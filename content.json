{"meta":{"title":"相思似海深旧事如天远","subtitle":"where there is a will there is a way","description":"江西理工大学2020级硕士研究生","author":null,"url":"http://pistachio0812.github.io","root":"/"},"pages":[{"title":"categories","date":"2022-03-13T07:36:30.000Z","updated":"2022-03-13T12:55:19.588Z","comments":true,"path":"categories/index.html","permalink":"http://pistachio0812.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2022-03-13T07:39:41.000Z","updated":"2022-03-13T13:01:34.747Z","comments":true,"path":"tags/index.html","permalink":"http://pistachio0812.github.io/tags/index.html","excerpt":"","text":""},{"title":"about","date":"2022-03-13T07:41:41.000Z","updated":"2022-03-13T13:48:25.663Z","comments":true,"path":"about/index.html","permalink":"http://pistachio0812.github.io/about/index.html","excerpt":"","text":"网站开发者是江西理工大学2020级硕士研究生，热衷于学习一些除学习之外的技能，硕士研究方向为目标检测，如果有志同道合的朋友看到这里，希望可以进一步探讨，可以通过CSDN留言给我。"}],"posts":[{"title":"MySQL学习笔记","slug":"MySQL学习笔记","date":"2022-03-26T11:07:11.619Z","updated":"2022-03-28T01:12:32.402Z","comments":true,"path":"2022/03/26/CN/MySQL学习笔记/","link":"","permalink":"http://pistachio0812.github.io/2022/03/26/CN/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"MySQL入门教程什么是数据库数据库是按照数据结构来组织、存储和管理数据的仓库 关系型数据库：建立在关系模型基础上的数据库，借助于集合代数等数学概念和方法来处理数据库中的数据 RDBMS即关系数据库管理系统的特点： 1.数据以表格的形式出现 2.每行为各种记录名称 3.每列为记录名称所对应的数据域 4.许多行和列组成一张表单 5.若干的表单组成数据库 RDBMS相关概念· MySQL创建数据表语法CREATE TABLE table_name (column_name column_type) 举例：在W3CSCHOOL数据库中创建数据表w3cschool_tbl: 1234567CREATE TABLE IF NOT EXISTS tutorials_tbl( tutorial_id INT NOT NULL AUTO_INCREMENT, tutorial_title VARCHAR(100) NOT NULL, tutorial_author VARCHAR(40) NOT NULL, submission_date DATE, PRIMARY KEY (tutorial_id) ); 注： ·如果你不想字段为NULL可以设置字段的属性为NOT NULL,在操作数据库如果输入该字段的数据为NULL,则会报错。 ·AUTO_INCREMENT定义列为自增的属性，一般用于主键，数值会自动加1。 ·PRIMARY KEY关键字用于定义列为主键。你可以使用多列来定义主键，列间以逗号分隔。 MySQL删除数据表语法DROP TABLE table_name MySQL插入数据语法123INSERT INTO table_name(field1, field2, ...fieldN) VALUES (value1, value2, ...valueN); 举例：使用SQL INSERT INTO语句向MySQL数据表w3cschool_tbl插入数据： 1234INSERT INTO w3cschool_tbl(w3cschool_title, w3cschool_author, submission_date)VALUES(&quot;Learn PHP&quot;, &quot;John Poul&quot;, NOW()); MySQL查询数据语法1234SELECT column_name, column_nameFROM table_name[WHERE Clause][OFFSET M][LIMIT N] 注： ·查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分隔，并使用WHERE语句来设定查询条件。 ·SELECT命令可以读取一条或者多条记录。 ·你可以使用星号(*)来代替其他字段，SELECT语句会返回表的所有字段数据。 ·你可以使用WHERE语句来包含任何条件。 ·你可以通过OFFSET指定SELECT语句开始查询的数据偏移量，默认偏移量为0。 ·你可以使用LIMIT属性来设定返回的记录数。 举例：通过SQL SELECT命令来获取MySQL数据表w3cschool_tbl的数据： SELECT * from w3cschool_tbl; MySQL where子句语法123SELECT field1, field2, ...fieldNFROM table_name1, table_name2...[WHERE condition1 [[AND][OR]] condition2...] 注： ·查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分隔，并使用WHERE语句来设定查询条件。 ·你可以在WHERE子句中指定任何条件。 ·你可以使用AND或者OR指定一个或多个条件。 ·WHERE子句也可以运用于SQL的DELETE或UPDATE命令。 ·WHERE子句类似于程序中的if条件，根据MySQL表中的字段值来读取指定的数据。 操作符列表，实例假定A&#x3D;10,B&#x3D;20: 操作符 描述 实例 &#x3D; 等号，检测两个值是否相等，如果相等返回True (A&#x3D;B)返回false &lt;&gt;或！&#x3D; 不等于，检测两个值是否相等，如果不相等返回True （A!&#x3D;B)返归true &gt; 大于号，检测左边的值是否大于右边的值，如果左边的值大于右边的值返回True (A&gt;B)返回false &lt; 小于号，检测左边的值是否小于右边的值，如果左边的值小于右边的值返回True (A&lt;B)返回true &gt;&#x3D; 大于等于号，检测左边的值是否大于等于右边的值，如果左边的值大于或等于右边的值返回True (A&gt;&#x3D;B)返回false &lt;&#x3D; 小于等于号，检测左边的值是否小于或等于右边的值，如果左边的值小于或等于右边的值返回True (A&lt;&#x3D;B)返回true 举例：读取w3cschool_tbl表中w3cschool_author字段值为Sanjay的所有记录： SELECT * from w3cschool_tbl WHERE w3cschool_author=&#39;Sanjay&#39;; 除非你使用LIKE来比较字符串，否则MySQL的WHERE子句的字符串比较是不区分大小写的。你可以使用BINARY关键字来设定WHERE子句的紫福春是区分大小写的。 SELECT * from w3cschool_tbl WHERE BINARY w3cschool_author=&#39;sanjay&#39;; MySQL UPDATE查询语法123UPDATE table_name SET field1=new-value1, field2=new-value2[WHERE Clause] 注： ·你可以同时更新一个或多个字段 ·你可以在WHERE字句中指定任何条件 ·你可以在一个单独表中同时更新数据 当你需要更新表中指定行的数据时，WHERE子句是非常有用的。 举例：更新数据表中w3cschool_id为3的w3cschool_title字段值： 123UPDATE w3cschool_tblSET w3cschool_title=&#x27;Learning JAVA&#x27;WHERE w3cschool_id=3; MySQL DELETE语句语法DELETE FROM table_name [WHERE Clause] 注： ·如果没有指定WHERE子句，MySQL表中的所有记录将被删除 ·你可以在WHERE字句中指定任何条件 ·你可以在单个表中一次性删除记录 当你想删除数据表中的指定记录时，WHERE子句是非常有用的 举例：删除w3cschool_tbl表中w3cschool_id为3的记录： 1DELETE FROM w3cschool_tbl WHERE w3cschool_id=3; MySQL LIKE子句SQL LIKE子句中使用百分号（%）字符来表示任意字符，类似于UNIX或者正则表达式中的星号（*）。 语法123SELECT field1, field2, ...fieldNFROM table_name1, table_name2...WHERE field1 LIKE condition [[AND][OR]] field2=&#x27;somevalue&#x27; 注： ·你可以在WHERE子句中指定任何条件 ·你可以在WHERE字句中使用LIKE子句 ·你可以使用LIKE子句代替等号 ·LIKE通常与%一同使用，类似于一个元字符的搜索 ·你可以使用AND或OR指定一个或多个条件 ·你可以在DELETE或UPDATE命令中使用WHERE…LIKE子句来指定条件 举例：查询w3cschool_tbl表中的w3cschool_author字段中以’jay’为结尾的所有记录： 12SELECT * from w3cschool_tblWHERE w3cschool_author LIKE &#x27;%jay&#x27;; MySQL排序语法12SELECT field1, field2,...fieldN FROM table_name1, table_name2...ORDER BY field1, [field2...] [ASC[DESC]] 注： ·你可以使用任何字段来作为排序的条件，从而返回排序后的查询结果 ·你可以设定多个字段来排序 ·你可以使用ASC或DESC关键字来设置查询结果是按升序或者降序排列。默认情况下，它是按升序排列 ·你可以添加WHERE…LIKE子句来设置条件 举例：使用ORDER BY子句来读取MySQL数据表w3cschool_tbl中的数据： SELECT * from w3cschool_tbl ORDER BY w3cschool_author ASC; SELECT * from w3cschool_tbl ORDER BY w3cschool_author DESC; MySQL 分组GROUP BY语句根据一个或者多个列对结果集进行分组，在分组的列上我们可以使用COUNT,SUM,AVG等函数。 语法1234SELECT column_name, function(column_name)FROM table_nameWHERE column_name operator valueGROUP BY column_name; employee_tbl表格信息如下： id name date singin 1 小明 2016-04-22 15:25:33 1 2 小王 2016-04-20 15:25:47 3 3 小丽 2016-04-19 15:26:02 2 4 小王 2016-04-07 15:26:14 4 5 小明 2016-04-11 15:26:40 4 6 小明 2016-04-04 15:26:54 2 举例：将数据表按名字进行分组，并统计每个人有多少条记录： SELECT name, COUNT(*) FROM employ_tbl GROUP BY name; 使用WITH ROLLUPWITH ROLLUP可以实现在分组统计数据基础上再进行相同的统计（SUM.AVG,COUNT). 举例：将以上的数据表按名字进行分组，再统计每个人登录的次数： 123SELECT name, SUM(singin) as singin_countFROM employee_tblGROUP BY name WITH ROLLUP; 其中结果如下： name singin_out 小丽 2 小明 7 小王 7 NULL 16 其中记录NULL表示所有人的登录次数，我们可以使用coalesce来设置一个可以取代NULL的名称。 语法select coalesce(a, b, c); 参数说明：如果a&#x3D;&#x3D;null,则选择b;如果b&#x3D;&#x3D;null,则选择c;如果a!&#x3D;null,则选择a;如果abc都为null,则返回null(没意义)。 举例：如果名字为空，使用总数代替： 123SELECT coalesce(name, &#x27;总数&#x27;), SUM(singin) as singin_outFROM employee_tblGROUP BY name WITH ROLLUP;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://pistachio0812.github.io/categories/MySQL/"}],"tags":[{"name":"Note","slug":"Note","permalink":"http://pistachio0812.github.io/tags/Note/"}]},{"title":"pytorch官方文档中文版","slug":"pytorch官方文档","date":"2022-03-25T13:26:45.398Z","updated":"2022-03-25T15:20:56.859Z","comments":true,"path":"2022/03/25/en/pytorch官方文档/","link":"","permalink":"http://pistachio0812.github.io/2022/03/25/en/pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/","excerpt":"","text":"torch.nnContainerModuleCLASS torch.nn.Module SOURCE 它是所有神经网络模型的基类，你的模块应该继承于该类。 模块还能包含其他模块，允许把它们嵌套在一个树结构中。你可以分配子模块作为常规属性： 123456789101112import torch.nn as nnimport torch.nn.functional as Fclass Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) 用这种方式分配的模块将会显示，并且当你调用to( )等等方法，它们的参数也将被转换。 注： 正如上面的例子一样，一个__init__()调用父类必须在子类赋值之前完成。 变量 training(bool)-布尔值代表这个模块是训练模式还是评估模式 add_module(name, module) SOURCE ​ 添加一个子模块到当前模块 ​ 这个模块可以用给定的名称作为属性访问模块 ​ 参数： ​ ·name(string)-子模块的名字，这个子模块可以用给定的名称作为属性访问。 ​ ·module(Module)-子模块添加到模块上","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://pistachio0812.github.io/categories/pytorch/"}],"tags":[{"name":"文档","slug":"文档","permalink":"http://pistachio0812.github.io/tags/%E6%96%87%E6%A1%A3/"}]},{"title":"github使用指南","slug":"github使用指南","date":"2022-03-25T10:46:42.593Z","updated":"2022-03-25T12:48:07.362Z","comments":true,"path":"2022/03/25/en/github使用指南/","link":"","permalink":"http://pistachio0812.github.io/2022/03/25/en/github%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/","excerpt":"","text":"git安装1.安装git OSX版 下载地址：http://git-scm.com/download/mac 2.安装git Windows版 下载地址：http://book.git-scm.com/download/win 3.安装git Linux版 下载地址：http://book.git-scm.com/download/linux 创建新仓库创建新文件夹，打开，然后执行git init以创建新的git仓库 检出仓库执行如下命令以创建一个本地仓库的克隆版本： git clone /path/to/repository 如果是远端服务器上的仓库，则使用如下命令： git clone username@host:/path/to/repository 工作流你的本地仓库由git维护的三棵“树”组成。第一个是你的工作目录，它持有实际文件；第二个是暂存区（index),它像个缓存区域，临时保存你的改动；最后是HEAD,它指向你最后一次提交的结果。 添加和提交你可以提出更改（把它们添加到暂存区），使用如下命令： git add &lt;filename&gt; git add * 这是git基本工作流程的第一步；使用如下命令以实际提交改动： git commit -m &quot;代码提交信息&quot; 现在，你的改动已经提交到了HEAD,但是还没到你的远端仓库。 推送改动你的改动现在已经在本地仓库的HEAD中了。执行如下命令以将这些改动提交到远端仓库： git push origin master 可以把master换成你想要推送的任何分支。 如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加： git remote add origin &lt;server&gt; 如此你就能够将你的改动推送到所添加的服务器上去了。 分支分支是用来将特性开发绝缘开来的。在你创建仓库的时候，master是默认的分支。在其他分支上进行开发，完成后再将它们合并到主分支上。 创建一个叫做“feature_x”的分支，并切换过去： git checkout -b feature_x 切换回主分支： git checkout master 再把新建的分支删掉： git branch -d feature_x 除非你将分支推送到远端仓库，不然该分支就是不为他人所见的： git push origin &lt;branch&gt; 更新与合并要更新你的本地仓库至最新改动，执行：git pull以在你的工作目录中 获取（fetch） 并 合并（merge） 远端的改动。要合并其他分支到你的当前分支（例如 master），执行：git merge &lt;branch&gt;在这两种情况下，git 都会尝试去自动合并改动。遗憾的是，这可能并非每次都成功，并可能出现冲突（conflicts）。 这时候就需要你修改这些文件来手动合并这些冲突（conflicts）。改完之后，你需要执行如下命令以将它们标记为合并成功： git add &lt;filename&gt;在合并改动之前，你可以使用如下命令预览差异：git diff &lt;source_branch&gt; &lt;target_branch&gt; 标签为软件发布创建标签是推荐的。这个概念早已存在，在 SVN 中也有。你可以执行如下命令创建一个叫做 1.0.0 的标签：git tag 1.0.0 1b2e1d63ff1b2e1d63ff 是你想要标记的提交 ID 的前 10 位字符。可以使用下列命令获取提交 ID：git log你也可以使用少一点的提交 ID 前几位，只要它的指向具有唯一性。 log如果你想了解本地仓库的历史记录，最简单的命令就是使用:git log你可以添加一些参数来修改他的输出，从而得到自己想要的结果。 只看某一个人的提交记录:git log --author=bob一个压缩后的每一条提交记录只占一行的输出:git log --pretty=oneline或者你想通过 ASCII 艺术的树形结构来展示所有的分支, 每个分支都标示了他的名字和标签:git log --graph --oneline --decorate --all看看哪些文件改变了:git log --name-status这些只是你可以使用的参数中很小的一部分。更多的信息，参考：git log --help 替换本地改动假如你操作失误（当然，这最好永远不要发生），你可以使用如下命令替换掉本地改动：git checkout -- &lt;filename&gt;此命令会使用 HEAD 中的最新内容替换掉你的工作目录中的文件。已添加到暂存区的改动以及新文件都不会受到影响。 假如你想丢弃你在本地的所有改动与提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它：git fetch origingit reset --hard origin/master 实用小贴士内建的图形化 git：gitk彩色的 git 输出：git config color.ui true显示历史记录时，每个提交的信息只显示一行：git config format.pretty oneline交互式添加文件到暂存区：git add -i 更多内容请参考：git - 简明指南","categories":[{"name":"github","slug":"github","permalink":"http://pistachio0812.github.io/categories/github/"}],"tags":[{"name":"使用指南","slug":"使用指南","permalink":"http://pistachio0812.github.io/tags/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"},{"name":"github","slug":"github","permalink":"http://pistachio0812.github.io/tags/github/"}]},{"title":"How to use Linux","slug":"Linux系统学习笔记","date":"2022-03-24T02:36:23.299Z","updated":"2022-03-24T03:08:07.695Z","comments":true,"path":"2022/03/24/en/Linux系统学习笔记/","link":"","permalink":"http://pistachio0812.github.io/2022/03/24/en/Linux%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[{"name":"Linux","slug":"Linux","permalink":"http://pistachio0812.github.io/categories/Linux/"}],"tags":[{"name":"Liunx system","slug":"Liunx-system","permalink":"http://pistachio0812.github.io/tags/Liunx-system/"}]},{"title":"色彩搭配","slug":"教你学会色彩搭配","date":"2022-03-22T10:22:49.177Z","updated":"2022-03-24T03:05:11.564Z","comments":true,"path":"2022/03/22/en/教你学会色彩搭配/","link":"","permalink":"http://pistachio0812.github.io/2022/03/22/en/%E6%95%99%E4%BD%A0%E5%AD%A6%E4%BC%9A%E8%89%B2%E5%BD%A9%E6%90%AD%E9%85%8D/","excerpt":"","text":"中国色中国色 COULEURcouleur color spacecolor space hyper colorHypercolor colorableColorable brandcolorsBrandColors 九月ppt九月PPT huemintHuemint - AI color palette generator Adobe Color色輪、調色盤產生器 | Adobe Color","categories":[],"tags":[]},{"title":"Q & A","slug":"遇到的问题","date":"2022-03-17T11:30:56.879Z","updated":"2022-03-27T08:53:10.340Z","comments":true,"path":"2022/03/17/CN/遇到的问题/","link":"","permalink":"http://pistachio0812.github.io/2022/03/17/CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"","text":"就地操作问题来源：看代码发现self.relu=nn.ReLU(inplace=True)不明白inplace=True什么意思。 解答：查看pytorch官网关于ReLU定义，ReLU其中inplace参数表示可以选择就地执行操作，默认为False,就地执行操作是指图像处理函数的输入图像和输出图像是同一对象，即同一张图像，常规的图像处理函数是不支持输入图像和输出图像是同一图像的。 eg:中值滤波函数 medianBlur(src, dst, 7); //常规操作 medianBlur(src, src, 7); //就地操作 就地操作直接更改张量的内容，而无需复制它。由于它不创建输入的副本，因此在处理高维数据时减少了内存使用，就地操作有助于使用更少的GPU内存，详情请看该博客如何在Pytorch中执行就地操作 torch.max中keepdim的作用torch.max的用法： (max, max_indices) &#x3D; torch.max(input, dim, keepdim&#x3D;False) 输入： input 是输入的tensor。 dim 是索引的维度，dim&#x3D;0寻找每一列的最大值，dim&#x3D;1寻找每一行的最大值。 keepdim 表示是否需要保持输出的维度与输入一样，keepdim&#x3D;True表示输出和输入的维度一样，keepdim&#x3D;False表示输出的维度被压缩了，也就是输出会比输入低一个维度。 输出： max 表示取最大值后的结果。 2max_indices 表示最大值的索引 import torch import numpy as np x = torch.randint(0,9,(2,4)) print(x) tensor([[7, 8, 7, 2], [6, 0, 3, 0]]) #取每一行的最大值，torch.max的输出结果 y = torch.max(x, 1) print(y) torch.return_types.max(values=tensor([8, 6]),indices=tensor([1, 0])) #索引值 y = torch.max(x, 1, keepdim=True)[0]print(y)print(np.shape(y)) # keepdim=True，输出仍然是二维的tensor([[8], [6]])torch.Size([2, 1])y = torch.max(x, 1, keepdim=False)[0]print(y)print(np.shape(y)) keepdim=False # 输出变成了一维 tensor([8, 6])torch.Size([2]) ConstantPad2d的用法torch.nn.ConstantPad2d(padding, value) 参数：padding(int, tuple)-padding的尺寸，如果是整型，那么所有的边界都使用相同的填充，如果是四元组，使用（padding_left, padding_right, padding_top, padding_bottom) 形状： 输入：$$(N, C, H_{in}, W_{in}) or (C, H_{in}, W_{in})$$ 输出：$$(N, C, H_{out}, W_{out}) or (C, H_{out}, W_{out})$$其中，$$H_{out} &#x3D; H_{in}+padding_{top}+padding_{bottom}$$ $$W_{out}&#x3D;W_{in}+padding_{left}+padding_{right}$$ 测试用例： 1234567891011121314import torchimport torch.nn as nnn1 = nn.ConstantPad2d(2, 0)n2 = nn.ConstantPad2d((0, 1, 0, 1), 0)n3 = nn.ConstantPad2d((-1, 0, -1, 0), 0)input = torch.randn(1, 2, 2)print(input)t = n1(input)print(t)x = n2(input)y = n3(x)print(x)print(y) 结果： tensor([[[ 1.0826, 0.1191], [-0.3506, 0.1677]]])tensor([[[ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 1.0826, 0.1191, 0.0000, 0.0000], [ 0.0000, 0.0000, -0.3506, 0.1677, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])tensor([[[ 1.0826, 0.1191, 0.0000], [-0.3506, 0.1677, 0.0000], [ 0.0000, 0.0000, 0.0000]]])tensor([[[0.1677, 0.0000], [0.0000, 0.0000]]]) 更多详情参考ConstantPad2d enumerate()函数描述enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。Python 2.3. 以上版本可用，2.6 添加 start 参数。 语法enumerate(sequence, [start=0]) 参数·sequence–一个序列、迭代器或其他支持迭代对象 ·start–下标起始位置的值 返回值返回enumerate(枚举)对象 实例以下展示了使用enumerate()方法的实例： seasons &#x3D; [‘Spring’, ‘Summer’, ‘Fall’, ‘Winter’] list(enumerate(seasons)) [(0, &#39;Spring&#39;), (1, &#39;Summer&#39;), (2, &#39;Fall&#39;), (3, &#39;Winter&#39;)] list(enumerate(seasons, start&#x3D;1)) # 下标从1开始 [(1, &#39;Spring&#39;), (2, &#39;Summer&#39;), (3, &#39;Fall&#39;), (4, &#39;Winter&#39;)] 普通的for循环 12345678910i = 0seq = [&#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;]for i in enumerate(seq): print(i, seq[i]) i += 1result:0 one1 two2 three for循环使用enumerate 12345678seq = [&#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;]for i, element in enumerate(seq): print(i, element)result:0 one1 two2 three torch.clamptorch.clamp(input, min=None, max=None, *, out=None)-&gt;Tensor Clamps中所有输入的元素都在[min, max]范围内，让 FPPI(68条消息) Recall&#x2F;Precision&#x2F;FPPI评价方式详解_Bruce_0712的博客-CSDN博客","categories":[],"tags":[]},{"title":"学习笔记","slug":"学习笔记","date":"2022-03-17T01:30:47.993Z","updated":"2022-03-24T03:05:35.837Z","comments":true,"path":"2022/03/17/en/学习笔记/","link":"","permalink":"http://pistachio0812.github.io/2022/03/17/en/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"tf中矩阵量存在形式常用有三种，具体如下：1.tf.Variable()表示神经网络中可变化的量（可以通过trainable&#x3D;False设置成不可变),可在运行中赋值，可以通过constant或者其他方式进行初始化。2.tf.constant()可以通过numpy中的array或者list,还有给定的shape和数值进行赋值3.tf.placeholder()相当于占位符，也是有shape的量，因为训练过程中需要不断赋值和替换值，而整体计算的结构是不变的。代码：&#x2F;&#x2F;导包 import tensorflow as tf &#x2F;&#x2F;定义变量A = tf.Variable(tf.ones([4,4])) &#x2F;&#x2F;变量初始化import numpy as np cst = tf.constant(np.ones([4,4]),dtype=tf.float32) #需要指定类型dtype&#x3D;tf.float32,tf中不能隐式转换浮点和整型#cst &#x3D; tf.constant(1.0,shape&#x3D;[4,4],dtype&#x3D;tf.float32)也是可以的A = tf.Variable(cst) &#x2F;&#x2F;定义placeholderX = tf.placeholder(dtype=tf.float32,shape=[4,1]) &#x2F;&#x2F;矩阵相乘C = tf.matmul(A,X) &#x2F;&#x2F;定义Sessionsess = tf.Session() &#x2F;&#x2F;初始化变量init = tf.global_variables_initializer() &#x2F;&#x2F;执行初始化sess.run(init) &#x2F;&#x2F;运行矩阵相乘sess.run(C,feed_dict=&#123;X:[[1],[1],[1],[1]]&#125;) &#x2F;&#x2F;获取变量值A_val = A.value() Avalue = sess.run(A_val) &#x2F;&#x2F;完整矩形相乘代码import tensorflow as tf import numpy as np X = tf.placeholder(dtype=tf.float32,shape=[4,1]) A = tf.Variable(tf.zeros([4,4])) C = tf.matmul(A,X) sess = tf.session() init = tf.global_variables_initializer() sess.run(init) print(sess.run(A)) &#x2F;&#x2F;为了使计算图更加清晰，可以使用variable_scope()&#x2F;&#x2F;定义变量名称with tf.variable_scope(&quot;first-nn-layer&quot;): W = tf.Variable(tf.zeros([784,10]),name=&quot;W&quot;) b = tf.Variable(tf.zeros([10]),name=&quot;b&quot;) y = tf.matmul(x,W)+b variable_summaries(W) &#x2F;&#x2F;标识不同的变量&#x2F;&#x2F;不同作用域下的同名变量with tf.variable_scope(&quot;first-nn-layer&quot;): W = tf.Variable(tf.zeros([784,10]),name=&quot;W&quot;) b = tf.Variable(tf.zeros([10]),name=&quot;b&quot;) W1 = tf.Variable(tf.zeros([784,10]),name=&quot;W&quot;) print(W.name) print(W1.name)#w、w1虽然name一样，但是计算中依然当成不同的变量，让同一个scope的同一变量可以通过get_variable()函数 &#x2F;&#x2F;获取变量with tf.variable_scope(&quot;first-nn-layer&quot;) as scope: W = tf.get_variable(&quot;W&quot;,[784, 10]) b = tf.get_variable(&quot;b&quot;,[10]) scope.reuse_variables()#缺少则会报错 W1 = tf.get_variables(&quot;W&quot;,shape=[784,10]) print(W.name) print(W1.name)#w、w1属于同一个变量 注：若此时缺少了scope.reuse_variables()则会报错，因为同时引用了同一个变量，对于不同层的变量，可以利用variable_scope进行区分，在再次引入相关变量时，需要加上reuse&#x3D;True,否则依然会报错。如果变量不存在时加上reuse&#x3D;True,依然会报错，因为该变量不存在 with tf.variable_scope(&quot;first-nn-layer&quot;) as scope: W = tf.get_variable(&quot;W&quot;,[784, 10]) b = tf.get_variable(&quot;b&quot;,[10]) with tf.variable_scope(&quot;second-nn-layer&quot;) as scope: W = tf.get_variable(&quot;W&quot;,[784, 10]) b = tf.get_variable(&quot;b&quot;,[10]) with tf.variable_scope(&quot;second-nn-layer&quot;, reuse=True) as scope: W3 = tf.get_variable(&quot;W&quot;,[784, 10]) b3 = tf.get_variable(&quot;b&quot;,[10]) print(W.name) print(W3.name) &#x2F;&#x2F;保存模型&#x2F;&#x2F;定义saversaver = tf.train.Saver() &#x2F;&#x2F;在训练过程中进行保存，保存为训练过程中的变量&#x2F;&#x2F;变量保存for itr in range(1000): ... saver.save(sess,&quot;model/al&quot;,global_step=itr) &#x2F;&#x2F;加载计算&#x2F;&#x2F;变量载入saver.restore(sess,&quot;model/v2-200&quot;) 3.4构建计算图&#x2F;&#x2F;前面在描述计算图，这里观察所建立的计算图&#x2F;&#x2F;定义summarytrain_writer = tf.summary.FileWriter(&quot;logdir&quot;,sess.graph) 注：sess.graph就是描绘的计算图，”logdir”是log的存储文件夹。在Shell中运行Tensorboard,在浏览器中输入localhost:6006,然后点击graph就可以看到设计的网络模型了。 &#x2F;&#x2F;问题：描绘的计算图非常杂乱无章，变量命名的可读性很差，需要进行整理。&#x2F;&#x2F;变量命名x = tf.placeholder(tf.float32,[None,784],name=&quot;input_x&quot;) label = tf.placeholder(tf.float32,[None,10],name=&quot;input_label&quot;) W = tf.Variable(tf.zeros([874,10]),name=&quot;W&quot;) b = tf.Variable(tf.zeros([10]),name=&quot;b&quot;) &#x2F;&#x2F;问题：依然不够清楚，可以将输入层的x和label归为一类&#x2F;&#x2F;定义作用域with tf.variable_scope(&quot;input&quot;): x = tf.placeholder(tf.float32,[None,784],name=&quot;input_x&quot;) label = tf.placeholder(tf.float32,[None,10],name=&quot;input_label&quot;) with tf.variable_scope(&quot;first-nn-layer&quot;): W = tf.Variable(tf.zeros([784,10]), name=&quot;W&quot;) b = tf.Variable(tf.zeros([10]),name=&quot;b&quot;) y = tf.matmul(x,W)+b with tf.variable_scope(&quot;loss&quot;): loss = tf.reduce_mean(tf.square(y-label)) &#x2F;&#x2F;同一作用域下的同名变量是相同的，涉及到变量复用的问题，以及后续变量的获取，为了观察变量的变化，需要观察的变量加入summary函数&#x2F;&#x2F;定义summary函数def variable_summaries(var): with tf.name_scope(&#39;summaries&#39;): mean = tf.reduce_mean(var) tf.summary.scalar(&#39;mean&#39;,mean) with tf.name_scope(&#39;stddev&#39;): stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean))) tf.summary.scalar(&#39;stddev&#39;,stddev) tf.summary.scalar(&#39;max&#39;,tf.reduce_max(var)) tf.summary.scalar(&#39;min&#39;,tf.reduce_min(var)) tf.summary.histogram(&#39;histogram&#39;,var) &#x2F;&#x2F;若要观测W的相关情况，调用summary函数&#x2F;&#x2F;调用summary函数variable_summaries(W) &#x2F;&#x2F;再用merge_all函数收集summary信息&#x2F;&#x2F;获取summary信息merged = tf.summary.merge_all() &#x2F;&#x2F;summary保存summary = sess.run(merged, feed_dict=&#123;x:batch_xs,label:batch_ys&#125;) train_writer.add_summary(summary,itr) 注：此时可以在网页中访问，观察变量随迭代变化的情况，可以通过不同的方式对变量进行观测，比如时序统计、histogram,这些统计信息对于分析训练过程是非常重要的 3.5全连接网络构建&#x2F;&#x2F;tf官方手写识别版本的简化版本&#x2F;&#x2F;单层全连接网络#引入库from tensorflow.examples.tutorials.mnist import input_data#产生数据，手写识别的图片和标签 import tensorflow as tf #获取数据mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;,one_hot=True)#构建网络模型#x,label分别为图形数据和标签数据x = tf.placeholder(tf.float32,[None,784]) label = tf.placeholder(tf.float32,[None,10])#构建单层网络中的权值和偏置W = tf.Variable(tf.zeros([784,10])) b = tf.Variable(tf.zeros([10])#本例中无非线性激活函数y = tf.matmul(x,W)+b#定义损失函数为欧氏距离，但这并不是最好的，多分类问题通常使用交叉熵loss = tf.reduce_mean(tf.square(y-label))#若使用交叉熵损失函数soft_max = tf.nn.softmax(logit, axis=1) loss = tf.reduce_mean(-label*tf.log(soft_max))#用梯度迭代算法train_step = tf.train.GradientDescentOptimizer(0.005).minimize(loss)#用于验证correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(label,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float.32))#定义会话sess = tf.Session()#初始化所有变量sess.run(tf.global_variable_initializer())#迭代过程for itr in range(3000): batch_xs,batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x:batch_xs,label:batch_ys&#125;) if itr%10==0: print(&quot;step:%6d accuracy:&quot;%iter, sess.run(accuracy, feed_dict=&#123;x:mnist.test.images, label:mnist.test.labels&#125;)) #获取W取值W_value = sess.run(W.value()) &#x2F;&#x2F;定义一个单层全连接函数def full_layer(input_tensor, out_dim, name=&quot;full&quot;): with tf.variable_scope(name): shape = input_tensor.get_shape()as_list() W = tf.get_variable(&#39;W&#39;,(shape[1],out_dim),dtype=tf.float32, initalizer=tf.truncated_normal_initializer(stddev=0.1)) b = tf.get_variable(&#39;b&#39;,[out_dim], dtype=tf.float32, initializer=tf.constant_initializer(0)) out = tf.matmul(input_tensor, W)+b return tf.nn.sigmoid(nn) 3.6CNN构建&#x2F;&#x2F;CNN手写识别#预读取MNIST手写字库from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True) import tensorflow as tf#用正态分布随机数初始化变量，本例中仅作为权值def weight_variable(shape): initial=tf.truncated_normal(shape,stddev=0.1) #正态分布 return tf.Variable(initial)#用常量方式初始化偏置def bias_variable(shape): initial=tf.constant(0.1,shape=shape) #常数分布 return tf.Variable(initial)#定义二维卷积的过程def conv2d(x,W): return tf.nn.conv2d(x,W,strides&#x3D;[1,1,1,1],padding&#x3D;’SAME’)#定义池化层，简单地说就是选个最大的数，进一步降低自由参数的个数def max_pool_2x2(x): return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=&#39;SAME&#39;) x = tf.placeholder(tf.float32,shape=[100,784]) y = tf.placeholder(tf.float32,shape=[100,10]) W_conv1 = weight_variable([5,5,1,32]) b_conv1 = bias_variable([32]) x_image = tf.reshape(x,[-1,28,28,1]) y_conv1 = tf.nn.relu(conv2d(x_iamge,W_conv1)+b_conv1) y_pool1 = max_pool_2x2(y_conv1) W_conv2 = weight_variable([5,5,32,64]) b_conv2 = weight_variable([64]) y_conv2 = tf.nn.relu(conv2d(y_pool1,W_conv2)+b_conv2) y_pool2 = max_pool_2x2(y_conv2) y_fc_flat = tf.reshape(y_pool2,[-1,7*7*64]) W_fc1 = weight_variable([7*7*64,10]) b_fc1 = bias_variable([10]) y_fc1 = tf.nn.relu(tf.matmul(y_fc_flat,W_fc1)+b_fc1) cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_fc1)) train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) sess = tf.Session() init = tf.global_variables_initializer() sess.run(init) for i in range(1000): bx,by = mnist.train.next_batch(100) sess.run(train_step,feed_dict=&#123;x:bx,y:by&#125;) import numpy as np import matplotlib.pyplot as plt#设置输出风格，为画图美观import matplotlib as mpl mpl.style.use(&#39;seaborn-darkgrid&#39;) val = W_conv1.value() convVal = np.array(sess.run(val)) convVal = np.reshape(convVal,[5,5,32]) plt.imshow(convVal[:,:,6]) plt.show() 3.8多架构运行&#x2F;&#x2F;GPU使用GPU可以加速深度学习作业的训练速度，如果服务器有多个GPU,那么tensorflow默认使用全部使用部分GPU: python程序启动时调用：CUDA_VISIBLE_DEVICES=0.2.3 python script.py python代码内进行调用：import os os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;1&quot; &#x2F;&#x2F;配置GPU显存某些情况下，多作业或者共享GPU的场景中，可以控制tf使用GPU显存大小gpuOptions = tf.GPUOptions(per_process_gpu_memory_fraction=0.8) sess = tf.Session(config=tf.ConfigProto(gpu_options=gpuOptions)) &#x2F;&#x2F;GPU运行代码#将变量的定义和分配定义到GPU上进行with tf.device(&#39;/gpu:0&#39;): W = tf.get_variable(&#39;W&#39;,(in_dim,out_dim),dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.1)) b = tf.get_variable(&#39;b&#39;),[out_dim],dtype=tf.float32,initializer=tf.constant_initializer(0)) net = tf.matmul(input_tensor,W)+b#在CPU上计算激活函数with tf.device(&#39;/cpu:0&#39;): net = tf.nn.sigmoid(net) &#x2F;&#x2F;多CPU使用，多设备计算&#x2F;&#x2F;利用标号简单的区分并运行#在CPU0上计算with tf.device(&#39;/cpu:0&#39;) ... net = tf.nn.sigmoid(net)#在CPU1上计算with tf.device(&#39;/cpu:1&#39;) ... net = tf.nn.sigmoid(net) &#x2F;&#x2F;在集群上运行，需要在多个主机上准备多份代码，代码前面部分相同，后续有所不同&#x2F;&#x2F;定义多主机运行参数#这里的地址形式为IP:Portcluster = tf.train.ClusterSpectf.train.ClusterSpec(&#123; &quot;worker&quot;:[ &quot;xx.xx.xx.xx:2222&quot;, #/job:worker/task:0 &quot;xx.xx.xx.xx:2222&quot;, #这里job的名称为自定义 &quot;xx.xx.xx.xx:2222&quot; #task编号同样需在Server中定义 ], &quot;ps&quot;:[ &quot;xx.xx.xx.xx:2222&quot;, &quot;xx.xx.xx.xx:2222&quot; ]&#125;) server = tf.train.Server(cluster, job_name=&quot;worker&quot;, task_index=0) &#x2F;&#x2F;定义第二个主机参数#这里的地址形式为IP:Portcluster = tf.train.ClusterSpectf.train.ClusterSpec(&#123; &quot;worker&quot;:[ &quot;xx.xx.xx.xx:2222&quot;, #/job:worker/task:0 &quot;xx.xx.xx.xx:2222&quot;, #这里job的名称为自定义 &quot;xx.xx.xx.xx:2222&quot; #task编号同样需在Server中定义 ], &quot;ps&quot;:[ &quot;xx.xx.xx.xx:2222&quot;, &quot;xx.xx.xx.xx:2222&quot; ]&#125;) server = tf.train.Server(cluster, job_name=&quot;worker&quot;, task_index=1) &#x2F;&#x2F;不同设备的运行代码with tf.device(&#39;/job:worker/task:0/cpu:0&#39;): ... &#x2F;&#x2F;将不同的任务分配到不同的计算节点上&#x2F;&#x2F;分配计算任务with tf.device(tf.train.replica_device_setter( worker_device=&quot;/job:worker/task:%d&quot; %task_index,cluster=cluster) &#x2F;&#x2F;函数replica_device_setter会将变量参数的定义部分自动定义到ps服务中，后续需要定义Session,用于执行这个过程&#x2F;&#x2F;多主机运行#定义句柄，迭代多少步后停止迭代hooks = [tf.train.StopAtStepHook(last_step=1000000)]#MonitoredTrainingSession函数会完成会话初始化工作#保存checkpoint,恢复checkpoint,异常判断等#这里需要定义master主机，定义保存、控制操作的masterwith tf.train.MonitroedTrainingSession( master=server.target, is_chief=(task_index==0), checkpoint_dir=&quot;dir/to/cp&quot;, hooks=hooks) as mon_sess: ... 注：在程序运行过程中，需要认为将程序分配到各个主机上，依次运行各个主机 &#x2F;&#x2F;队列用于数据读取和处理，队列可以是先进先出队列，也可以是随机队列，用于随机化输出&#x2F;&#x2F;tf中队列的操作是对于训练前的过程而言的，有以下作用1.多线程数据预处理并将其推入队列2.在执行过程中，队列不断提供训练数据&#x2F;&#x2F;简单实例说明队列使用def simple_shuffle_batch(source,capacity,batch_size=10): #定义随机序列 queue = tf.RandomShuffleQueue( capacity=capacity, min_after_dequeue=int(0.9*capacity), shapes=source.shape, dtypes=source.dtype) #定义enqueue过程 enqueue = queue.enqueue(source) #定义执行进程个数 num_threads = 4 qr = tf.train.QueueRunner(queue,[enqueue]*num_threads) #声明Queue runner,使得其可以被执行 tf.train.add_queue_runner(qr) #获取数据 return queue.dequeue_many(batch_size)#产生测试数据input = tf.constant(list(range(100))) input = tf.data.Dataset.from_tensor_slices(input) input = input.make_one_shot_iterator().get_next() #定义函数get_batch = simple_shuffle_batch(input,capacity=20) #定义sessionwith tf.train.MonitoredSession() as sess: while not sess.should_stop(): print(sess.run(get_batch)) 注：队列操作可以使得数据读取过程得到并行的优化，这对于提升程序的运行速度是很有利的。 &#x2F;&#x2F;tf相关扩展4.2.1 tf Layers&#x2F;&#x2F;全连接网络#layers定义全连接网络net = tf.layers.dense(inputs=net, units=units, activation=tf.nn.relu) #卷积网络net = tf.layers.conv2d( inputs=net, #输入 filters=n_features, #输出特征数 kernel-size=[5, 5], #卷积核心大小 padding=&quot;same&quot;, #边界 activation=tf.nn.relu #激活函数 ) &#x2F;&#x2F;前馈神经网络函数#二维最大池化net = tf.layers.max_pooling2d(...)#二维平均池化net = tf.layers.average_pooling2d(...)#二维卷积net = tf.layers.conv2d(...)#dropoutnet = tf.layers.dropout(...)#展开net = tf.layers.flatten(...)#BNnet = tf.layers.batch_normalization(...) 4.2.2 tf Slim#卷积函数def conv2d_layer(input_tensor, size=1, feature=128, name=&#39;conv1d&#39;): with tf.variable_scope(name): shape = input_tensor.get_shape.as_list() kernel = tf.get_variable(&#39;kernel&#39;, (size, size, shape[-1], feature), dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1)) b = tf.get_variable(&#39;b&#39;, [feature], dtype=tf.float32, initializer=tf.constant_initializer(0)) out = tf.nn.conv2d(input_tensor, kernel, strides=[1,2,2,1],padding=&#39;SAME&#39;)+b return tf.nn.relu(out)#全连接函数def full_layer(input_tensor, out_dim, name=&#39;full&#39;): with tf.variable_scope(name): shape = input_tensor.get_shape.as_list() W = tf.get_variable(&#39;W&#39;, (shape[1], out_dim), dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1)) b = tf.get_variabel(&#39;b&#39;, [out_dim], dtype=tf.float32, initializer=tf.constant_initializer(0)) out = tf.matmul(input_tensor, W)+b return out &#x2F;&#x2F;slim实现卷积，tfv2取消该库#引入slim库import tensorflow.contrib.slim as slim#定义卷积层net = slim.conv2d(inputs, 16, 4, strides=2, activation_fn=tf.nn.relu, scope=&#39;conv1&#39;)#加入池化层net = tf.nn.max_pool(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) net = slim.conv2d(net, 32, 4, strides=2, activation_fn=tf.nn.relu, scope=&#39;conv2&#39;) net = tf.nn.max_pool(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) #flatten层，用于将三维的图形数据展开成一维数据，用于全连接层net = slim.flatten(net)#全连接层y = slim.fully_connected(net, 10, activation_fn=line, scope=&#39;full&#39;, reuse=False) 4.2.3 tfLearn&#x2F;&#x2F;tflearn抽象层次更高，代码可读性更好,其是一个完整的生态&#x2F;&#x2F;基础网络架构#全连接net = tflearn.fully_connected(...)#卷积net = tflearn.conv_2d(...)#LSTMnet = tflearn.lstm(...)#dropoutnet = tflearn.dropout(...) &#x2F;&#x2F;输入函数network = tflearn.input_data(shape=[None, 28, 28, 1], name=&#39;input&#39;) &#x2F;&#x2F;优化部分#定义优化过程network = tflearn.layers.estimator.regression( network, optimizer=&#39;adam&#39;, #优化方法 learning_rate=0.01, #学习率 loss=&#39;categorical_crossentropy&#39;, #损失函数 name=&#39;target&#39;) &#x2F;&#x2F;利用tflearn完成手写数字的识别任务import tflearn from tflearn.layers.core import input_data,dropout, fully_connected from tflearn.layers.conv import conv_2d, , max_pool_2d from tflearn.layers.normalization import local_response_normalization from tflearn.layers.estimator import regression #载入并处理数据import tflearn.datasets.mnist as mnist X, Y, testX, testY = mnist.load_data(one_hot=True)#转换为二维图形X = X.reshape([-1, 28, 28, 1]) testX = testX.reshape([-1, 28, 28, 1]) #建立神经网络network = tflearn.input_data(shape=[None, 28, 28, 1], name=&#39;input&#39;) network = conv_2d(network, 32, 3, activation=&#39;relu&#39;, regularizer=&#39;L2&#39;) network = max_pool_2d(network) network = local_response_normalization(network) network = fully_connected(network, 128, activation=&#39;tanh&#39;) network = dropout(network, 0.8) network = fully_connected(network, 256, activation=&#39;tanh&#39;) network = dropout(network, 0.8) network = fully_connected(network, 10, activation=&#39;softmax&#39;) #定义优化过程network = regression( network, optimizer=&#39;adam&#39;, learning_rate=0.01, loss=&#39;categorical_crossentropy&#39;, name=&#39;target&#39;) #训练过程model = tflearn.DNN(network, tensorboard_verbose=0) model.fit(&#123;&#39;input&#39;:X&#125;, &#123;&#39;target&#39;:Y&#125;, n_epoch=20, validation_set=(&#123;&#39;input&#39;:testX&#125;, &#123;&#39;target&#39;:testY&#125;), snapshot_step=100, show_metric=True, run_id=&#39;convnet_mnist&#39;) &#x2F;&#x2F;Keras代码可读性好，并且横跨多个机器学习框架，但其扩展性较差&#x2F;&#x2F;引入库from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D &#x2F;&#x2F;Keras直接顺序加入模型，无需通过数据方式进行传递&#x2F;&#x2F;基础网络层from keras.models import Sequential model = Sequential()#加入卷积层model.add(Conv2D(...))#加入池化层model.add(MaxPooling2D(...))#加入全连接层model.add(Dense(...))#dropoutmodel.add(Dropout(0.25)) &#x2F;&#x2F;定义model后可直接加入多种层进行操作，同样其需要定义训练函数&#x2F;&#x2F;定义优化过程from keras.optimizers import SGD#定义迭代算法sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=sgd)#训练过程model.fit(x_train, y_train, batch_szie=32, epochs=10)#评估训练效果score = model.evaluate(x_test, y_test, batch_size=32) &#x2F;&#x2F;完整代码import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D from keras.optimizers import SGD #这里utils为自己定义的库函数，用于载入数据import utils X, Y, testX, testY = utils.load_data(one_hot=True) model = Sequential()#定义神经网络过程model.add(Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(100, 100, 3))) model.add(Conv2D(32, (3, 3), activation=&#39;relu&#39;)) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(64, (3, 3), activation=&#39;relu&#39;)) model.add(Conv2D(64, (3, 3), activatin=&#39;relu&#39;)) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) #展开为一维数据用于全连接层model.add(Flatten()) model.add(Dense(256, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(10, activation=&#39;softmax&#39;))#梯度迭代算法sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=sgd)#训练过程model.fit(x_train, y_train, batch_size=32, epochs=10)#效果评估score = model.evaluate(x_test, y_test, batch_size=32) 4.3 Tensorboard与问题监控&#x2F;&#x2F;tensorboard最重要的作用就在于观察训练过程中的各种问题并改善，包括梯度消失、过拟合等问题&#x2F;&#x2F;获取所有可训练的参数var_list_w = [var for var in tf.trainable_variables() if &#39;w&#39; in var.name] var_list_b = [var for var in tf.trainable_variables() if &#39;b&#39; in var.name] &#x2F;&#x2F;利用定义的梯度算法来计算梯度gradient_w = optimizer.compute_gradients(loss, var_list=var_list_w) gradient_b = optimizer.compute_gradients(loss, var_list=var_list_b) &#x2F;&#x2F;返回的梯度是一个列表，可对其进行各种列表操作&#x2F;&#x2F;加入summary操作for idx, itr_g in enumerate(gradient_w): variable_summaries(itr_g[0], &#39;layer%d-w-grad&#39;%idx) for idx, itr_g in enumerate(gradient_b): variable_summaries(itr_g[0], &#39;layer%d-b-grad&#39;%idx for idx, itr_g in enumerate(var_list_w): variable_summaries(itr_g, &#39;layer%d-w-grad&#39;%idx) for idx, itr_g in enumerate(var_list_b): variable_summaries(itr_g, &#39;layer%d-b-grad&#39;%idx) 4.4改善深度神经网络&#x2F;&#x2F;出现梯度消失一种最有效的方式就是进行BN操作&#x2F;&#x2F;batchnorm层net = tf.contrib.layers.batch_norm(net) &#x2F;&#x2F;加入BN层的神经网络#对于sigmoid激活函数来讲，BN操作效果可能不理想net = slim.fully_connected(x, 4, activation_fn=tf.nn.sigmoid, scope=&#39;full1&#39;, reuse=False) net = tf.contrib.layers.batch_norm(net) net = slim.fully_connected(net, 8, activation_fn=tf.nn.sigmoid, scope=&#39;full2&#39;, reuse=False) net = tf.contrib.layers.batch_norm(net) net = slim.fully_connected(net, 8, activation_fn=tf.nn.sigmoid, scope=&#39;full3&#39;, reuse=False) net = tf.contrib.layers.batch_norm(net) net = slim.fully_connected(net, 4, activation_fn=tf.nn.sigmoid, scope=&#39;full4&#39;, reuse=False) net = tf.contrib.layers.batch_norm(net) net = slim.fully_connected(net, 3, activation_fn=tf.nn.sigmoid, scope=&#39;full5&#39;, reuse=False) loss = tf.reduce_mean(tf.square(y-label)) 4.5性能优化建议&#x2F;&#x2F;训练前的优化技巧1.网络结构优化Relu和BN能够有效加快神经网络训练速度卷积核心的选取可以从大的卷积核心修改为多个小的卷积核心将nxn修改为nx1+1xn，减少参数量，不同的输出内容之间可以进行concat引入跨层支路解决梯度问题（ResNet)2.初始值的选取不好的初始值对训练的影响非常大，有效的初始化方法包括xavier初始化方法和He初始化方法3.数据预处理包括去均值和方差均衡&#x2F;&#x2F;训练过程中的优化技巧1）优化算法的选择Adam2）学习率的选取从大的步长开始进行迭代，逐步减少学习率3）Batchsize选择4）model ensembles使用不同初始值同时训练多个模型，预测过程中将多个模型输出结果做平均，有效提升结果精度5)dropout选择从0.5附近进行调整，调整步长为0.05左右 &#x2F;&#x2F;物体检测1.传统检测方法2001年，基于Haar特征和Adaboost检测方法引起轰动2012年之前，三方面不断创新与优化：特征的设计更新、检测窗口的选择、分类器的设计更新 2.深度学习的物体检测1）基于分类的物体检测处理过程：图像被分解成多个小区域，每个小区域将运行一个分类算法以确定区域是否包含待检测物体，之后再在这个小区域的周围确认物体的边界框。代表算法：R-CNN、Fast-RCNN、Faster-RCNN2) 基于回归的物体检测将问题建模为回归问题，通过深度神经网络直接预测出边界框和所属类别的置信度。代表算法：SSD、YOLO模型 &#x2F;&#x2F;YOLO模型官网：https://pjreddie.com/darknet/yolo/&#x2F;&#x2F;选讲tiny YOLO v1模型，由9个卷积层和3个全连接层组成，每个卷积层都由卷积层、LeakyRelu和Max Pooling操作组成，前9个卷积层可被理解为特征提取器，最后三个全连接层可被理解为预测边界框的回归器。参考论文：You Only Look Once:Unified, Real-Time Object Detection参考实例：https://github.com/xslittlegrass/CarND-Vehicle-Detection模型参数：45089374深度学习框架：Keras 1.2.2 &#x2F;&#x2F;构建YOLO模型网络结构import keras from keras.models import Sequential from keras.layers.convolutional import Convlution2D, MaxPooling2D from keras.layers.advanced_activations import LeakyReLU from keras.layers.core import Flatten, Dense, Activation, Reshape from utils import load_weights, Box, yolo_net_out_to_car_boxes, draw_box def construct_yolo_model(): keras.backend.set_image_dim_ordering(&#39;th&#39;) model = Sequential() model.add(Convolution2D(16, 3, 3, input_shape=(3, 448, 448), border_mode=&#39;same&#39;, subsample=(1, 1))) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Convolution2D(32, 3, 3, border_mode=&#39;same&#39;)) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2), border_mode=&#39;valid&#39;)) model.add(Convolution2D(32, 3, 3, border_mode=&#39;same&#39;)) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2), border_mode=&#39;valid&#39;)) model.add(Convolution2D(64, 3, 3, border_mode=&#39;same&#39;)) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2), border_mode=&#39;valid&#39;)) model.add(Convolution2D(128, 3, 3, border_mode=&#39;same&#39;)) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2), border_mode=&#39;valid&#39;)) model.add(Convolution2D(256, 3, 3, border_mode=&#39;same&#39;)) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2), border_mode=&#39;valid&#39;)) model.add(Convolution2D(512, 3, 3, border_mode=&#39;same&#39;)) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2), border_mode=&#39;valid&#39;)) model.add(Convolution2D(1024, 3, 3, border_mode=&#39;same&#39;)) model.add(LeakyReLU(alpha=0.1)) model.add(Convolution2D(1024, 3, 3, border_mode=&#39;same&#39;)) model.add(LeakyReLU(alpha=0.1)) model.add(Convolution2D(1024, 3, 3, border_mode=&#39;same&#39;)) model.add(LeakyReLU(alpha=0.1 model.add(Flatten()) model.add(Dense(256)) model.add(Dense(4096)) model.add(LeakyReLU(alpha=0.1)) model.add(Dense(1470)) model.summary() return model 注：网络的输入是形状为（3,448,448)的图像，其输出是一个1470维度的向量，它包含预测边界框、物体类别信息。1470矢量被分成三个部分，分别给出了所属类别概率、置信度和边框坐标。这三个部分进一步划分为49个小区域，与每个单元的预测相对应。输出向量信息组织方式：probability:49*20=980判断类别，20个类别confidence:49*2=98是否包含物体（0,1）box coordinates:49*8=392 (x_min,y_min,x_max,y_max),(c_x,c_y,w,h) 8.4.3车辆图像数据探索1.车辆视频数据预处理&#x2F;&#x2F;预处理及可视化图像def visualize_images(): imagePath = &#39;./test_images/test1.jpg&#39; image = plt.imread(imagePath) #去除顶部和底部图片 image_crop = image[300:650,500:,:] #将图片转换为模型所需要的输入格式 resized = cv2.resize(image_crop, (448, 448)) f1,(ax11,ax22,ax33) = plt.subplot(1, 3, figsize=(16, 6)) ax11.imshow(image) ax22.imshow(image_crop) ax33.imshow(resized) pylab.show() return resized 8.4.5迁移学习通过迁移学习加载使用Pre-trained YOLO模型进行行车检测。具体做法是将pre-trained模型中的权重加载进之前构造的模型结构中，官网提供的权重，可以通过脚本解析成Keras能够加载的格式。&#x2F;&#x2F;加载YOLO模型权重def load_model_weights(model): #预训练权重网址：https://pjreddie.com/darknet/yolo/ load_weights(model, &#39;./yolo-tiny.weights&#39;) &#x2F;&#x2F;加载模型权重的具体逻辑def load_weights(model, yolo_weight_file): data = np.fromfile(yolo_weight_file, np.float32) data = data[4:] index = 0 for layer in model.layers: shape = [w.shape for w in layer.get_weights()] if shape !=[]: kshape, bshape = shape bia = data[index:index+np.prod(bshape)].reshape(bshape) index += np.prod(bshape) ker = data[index:index:index+np.prod(kshape)].reshape(kshape) index += np.prod(kshape) layer.set_weights([ker, bia]) &#x2F;&#x2F;模型推断&#x2F;&#x2F;使用模型进行在线推断，预测出车辆区域def inference_image(model, resized): #转置 batch = np.transpose(resized, (2, 0, 1)) #将像素值变换到-1~1 batch = 2*(batch/255.) - 1 #将一张图片转为数组 batch = np.expand_dims(batch, axis=0） out = model.predict(batch) return out &#x2F;&#x2F;绘制检测结果&#x2F;&#x2F;将上述的预测结果转换为边框坐标，同时基于阈值进行预测th = 0.17 boxes = yolo_net_to_out_to_car_boxes(out[0], threshold=th) &#x2F;&#x2F;定义box边框对象，判断是否保留预测的边框结果通过c,在图像上绘制车辆位置通过对象中的坐标信息#定义box类，存储边框信息和物体检测类别等信息class Box: def __init__(self): #x, y轴坐标 self.x, self.y = float(), float() #边框宽度和长度 self.w, self.h = float(), float() #置信度 self.c = float() #所属类别概率 self.prob = float() &#x2F;&#x2F;通过yolo_net_to_out_to_car_boxes方法，将预测出的Vector转换为Box对象信息。其核心逻辑是解析模型预测输出向量中的坐标、类别和置信度信息&#x2F;&#x2F;置信度大于阈值边界框则进行保留class_num = 6 #yolo模型可以预测多种类别，6为车辆所属类别 p = probs[grid, :] *bx.c if p[class_num]&gt;=threshold: bx.prob = p[class_num] boxes.append(bx) &#x2F;&#x2F;将结果绘制在图像上def visualize_image_car_detection(boxes): imagePath = &#39;./test_images/test1.jpg&#39; image = plt.imread(imagePath) f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6)) ax1.imshow(image) ax2.imshow(draw_box(boxes, plt.imread(imagePath), [[500, 1280],[300,650]])) pylab.show() &#x2F;&#x2F;将边框绘制在图像上def draw_box(boxes, im, crop_dim): imgcv = im [xmin, xmax] = crop_dim[0] [ymin, ymax] = crop_dim[1] for b in boxes: h, w, _ = imgcv.shape left = int((b.x-b.w/2.)*w) right = int((b.x+b.w/2.)*w) top = int((b.y-b.h/2.)*h) bot = int((b.y+b.h/2.)*h) left = int(left*(xmax-xmin)/w+xmin) right = int(right*(xmax-xmin)/w+xmin) top = int(top*(ymax-ymin)/h+ymin) bot = int(bot*(ymax-ymin)/h+ymin) if left&lt;0 : left=0 if right&gt;w-1 : right=w-1 if top&lt;0 : top=0 if bot&gt;h-1 : bot=h-1 thick = int((h+w)//150) cv2.rectangle(imgcv, (left, top), (right, bot), (255,0,0), thick) return imgcv 8.5.1英伟达End to End模型End to End的好处：通过缩减人工预处理和后续处理，尽可能使模型从原始输入到输出，使得其根据网络模型能够有足够多的空间进行自动调节，从而减少基于规则的复杂变化。缺点：可解释性较差，准确度和精度不容易受控制。&#x2F;&#x2F;构建英伟达模型import tensorflow as tf import keras from keras.models import Sequential from keras.layers import Dense, Activation, Flatten, Lambda from keras.layers import Conv2D, Dropout from keras import losses def nvida_model(): model = Sequential() model.add(Lambda(lambda x: x/127.5-1., input_shape=(img_height, img_width, img_channels))) model.add(Conv2D(24, kernel_size=(5, 5), strides=(2, 2), padding=&#39;valid&#39;, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;)) model.add(Conv2D(36, kernel_size=(5, 5), strides=(2, 2), padding=&#39;valid&#39;, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;)) model.add(Conv2D(48, kernel_size=(5, 5), strides=(2, 2), padding=&#39;valid&#39;, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;)) model.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding=&#39;valid&#39;, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;)) model.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding=&#39;valid&#39;, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu model.add(Flatten()) model.add(Dense(1164, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;)) model.add(Dense(100, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;)) model.add(Dense(50, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;)) model.add(Dense(10, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;)) model.add(Dense(1, kernel_initializer=&#39;he_normal&#39;)) model.compile(loss=&#39;mse&#39;, optimizer=&#39;Adadelta&#39;) return model &#x2F;&#x2F;8.5.3数据分析1）转向控制数据分布#绘制转向分布def steering_distribution(): wheel_sig = pd.read_csv(params.data_dir+&#39;/epoch01_steering.csv&#39;) wheel_sig.head() wheel_sig.wheel.hist(bins=50) 2)数据变化幅度#绘制转向变化幅度def angel_visualize(): wheel_sig = pd.read_csv(params.data_dir+&#39;/epoch01_steering.csv&#39;) wheel_sig.plot(x=&#39;frame&#39;, y=&#39;wheel&#39;) plt.show() 8.5.4读入视频，并处理图像&#x2F;&#x2F;使用OpenCV从视频中提取图像，以及与其对应的转向角度并返回#提取图像并处理imgs = [] wheels = [] epochs = [10] for epoch in epochs: vid_path = utils.join_dir(params.data_dir, &#39;epoch&#123;:0&gt;2&#125;_front.mp4&#39;.format(epoch)) assert os.path.isfile(vid_path) frame_count = frame_count_func(vid_path) cap = cv2.VideoCapture(vid_path) for frame_id in range(frame_count): while True: #通过OpenCV中的VideoCapture进行视频中图像的提取 ret, img = cap.read() if not ret: break #用户可以自定义对图像的处理、扩展和增强操作 img = a_image_convert.img_preprocess(img, color_mode, flip=False) imgs.append(img) csv.path = os.path.join(data_dir, &#39;epoch&#123;:0&gt;2&#125;_steering.csv&#39;.format(epoch)) rows = pd.read_csv(csv.path) yy = rows[&#39;wheel&#39;].values wheels.extend(yy) cap.release() imgs = np.array(imgs) wheels = np.array(wheels) wheels = np.reshape(wheels, (len(wheels), 1) return imgs, wheels 8.5.5深度学习模型构建与训练&#x2F;&#x2F;训练模型def training(model, X_train_RGB, y_train_RGB): RGB_model = model time_start = time.time() #fit the model RGB_history = RGB_model.fit(X_train_RGB, y_train_RGB, epochs=30, batch_size=batch_size) return RGB_model, RGB_history &#x2F;&#x2F;可视化结果#将训练过程中的loss误差进行可视化def visualize_label(RGB_history): print(RGB_history.history[&#39;loss&#39;] plt.figure(figsize=(9, 6)) plt.plot(RGB_history.history[&#39;loss&#39;]) plt.title(&#39;model loss&#39;) plt.ylabel(&#39;Loss&#39;, fontsize=12) plt.xlabel(&#39;Epoch&#39;, fontsize=12) plt.legend([&#39;train RGB&#39;], loc=&#39;upper right&#39;) plt.grid() plt.show() &#x2F;&#x2F;可视化&#x2F;&#x2F;数据的绘图过程就是将前面所得到的一系列数据，通过静态、动态的二维、三维图形进行展示1.Matplotlib&#x2F;&#x2F;绘制y&#x3D;sinx图像import numpy as np import matplotlib.pyplot as plt x = np.linspace(0, 4*np.pi, 1000) y = np.sin(x) plt.plot(x,y) &#x2F;&#x2F;利用API,绘制更加审美要求的图像import numpy as np import matplotlib.pyplot as plt import matplotlib as mpl #设置图片风格 mpl.style.use(&#39;seaborn-darkgrid&#39;) #定义曲线 x = np.linspace(0, 4*np.pi, 100) y1 = np.sin(x) y2 = np.sin(x+1) y3 = np.sin(x+2) #绘图 plt.plot(x, y1, color=&#39;#009900&#39;, lw=6, alpha=0.6) plt.plot(x, y2, color=&#39;#990000&#39;, lw=6, alpha=0.6) plt.plot(x, y3, color=&#39;#000099&#39;, lw=6, alpha=0.6) #展示 plt.show() 9.4ECharts&#x2F;&#x2F;ECharts提供了常规的折线图、柱状图、散点图、饼图、K线图等等，功能强大。&#x2F;&#x2F;ECharts图形绘制略 &#x2F;&#x2F;文本向量化&#x2F;&#x2F;文本向量化函数#文本TfIdf向量化from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidVectorizer() vectors = vectorizer.fit_transform(datas) &#x2F;&#x2F;文本向量化的数据进行降维&#x2F;&#x2F;LDA降维from sklearn.decomposition import LatentDirichletAllocation lda = LatenDirichletAllocation(n_components=n_topic, max_iter=5, learning_method = &#39;online&#39;, learning_offset = 50., radom_state = 0)#用LDA方法降维数据dr_vectors = lad.fit_transform(vectors) 9.6三维可视化&#x2F;&#x2F;ECharts地图柱状图myChart.setOption(&#123; visualMap: &#123; show: flase, calculable: true, realtime: false, inRange: &#123; color: [&#39;#313695&#39;, &#39;#4575b4&#39;, &#39;#74add1&#39;, &#39;#abd9e9&#39;, &#39;#e0f3f8&#39;, &#39;#ffffbf&#39;, &#39;#fee090&#39;, &#39;#fdae61&#39;, &#39;#f46d43&#39;, &#39;#d73027&#39;, &#39;#d73027&#39;, &#39;a50026&#39;] &#125;, outOfRange: &#123; colorAlpha: 0 &#125;, max: linedata[1] &#125;, ... series: [&#123; type: &#39;bar3D&#39;, shading: &#39;realistic&#39;, coordinateSystem: &#39;mapbox&#39;, barSize: 0.2, silent: true, data: linedata[0] &#125;] &#125;); &#x2F;&#x2F;利用Matplotlib完成对三维数据的可视化任务from mpl_toolkits.mplot3d import axes3d import matplotlib.pyplot as plt from matplotlib import cm import matplotlib.style as style style.use(&#39;seaborn-darkgrid&#39;) #定义三维画布fig = plt.figure() ax = fig.gca(projection=&#39;3d&#39;)#获取数据X, Y, Z = axes3d.get_test_data(0.05)#绘制surfaceax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.3)#绘制等值线cst = ax.contourf(X, Y, Z, zdir=&#39;z&#39;, offset=-100, cmap=cm.coolwarm) cst = ax.contourf(X, Y, Z, zdir=&#39;x&#39;, offset=-40, cmap=cm.coolwarm) cst = ax.contourf(X, Y, Z, zdir=&#39;y&#39;, offset=40, cmap=cm.coolwarm) plt.show() 9.7动态可视化&#x2F;&#x2F;Matplotlib中用于数据动态演示的方法为animation,其可以通过函数进行简单的调用，以进行动态图形的演示工作&#x2F;&#x2F;动画展示import matplotlib.animation as animation animation.FuncAniamtion( ... ) &#x2F;&#x2F;动态可视化的展示方式是在普通的图表之上通过不断地修改数据并进行展示，这种修改可以通过setOption而得到的，在实现上可以通过函数递归的方式实现动态数据的可视化工作function update()&#123; myChart.setOption(...); setTimeout(update, UPDATE_DURATION); &#125; update(); &#x2F;&#x2F;优化实践10.1通用深度神经网络训练优化建议 1）通用的较为优化的训练过程1.将问题转换为相似的经典问题场景，参照paper中的配置和调优技巧进行最初的实验与优化2.优化算法：选用随机梯度下降（SGD)算法，虽然批量梯度下降（BGD)相比SGD有一些优势，但是在处理大规模数据时，SGD及其优化变种更加简单和快速。3.随机Shuffle样本：应尽量避免连续处理的样本属于同一类别的情况。尽量选择当前样本能让模型产生较大的误差，而不是较小的误差4.规范化数据：输入的每个变量均值最好趋近于0.变换输入变量，使其协方差相近，变量间尽量不要相关5.激活函数的选取：相比Sigmoid函数，tanh和Relu有更好的收敛速度。6.权重初始化：可以随机通过一种分布，均值为0.7.选择学习率：每个权重都可以选取属于自己的学习率。处于低层的权重学习率最好大于高层的权重学习率。学习率最好正比于每个单元的输入数量。 2）CNN训练过程中通常关注的优化点和参数一般比较关注：Learning Rate,Weight Decay,Momentum,Batchsize,Init Weights,数据增强eg:在Resnet中，使用SGD优化算法优化方法训练，mini-batch的大小设置为256，学习率初始化为0.1.随着训练进行，当Loss不再下降，会每次自适应以10倍进行缩减学习率。模型训练用了60x10^4轮迭代。Weight Decay设置为0.0001，同时设置momentum为0.9 3)RNN训练过程中通常关注的优化点和参数一般比较关注：SGD,正则化，规范化梯度，Pad Sentence,Init Weight, Batch Size, Embedding输入，输出控制，Vacabulary Size, Sampled Softmaxeg:Google发布的TTS模型TACOTRON为例 10.1.1 过拟合和欠拟合欠拟合：若训练集和测试集的误差有收敛但很高时，则为高偏差过拟合：若训练集和测试集的误差较大时，则为高方差 解决过拟合的方法：正则化，数据增强，Early Stop, Dropout, Batch Normalization 解决欠拟合的方法：1.使用更加复杂的深度学习网络架构2.添加其他特征项，有时候模型出现欠拟合的情况是因为特征项不够导致的，可以添加其他特征项来很好的解决这个问题3.减少正则化参数和组件，正则化的目的是用来防止过拟合。 10.1.2数据增强&#x2F;&#x2F;数据增强的根本原因在于机器在学习的过程中会在模型中遇到大量的参数，同时为了防止过拟合1）对于图像数据，可采取：1.图像平移：使得网络学习到平移不变的特性2.图像旋转：使得网络学习到旋转不变的特性3.图像亮度变化4.裁剪5.缩放6.图像模糊:用不同的卷积模板产生模糊图像2）语音识别中对输入数据添加随机噪声等方式3）NLP中最常用的方式就是进行近义词替换等方式4）噪声注入，可以对输入添加噪声，也可以对隐藏层或者输出层添加噪声 10.1.3梯度消失&#x2F;&#x2F;实验数据显示了深度神经网络在训练过程中，随着epoch的增加各隐藏层的学习率变化。前面隐藏层的学习速度要低于后面的隐藏层&#x2F;&#x2F;梯度消失的原因：根据链式法则，如果每一层神经元对上一层输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层的传播后，误差对输入层的偏导也会趋近于0解决梯度消失的策略：1.BN2.RNN中使用LSTM:适用于RNN,门控制和长时记忆可缓解和解决梯度消失问题3.激活函数Relu:新的激活函数解析性质更好，其在一定程度上克服了sigmoid函数和tanh函数的梯度消失问题。4.在RNN反向传播过程中减少时间步长度。 10.1.4初始化权重&#x2F;&#x2F;在参数解空间内，好的权重初始化方式，意味着离全局最小值更近。1.高斯初始化，为权重初始化较小的值，权重按照高斯分布随机进行初始化，固定均值和方差2.Xaiver更新方法，使用tanh为激活函数，效果较好。进行梯度更新时，收敛速度较快，然而没有考虑Relu3.MSRA方法，适用于从头训练深层深度神经网络的网络结构。权重以高斯分布随机进行初始化，方差需要考虑空间过滤器的大小和过滤器数量的影响。 10.1.5优化算法近些年最常用的是采用Adam优化算法，也可以采用自适应学习率的方法实现快速收敛。 10.1.6超参数选择一些实践经验：1.在验证集上进行调参2.优先调Learning Rate3.通过初期设计卷积层尽量深、卷积核尽量多的模型，强行让模型拟合训练集，这时会出现过拟合，之后通过Dropout、正则化和Data Augument等等方式去改善模型结果4.调整模型的层数和卷积核数量 &#x2F;&#x2F;通过Scikit-learn的网格搜索库进行参数调优实例1.常见搜索参数学习率、Dropout、Epochs和神经元数量2.数据集下载数据集为Pima Indians Onset of Diabetes分类数据集下载地址：https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/3.搜索最优batchsize和epochs&#x2F;&#x2F;以20的步长，从10到100逐步评估不同的微型批尺寸，epochs分别设置为10、50、100import numpy from sklearn.grida_search import GridSearchCV from keras.models import Sequential from keras.layers import Dense from keras.wrappers.scikit_learn import KerasClassifier #Function to create model, required for KerasClassifierdef create_model(): #create model model = Sequential() model.add(Dense(12, input_dim=8, activation=&#39;relu&#39;)) model.add(Dense(1, activation=&#39;sigmoid&#39;)) #compile model model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) return model #fix random seed for reproducibility seed = 7 numpy.random.seed(seed) #load dataset dataset = numpy.loadtxt(&quot;pima-indians-diabetes.csv&quot;, delimiter=&#39;,&#39;) #split into input (x) and output (Y) variables X = [:, 0:8] Y = [:, 8] #create model model = KerasClassifier(build_fn=create_model, verbose=0) #define the grid search parameters batch_size = [10, 20, 40, 60, 80, 100] epochs = [10, 50, 100] param_grid = dict(batch_size=batch_size, nb_epoch=epochs) grid = GridSearchCV(estimator=model, param_grid=parm_grid, n_jobs=-1) grid_result = grid_fit(X,Y) #summarize results print(&quot;Best: %f using %s&quot; % (grid_result.best_score_, grid_result.best_params)) for params, mean_score, scores in grid_result.grid_scores_: print(&quot;%f (%f) with: %r&quot; % (scores.mean(), scores.std(), params)) 10.2深度学习系统性能优化建议10.2.1输入及预处理流水线优化输入流水线：从磁盘读取图像，将JPEG预处理为张量，进行数据预处理裁剪、翻转等，然后进行批处理操作1.在CPU端进行预处理&#x2F;&#x2F;在CPU端上放置输入预处理操作可以显著提高性能，GPU专注训练&#x2F;&#x2F;控制代码在CPU端执行with tf.device(&quot;/cpu:0&quot;): # function to get and process data. ​ distored_inputs = load_and_preprocess_images() 2.使用大文件读取大量的小文件会显著影响I&#x2F;O性能1）转换为TFRecord格式2）小数据集加载到内存 10.2.2数据格式NHWC的方存局部性更好（每三个输入像素即可得到一个输出像素），NCHW则必须等所有通道输入都准备好后才能得到最终的输出结果，需要占用较大的临时空间。tf默认NHWC格式，Nvidia cuDNN默认NCHW格式注：设计网络时充分考虑这两种格式，最好能够灵活切换，在GPU上训练时使用NCHW格式，在CPU上做预测时使用NHWC格式 10.2.3编译优化&#x2F;&#x2F;通过bazel命令对特定平台对tf进行编译bazel build -c opt --copt=-march=&quot;brodewell&quot; --config=cuda //tensorflow/tools/pip_package:build_pip_package 10.2.4GPU性能瓶颈诊断&#x2F;&#x2F;参考如下分析步骤对作业进行优化1）对代码进行性能分析2）找到运行慢的阶段3）分析慢的原因4）修改成更快的实现5）再次对代码进行性能分析 &#x2F;&#x2F;处理器有两个关键的性能瓶颈：浮点计算量和内存吞吐量。&#x2F;&#x2F;可通过以下工具进行深度学习作业的性能分析1.Tensorflow性能分析工具Timeline(获取执行图中每个节点的执行时间）1）创建metadata运行时对象2）获取运行时信息创建Timeline对象3）将Timeline对象写入json文件4）Chrome加载trace的json文件 &#x2F;&#x2F;tensorflow使用Timeline进行性能分析import tensorflow as tf from tensorflow.python.client import timeline x = tf.random_normal([1000, 1000]) y = tf.random_normal([1000, 1000]) res = tf.matmul(x, y) #run the graph with full trace option with tf.Session() as sess: run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) run_metadata = tf.RunMetadata() sess.run(res, options=run_options, run_metadata=run_metadata) #create Timeline variable，then write it into json file t1 = timeline.Timeline(run_metadata.step_stats) ctf = t1.generate_chrome_trace_format() with open(&#39;timeline.json&#39;, &#39;w&#39;) as f: f.write(ctf) 可以打开谷歌chrome浏览器，转到chrome:&#x2F;&#x2F;tracing页并加载timeline.json文件，接下来，可以进行程序的profiling 2.常用的GPU分析工具1）nvprof是英伟达性能分析工具2）nvvp则是带GUI的英伟达可视化性能分析工具 10.2.5CPU瓶颈优化1）多线程方式优化以下两个针对tensorflow的配置可以通过适配线程池进行CPU的性能优化intra_op_parallelism_threads：对tf操作符内部的任务进行并行化inter_op_parallelism_threads: 控制多个运算符之间的并行化运算&#x2F;&#x2F;多线程优化config = tf.ConfigProto() config.intra_op_parallelism_threads = 22 config.inter_op_parallelism_threads = 22 tf.session(config=config) 2)使用SIMD高级指令集参考tf官方文档的”Performance Guide”章节 10.2.6模型压缩模型小型化：从模型权重的角度进行压缩和从网络架构的角度进行压缩网络架构角度：提出新的网络结构或卷积方法进行压缩优化，如SqueezeNet, MobileNets等模型权重角度：一般是在已经训练好的模型上进行裁剪，然后fine-tuning到原有模型的准确率，一般的优化方式包括剪枝、权值共享、神经网络二值化等。 10.3工程实践建议10.3.1Model格式转换框架间的模型转换参考链接：1.https://github.com/ysh329/deep-learning-model-convertor2.https://github.com/Microsoft/MMdnn 10.3.2迁移学习（Transfer Learning)其思想是将训练好的模型参数迁移到新的模型来帮助新模型的训练和预测。 &#x2F;&#x2F;通过MNIST数据集04的数字训练一个模型，然后将模型迁移到59数据集上进行迁移学习1）在MNIST数据集上训练一个简单的卷积神经网络，只预测04的数字2）将训练好的预测04数据集的模型，应用到5~9数据集上。对模型冻结卷积层参数，Fine-Tuning全连接层。&#x2F;&#x2F;keras迁移学习实例from __future__ import print_function import datetime import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Conv2D, MaxPooling2D from keras import backend as K now = datetime.datetime.now batch_size = 128 num_classes = 5 epochs = 5 #input images dimensions img_rows, img_cols = 28, 28 #number of convolutional filters to use filters = 32 #size of pooling area for max pooling pool_size = 2 #convolution kernel size kernel_size = 3 if K.image_data_format()==&#39;channels_first&#39;: input_shape = (1, img_rows, img_cols) else: input_shape = (img_rows, img_cols, 1) def train_model(model, train, test, num_classes): x_train = train[0].reshape((train[0].shape[0],)+input_shape) x_test = test[0].reshape((test[0].shape[0],)+input_shape) x_train = x_train.astype(&#39;float32&#39;) x_test = x_test.astype(&#39;float32&#39;) x_train /= 255 x_test /= 255 print(&#39;x_train shape:&#39;, x_train.shape) print(x_train.shape[0], &#39;train samples&#39;) print(x_test.shape[0], &#39;test samples&#39;) #convert class vectors to binary class matrics y_train = keras.utils.to_categorical(train[1], num_classes) y_test = keras.utils.to_categorical(test[1], num_classes) model.compile( loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adadelta&#39;, metrics = [&#39;accuracy&#39;] ) t = now() model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, verbose = 1, validation_data = (x_test, y_test)) print(&#39;Training time: %s&#39; %(now() -t)) score = model.evaluate(x_test, y_test, verbose=0) print(&#39;Test score:&#39;, score[0]) #the data,shuffled and spilt between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load-data() #create two datasets one with digits below 5 and one with 5 and above x_train_lt5 = x_train[y_train&lt;5] y_train_lt5 = x_train[y_train&lt;5] x_test_lt5 = x_test[y_test&lt;5] y_test_lt5 = y_test[y_test&lt;5] x_train_get5 = x_train[y_train&gt;=5] y_train_get5 = y_train[y_train&gt;=5]-5 x_test_get5 = x_test[y_test&gt;=5] y_test_get5 = y_test[y_test&gt;=5]-5 #define two groups of layers:feature(convolutions) and classification(dense) feature_layers = [ Conv2D(filters, kernel_size, padding=&#39;valid&#39;, input_shape=input_shape), Activation(&#39;relu&#39;), Conv2D(filters, kernel_size), Activation(&#39;relu&#39;), MaxPooling2D(pool_size=pool_size), Dropout(0.5), Flatten()] classification_layers=[ Dense(128), Activation(&#39;relu&#39;), Dropout(0.5), Dense(num_classes), Activation(&#39;softmax&#39;)] #create complete model model = Sequential(feature_layers+classification_layers) #train model for 5-digit classification(0~4) train_model(model, (x_train_lt5, y_train_lt5), (x_test_lt5, y_test_lt5), num_classes) #freeze feature layers and rebuild model for l in feature_layers: l.trainable = False #transfer: train dense layers for new classification task(5~9) train_model(model, (x_train_gte5, y_train_get5), (x_test_get5, y_test_get5), num_classes) ​​​","categories":[],"tags":[]},{"title":"学术搜索网站集合","slug":"学术搜索网站导航","date":"2022-03-15T01:58:59.113Z","updated":"2022-03-26T01:55:30.522Z","comments":true,"path":"2022/03/15/CN/学术搜索网站导航/","link":"","permalink":"http://pistachio0812.github.io/2022/03/15/CN/%E5%AD%A6%E6%9C%AF%E6%90%9C%E7%B4%A2%E7%BD%91%E7%AB%99%E5%AF%BC%E8%88%AA/","excerpt":"","text":"搞学术研究的必不可少的就是论文，而且是大量的论文。因此，在这里我特意把我平时用到的积累到的论文搜索网站集中到了这里，后续遇到了其他的还会继续补充。 谷歌学术谷歌学术是一个可以免费搜索学术文章的Google网络应用。2004年11月，Google第一次发布了Google学术搜索的试用版。该项索引包括了世界上绝大部分出版的学术期刊， 可广泛搜索学术文献的简便方法。您可以从一个位置搜索众多学科和资料来源：来自学术著作出版商、专业性社团、预印本、各大学及其他学术组织的经同行评论的文章、论文、图书、摘要和文章。Google 学术搜索可帮助您在整个学术领域中确定相关性最强的研究。 相关页面如下： githubgithub于2008年4月10日正式上线，除了代码仓库托管及基本的Web管理界面以外，还提供了订阅、讨论组、文本渲染、在线文件编辑器、协作图谱（报表）、代码片段分享（Gist）等功能。目前，其注册用户已经超过350万，托管版本数量也是非常之多，其中不乏知名开源项目Ruby on Rails、jQuery、python等。 相关页面如下： CVPR国际计算机视觉与模式识别会议（CVPR）是IEEE一年一度的学术性会议，会议的主要内容是计算机视觉与模式识别技术。CVPR是世界顶级的计算机视觉会议（三大顶会之一，另外两个是ICCV和ECCV，近年来每年有约1500名参加者，收录的论文数量一般300篇左右。本会议每年都会有固定的研讨主题，而每一年都会有公司赞助该会议并获得在会场展示的机会。 相关页面如下： CVFCVF研究论文是由计算机视觉基金会提供的开放获取版本。除水印外，它们与接受的版本相同;最后发表的论文集可以在IEEE Xplore上找到。本材料的提出，以确保及时传播学术和技术工作。版权和其中的所有权利由作者或其他版权持有人保留。所有复制此信息的人都应遵守每个作者的版权所援引的条款和约束。 arXivarXiv是一个免费分发服务和开放获取的档案，涵盖物理、数学、计算机科学、定量生物学、定量金融学、统计学、电气工程和系统科学以及经济学领域的2,040,232篇学术文章。 paperswithcodePapers With Code代码论文的任务是创建一个免费和开放的资源与机器学习论文，代码，数据集，方法和评估表。我们相信，在NLP和ML的支持下，与社区合作是最好的。这个网站上的所有内容都是公开许可的CC-BY-SA(与维基百科一样)，每个人都可以贡献——寻找“编辑”按钮!我们还运营专门的门户网站，提供天文学、物理学、计算机科学、数学和统计学的论文代码。","categories":[],"tags":[]},{"title":"学习网站集合","slug":"学习网站集合","date":"2022-03-14T05:07:01.833Z","updated":"2022-03-26T10:51:27.449Z","comments":true,"path":"2022/03/14/en/学习网站集合/","link":"","permalink":"http://pistachio0812.github.io/2022/03/14/en/%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99%E9%9B%86%E5%90%88/","excerpt":"","text":"万门好课万门好课是一家提供多品类原创精品课程的在线教育平台 ，课程覆盖IT与互联网类、职业成长类、经济金融类、本科学习类等领域 。 整体课程定位侧重于“用户刚需”类课程，如语言版块的出国英语考试类课程、小语种培训课程，本科学习版块的各学科基础大课，以及特色的万门通识课程包括PS、化妆等。 网易云课堂网易云课堂立足于实用性的要求，网易云课堂与多家教育、培训机构建立合作，课程数量已达4100+，课时总数超50000,涵盖实用软件、IT与互联网、外语学习、生活家居、兴趣爱好、职场技能、金融管理、考试认证、中小学、亲子教育等十余大门类。 网易公开课网易公开课首批1200集课程上线，其中有200多集配有中文字幕。用户可以在线免费观看来自于哈佛大学等世界级名校的公开课课程，可汗学院，TED等教育性组织的精彩视频，内容涵盖人文、社会、艺术、科学、金融等领域。 力求为爱学习的网友创造一个公开的免费课程平台，借此向外界公开招聘兼职字幕翻译。 爱课程网爱课程网利用现代信息技术和网络技术， 面向高校师生和社会大众。提供优质教育资源共享和个性化教学资源服务，具有资源浏览、搜索、重组、评价、课程包的导入导出、发布、互动参与和“教”“学”兼备等功能。 粉笔网粉笔网是一个互联网教育平台，业务包含：公务员考试，考研、教师资格、事业单位、英语、建造、财会等技能培训；利用技术手段实现智能批改功能，并提供免费题库，供用户查阅学习，利用网络直播，进行线上授课，同时提供实物图书、试卷以及客户服务。","categories":[{"name":"学习网站","slug":"学习网站","permalink":"http://pistachio0812.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"学习网站","slug":"学习网站","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99/"}]},{"title":"计算机视觉相关单词","slug":"计算机视觉单词表","date":"2022-03-12T08:48:16.114Z","updated":"2022-03-13T13:03:48.618Z","comments":true,"path":"2022/03/12/CN/计算机视觉单词表/","link":"","permalink":"http://pistachio0812.github.io/2022/03/12/CN/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%8D%95%E8%AF%8D%E8%A1%A8/","excerpt":"","text":"artificial neural network,ANN 人工神经网络 perceptron 感知机，人工神经元 activation function 激活函数 rectified linear unit,RELU 修正线性单元 bias 偏置 loss function 损失函数 universal approximation theorem 万能逼近定理 one-hot encoding 独热编码 cross-entropy 交叉熵 dropout 丢弃 bagging 装袋 model averaging 模型平均 batch normalization 批归一化 backpropagation 反向传播 stochastic gradient descent,SGD 随机梯度下降 acquisition 学习，获得 integrate 整合，集成，合并 diverse 多样化的，不同的 tune 调优 curation 内容管理 projection 投影，预测 coherent 有条理的，连贯的 redundant 冗余的 entity 实体 synthetic 合成的，虚假的，不诚恳的 spammy 垃圾邮件式的，无聊的 crowdsourcing 众包 continuity 连续性，连贯性 manifold 多种多样的 inherent 固有的，内在的 pseudo 假的，仿冒的 ensemble 套 heuristic 启发式的；启发式教育法 erroneous 错误的，不正确的 resilient 有弹性的，可迅速恢复的 degraded 堕落的，退化的 converge 收敛，集中 outlier 离群值，异常值 violate 违反，违背 syntactic 语法的 cartesian 笛卡尔的 categorical 分类，绝对的 prune 修剪 param 停止 translation invariance 平移不变性 suppress 抑制，镇压，阻止 bidirectional 双向 tabular 扁平的，列成表格的 revenue 收入，税收 latency 延迟 harmonic 和声的，谐和的，音乐般的 harmonic mean 调和平均数 harmonic series 调和级数 rote 死记硬背，生搬硬套 bid 出价，投标 leaderboard 排行榜，通栏广告 minor 较小的，次要的，轻微的 contaminated 受污染的，弄脏的 tradeoff 权衡，折中 ensemble learning 集成学习 decompose 分解，使腐烂 intrinsic 内在的，固有的 notable 显要的，值得注意的；非常成功的，令人尊敬的 camouflaged 伪装的 facilitate 促进，使便利 overlap:与……重叠，部分地相同；重叠的部分，互搭量 threshold:入口，门槛，开始，极限，临界值 conjecture:猜测，推测 within:在……之内 oversample:过采样 trade off:权衡，卖掉，折中方案 ultimately:最后，根本，基本上 robotics：机器人学 areial:空中的，航空的，空气的 underperform:表现不佳，工作不如预期 crucial:重要的，决定性的 high-resolution:高分辨率的 deploy:配置，展开，部署 barely:仅仅，勉强，几乎不 tumor:肿瘤，肿块 diagnosis:诊断 inspection:检查，视察 defect:缺陷，缺点，不足之处 annotate:注释，作注解 address:地址，编址 potentially:可能地，潜在地 imply:意味，暗示，隐含 diversity:多样性，差异 generalize:概括，推广，使……一般化 portion:部分 crop:裁剪 merge:合并 align:匹配，排列，对齐，对准 mask:掩码，掩膜 cascade:小瀑布，串联，级联 fuse:融合，熔接，熔化 computational:计算的 overhead:经常性费用，运营费用 fraction:分数，部分，小部分，稍微 schematic illustration:示意图 respect to:关于，考虑 validate:验证，确认，使生效 stochastic:随机的，猜测的 decay:衰退，衰减 coefficient:系数，率 explicitly:明确地，明白地 outline:大纲，概要 distillation:蒸馏 curvature:曲率 stochastic:随机 variance:差异，方差 spectrum:光谱，频谱；范围 neat:灵巧的，整洁的；优雅的，平滑的 heterogenerous:由很多种类组成的 intricate:复杂的，错综的 arbitrary:任意的，武断的 vanilla:香草，比较原始的 sketch:示意图 incarnation:化身，典型 waive:放弃，搁置 shrinkage 收缩，皱缩，缩水; 跌价; 抽缩 alleviate 缓解，减轻 de-facto 事实上 corpus 文集，语料库 unprecedented 前所未有的 inductive 归纳的 empirical 经验主义的 allergic 过敏的，反感的 pollen 花粉 badminton 羽毛球运动 pharmacy 药房 jasmine 茉莉 latent 潜在的，潜伏的，潜意识的 prepend 预先考虑 embedding 编码 alternating 交互的 interpolation 插入，篡改，添写 de-duplicate 删除重复数据 suite 一套，套件 geometric 几何图形的，几何的 intermediate 中间的 fine-tuning 微调 appendix 附录 warmup 预热 least-squares regression 最小二乘回归 on-the-fly 匆匆忙忙地；在空中；（计）运行中 literature 文献 outperform 胜过，做的比……好 substantially 实质上；大体上；充分地 standard deviation 标准差 co-training 协同训练 boost 促进，增加 overtake 赶上，压倒，突然来袭 plateau 趋于平稳，进入停滞期 vanish 消失 versus 与 saturate 饱和的 principal 最主要的 plausible 貌似可信的，花言巧语的；貌似真实的，貌似有理的 sinusoidal 正弦曲线的 degree 程度 analogous 类似的 preliminary 初步的 manual 手动的，手工的 insight 洞察力，领悟 exponentially 以指数方式的 holistically 整体论地 unidirectional 单向性的 incorporate 包含，吸收，体现；把……合并 alleviate 减轻 shallow 浅的，肤浅的 discriminate 区分，辨别 coarser 粗糙的 granularity 间隔尺寸，粒度 derived 导出的，衍生的，派生的 predecessor 前任，前辈;(被取代的)原有事物，前身 cloze adj. 完形的；填充测验法的 cloze task 完形填空 recipe 秘诀，处方 distinctive 有特色的，与众不同的 unambiguously 不含糊地，明白地 intuitively 直观地；直觉地 trivially 琐细地，平凡地，无能地 mitigate 使缓和，使减轻 monolingual 单语的；仅用一种语言的；仅懂一种语言的 procedure 程序，手续，步骤 degenerate 使退化，恶化 de-facto (法)实际上的 explicitly 显式地 reformulate 重新构造 ensemble 全体，总效果 nontrivial 重要的，显著的 obstacle 阻碍，障碍 notorious 臭名昭著的，声名狼藉的 vanishing&#x2F;exploding gradients 梯度消失&#x2F;梯度爆炸 hamper 妨碍，束缚 degradation 退化，降级，堕落 thoroughly 完全地，彻底地 counterpart 副本，配对物 feasible 可行的，可能的 akin to 类似于 generic 类的，属性的; 一般的; 不受商标保护的; [生]属的，类的 retrieval 检索 quantization 量化 partial differential equations 偏微分方程 auxiliary 辅助的，备用的 Concurrent 并发的，同时发生的 asymptotically 渐近地 counterintuitive 违反直觉的 perturbations [流]扰动，不安 trial 测试 curse 咒骂，诅咒 estimation 评估，评价，判断 surrogate 代理的 prominent 突出的，显著的，卓越的，杰出的 thes 命题，论文 recalibrate 重新校准 pruning 剪枝 proxy 代理人，代表权 compound 加重; 使复杂化; 混合;混合的 criteria 标准，条件 panoptic 全景的 controversial 有争议的 problematic 有疑问的，有问题的 contrastive 对比的 intuitive 直觉的; 凭直觉获知的; 直观的 preserve 保存；保护；维持；腌；禁猎 intractable 棘手的；难治的；倔强的；不听话的 pretext 借口，托辞; 假象，掩饰 permutation 排列，置换 discrimination 区别对待; 鉴别力; 区别 shuffle 洗牌; 曳脚而行; 搬移; 搁置，随手放 neatness 整洁，干净 blur 模糊 permutation 排列，置换 infrared 红外线的 attenuation 衰减，衰变 tricky 棘手的，难对付的 plethora 过多，过剩 deluge 泛滥，淹没 elaborate 精心制作的，详尽的 repurpose 改换意图，重新 assistive 辅助性的 eliminate 消除 duplicate 重复的 coordinate 坐标 refreshingly 清爽地，有精神地，令人耳目一新地 millisecond 毫秒 implicitly 隐式地 delimiter 分隔符 diverge 分歧，相异 remedy 解决方法，纠正方法 deviation 偏差 coarse 粗糙的 begnign 无有害的，认为无关紧要的 malicious 恶意的，怀恨的 rigorous 严格的 outlier 离群值 deliberate 故意的；深思熟虑的；从容的 susceptible 易受影响的；易感动的；容许…的 leverage 利用 kinda 有点，有几分 centroid 形心，重心 exclusively 专门地，唯一地 collision 碰撞，警告 hint 暗示，示意 stand-alone （计算机）独立运行的；（公司、组织）独立的 photometric distortion 光度失真 geometric 几何失真 hue 色调 saturation 饱和度 superimpose 叠加","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"CV","slug":"CV","permalink":"http://pistachio0812.github.io/tags/CV/"},{"name":"English","slug":"English","permalink":"http://pistachio0812.github.io/tags/English/"}]},{"title":"Hello World","slug":"hello-world","date":"2022-03-11T08:11:46.667Z","updated":"2022-03-14T04:42:29.621Z","comments":true,"path":"2022/03/11/CN/hello-world/","link":"","permalink":"http://pistachio0812.github.io/2022/03/11/CN/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"Hexo博客搭建","slug":"Hexo博客搭建","permalink":"http://pistachio0812.github.io/categories/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"博客搭建","slug":"博客搭建","permalink":"http://pistachio0812.github.io/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"},{"name":"Hexo","slug":"Hexo","permalink":"http://pistachio0812.github.io/tags/Hexo/"}]}],"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://pistachio0812.github.io/categories/MySQL/"},{"name":"pytorch","slug":"pytorch","permalink":"http://pistachio0812.github.io/categories/pytorch/"},{"name":"github","slug":"github","permalink":"http://pistachio0812.github.io/categories/github/"},{"name":"Linux","slug":"Linux","permalink":"http://pistachio0812.github.io/categories/Linux/"},{"name":"学习网站","slug":"学习网站","permalink":"http://pistachio0812.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99/"},{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"Hexo博客搭建","slug":"Hexo博客搭建","permalink":"http://pistachio0812.github.io/categories/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"Note","slug":"Note","permalink":"http://pistachio0812.github.io/tags/Note/"},{"name":"文档","slug":"文档","permalink":"http://pistachio0812.github.io/tags/%E6%96%87%E6%A1%A3/"},{"name":"使用指南","slug":"使用指南","permalink":"http://pistachio0812.github.io/tags/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"},{"name":"github","slug":"github","permalink":"http://pistachio0812.github.io/tags/github/"},{"name":"Liunx system","slug":"Liunx-system","permalink":"http://pistachio0812.github.io/tags/Liunx-system/"},{"name":"学习网站","slug":"学习网站","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99/"},{"name":"CV","slug":"CV","permalink":"http://pistachio0812.github.io/tags/CV/"},{"name":"English","slug":"English","permalink":"http://pistachio0812.github.io/tags/English/"},{"name":"博客搭建","slug":"博客搭建","permalink":"http://pistachio0812.github.io/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"},{"name":"Hexo","slug":"Hexo","permalink":"http://pistachio0812.github.io/tags/Hexo/"}]}