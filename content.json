{"meta":{"title":"阿月浑子-Hexo博客","subtitle":"","description":"计算机视觉,文学,pytorch,hexo,江西理工大学,目标检测","author":"pistachio","url":"http://pistachio0812.github.io","root":"/"},"pages":[{"title":"","date":"2022-04-10T07:16:49.419Z","updated":"2022-04-10T07:16:49.419Z","comments":true,"path":"404.html","permalink":"http://pistachio0812.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2022-04-10T07:24:28.965Z","updated":"2022-04-10T07:24:28.965Z","comments":false,"path":"artitalk/index.html","permalink":"http://pistachio0812.github.io/artitalk/index.html","excerpt":"","text":""},{"title":"关于博主的那点事儿","date":"2022-03-13T07:41:41.000Z","updated":"2022-05-01T09:55:28.855Z","comments":true,"path":"about/index.html","permalink":"http://pistachio0812.github.io/about/index.html","excerpt":"","text":"网站开发者是江西理工大学2020级硕士研究生，平时也没啥爱好，会打打乒乓球，但很久没碰了，早就手生了，但要是认为随随便便一个人能赢我，那真是痴心妄想了。另外，热衷于学习一些除学习之外的技能，硕士研究方向为目标检测，如果有志同道合的朋友看到这里，希望可以进一步探讨，可以通过CSDN和博客园 留言给我。"},{"title":"categories","date":"2022-03-13T07:36:30.000Z","updated":"2022-04-10T03:25:17.792Z","comments":true,"path":"categories/index.html","permalink":"http://pistachio0812.github.io/categories/index.html","excerpt":"","text":""},{"title":"将你撇在这里 好朋友也","date":"2024-05-02T09:52:26.593Z","updated":"2024-05-02T09:52:26.593Z","comments":true,"path":"friends/index.html","permalink":"http://pistachio0812.github.io/friends/index.html","excerpt":"","text":"君行日已远，我思日益长。何物可谕此，山高水茫茫。人生要有别，男儿志四方。奈此老多感，况于朋友良。 友链规范： 1.这里的链接为对我帮助的人的社交软件链接或者博客链接 2.此链接如本人不愿上榜，可私我进行下线处理 3.欢迎毛遂自荐，共同进步 {friends} {tabs tab-id} title:#网站名称 url:#访问地址 avatar:#头像地址 description:#描述/一句话概述/格言 screenshot:#网站截图/展示图 backgroundColor:#头像背景颜色 textColor:#文本颜色 keywords:#标签 green checked::网站访问稳定 green checked::网站稳定更新 times red checked::不接受一切商业性或者广告类站点 times red checked::不接受违反中华人民共和国法律法规的网站 网站名称：阿月浑子-Hexo博客 网站链接：https://pistachio0812.github.io 头像：https://profile.csdnimg.cn/D/8/6/1_qq_38452951 描述：越努力越幸运 {endtabs}"},{"title":"指点江山","date":"2022-03-13T07:39:41.000Z","updated":"2022-05-01T02:54:25.008Z","comments":true,"path":"tags/index.html","permalink":"http://pistachio0812.github.io/tags/index.html","excerpt":"","text":"这里是留言箱，请把你对网站的建议用评论的形式留下来吧，万分感谢，一经采纳，重重有赏！！！"}],"posts":[{"title":"tensorboard","slug":"如何使用tensorboard","date":"2022-11-27T02:29:16.179Z","updated":"2022-11-27T02:43:36.243Z","comments":true,"path":"zh-CN/如何使用tensorboard/","permalink":"http://pistachio0812.github.io/zh-CN/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/","excerpt":"","text":"参考博文： 1.✔✔✔ TensorBoard 的正确打开方法（含错误解决方法，超详细 安装tensorboard1pip install tensorboard 环境配置略 打开文件tf_logs文件下存放的就是训练模型时的日志文件，如下图所示： .hmlpgvhqlqej{zoom:80%;} cmd下用如下命令打开: 1tensorboard --logdir \"absolute_path\" .tqirgrbmprtt{zoom:80%;} 打开网页打开http://localhost:6006/ .dpelhpsxnrrk{zoom:80%;} 如果不懂，参考博文","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"tensorboard","slug":"tensorboard","permalink":"http://pistachio0812.github.io/tags/tensorboard/"}],"author":"pistachio"},{"title":"SSD300转换到SSD512","slug":"SSD300_to_SSD512","date":"2022-10-31T07:52:05.818Z","updated":"2022-10-31T07:59:11.786Z","comments":true,"path":"zh-CN/SSD300_to_SSD512/","permalink":"http://pistachio0812.github.io/zh-CN/SSD300_to_SSD512/","excerpt":"","text":"参考博文： 1.mirrors / bubbliiiing / ssd-pytorch · GitCode 2.https://github.com/midasklr/SSD.Pytorch 由于我使用的第一篇参考链接里的代码，只有SSD300，而我又想升级改造成SSD512，因此找了一些参考，比如第二个链接，现在，让我们尝试着改造它吧。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"SSD","slug":"SSD","permalink":"http://pistachio0812.github.io/tags/SSD/"}],"author":"pistachio"},{"title":"pth权重文件转换为onnx权重文件","slug":"pth_to_onnx","date":"2022-10-31T02:59:04.886Z","updated":"2022-10-31T03:25:29.398Z","comments":true,"path":"zh-CN/pth_to_onnx/","permalink":"http://pistachio0812.github.io/zh-CN/pth_to_onnx/","excerpt":"","text":"参考博文： 1.pth文件转为onnx格式业精于勤。荒于嬉。的博客-CSDN博客.pth转onnx 2.Pytorch——初探onnx（1）解决upsamplebilinear2d转换问题零壹博弈的博客-CSDN博客 3.https://blog.csdn.net/magic_show_time/article/details/122476306 由于想利用zetane这个软件可视化目标检测模型里面的特征变化过程，zetane只支持onnx、h5、ZTN文件,而我的目标检测模型得到的权重文件是pth文件,因此需要用到权重转换 经过一系列的摸索，得出以下代码（亲测可用）： 123456789101112131415161718192021222324import torch.onnximport onnxruntime as ortfrom nets import ssd# 创建.pth模型model = ssd.SSD300(21,'vgg')# 加载权重model_path = './model_data/ep250-loss1.474-val_loss2.826.pth'device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')model_statedict = torch.load(model_path, map_location=device)model.load_state_dict(model_statedict)model.to(device)model.eval()input_data = torch.randn(1, 3, 300, 300, device=device)# 转化为onnx模型input_names = ['input']output_names = ['output']torch.onnx.export(model, input_data, 'ssdv7.onnx', opset_version=11, verbose=True, input_names=input_names, output_names=output_names) 遇到的问题： 如果你是按照我给的两篇文章写代码，你会发现很简单 1.模型无法加载，load_state_dict出现问题 12345# 记住，模型参数一定要写全，根据自己的网络模型写即可# 错误代码model = ssd.SSD300()# 正确代码model = ssd.SSD300(21, 'vgg') 2.权重文件因为模型中上采样导出失败问题 .bxcpglnbmfbi{zoom:80%;} 按方法2解决即可，pytorch版本问题 1234567# 错误代码torch.onnx.export(model, input_data, 'ssdv7.onnx', opset_version=9, verbose=True, input_names=input_names, output_names=output_names)# 正确代码torch.onnx.export(model, input_data, 'ssdv7.onnx', opset_version=11, verbose=True, input_names=input_names, output_names=output_names) ok,接下来就是zetane加载模型了，看看最终的模型吧。 .lsgvnjncodbt{zoom:80%;}","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"文件转换","slug":"文件转换","permalink":"http://pistachio0812.github.io/tags/%E6%96%87%E4%BB%B6%E8%BD%AC%E6%8D%A2/"}],"author":"pistachio"},{"title":"vue3学习笔记","slug":"vue3","date":"2022-10-20T13:29:38.596Z","updated":"2022-10-20T13:31:02.923Z","comments":true,"path":"zh-CN/vue3/","permalink":"http://pistachio0812.github.io/zh-CN/vue3/","excerpt":"","text":"","categories":[{"name":"vue","slug":"vue","permalink":"http://pistachio0812.github.io/categories/vue/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"vue","slug":"vue","permalink":"http://pistachio0812.github.io/tags/vue/"}],"author":"pistachio"},{"title":"react学习笔记","slug":"React","date":"2022-10-20T11:45:57.275Z","updated":"2022-10-20T11:55:38.020Z","comments":true,"path":"zh-CN/React/","permalink":"http://pistachio0812.github.io/zh-CN/React/","excerpt":"","text":"react特点 1.声明式设计 −React采用声明范式，可以轻松描述应用。 2.高效 −React通过对DOM的模拟，最大限度地减少与DOM的交互。 3.灵活 −React可以与已知的库或框架很好地配合。 4.JSX − JSX 是 JavaScript 语法的扩展。React 开发不一定使用 JSX ，但我们建议使用它。 5.组件 − 通过 React 构建组件，使得代码更加容易得到复用，能够很好的应用在大项目的开发中。 6.单向响应的数据流 − React 实现了单向响应的数据流，从而减少了重复代码，这也是它为什么比传统数据绑定更简单。 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\" /&gt;&lt;title&gt;Hello React!&lt;/title&gt;&lt;script src=\"https://cdn.staticfile.org/react/16.4.0/umd/react.development.js\"&gt;&lt;/script&gt;&lt;script src=\"https://cdn.staticfile.org/react-dom/16.4.0/umd/react-dom.development.js\"&gt;&lt;/script&gt;&lt;script src=\"https://cdn.staticfile.org/babel-standalone/6.26.0/babel.min.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"example\"&gt;&lt;/div&gt;&lt;script type=\"text/babel\"&gt;ReactDOM.render( &lt;h1&gt;Hello, world!&lt;/h1&gt;, document.getElementById('example'));&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; react安装本教程使用了 React 的版本为 16.4.0，你可以在官网 https://reactjs.org/ 下载最新版 官方提供的CDN 12345&lt;script src=\"https://unpkg.com/react@16/umd/react.development.js\"&gt;&lt;/script&gt;&lt;script src=\"https://unpkg.com/react-dom@16/umd/react-dom.development.js\"&gt;&lt;/script&gt;&lt;!-- 生产环境中不建议使用 --&gt;&lt;!-- 在浏览器中使用 Babel 来编译 JSX 效率是非常低的 --&gt;&lt;script src=\"https://unpkg.com/babel-standalone@6.15.0/babel.min.js\"&gt;&lt;/script&gt;","categories":[{"name":"React","slug":"React","permalink":"http://pistachio0812.github.io/categories/React/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"React","slug":"React","permalink":"http://pistachio0812.github.io/tags/React/"}],"author":"pistachio"},{"title":"highcharts学习笔记","slug":"highcharts教程","date":"2022-10-16T11:44:53.284Z","updated":"2022-10-16T12:14:01.594Z","comments":true,"path":"zh-CN/highcharts教程/","permalink":"http://pistachio0812.github.io/zh-CN/highcharts%E6%95%99%E7%A8%8B/","excerpt":"","text":"环境配置建议使用cdn 安装jQuery 1234# 使用 Staticfile CDN 静态资源库的jQuery资源http://cdn.staticfile.org/jquery/2.1.4/jquery.min.js# 使用百度静态资源库的jQuery资源http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js 安装Highcharts 12# 官方CDN地址http://code.highcharts.com/highcharts.js 配置语法可以将下面文件保存在highcharts.html文件中，用浏览器打开 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&lt;html&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\" /&gt;&lt;title&gt;Highcharts&lt;/title&gt;&lt;script src=\"http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js\"&gt;&lt;/script&gt;&lt;script src=\"http://code.highcharts.com/highcharts.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"container\" style=\"width: 550px; height: 400px; margin: 0 auto\"&gt;&lt;/div&gt;&lt;script language=\"JavaScript\"&gt;$(document).ready(function() { // 为图表配置标题 var title = { text: '月平均气温' }; // 为图表配置副标题 var subtitle = { text: 'Source: runoob.com' }; // 配置要在x轴显示的项 var xAxis = { categories: ['一月', '二月', '三月', '四月', '五月', '六月' ,'七月', '八月', '九月', '十月', '十一月', '十二月'] }; // 配置要在y轴显示的项 var yAxis = { title: { text: 'Temperature (\\xB0C)' }, plotLines: [{ value: 0, width: 1, color: '#808080' }] }; // 配置提示信息 var tooltip = { valueSuffix: '\\xB0C' } // 配置图表向右对齐 var legend = { layout: 'vertical', align: 'right', verticalAlign: 'middle', borderWidth: 0 }; // 配置图表要展示的数据。每个系列是个数组，每一项在图片中都会生成一条曲线 var series = [ { name: 'Tokyo', data: [7.0, 6.9, 9.5, 14.5, 18.2, 21.5, 25.2, 26.5, 23.3, 18.3, 13.9, 9.6] }, { name: 'New York', data: [-0.2, 0.8, 5.7, 11.3, 17.0, 22.0, 24.8, 24.1, 20.1, 14.1, 8.6, 2.5] }, { name: 'Berlin', data: [-0.9, 0.6, 3.5, 8.4, 13.5, 17.0, 18.6, 17.9, 14.3, 9.0, 3.9, 1.0] }, { name: 'London', data: [3.9, 4.2, 5.7, 8.5, 11.9, 15.2, 17.0, 16.6, 14.2, 10.3, 6.6, 4.8] } ]; var json = {}; json.title = title; json.subtitle = subtitle; json.xAxis = xAxis; json.yAxis = yAxis; json.tooltip = tooltip; json.legend = legend; json.series = series; &lt;!-- highcharts库使用json格式配置--&gt; $('#container').highcharts(json);});&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; .jberqhwmxcte{zoom:50%;}","categories":[],"tags":[]},{"title":"echarts学习笔记","slug":"echarts","date":"2022-10-13T03:13:43.996Z","updated":"2022-10-16T06:42:52.968Z","comments":true,"path":"zh-CN/echarts/","permalink":"http://pistachio0812.github.io/zh-CN/echarts/","excerpt":"","text":"参考博文： 1.ECharts 教程 | 菜鸟教程 (runoob.com) ECharts 是一个使用 JavaScript 实现的开源可视化库，涵盖各行业图表，满足各种需求。 ECharts 遵循 Apache-2.0 开源协议，免费商用。 ECharts 兼容当前绝大部分浏览器（IE8/9/10/11，Chrome，Firefox，Safari等）及兼容多种设备，可随时随地任性展示。 echarts安装1.独立版本 我们可以在直接下载 echarts.min.js 并用 标签引入。 2.使用CDN方法 以下推荐国外比较稳定的两个 CDN，国内还没发现哪一家比较好，目前还是建议下载到本地。 Staticfile CDN（国内） : https://cdn.staticfile.org/echarts/4.3.0/echarts.min.js jsDelivr：https://cdn.jsdelivr.net/npm/echarts@4.3.0/dist/echarts.min.js。 cdnjs : https://cdnjs.cloudflare.com/ajax/libs/echarts/4.3.0/echarts.min.js 3.npm方法 1npm install echarts --save echarts配置语法下面实例可以放在一个html文件里显示效果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 创建html页面&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;第一个 ECharts 实例&lt;/title&gt; &lt;!-- 引入 echarts.js --&gt; &lt;script src=&quot;https://cdn.staticfile.org/echarts/4.3.0/echarts.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- 为ECharts准备一个具备大小（宽高）的Dom --&gt; &lt;div id=&quot;main&quot; style=&quot;width: 600px;height:400px;&quot;&gt;&lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById(&#x27;main&#x27;)); // 指定图表的配置项和数据 var option = &#123; // 为图表配置标题 title: &#123; text: &#x27;第一个 ECharts 实例&#x27; &#125;, // 配置提示信息 tooltip: &#123;&#125;, // 图例组件展现了不同系列的标记(symbol)，颜色和名字。可以通过点击图例控制哪些系列不显示。 legend: &#123; data:[&#x27;销量&#x27;] &#125;, // 配置要在x轴显示的项 xAxis: &#123; data: [&quot;衬衫&quot;,&quot;羊毛衫&quot;,&quot;雪纺衫&quot;,&quot;裤子&quot;,&quot;高跟鞋&quot;,&quot;袜子&quot;] &#125;, // 配置要在y轴显示的项 yAxis: &#123;&#125;, // 每个系列通过type决定自己的图表类型 series: [&#123; name: &#x27;销量&#x27;, // 系列名称 type: &#x27;bar&#x27;, // 系列图表类型 data: [5, 20, 36, 10, 10, 20] // 系列中的数据内容 &#125;] &#125;; // 使用刚指定的配置项和数据显示图表。 myChart.setOption(option); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 特别说明： 图例组件： 1234567891011legend: &#123; data: [&#123; name: &#x27;系列1&#x27;, // 强制设置图形为圆。 icon: &#x27;circle&#x27;, // 设置文本为红色 textStyle: &#123; color: &#x27;red&#x27; &#125; &#125;]&#125; 系列列表： 12345series: [&#123; name: &#x27;销量&#x27;, // 系列名称 type: &#x27;bar&#x27;, // 系列图表类型 data: [5, 20, 36, 10, 10, 20] // 系列中的数据内容&#125;] 每个系列通过 type 决定自己的图表类型： type: ‘bar’：柱状/条形图 type: ‘line’：折线/面积图 type: ‘pie’：饼图 type: ‘scatter’：散点（气泡）图 type: ‘effectScatter’：带有涟漪特效动画的散点（气泡） type: ‘radar’：雷达图 type: ‘tree’：树型图 type: ‘treemap’：树型图 type: ‘sunburst’：旭日图 type: ‘boxplot’：箱形图 type: ‘candlestick’：K线图 type: ‘heatmap’：热力图 type: ‘map’：地图 type: ‘parallel’：平行坐标系的系列 type: ‘lines’：线图 type: ‘graph’：关系图 type: ‘sankey’：桑基图 type: ‘funnel’：漏斗图 type: ‘gauge’：仪表盘 type: ‘pictorialBar’：象形柱图 type: ‘themeRiver’：主题河流 type: ‘custom’：自定义系列 echarts饼图饼图主要是通过扇形的弧度表现不同类目的数据在总和中的占比，它的数据格式比柱状图更简单，只有一维的数值，不需要给类目。因为不在直角坐标系上，所以也不需要 xAxis，yAxis。 12345678910111213141516myChart.setOption(&#123; series : [ &#123; name: &#x27;访问来源&#x27;, type: &#x27;pie&#x27;, // 设置图表类型为饼图 radius: &#x27;55%&#x27;, // 饼图的半径，外半径为可视区尺寸（容器高宽中较小一项）的 55% 长度。 data:[ // 数据数组，name 为数据项名称，value 为数据项值 &#123;value:235, name:&#x27;视频广告&#x27;&#125;, &#123;value:274, name:&#x27;联盟广告&#x27;&#125;, &#123;value:310, name:&#x27;邮件营销&#x27;&#125;, &#123;value:335, name:&#x27;直接访问&#x27;&#125;, &#123;value:400, name:&#x27;搜索引擎&#x27;&#125; ] &#125; ]&#125;) 完整源码： 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;第一个 ECharts 实例&lt;/title&gt; &lt;!-- 引入 echarts.js --&gt; &lt;script src=&quot;https://cdn.staticfile.org/echarts/4.3.0/echarts.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- 为ECharts准备一个具备大小（宽高）的Dom --&gt; &lt;div id=&quot;main&quot; style=&quot;width: 600px;height:400px;&quot;&gt;&lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById(&#x27;main&#x27;)); myChart.setOption(&#123; series : [ &#123; name: &#x27;访问来源&#x27;, type: &#x27;pie&#x27;, // 设置图表类型为饼图 radius: &#x27;55%&#x27;, // 饼图的半径，外半径为可视区尺寸（容器高宽中较小一项）的 55% 长度。 // roseType: &#x27;angle&#x27; // 饼图显示为南丁格尔图 data:[ // 数据数组，name 为数据项名称，value 为数据项值 &#123;value:235, name:&#x27;视频广告&#x27;&#125;, &#123;value:274, name:&#x27;联盟广告&#x27;&#125;, &#123;value:310, name:&#x27;邮件营销&#x27;&#125;, &#123;value:335, name:&#x27;直接访问&#x27;&#125;, &#123;value:400, name:&#x27;搜索引擎&#x27;&#125; ] //itemStyle 参数可以设置诸如阴影、透明度、颜色、边框颜色、边框宽度等 itemStyle:&#123; normal:&#123; shadowBlur: 200, shadowColor: &#x27;rgba(0, 0, 0, 0.5)&#x27; &#125; &#125; &#125; ] &#125;) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 从上面两个来看，接下来我们只会更改变量myChart的配置，到时候进行替换就行了 样式设置ECharts 可以通过样式设置来改变图形元素或者文字的颜色、明暗、大小等。 颜色主题ECharts4 开始，除了默认主题外，内置了两套主题，分别为 light 和 dark。 1234# light themevar chart = echarts.init(dom, &#x27;light&#x27;);# dark themevar chart = echarts.init(dom, &#x27;dark&#x27;); 另外，我们也可以在官方的 主题编辑器 选择自己喜欢的主题下载。 .uctliubttkqd{zoom:50%;} 目前主题下载提供了 JS 版本和 JSON 版本。 如果你使用 JS 版本，可以将 JS 主题代码保存一个 主题名.js 文件，然后在 HTML 中引用该文件，最后在代码中使用该主题。 比如上图中我们选中了一个主题，将 JS 代码保存为 wonderland.js。 1234567&lt;!-- 引入主题 --&gt;&lt;script src=&quot;https://www.runoob.com/static/js/wonderland.js&quot;&gt;&lt;/script&gt;...// HTML 引入 wonderland.js 文件后，在初始化的时候调用var myChart = echarts.init(dom, &#x27;wonderland&#x27;);// ... 如果主题保存为 JSON 文件，那么可以自行加载和注册。 比如上图中我们选中了一个主题，将 JSON 代码保存为 wonderland.json。 12345//主题名称是 wonderland$.getJSON(&#x27;wonderland.json&#x27;, function (themeJSON) &#123; echarts.registerTheme(&#x27;wonderland&#x27;, themeJSON) var myChart = echarts.init(dom, &#x27;wonderland&#x27;);&#125;); 注意：我们使用了 $.getJSON，所以需要引入 jQuery。 调色盘调色盘可以在 option 中设置。 调色盘给定了一组颜色，图形、系列会自动从其中选择颜色。 可以设置全局的调色盘，也可以设置系列自己专属的调色盘。 12345678910111213141516option = &#123; // 全局调色盘。 color: [&#x27;#c23531&#x27;,&#x27;#2f4554&#x27;, &#x27;#61a0a8&#x27;, &#x27;#d48265&#x27;, &#x27;#91c7ae&#x27;,&#x27;#749f83&#x27;, &#x27;#ca8622&#x27;, &#x27;#bda29a&#x27;,&#x27;#6e7074&#x27;, &#x27;#546570&#x27;, &#x27;#c4ccd3&#x27;], series: [&#123; type: &#x27;bar&#x27;, // 此系列自己的调色盘。 color: [&#x27;#dd6b66&#x27;,&#x27;#759aa0&#x27;,&#x27;#e69d87&#x27;,&#x27;#8dc1a9&#x27;,&#x27;#ea7e53&#x27;,&#x27;#eedd78&#x27;,&#x27;#73a373&#x27;,&#x27;#73b9bc&#x27;,&#x27;#7289ab&#x27;, &#x27;#91ca8c&#x27;,&#x27;#f49f42&#x27;], ... &#125;, &#123; type: &#x27;pie&#x27;, // 此系列自己的调色盘。 color: [&#x27;#37A2DA&#x27;, &#x27;#32C5E9&#x27;, &#x27;#67E0E3&#x27;, &#x27;#9FE6B8&#x27;, &#x27;#FFDB5C&#x27;,&#x27;#ff9f7f&#x27;, &#x27;#fb7293&#x27;, &#x27;#E062AE&#x27;, &#x27;#E690D1&#x27;, &#x27;#e7bcf3&#x27;, &#x27;#9d96f5&#x27;, &#x27;#8378EA&#x27;, &#x27;#96BFFF&#x27;], ... &#125;]&#125; 高亮样式emphasis直接的样式设置是比较常用设置方式。纵观 ECharts 的 option 中，很多地方可以设置 itemStyle、lineStyle、areaStyle、label 等等。这些的地方可以直接设置图形元素的颜色、线宽、点的大小、标签的文字、标签的样式等等。 123456789101112// 高亮样式。emphasis: &#123; itemStyle: &#123; // 高亮时点的颜色 color: &#x27;red&#x27; &#125;, label: &#123; show: true, // 高亮时标签的文字 formatter: &#x27;高亮时显示的标签内容&#x27; &#125;&#125;, 异步加载数据ECharts 通常数据设置在 setOption 中，如果我们需要异步加载数据，可以配合 jQuery等工具，在异步获取数据后通过 setOption 填入数据和配置项就行。 1234567891011// echarts_test_data.json&#123; &quot;data_pie&quot; : [ &#123;&quot;value&quot;:235, &quot;name&quot;:&quot;视频广告&quot;&#125;, &#123;&quot;value&quot;:274, &quot;name&quot;:&quot;联盟广告&quot;&#125;, &#123;&quot;value&quot;:310, &quot;name&quot;:&quot;邮件营销&quot;&#125;, &#123;&quot;value&quot;:335, &quot;name&quot;:&quot;直接访问&quot;&#125;, &#123;&quot;value&quot;:400, &quot;name&quot;:&quot;搜索引擎&quot;&#125; ]&#125; 12345678910111213141516// 实例var myChart = echarts.init(document.getElementById(&#x27;main&#x27;));myChart.showLoading(); // 开启 loading 效果$.get(&#x27;https://www.runoob.com/static/js/echarts_test_data.json&#x27;, function (data) &#123; myChart.hideLoading(); // 隐藏 loading 效果 myChart.setOption(&#123; series : [ &#123; name: &#x27;访问来源&#x27;, type: &#x27;pie&#x27;, // 设置图表类型为饼图 radius: &#x27;55%&#x27;, // 饼图的半径，外半径为可视区尺寸（容器高宽中较小一项）的 55% 长度。 data:data.data_pie &#125; ] &#125;)&#125;, &#x27;json&#x27;) 数据的动态更新ECharts 由数据驱动，数据的改变驱动图表展现的改变，因此动态数据的实现也变得异常简单。 所有数据的更新都通过 setOption 实现，你只需要定时获取数据，setOption 填入数据，而不用考虑数据到底产生了那些变化，ECharts 会找到两组数据之间的差异然后通过合适的动画去表现数据的变化。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;第一个 ECharts 实例&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/2.2.4/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;!-- 引入 echarts.js --&gt; &lt;script src=&quot;https://cdn.staticfile.org/echarts/4.3.0/echarts.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- 为ECharts准备一个具备大小（宽高）的Dom --&gt; &lt;div id=&quot;main&quot; style=&quot;width: 600px;height:400px;&quot;&gt;&lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; var base = +new Date(2022, 10, 13); var oneDay = 24 * 3600 * 1000; var date = []; var data = [Math.random() * 150]; var now = new Date(base); function addData(shift) &#123; now = [now.getFullYear(), now.getMonth() + 1, now.getDate()].join(&#x27;/&#x27;); date.push(now); data.push((Math.random() - 0.4) * 10 + data[data.length - 1]); if (shift) &#123; date.shift(); data.shift(); &#125; now = new Date(+new Date(now) + oneDay); &#125; for (var i = 1; i &lt; 100; i++) &#123; addData(); &#125; option = &#123; xAxis: &#123; type: &#x27;category&#x27;, boundaryGap: false, data: date &#125;, yAxis: &#123; boundaryGap: [0, &#x27;50%&#x27;], type: &#x27;value&#x27; &#125;, series: [ &#123; name:&#x27;成交&#x27;, type:&#x27;line&#x27;, smooth:true, symbol: &#x27;none&#x27;, stack: &#x27;a&#x27;, areaStyle: &#123; normal: &#123;&#125; &#125;, data: data &#125; ] &#125;; setInterval(function () &#123; addData(true); myChart.setOption(&#123; xAxis: &#123; data: date &#125;, series: [&#123; name:&#x27;成交&#x27;, data: data &#125;] &#125;); &#125;, 500); // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById(&#x27;main&#x27;)); myChart.setOption(option) &lt;/script&gt;&lt;/body&gt; 数据集ECharts 使用 dataset 管理数据。 dataset 组件用于单独的数据集声明，从而数据可以单独管理，被多个组件复用，并且可以基于数据指定数据到视觉的映射。 123456789101112131415161718192021222324option = &#123; legend: &#123;&#125;, tooltip: &#123;&#125;, dataset: &#123; // 提供一份数据。 source: [ [&#x27;product&#x27;, &#x27;2015&#x27;, &#x27;2016&#x27;, &#x27;2017&#x27;], [&#x27;Matcha Latte&#x27;, 43.3, 85.8, 93.7], [&#x27;Milk Tea&#x27;, 83.1, 73.4, 55.1], [&#x27;Cheese Cocoa&#x27;, 86.4, 65.2, 82.5], [&#x27;Walnut Brownie&#x27;, 72.4, 53.9, 39.1] ] &#125;, // 声明一个 X 轴，类目轴（category）。默认情况下，类目轴对应到 dataset 第一列。 xAxis: &#123;type: &#x27;category&#x27;&#125;, // 声明一个 Y 轴，数值轴。 yAxis: &#123;&#125;, // 声明多个 bar 系列，默认情况下，每个系列会自动对应到 dataset 的每一列。 series: [ &#123;type: &#x27;bar&#x27;&#125;, &#123;type: &#x27;bar&#x27;&#125;, &#123;type: &#x27;bar&#x27;&#125; ]&#125; 或者使用对象数组的格式 12345678910111213141516171819202122option = &#123; legend: &#123;&#125;, tooltip: &#123;&#125;, dataset: &#123; // 这里指定了维度名的顺序，从而可以利用默认的维度到坐标轴的映射。 // 如果不指定 dimensions，也可以通过指定 series.encode 完成映射，参见后文。 dimensions: [&#x27;product&#x27;, &#x27;2015&#x27;, &#x27;2016&#x27;, &#x27;2017&#x27;], source: [ &#123;product: &#x27;Matcha Latte&#x27;, &#x27;2015&#x27;: 43.3, &#x27;2016&#x27;: 85.8, &#x27;2017&#x27;: 93.7&#125;, &#123;product: &#x27;Milk Tea&#x27;, &#x27;2015&#x27;: 83.1, &#x27;2016&#x27;: 73.4, &#x27;2017&#x27;: 55.1&#125;, &#123;product: &#x27;Cheese Cocoa&#x27;, &#x27;2015&#x27;: 86.4, &#x27;2016&#x27;: 65.2, &#x27;2017&#x27;: 82.5&#125;, &#123;product: &#x27;Walnut Brownie&#x27;, &#x27;2015&#x27;: 72.4, &#x27;2016&#x27;: 53.9, &#x27;2017&#x27;: 39.1&#125; ] &#125;, xAxis: &#123;type: &#x27;category&#x27;&#125;, yAxis: &#123;&#125;, series: [ &#123;type: &#x27;bar&#x27;&#125;, &#123;type: &#x27;bar&#x27;&#125;, &#123;type: &#x27;bar&#x27;&#125; ]&#125;; 数据到图形的映射我们可以在配置项中将数据映射到图形中。 我们可以使用 series.seriesLayoutBy 属性来配置 dataset 是列（column）还是行（row）映射为图形系列（series），默认是按照列（column）来映射。 1234567891011121314151617181920212223242526272829303132333435option = &#123; legend: &#123;&#125;, tooltip: &#123;&#125;, dataset: &#123; source: [ [&#x27;product&#x27;, &#x27;2012&#x27;, &#x27;2013&#x27;, &#x27;2014&#x27;, &#x27;2015&#x27;], [&#x27;Matcha Latte&#x27;, 41.1, 30.4, 65.1, 53.3], [&#x27;Milk Tea&#x27;, 86.5, 92.1, 85.7, 83.1], [&#x27;Cheese Cocoa&#x27;, 24.1, 67.2, 79.5, 86.4] ] &#125;, xAxis: [ &#123;type: &#x27;category&#x27;, gridIndex: 0&#125;, &#123;type: &#x27;category&#x27;, gridIndex: 1&#125; ], yAxis: [ &#123;gridIndex: 0&#125;, &#123;gridIndex: 1&#125; ], grid: [ &#123;bottom: &#x27;55%&#x27;&#125;, &#123;top: &#x27;55%&#x27;&#125; ], series: [ // 这几个系列会在第一个直角坐标系中，每个系列对应到 dataset 的每一行。 &#123;type: &#x27;bar&#x27;, seriesLayoutBy: &#x27;row&#x27;&#125;, &#123;type: &#x27;bar&#x27;, seriesLayoutBy: &#x27;row&#x27;&#125;, &#123;type: &#x27;bar&#x27;, seriesLayoutBy: &#x27;row&#x27;&#125;, // 这几个系列会在第二个直角坐标系中，每个系列对应到 dataset 的每一列。 &#123;type: &#x27;bar&#x27;, xAxisIndex: 1, yAxisIndex: 1&#125;, &#123;type: &#x27;bar&#x27;, xAxisIndex: 1, yAxisIndex: 1&#125;, &#123;type: &#x27;bar&#x27;, xAxisIndex: 1, yAxisIndex: 1&#125;, &#123;type: &#x27;bar&#x27;, xAxisIndex: 1, yAxisIndex: 1&#125; ]&#125; .ojmdzaznuxxq{zoom:50%;} 常用图表所描述的数据大部分是”二维表”结构，我们可以使用 series.encode 属性将对应的数据映射到坐标轴（如 X、Y 轴） 123456789101112131415161718192021222324252627282930var option = &#123; dataset: &#123; source: [ [&#x27;score&#x27;, &#x27;amount&#x27;, &#x27;product&#x27;], [89.3, 58212, &#x27;Matcha Latte&#x27;], [57.1, 78254, &#x27;Milk Tea&#x27;], [74.4, 41032, &#x27;Cheese Cocoa&#x27;], [50.1, 12755, &#x27;Cheese Brownie&#x27;], [89.7, 20145, &#x27;Matcha Cocoa&#x27;], [68.1, 79146, &#x27;Tea&#x27;], [19.6, 91852, &#x27;Orange Juice&#x27;], [10.6, 101852, &#x27;Lemon Juice&#x27;], [32.7, 20112, &#x27;Walnut Brownie&#x27;] ] &#125;, grid: &#123;containLabel: true&#125;, xAxis: &#123;&#125;, yAxis: &#123;type: &#x27;category&#x27;&#125;, series: [ &#123; type: &#x27;bar&#x27;, encode: &#123; // 将 &quot;amount&quot; 列映射到 X 轴。 x: &#x27;amount&#x27;, // 将 &quot;product&quot; 列映射到 Y 轴。 y: &#x27;product&#x27; &#125; &#125; ]&#125;; encode 声明的基本结构如下，其中冒号左边是坐标系、标签等特定名称，如 ‘x’, ‘y’, ‘tooltip’ 等，冒号右边是数据中的维度名（string 格式）或者维度的序号（number 格式，从 0 开始计数），可以指定一个或多个维度（使用数组）。通常情况下，下面各种信息不需要所有的都写，按需写即可。 1234567891011121314151617181920212223242526272829303132333435363738394041// 在任何坐标系和系列中，都支持：encode: &#123; // 使用 “名为 product 的维度” 和 “名为 score 的维度” 的值在 tooltip 中显示 tooltip: [&#x27;product&#x27;, &#x27;score&#x27;] // 使用 “维度 1” 和 “维度 3” 的维度名连起来作为系列名。（有时候名字比较长，这可以避免在 series.name 重复输入这些名字） seriesName: [1, 3], // 表示使用 “维度2” 中的值作为 id。这在使用 setOption 动态更新数据时有用处，可以使新老数据用 id 对应起来，从而能够产生合适的数据更新动画。 itemId: 2, // 指定数据项的名称使用 “维度3” 在饼图等图表中有用，可以使这个名字显示在图例（legend）中。 itemName: 3&#125;// 直角坐标系（grid/cartesian）特有的属性：encode: &#123; // 把 “维度1”、“维度5”、“名为 score 的维度” 映射到 X 轴： x: [1, 5, &#x27;score&#x27;], // 把“维度0”映射到 Y 轴。 y: 0&#125;// 单轴（singleAxis）特有的属性：encode: &#123; single: 3&#125;// 极坐标系（polar）特有的属性：encode: &#123; radius: 3, angle: 2&#125;// 地理坐标系（geo）特有的属性：encode: &#123; lng: 3, lat: 2&#125;// 对于一些没有坐标系的图表，例如饼图、漏斗图等，可以是：encode: &#123; value: 3&#125; 视觉通道的映射我们可以使用 visualMap 组件进行视觉通道的映射。 视觉元素可以是： symbol: 图元的图形类别。 symbolSize: 图元的大小。 color: 图元的颜色。 colorAlpha: 图元的颜色的透明度。 opacity: 图元以及其附属物（如文字标签）的透明度。 colorLightness: 颜色的明暗度。 colorSaturation: 颜色的饱和度。 colorHue: 颜色的色调。 visualMap 组件可以定义多个，从而可以同时对数据中的多个维度进行视觉映射。 12345678910111213141516171819202122232425262728293031323334353637383940414243var option = &#123; dataset: &#123; source: [ [&#x27;score&#x27;, &#x27;amount&#x27;, &#x27;product&#x27;], [89.3, 58212, &#x27;Matcha Latte&#x27;], [57.1, 78254, &#x27;Milk Tea&#x27;], [74.4, 41032, &#x27;Cheese Cocoa&#x27;], [50.1, 12755, &#x27;Cheese Brownie&#x27;], [89.7, 20145, &#x27;Matcha Cocoa&#x27;], [68.1, 79146, &#x27;Tea&#x27;], [19.6, 91852, &#x27;Orange Juice&#x27;], [10.6, 101852, &#x27;Lemon Juice&#x27;], [32.7, 20112, &#x27;Walnut Brownie&#x27;] ] &#125;, grid: &#123;containLabel: true&#125;, xAxis: &#123;name: &#x27;amount&#x27;&#125;, yAxis: &#123;type: &#x27;category&#x27;&#125;, visualMap: &#123; orient: &#x27;horizontal&#x27;, left: &#x27;center&#x27;, min: 10, max: 100, text: [&#x27;High Score&#x27;, &#x27;Low Score&#x27;], // Map the score column to color dimension: 0, inRange: &#123; color: [&#x27;#D7DA8B&#x27;, &#x27;#E15457&#x27;] &#125; &#125;, series: [ &#123; type: &#x27;bar&#x27;, encode: &#123; // Map the &quot;amount&quot; column to X axis. x: &#x27;amount&#x27;, // Map the &quot;product&quot; column to Y axis y: &#x27;product&#x27; &#125; &#125; ]&#125;; 交互联动以下实例多个图表共享一个 dataset，并带有联动交互： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364setTimeout(function () &#123; option = &#123; legend: &#123;&#125;, tooltip: &#123; trigger: &#x27;axis&#x27;, showContent: false &#125;, dataset: &#123; source: [ [&#x27;product&#x27;, &#x27;2012&#x27;, &#x27;2013&#x27;, &#x27;2014&#x27;, &#x27;2015&#x27;, &#x27;2016&#x27;, &#x27;2017&#x27;], [&#x27;Matcha Latte&#x27;, 41.1, 30.4, 65.1, 53.3, 83.8, 98.7], [&#x27;Milk Tea&#x27;, 86.5, 92.1, 85.7, 83.1, 73.4, 55.1], [&#x27;Cheese Cocoa&#x27;, 24.1, 67.2, 79.5, 86.4, 65.2, 82.5], [&#x27;Walnut Brownie&#x27;, 55.2, 67.1, 69.2, 72.4, 53.9, 39.1] ] &#125;, xAxis: &#123;type: &#x27;category&#x27;&#125;, yAxis: &#123;gridIndex: 0&#125;, grid: &#123;top: &#x27;55%&#x27;&#125;, series: [ &#123;type: &#x27;line&#x27;, smooth: true, seriesLayoutBy: &#x27;row&#x27;&#125;, &#123;type: &#x27;line&#x27;, smooth: true, seriesLayoutBy: &#x27;row&#x27;&#125;, &#123;type: &#x27;line&#x27;, smooth: true, seriesLayoutBy: &#x27;row&#x27;&#125;, &#123;type: &#x27;line&#x27;, smooth: true, seriesLayoutBy: &#x27;row&#x27;&#125;, &#123; type: &#x27;pie&#x27;, id: &#x27;pie&#x27;, radius: &#x27;30%&#x27;, center: [&#x27;50%&#x27;, &#x27;25%&#x27;], label: &#123; formatter: &#x27;&#123;b&#125;: &#123;@2012&#125; (&#123;d&#125;%)&#x27; &#125;, encode: &#123; itemName: &#x27;product&#x27;, value: &#x27;2012&#x27;, tooltip: &#x27;2012&#x27; &#125; &#125; ] &#125;; myChart.on(&#x27;updateAxisPointer&#x27;, function (event) &#123; var xAxisInfo = event.axesInfo[0]; if (xAxisInfo) &#123; var dimension = xAxisInfo.value + 1; myChart.setOption(&#123; series: &#123; id: &#x27;pie&#x27;, label: &#123; formatter: &#x27;&#123;b&#125;: &#123;@[&#x27; + dimension + &#x27;]&#125; (&#123;d&#125;%)&#x27; &#125;, encode: &#123; value: dimension, tooltip: dimension &#125; &#125; &#125;); &#125; &#125;); myChart.setOption(option);&#125;); .okfqmchioitk{zoom:50%;} 交互组件ECharts 提供了很多交互组件：例组件 legend、标题组件 title、视觉映射组件 visualMap、数据区域缩放组件 dataZoom、时间线组件 timeline。 dataZoomdataZoom 组件可以实现通过鼠标滚轮滚动，放大缩小图表的功能。 默认情况下 dataZoom 控制 x 轴，即对 x 轴进行数据窗口缩放和数据窗口平移操作。 123456789101112131415161718192021222324252627option = &#123; xAxis: &#123; type: &#x27;value&#x27; &#125;, yAxis: &#123; type: &#x27;value&#x27; &#125;, dataZoom: [ &#123; // 这个dataZoom组件，默认控制x轴。 type: &#x27;slider&#x27;, // 这个 dataZoom 组件是 slider 型 dataZoom 组件 start: 10, // 左边在 10% 的位置。 end: 60 // 右边在 60% 的位置。 &#125; ], series: [ &#123; type: &#x27;scatter&#x27;, // 这是个『散点图』 itemStyle: &#123; opacity: 0.8 &#125;, symbolSize: function (val) &#123; return val[2] * 40; &#125;, data: [[&quot;14.616&quot;,&quot;7.241&quot;,&quot;0.896&quot;],[&quot;3.958&quot;,&quot;5.701&quot;,&quot;0.955&quot;],[&quot;2.768&quot;,&quot;8.971&quot;,&quot;0.669&quot;],[&quot;9.051&quot;,&quot;9.710&quot;,&quot;0.171&quot;],[&quot;14.046&quot;,&quot;4.182&quot;,&quot;0.536&quot;],[&quot;12.295&quot;,&quot;1.429&quot;,&quot;0.962&quot;],[&quot;4.417&quot;,&quot;8.167&quot;,&quot;0.113&quot;],[&quot;0.492&quot;,&quot;4.771&quot;,&quot;0.785&quot;],[&quot;7.632&quot;,&quot;2.605&quot;,&quot;0.645&quot;],[&quot;14.242&quot;,&quot;5.042&quot;,&quot;0.368&quot;]] &#125; ]&#125; 上面的实例只能拖动 dataZoom 组件来缩小或放大图表。如果想在坐标系内进行拖动，以及用鼠标滚轮（或移动触屏上的两指滑动）进行缩放，那么需要 再再加上一个 inside 型的 dataZoom 组件。 在以上实例基础上我们再增加 type: ‘inside’ 的配置信息 12345678910111213141516option = &#123; ..., dataZoom: [ &#123; // 这个dataZoom组件，默认控制x轴。 type: &#x27;slider&#x27;, // 这个 dataZoom 组件是 slider 型 dataZoom 组件 start: 10, // 左边在 10% 的位置。 end: 60 // 右边在 60% 的位置。 &#125;, &#123; // 这个dataZoom组件，也控制x轴。 type: &#x27;inside&#x27;, // 这个 dataZoom 组件是 inside 型 dataZoom 组件 start: 10, // 左边在 10% 的位置。 end: 60 // 右边在 60% 的位置。 &#125; ], ...&#125; 当然我们可以通过 dataZoom.xAxisIndex 或 dataZoom.yAxisIndex 来指定 dataZoom 控制哪个或哪些数轴。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108var data1 = [];var data2 = [];var data3 = [];var random = function (max) &#123; return (Math.random() * max).toFixed(3);&#125;;for (var i = 0; i &lt; 500; i++) &#123; data1.push([random(15), random(10), random(1)]); data2.push([random(10), random(10), random(1)]); data3.push([random(15), random(10), random(1)]);&#125;option = &#123; animation: false, legend: &#123; data: [&#x27;scatter&#x27;, &#x27;scatter2&#x27;, &#x27;scatter3&#x27;] &#125;, tooltip: &#123; &#125;, xAxis: &#123; type: &#x27;value&#x27;, min: &#x27;dataMin&#x27;, max: &#x27;dataMax&#x27;, splitLine: &#123; show: true &#125; &#125;, yAxis: &#123; type: &#x27;value&#x27;, min: &#x27;dataMin&#x27;, max: &#x27;dataMax&#x27;, splitLine: &#123; show: true &#125; &#125;, dataZoom: [ &#123; type: &#x27;slider&#x27;, show: true, xAxisIndex: [0], start: 1, end: 35 &#125;, &#123; type: &#x27;slider&#x27;, show: true, yAxisIndex: [0], left: &#x27;93%&#x27;, start: 29, end: 36 &#125;, &#123; type: &#x27;inside&#x27;, xAxisIndex: [0], start: 1, end: 35 &#125;, &#123; type: &#x27;inside&#x27;, yAxisIndex: [0], start: 29, end: 36 &#125; ], series: [ &#123; name: &#x27;scatter&#x27;, type: &#x27;scatter&#x27;, itemStyle: &#123; normal: &#123; opacity: 0.8 &#125; &#125;, symbolSize: function (val) &#123; return val[2] * 40; &#125;, data: data1 &#125;, &#123; name: &#x27;scatter2&#x27;, type: &#x27;scatter&#x27;, itemStyle: &#123; normal: &#123; opacity: 0.8 &#125; &#125;, symbolSize: function (val) &#123; return val[2] * 40; &#125;, data: data2 &#125;, &#123; name: &#x27;scatter3&#x27;, type: &#x27;scatter&#x27;, itemStyle: &#123; normal: &#123; opacity: 0.8, &#125; &#125;, symbolSize: function (val) &#123; return val[2] * 40; &#125;, data: data3 &#125; ]&#125; .hsjyjsjythfx{zoom:50%;} 响应式ECharts 图表显示在用户指定高宽的 DOM 节点（容器）中。 有时候我们希望在 PC 和 移动设备上都能够很好的展示图表的内容，实现响应式的设计，为了解决这个问题，ECharts 完善了组件的定位设置，并且实现了类似 CSS Media Query 的自适应能力。 组件的定位和布局left/right/top/bottom/width/height 定位方式这六个量中，每个量都可以是『绝对值』或者『百分比』或者『位置描述』。 绝对值 单位是浏览器像素（px），用 number 形式书写（不写单位）。例如 &#123;left: 23, height: 400&#125;。 百分比 表示占 DOM 容器高宽的百分之多少，用 string 形式书写。例如 &#123;right: &#39;30%&#39;, bottom: &#39;40%&#39;&#125;。 位置描述 可以设置 left: &#39;center&#39;，表示水平居中。 可以设置 top: &#39;middle&#39;，表示垂直居中。 这六个量的概念，和 CSS 中六个量的概念类似： left：距离 DOM 容器左边界的距离。 right：距离 DOM 容器右边界的距离。 top：距离 DOM 容器上边界的距离。 bottom：距离 DOM 容器下边界的距离。 width：宽度。 height：高度。 在横向，left、right、width 三个量中，只需两个量有值即可，因为任两个量可以决定组件的位置和大小，例如 left 和 right 或者 right 和 width 都可以决定组件的位置和大小。 纵向，top、bottom、height 三个量，和横向类同不赘述。 center / radius 定位方式 center 是一个数组，表示 [x, y]，其中，x、y可以是『绝对值』或者『百分比』，含义和前述相同。 radius 是一个数组，表示 [内半径, 外半径]，其中，内外半径可以是『绝对值』或者『百分比』，含义和前述相同。 在自适应容器大小时，百分比设置是很有用的。 横向（horizontal）和纵向（vertical）ECharts的『外观狭长』型的组件（如 legend、visualMap、dataZoom、timeline等），大多提供了『横向布局』『纵向布局』的选择。例如，在细长的移动端屏幕上，可能适合使用『纵向布局』；在PC宽屏上，可能适合使用『横向布局』。 横纵向布局的设置，一般在『组件』或者『系列』的 orient 或者 layout 配置项上，设置为 ‘horizontal’ 或者 ‘vertical’。 以下实例中我们可以可尝试拖动右下角的圆点，图表会随着屏幕尺寸变化，legend 和 系列会自动改变布局位置和方式。 实例中我们使用了 jQuery 来加载外部数据，使用时我们需要引入 jQuery 库。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190$.when( $.getScript(&#x27;https://www.runoob.com/static/js/timelineGDP.js&#x27;), $.getScript(&#x27;https://www.runoob.com/static/js/draggable.js&#x27;)).done(function () &#123; draggable.init( $(&#x27;div[_echarts_instance_]&#x27;)[0], myChart, &#123; width: 700, height: 400, throttle: 70 &#125; ); myChart.hideLoading(); option = &#123; baseOption: &#123; title : &#123; text: &#x27;南丁格尔玫瑰图&#x27;, subtext: &#x27;纯属虚构&#x27;, x:&#x27;center&#x27; &#125;, tooltip : &#123; trigger: &#x27;item&#x27;, formatter: &quot;&#123;a&#125; &lt;br/&gt;&#123;b&#125; : &#123;c&#125; (&#123;d&#125;%)&quot; &#125;, legend: &#123; data:[&#x27;rose1&#x27;,&#x27;rose2&#x27;,&#x27;rose3&#x27;,&#x27;rose4&#x27;,&#x27;rose5&#x27;,&#x27;rose6&#x27;,&#x27;rose7&#x27;,&#x27;rose8&#x27;] &#125;, toolbox: &#123; show : true, feature : &#123; mark : &#123;show: true&#125;, dataView : &#123;show: true, readOnly: false&#125;, magicType : &#123; show: true, type: [&#x27;pie&#x27;, &#x27;funnel&#x27;] &#125;, restore : &#123;show: true&#125;, saveAsImage : &#123;show: true&#125; &#125; &#125;, calculable : true, series : [ &#123; name:&#x27;半径模式&#x27;, type:&#x27;pie&#x27;, roseType : &#x27;radius&#x27;, label: &#123; normal: &#123; show: false &#125;, emphasis: &#123; show: true &#125; &#125;, lableLine: &#123; normal: &#123; show: false &#125;, emphasis: &#123; show: true &#125; &#125;, data:[ &#123;value:10, name:&#x27;rose1&#x27;&#125;, &#123;value:5, name:&#x27;rose2&#x27;&#125;, &#123;value:15, name:&#x27;rose3&#x27;&#125;, &#123;value:25, name:&#x27;rose4&#x27;&#125;, &#123;value:20, name:&#x27;rose5&#x27;&#125;, &#123;value:35, name:&#x27;rose6&#x27;&#125;, &#123;value:30, name:&#x27;rose7&#x27;&#125;, &#123;value:40, name:&#x27;rose8&#x27;&#125; ] &#125;, &#123; name:&#x27;面积模式&#x27;, type:&#x27;pie&#x27;, roseType : &#x27;area&#x27;, data:[ &#123;value:10, name:&#x27;rose1&#x27;&#125;, &#123;value:5, name:&#x27;rose2&#x27;&#125;, &#123;value:15, name:&#x27;rose3&#x27;&#125;, &#123;value:25, name:&#x27;rose4&#x27;&#125;, &#123;value:20, name:&#x27;rose5&#x27;&#125;, &#123;value:35, name:&#x27;rose6&#x27;&#125;, &#123;value:30, name:&#x27;rose7&#x27;&#125;, &#123;value:40, name:&#x27;rose8&#x27;&#125; ] &#125; ] &#125;, media: [ &#123; option: &#123; legend: &#123; right: &#x27;center&#x27;, bottom: 0, orient: &#x27;horizontal&#x27; &#125;, series: [ &#123; radius: [20, &#x27;50%&#x27;], center: [&#x27;25%&#x27;, &#x27;50%&#x27;] &#125;, &#123; radius: [30, &#x27;50%&#x27;], center: [&#x27;75%&#x27;, &#x27;50%&#x27;] &#125; ] &#125; &#125;, &#123; query: &#123; minAspectRatio: 1 &#125;, option: &#123; legend: &#123; right: &#x27;center&#x27;, bottom: 0, orient: &#x27;horizontal&#x27; &#125;, series: [ &#123; radius: [20, &#x27;50%&#x27;], center: [&#x27;25%&#x27;, &#x27;50%&#x27;] &#125;, &#123; radius: [30, &#x27;50%&#x27;], center: [&#x27;75%&#x27;, &#x27;50%&#x27;] &#125; ] &#125; &#125;, &#123; query: &#123; maxAspectRatio: 1 &#125;, option: &#123; legend: &#123; right: &#x27;center&#x27;, bottom: 0, orient: &#x27;horizontal&#x27; &#125;, series: [ &#123; radius: [20, &#x27;50%&#x27;], center: [&#x27;50%&#x27;, &#x27;30%&#x27;] &#125;, &#123; radius: [30, &#x27;50%&#x27;], center: [&#x27;50%&#x27;, &#x27;70%&#x27;] &#125; ] &#125; &#125;, &#123; query: &#123; maxWidth: 500 &#125;, option: &#123; legend: &#123; right: 10, top: &#x27;15%&#x27;, orient: &#x27;vertical&#x27; &#125;, series: [ &#123; radius: [20, &#x27;50%&#x27;], center: [&#x27;50%&#x27;, &#x27;30%&#x27;] &#125;, &#123; radius: [30, &#x27;50%&#x27;], center: [&#x27;50%&#x27;, &#x27;75%&#x27;] &#125; ] &#125; &#125; ] &#125;; myChart.setOption(option);&#125;); 数据的视觉映射visualMap 组件中可以使用的视觉元素有： 图形类别（symbol） 图形大小（symbolSize） 颜色（color） 透明度（opacity） 颜色透明度（colorAlpha） 颜色明暗度（colorLightness） 颜色饱和度（colorSaturation） 色调（colorHue） 数据和维度ECharts 中的数据，一般存放于 series.data 中。 不同的图表类型，数据格式有所不一样，但是他们的共同特点就都是数据项（dataItem） 的集合。每个数据项含有 数据值（value） 和其他信息（可选）。每个数据值，可以是单一的数值（一维）或者一个数组（多维）。 series.data 最常见的形式 是线性表，即一个普通数组： 12345678910111213141516171819202122232425series: &#123; data: [ &#123; // 这里每一个项就是数据项（dataItem） value: 2323, // 这是数据项的数据值（value） itemStyle: &#123;...&#125; &#125;, 1212, // 也可以直接是 dataItem 的 value，这更常见。 2323, // 每个 value 都是『一维』的。 4343, 3434 ]&#125;series: &#123; data: [ &#123; // 这里每一个项就是数据项（dataItem） value: [3434, 129, &#x27;圣马力诺&#x27;], // 这是数据项的数据值（value） itemStyle: &#123;...&#125; &#125;, [1212, 5454, &#x27;梵蒂冈&#x27;], // 也可以直接是 dataItem 的 value，这更常见。 [2323, 3223, &#x27;瑙鲁&#x27;], // 每个 value 都是『三维』的，每列是一个维度。 [4343, 23, &#x27;图瓦卢&#x27;] // 假如是『气泡图』，常见第一维度映射到x轴， // 第二维度映射到y轴， // 第三维度映射到气泡半径（symbolSize） ]&#125; 在图表中，往往默认把 value 的前一两个维度进行映射，比如取第一个维度映射到x轴，取第二个维度映射到y轴。如果想要把更多的维度展现出来，可以借助 visualMap 。 visualMap组件visualMap 组件定义了把数据的指定维度映射到对应的视觉元素上。 visualMap 组件可以定义多个，从而可以同时对数据中的多个维度进行视觉映射。 visualMap 组件可以定义为 分段型（visualMapPiecewise） 或 连续型（visualMapContinuous），通过 type 来区分。例如 12345678910111213option = &#123; visualMap: [ &#123; // 第一个 visualMap 组件 type: &#x27;continuous&#x27;, // 定义为连续型 visualMap ... &#125;, &#123; // 第二个 visualMap 组件 type: &#x27;piecewise&#x27;, // 定义为分段型 visualMap ... &#125; ], ...&#125;; 分段型视觉映射组件，有三种模式： 连续型数据平均分段: 依据 visualMap-piecewise.splitNumber 来自动平均分割成若干块。 连续型数据自定义分段: 依据 visualMap-piecewise.pieces 来定义每块范围。 离散数据根据类别分段: 类别定义在 visualMap-piecewise.categories 中。 映射方式配置visualMap 中可以指定数据的指定维度映射到对应的视觉元素上。 12345678910111213141516171819202122232425option = &#123; visualMap: [ &#123; type: &#x27;piecewise&#x27; min: 0, max: 5000, dimension: 3, // series.data 的第四个维度（即 value[3]）被映射 seriesIndex: 4, // 对第四个系列进行映射。 inRange: &#123; // 选中范围中的视觉配置 color: [&#x27;blue&#x27;, &#x27;#121122&#x27;, &#x27;red&#x27;], // 定义了图形颜色映射的颜色列表， // 数据最小值映射到&#x27;blue&#x27;上， // 最大值映射到&#x27;red&#x27;上， // 其余自动线性计算。 symbolSize: [30, 100] // 定义了图形尺寸的映射范围， // 数据最小值映射到30上， // 最大值映射到100上， // 其余自动线性计算。 &#125;, outOfRange: &#123; // 选中范围外的视觉配置 symbolSize: [30, 100] &#125; &#125;, ... ]&#125;; 1234567891011121314option = &#123; visualMap: [ &#123; ..., inRange: &#123; // 选中范围中的视觉配置 colorLightness: [0.2, 1], // 映射到明暗度上。也就是对本来的颜色进行明暗度处理。 // 本来的颜色可能是从全局色板中选取的颜色，visualMap组件并不关心。 symbolSize: [30, 100] &#125;, ... &#125;, ... ]&#125;; 事件处理ECharts 中我们可以通过监听用户的操作行为来回调对应的函数。 ECharts 通过 on 方法来监听用户的行为，例如监控用户的点击行为。 ECharts 中事件分为两种类型: 用户鼠标操作点击，如 ‘click’、’dblclick’、’mousedown’、’mousemove’、’mouseup’、’mouseover’、’mouseout’、’globalout’、’contextmenu’ 事件。 还有一种是用户在使用可以交互的组件后触发的行为事件，例如在切换图例开关时触发的 ‘legendselectchanged’ 事件），数据区域缩放时触发的 ‘datazoom’ 事件等等。 12345678910111213141516myChart.on(&#x27;click&#x27;, function (params) &#123; // 在用户点击后控制台打印数据的名称 console.log(params);&#125;);myChart.on(&#x27;legendselectchanged&#x27;, function (params) &#123; console.log(params);&#125;);chart.on(&#x27;click&#x27;, &#x27;series.line&#x27;, function (params) &#123; console.log(params);&#125;);chart.on(&#x27;mouseover&#x27;, &#123;seriesIndex: 1, name: &#x27;xx&#x27;&#125;, function (params) &#123; console.log(params);&#125;); 鼠标事件ECharts 支持的鼠标事件类型，包括 ‘click’、’dblclick’、’mousedown’、’mousemove’、’mouseup’、’mouseover’、’mouseout’、’globalout’、’contextmenu’ 事件。 以下实例在点击柱形图时会弹出对话框： 12345678910111213141516171819202122// 基于准备好的dom，初始化ECharts实例var myChart = echarts.init(document.getElementById(&#x27;main&#x27;));// 指定图表的配置项和数据var option = &#123; xAxis: &#123; data: [&quot;衬衫&quot;,&quot;羊毛衫&quot;,&quot;雪纺衫&quot;,&quot;裤子&quot;,&quot;高跟鞋&quot;,&quot;袜子&quot;] &#125;, yAxis: &#123;&#125;, series: [&#123; name: &#x27;销量&#x27;, type: &#x27;bar&#x27;, data: [5, 20, 36, 10, 10, 20] &#125;]&#125;;// 使用刚指定的配置项和数据显示图表。myChart.setOption(option);// 处理点击事件并且弹出数据名称myChart.on(&#x27;click&#x27;, function (params) &#123; alert(params.name);&#125;); 所有的鼠标事件包含参数 params，这是一个包含点击图形的数据信息的对象，格式如下： 12345678910111213141516171819202122232425&#123; // 当前点击的图形元素所属的组件名称， // 其值如 &#x27;series&#x27;、&#x27;markLine&#x27;、&#x27;markPoint&#x27;、&#x27;timeLine&#x27; 等。 componentType: string, // 系列类型。值可能为：&#x27;line&#x27;、&#x27;bar&#x27;、&#x27;pie&#x27; 等。当 componentType 为 &#x27;series&#x27; 时有意义。 seriesType: string, // 系列在传入的 option.series 中的 index。当 componentType 为 &#x27;series&#x27; 时有意义。 seriesIndex: number, // 系列名称。当 componentType 为 &#x27;series&#x27; 时有意义。 seriesName: string, // 数据名，类目名 name: string, // 数据在传入的 data 数组中的 index dataIndex: number, // 传入的原始数据项 data: Object, // sankey、graph 等图表同时含有 nodeData 和 edgeData 两种 data， // dataType 的值会是 &#x27;node&#x27; 或者 &#x27;edge&#x27;，表示当前点击在 node 还是 edge 上。 // 其他大部分图表中只有一种 data，dataType 无意义。 dataType: string, // 传入的数据值 value: number|Array // 数据图形的颜色。当 componentType 为 &#x27;series&#x27; 时有意义。 color: string&#125; 如何区分鼠标点击到了哪里： 123456789101112131415161718myChart.on(&#x27;click&#x27;, function (params) &#123; if (params.componentType === &#x27;markPoint&#x27;) &#123; // 点击到了 markPoint 上 if (params.seriesIndex === 5) &#123; // 点击到了 index 为 5 的 series 的 markPoint 上。 &#125; &#125; else if (params.componentType === &#x27;series&#x27;) &#123; if (params.seriesType === &#x27;graph&#x27;) &#123; if (params.dataType === &#x27;edge&#x27;) &#123; // 点击到了 graph 的 edge（边）上。 &#125; else &#123; // 点击到了 graph 的 node（节点）上。 &#125; &#125; &#125;&#125;); 使用 query 只对指定的组件的图形元素的触发回调： 1chart.on(eventName, query, handler); query 可为 string 或者 Object。 如果为 string 表示组件类型。格式可以是 ‘mainType’ 或者 ‘mainType.subType’。例如： 1234chart.on(&#x27;click&#x27;, &#x27;series&#x27;, function () &#123;...&#125;);chart.on(&#x27;click&#x27;, &#x27;series.line&#x27;, function () &#123;...&#125;);chart.on(&#x27;click&#x27;, &#x27;dataZoom&#x27;, function () &#123;...&#125;);chart.on(&#x27;click&#x27;, &#x27;xAxis.category&#x27;, function () &#123;...&#125;); 如果为 Object，可以包含以下一个或多个属性，每个属性都是可选的： 123456789&#123; &lt;mainType&gt;Index: number // 组件 index &lt;mainType&gt;Name: string // 组件 name &lt;mainType&gt;Id: string // 组件 id dataIndex: number // 数据项 index name: string // 数据项 name dataType: string // 数据项 type，如关系图中的 &#x27;node&#x27;, &#x27;edge&#x27; element: string // 自定义系列中的 el 的 name&#125; 例如： 12345678910chart.setOption(&#123; // ... series: [&#123; name: &#x27;uuu&#x27; // ... &#125;]&#125;);chart.on(&#x27;mouseover&#x27;, &#123;seriesName: &#x27;uuu&#x27;&#125;, function () &#123; // series name 为 &#x27;uuu&#x27; 的系列中的图形元素被 &#x27;mouseover&#x27; 时，此方法被回调。&#125;); 例如： 123456789101112131415chart.setOption(&#123; // ... series: [&#123; // ... &#125;, &#123; // ... data: [ &#123;name: &#x27;xx&#x27;, value: 121&#125;, &#123;name: &#x27;yy&#x27;, value: 33&#125; ] &#125;]&#125;);chart.on(&#x27;mouseover&#x27;, &#123;seriesIndex: 1, name: &#x27;xx&#x27;&#125;, function () &#123; // series index 1 的系列中的 name 为 &#x27;xx&#x27; 的元素被 &#x27;mouseover&#x27; 时，此方法被回调。&#125;); 例如： 1234567891011121314chart.setOption(&#123; // ... series: [&#123; type: &#x27;graph&#x27;, nodes: [&#123;name: &#x27;a&#x27;, value: 10&#125;, &#123;name: &#x27;b&#x27;, value: 20&#125;], edges: [&#123;source: 0, target: 1&#125;] &#125;]&#125;);chart.on(&#x27;click&#x27;, &#123;dataType: &#x27;node&#x27;&#125;, function () &#123; // 关系图的节点被点击时此方法被回调。&#125;);chart.on(&#x27;click&#x27;, &#123;dataType: &#x27;edge&#x27;&#125;, function () &#123; // 关系图的边被点击时此方法被回调。&#125;); 例如： 1234567891011121314151617181920212223chart.setOption(&#123; // ... series: &#123; // ... type: &#x27;custom&#x27;, renderItem: function (params, api) &#123; return &#123; type: &#x27;group&#x27;, children: [&#123; type: &#x27;circle&#x27;, name: &#x27;my_el&#x27;, // ... &#125;, &#123; // ... &#125;] &#125; &#125;, data: [[12, 33]] &#125;&#125;)chart.on(&#x27;mouseup&#x27;, &#123;element: &#x27;my_el&#x27;&#125;, function () &#123; // name 为 &#x27;my_el&#x27; 的元素被 &#x27;mouseup&#x27; 时，此方法被回调。&#125;); 你可以在回调函数中获得这个对象中的数据名、系列名称后在自己的数据仓库中索引得到其它的信息候更新图表，显示浮层等等，如下示例代码： 1234567891011myChart.on(&#x27;click&#x27;, function (parmas) &#123; $.get(&#x27;detail?q=&#x27; + params.name, function (detail) &#123; myChart.setOption(&#123; series: [&#123; name: &#x27;pie&#x27;, // 通过饼图表现单个柱子中的数据分布 data: [detail.data] &#125;] &#125;); &#125;);&#125;); 行为事件在 ECharts 中基本上所有的组件交互行为都会触发相应的事件，常用的事件和事件对应参数在 events 文档中有列出。 下面是监听一个图例开关的示例： 123456789// 图例开关的行为只会触发 legendselectchanged 事件myChart.on(&#x27;legendselectchanged&#x27;, function (params) &#123; // 获取点击图例的选中状态 var isSelected = params.selected[params.name]; // 在控制台中打印 console.log((isSelected ? &#x27;选中了&#x27; : &#x27;取消选中了&#x27;) + &#x27;图例&#x27; + params.name); // 打印所有图例的状态 console.log(params.selected);&#125;); 触发组件行为上面我们只说明了用户的交互操作，但有时候我们也会需要在程序里调用方法并触发图表的行为，比如显示 tooltip。 ECharts 通过 dispatchAction({ type: ‘’ }) 来触发图表行为，统一管理了所有动作，也可以根据需要去记录用户的行为路径。 以上实例用于轮播饼图中的 tooltip： 12345678910111213141516171819202122setInterval(function () &#123; var dataLen = option.series[0].data.length; // 取消之前高亮的图形 myChart.dispatchAction(&#123; type: &#x27;downplay&#x27;, seriesIndex: 0, dataIndex: app.currentIndex &#125;); app.currentIndex = (app.currentIndex + 1) % dataLen; // 高亮当前图形 myChart.dispatchAction(&#123; type: &#x27;highlight&#x27;, seriesIndex: 0, dataIndex: app.currentIndex &#125;); // 显示 tooltip myChart.dispatchAction(&#123; type: &#x27;showTip&#x27;, seriesIndex: 0, dataIndex: app.currentIndex &#125;);&#125;, 1000); 旭日图旭日图（Sunburst）由多层的环形图组成，在数据结构上，内圈是外圈的父节点。因此，它既能像饼图一样表现局部和整体的占比，又能像矩形树图一样表现层级关系。 ECharts 创建旭日图很简单，只需要在 series 配置项中声明类型为 sunburst 即可，data 数据结构以树形结构声明，看下一个简单的实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;ECharts 实例&lt;/title&gt; &lt;!-- 引入 echarts.js --&gt; &lt;script src=&quot;https://cdn.staticfile.org/echarts/4.3.0/echarts.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- 为ECharts准备一个具备大小（宽高）的Dom --&gt; &lt;div id=&quot;main&quot; style=&quot;width: 600px;height:400px;&quot;&gt;&lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById(&#x27;main&#x27;)); // 指定图表的配置项和数据 var option = &#123; series: &#123; type: &#x27;sunburst&#x27;, data: [&#123; name: &#x27;A&#x27;, value: 10, children: [&#123; value: 3, name: &#x27;Aa&#x27; &#125;, &#123; value: 5, name: &#x27;Ab&#x27; &#125;] &#125;, &#123; name: &#x27;B&#x27;, children: [&#123; name: &#x27;Ba&#x27;, value: 4 &#125;, &#123; name: &#x27;Bb&#x27;, value: 2 &#125;] &#125;, &#123; name: &#x27;C&#x27;, value: 3 &#125;] &#125; &#125;; // 使用刚指定的配置项和数据显示图表。 myChart.setOption(option); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; .mhsrmaaubjin{zoom:50%;} 更多详情ECharts 旭日图 | 菜鸟教程 (runoob.com) 累了，累了，学习笔记就写到这吧，不懂的去菜鸟教程自学","categories":[{"name":"echarts","slug":"echarts","permalink":"http://pistachio0812.github.io/categories/echarts/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"echarts","slug":"echarts","permalink":"http://pistachio0812.github.io/tags/echarts/"}],"author":"pistachio"},{"title":"微信小程序开发","slug":"微信小程序开发","date":"2022-10-12T11:53:01.972Z","updated":"2022-10-12T12:06:32.648Z","comments":true,"path":"zh-CN/微信小程序开发/","permalink":"http://pistachio0812.github.io/zh-CN/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91/","excerpt":"","text":"参考博文： 1.微信小程序开发文档_w3cschool 2.https://mp.weixin.qq.com/ 3.微信开放文档 (qq.com) 4.微信开发者工具下载地址与更新日志 | 微信开放文档 (qq.com)","categories":[{"name":"开发日记","slug":"开发日记","permalink":"http://pistachio0812.github.io/categories/%E5%BC%80%E5%8F%91%E6%97%A5%E8%AE%B0/"}],"tags":[{"name":"微信小程序","slug":"微信小程序","permalink":"http://pistachio0812.github.io/tags/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"}],"author":"pistachio"},{"title":"Logo语言的学习","slug":"Logo学习笔记","date":"2022-10-11T10:51:39.314Z","updated":"2022-10-11T14:26:40.541Z","comments":true,"path":"zh-CN/Logo学习笔记/","permalink":"http://pistachio0812.github.io/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"参考博文： 1.logo（程序设计语言） 2.Logo 简介_w3cschool Logo 是一种易于学习的编程语言。它用于教学生和儿童如何编程计算机。它被开发用于处理单词列表。 安装logo如果想要安装logo编程软件，点击MSWLogo Downloads – Softronics Inc. (softronix.com) 下载setup kit安装即可 turtle简单的 Logo 绘制命令可以前后移动 Turtle，也可以向右或向左转动。命令及其缩写如下: fd – 前进 bk – 向后 rt – 右 lt – 左 cs – 清屏 可以使用这些命令的任一版本。除了cs命令，这些命令中的每一个都必须跟一个称为参数的值。fd和bk的参数是单位；rt和lt的角度可以是任何整数。旋转 360 度是完整的旋转，因此旋转 375 度与 1/15 度相同。 forward 60或fd 60表示前进 60 步 right 90或rt 90表示右转 90 度 left 90或lt 90表示左转 90 度 back 60或bk 60表示返回 60 步 clearscreen或cs表示擦除所有绘图。这将 Turtle 设置在中心 图形窗口有一个坐标系。中心的两个坐标（通常称为 x 和 y）的值为0、0。在东北角，它们是250、250；在东南角，它们是 250，-250。在西南角，它们是-250、-250；等等。如果 Turtle 试图走到屏幕的一侧，它会绕过去。右侧绕到左侧，顶部绕到底部。 turtle命令现在让我们尝试一些命令。命令将每行发出一个，然后是回车。可以在命令窗口中连续键入其中几个命令，然后按回车符。对 Turtle 的效果是一样的。但是，如果您键入一个命令，该命令需要一个或多个输入并在下一行提供缺少的输入，Logo 将显示错误。 示例1： 12# 尝试画一个领奖台fd 40 rt 90 fd 40 lt 90 fd 40 rt 90 fd 40 rt 90 fd 40 lt 90 fd 40 rt 90 fd 40 rt 90 fd 120 .exmdcfndybum{zoom:50%;} 示例2： 12# 尝试画一个正五边形fd 60 rt 72 fd 60 rt 72 fd 60 rt 72 fd 60 rt 72 fd 60 .qlwendiriqio{zoom:50%;} 现在你可以尝试画出以下图形或者任何你想画的了 .rchyphtakrzr{zoom:50%;} 控制turtle和笔Logo 还有许多其他绘图命令，其中一些在下面给出。 pu - penup pd - pendown ht - hideturtle dt - showturtle setpensize pendown 和 penup 命令分别告诉 turtle 在移动时在屏幕上留下墨迹或不留下墨迹。该hideturtle和showturtle命令隐藏或显示 turtle，但不影响其离开墨因为它移动的能力。home 命令使 turtle 返回到屏幕的中心。当 turtle 回到屏幕中心时，它可能会留下墨水。setpensize 命令决定绘图笔的大小。 penup或pu表示拿起笔，因此您可以移动 turtle 而不会留下痕迹。 pendown或pd表示放下笔，因此您可以移动 turtle 并留下轨迹。 hideturtle或ht表示隐藏 turtle，这样您就可以欣赏您的画作。 showturtle或st表示显示 turtle，因此您可以继续绘图。 setpensize意味着它可以使笔更大，更容易看到。默认笔大小为–[1 1]。 示例1： 12# 设置笔的大小，是[1 1],中间有空格setpensize [1 1] fd 40 setpensize [3 3] fd 40 .ppdsjsqimvxm{zoom:50%;} 示例2： 123# repeat表示重复， 4为重复次数# 展示pu, pd功能repeat 4[fd 30 pu fd 30 pd fd 30 rt 90] .iektiyfjcycb{zoom:50%;} 示例3： 12# 展示ht, 默认st,如果你想重新显示turtle, 后面加上st就行了repeat 4[fd 30 pu fd 30 pd fd 30 rt 90] ht .bjeilrmkbfvj{zoom:50%;} turtle世界Logo 还有许多其他附加绘图命令，其中一些在下面给出。 Home cleartext 或 ct label setxy label命令将单个单词作为带引号的字符串（例如“a_string”）或 [ ] 括号中不带引号的单词列表（例如 [Programming]），并将它们打印在 turtle 所在位置的图形窗口上. 让我们考虑以下代码。 1label \"Programming repeat 3 [rt 90 label \"Programming ] ht .apseuxhpjicd{zoom:50%;} setxy命令采用两个参数，第一个参数作为横坐标（水平轴）的值和第二个参数为纵坐标（垂直轴）的值。它将 turtle 放置在这些坐标处，可能在到达这些坐标时留下墨迹。 示例1： 1setxy 60 60 .okiybgriyiav{zoom:50%;} 示例2： 1setxy 60 60 pu setxy -60 60 .tkvyywsptcxe{zoom:50%;} cleartext命令，缩写为ct，用于清除命令窗口的文本区域。 尝试一下下面的命令 1cs pu setxy -60 60 pd home rt 45 fd 85 lt 135 fd 120 .tmjpucxsaijx{zoom:50%;} 当您从左到右阅读这些命令时，请对其进行解释。尝试找出结果。 以下是命令摘要表。 命令名称 目的 setx 100 将turtle的 x 坐标设置为 +100;将其移动到中心右侧 100 点;无垂直变化 setx -200 将turtle移动到中心左侧 200 点;无垂直变化 150 将turtle的 y 坐标设置为 150;将其移动到中心上方 150 点;无横向变化 sety - 50 将turtle移动到中心下方 50 点; 无横向变化 setxy 100 100 将turtle移动到 xy 坐标 (100,100) show xcor;show ycor 报告turtle的 x 坐标;报告turtle的 y 坐标 setheading 0;seth 0 将turtle直指，“正午” seth 120 将turtle移动 120 度以指向四点钟位置 logo变量变量是可以包含值的内存位置的名称。在计算机中，每个内存位置都有一个整数地址。由于很难记住包含程序使用的值的每个位置的地址，计算机科学家已经找到了给这些位置、符号名称的方法。一旦变量有了名称，我们就可以使用和操作它。 变量的名称是字母串。变量名可以包含字母（不区分大小写）、数字和下划线。变量名可以在计算中使用:在它之前访问。让我们在屏幕截图中考虑以下示例。 示例： 12345678910make \"val1 100make \"val2 200print :val1100print : val2 has no valueprint :val1 + :val2300print :val1 - :val2-100 logo运算符Logo 提供了常用的加、减、乘、除算术运算，用符号+、-、*、/表示。这些操作中的每一个都会产生一个结果。如果对结果不做任何处理，例如打印它，Logo 将显示错误。 使用打印命令，可以在命令窗口中使用和打印算术运算的结果。以下屏幕截图中给出的示例演示了相同的内容。 其他有用的命令是： sqrt - 它接受一个非负参数并返回其平方根。 power - 它需要两个参数，称为“a”和“b”，并生成 a 的 b 次幂。 ln - 它接受一个参数并返回其自然对数。 exp - 它需要一个参数并计算 e 的那个幂，e 是自然数 2.718281828。 log10 - 取其一个参数的以 10 为底的对数。 12345678910print sqrt 164print power 10 31000print ln 20.693147180559945print exp 27.38905609893065print log10 10003 算术运算符的优先级决定了它们的计算顺序。 注意: 打印60 * sqrt 2和打印sqrt 2 * 60会产生不同的答案。这里*运算符优先于 sqrt 运算符。因此，如果有选择，*将在 sqrt 之前完成，就像在第二种情况下一样。 logo重复让我们假设我们想画一个边长为 100 的正方形，我们可以使用以下程序来做到这一点： 12345678910fd 100rt 90fd 100rt 90fd 100rt 90fd 100rt 90# 你可以写成下面的形式repeat 4[fd 100 rt 90] logo随机化有时，计算出不可预测的结果很有趣。Logo 提供了一个随机程序来生成一个随机数。它有一个参数并产生一个随机统一选择的整数值，该值大于或等于 0 且小于其参数的值。因此，如果您想要一个 0 到 359 度之间的随机角度，您可以使用命令random 360来生成它。请记住，除非您对结果执行某些操作（例如打印），否则 Logo 将显示错误。 示例1： 12print random 360231 示例2： 1repeat 100[fd random 80 rt 90] .uhxinkknlmes{zoom:50%;} 示例3： 1repeat 1000[fd 10 rt random 360] .abumdmnwsfyd{zoom:50%;} logo程序过程提供了一种封装命令集合的方法。一旦创建了过程，就可以像使用内置命令一样使用它。一个过程的“意义”就是它的各个命令的意义。 没有参数的过程在第一行有单词to（保留字）和过程名称。（Logo 中的保留字不能作为变量使用，有明确的含义和用途。）它在最后一行有保留字end。 子程序是供另一个程序执行的命名步骤序列。子程序的其他名称是过程和函数。在 Logo 中，你告诉计算机如何做某事——例如: 123to squarerepeat 4 [fd 100 rt 90]end 一旦我们向 Logo 描述了我们的过程，我们就可以在命令行上输入它的名称，就像我们对任何内置的东西所做的一样。在这种情况下，我们将在命令行上输入square，Logo 会查找命令以制作一个正方形。 单击显示Edall（用于编辑全部）的按钮以调出 Logo 的内置编辑器。（如果您的徽标没有 Edall 按钮，请在命令行中输入edall）。以下代码块具有子程序所需的结构。 123to procedurenamesteps of your procedure hereend 过程或子程序必须以to这个词开头，后面跟着一个我们想到的名字。下一步是键入我们将在命令行上编写的所有相同步骤。该过程必须以end一词结尾。所有注释或备注行都应以分号 ;开头。 我们不希望每个方格的大小都一样——我们想要多样性。在 Logo 中，我们创建了变量，我们可以更改其值。在以下示例中，我们将使用相同的平方程序，但稍作改动。 123to square :nrepeat 4 [fd :n rt 90]end 我们在命令行上给 Logo 一个:n的替换值，如下所示。 123square 40square 50square 70 .dlahwilqxddk{zoom:50%;} logo递归过程在递归过程中，过程中会有一个过程的递归调用。让我们考虑以下代码 1234567# 螺旋线， 图形中n=30to spiral_recur :n if :n &lt; 1 [stop] fd :n rt 20 spiral_recur 0.95 * :nend .bkcsgeeokyaw{zoom:50%;} logo决策决策和变量相辅相成。程序需要能够根据情况改变课程。例如，下面是绘制螺旋的框架。它有一个循环，是前面显示的重复的变体，循环的主体供我们填写。 1234567to spiral make \"n 1 while [:n &lt; 100] [ make \"n :n + 5 fd :n rt 90 ]end 上面的代码展示了 MSW Logo 语法的几个新特性。我们通过键入make将一个变量设置为一个新值，然后变量的名称前面是双引号\"而不是冒号: ，如下所示。 1make \"n 1 不过，我们使用了一个变量，在它的名称前有一个冒号: 1while [:n &lt; 100] while [condition]后括号内的代码被执行，而条件为真。当它不再为真时，因为（在这种情况下）:n的值增长大于 100，执行括号后面的代码。 .rbcurmbxrmdo{zoom:50%;} 现在，我们将讨论if 语句的使用，它具有仅在给定条件为真时才会执行的代码。 它还显示了一个生成随机数的内置徽标。语句random 3在随机序列中任意生成任意数字 0 或 1 或 2。然后程序决定“随机”走哪条路。生成的随机数将保存在 r 中，稍后将根据变量r的值执行 if 语句之一，这将满足条件。因此，如果: r 的值为 0，则将执行[fd 20]。 r 的值为 1，则将执行[rt 90 fd 20]。 r 的值为 2，则将执行[lt 90 fd 20]。 代码： 12345678to randomwalkrepeat 100[ make \"r random 3 if :r = 0 [fd 20] if :r = 1 [rt 90 fd 20] if :r = 2 [lt 90 fd 20]]end .wocjevfbxgpp{zoom:50%;} logo字符串任何字母数字字符序列，例如america、emp1234等，都是字符串的示例。计算字符数是所有字符串过程中最基本的。问题stringlength \"abc12ef的答案由以下过程给出: 12345678910to stringlength :s make \"inputstring :s make \"count 0 while [not emptyp :s] [ make \"count :count + 1 print first :s make \"s butfirst :s ] print (sentence :inputstring \"has :count \"letters)end 在上面的过程中s是包含输入字符串的变量。变量inputstring包含输入字符串的副本。变量计数初始化为 0。在 while 循环中，条件检查字符串是否为空。在每个循环计数中，一个变量增加 1 以保持长度计数。语句print first :s仅打印存储在s中的字符串的第一个字符。 语句make \"s butfirst :s，检索不包括第一个字符的子字符串。退出while循环后，我们打印了输入字符串的字符数或长度。 logo颜色计算机屏幕使用红色、绿色和蓝色的光成分，因此它们有时被称为RGB 屏幕。 在 Logo 的设置菜单上，我们可以设置三个屏幕元素的颜色: turtle 的笔 turtle 的填充物（就像围栏的油漆桶） 画面背景 .nsiwodqfcxfg{zoom:50%;} 我们通过左右移动这三个滑块来设置颜色。请记住，黑色是所有颜色的缺失，而白色是所有颜色的结合。混合光不像混合油漆。例如，如果您将红色和绿色颜料混合，则会得到浑浊的颜色。由于这是一台计算机，因此每种颜色都有一个内部数字表示。 滑动刻度的左端为零 (0)。右端是 255，这有点像计算机的 99（它是 2 8 - 1）。因此黑色是[0 0 0]，红色是[255 0 0]，绿色是[0 255 0]，蓝色是[0 0 255]。你可以在这些颜色之间制作任何东西，在所有这些颜色中，有256 * 256 * 256种可能的颜色。那是2^8 * 2^8 * 2^8，或 24 位颜色 — 机器内部的 24 位二进制数字。 以下命令会给你一个大红笔 - 12setpensize [5 5]setpencolor [255 0 0] 当您使用滑块找到您喜欢的颜色时，您可以询问 Logo 它是什么：选择笔的颜色，然后在命令窗口中输入以下命令。 1show pencolor 您可以使用以下步骤制作彩色方块 - 步骤 1 - 使用以下命令绘制边长为 40 的正方形。 1repeat 4 [fd 40 rt 90] 步骤 2 - 使用以下命令完成。 1pu 步骤 3 - 转到正方形内的一个点。例如，使用以下命令将海龟放置在坐标 (20, 20) 处。 1setxy 20 20 步骤 4 - 用设置的泛色填充正方形。例如，要将泛光颜色设置为蓝色，请使用以下命令。 1setfloodcolor [0 0 255] 下表列出了更多与颜色和笔相关的命令。 颜色和笔命令 命令的目的 setpencolor [rgb]；setpc [rgb] 设置turtle笔的颜色 rgb 是 [0, 255] 范围内的数字 setfloodcolor [rgb]； setfc [rgb] 设置批注区域的颜色 设置屏幕颜色 [rgb]; 设置sc [rgb] 设置背景颜色 显示笔色; 显示泛色; 显示屏幕颜色 指定命名项的 [rgb] 的当前值 填充 在光标位置倾倒一桶当前的泛色 尝试执行以下命令集： cs - 清除屏幕。 home - 将turtle放置在初识位置。 setpensize [5 5] - 设置笔的大小。 setpencolor [255 0 0] - 将笔颜色设置为红色。 setfloodcolor [0 0 255] - 将泛色设置为蓝色。 setscreencolor [0 255 0] - 将屏幕颜色设置为绿色。 repeat 4 [fd 40 rt 90] - 画一个边长为 40 的正方形。 pu - 拿起钢笔。 setxy 20 20 - 将turtle放在坐标 (20, 20) 处。 fill- 用设置的泛光蓝色填充正方形。 ht - 隐藏turtle。 .fvxptucbwbwg{zoom:50%;} 好了，到此结束，有点像python里的turtle库，或许就是，再见！！！","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"logo","slug":"logo","permalink":"http://pistachio0812.github.io/tags/logo/"},{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"author":"coolboy"},{"title":"rss学习笔记","slug":"RSS","date":"2022-10-10T09:58:38.565Z","updated":"2022-10-10T12:16:05.872Z","comments":true,"path":"zh-CN/RSS/","permalink":"http://pistachio0812.github.io/zh-CN/RSS/","excerpt":"","text":"参考博文： 1.RSS 教程 | 菜鸟教程 (runoob.com) 通过使用 RSS，您可以有选择地浏览您感兴趣的以及与您的工作相关的新闻。 通过使用 RSS，您可以把需要的信息从不需要的信息（兜售信息，垃圾邮件等）中分离出来。 通过使用 RSS，您可以创建自己的新闻频道，并将之发布到因特网。 rss工作原理RSS 用于在网站间分享信息。 使用 RSS，您在名为聚合器的公司注册您的内容。 步骤之一是，创建一个 RSS 文档，然后使用 .xml 后缀来保存它。然后把此文件上传到您的网站。接下来，通过一个 RSS 聚合器来注册。每天，聚合器都会到被注册的网站搜索 RSS 文档，校验其链接，并显示有关 feed 的信息，这样客户就能够链接到使他们产生兴趣的文档。 rss实例1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;rss version=\"2.0\"&gt;&lt;channel&gt; &lt;title&gt;菜鸟教程首页&lt;/title&gt; &lt;link&gt;http://www.runoob.com&lt;/link&gt; &lt;description&gt;免费编程教程&lt;/description&gt; &lt;item&gt; &lt;title&gt;RSS 教程&lt;/title&gt; &lt;link&gt;http://www.runoob.com/rss&lt;/link&gt; &lt;description&gt;菜鸟教程 Rss 教程&lt;/description&gt; &lt;/item&gt; &lt;item&gt; &lt;title&gt;XML 教程&lt;/title&gt; &lt;link&gt;http://www.runoob.com/xml&lt;/link&gt; &lt;description&gt;菜鸟教程 XML 教程&lt;/description&gt; &lt;/item&gt;&lt;/channel&gt;&lt;/rss&gt; 文档中的第一行：XML 声明 - 定义了文档中使用的 XML 版本和字符编码。此例子遵守 1.0 规范，并使用 UTF-8 字符集(可支持中文)。 下一行是标识此文档是一个 RSS 文档的 RSS 声明（此例是 RSS version 2.0）。 下一行含有 元素。此元素用于描述 RSS feed。 元素有三个必需的子元素： - 定义频道的标题。（比如 菜鸟教程首页） - 定义到达频道的超链接。（比如 www.runoob.com） - 描述此频道（比如 免费编程教程） 每个 元素可拥有一个或多个 元素。 每个 元素可定义 RSS feed 中的一篇文章或 “story”。 元素拥有三个必需的子元素： - 定义项目的标题。（比如 RSS 教程） - 定义到达项目的超链接。（比如 http://www.runoob.com/rss） - 描述此项目（比如 菜鸟教程 Rss 教程） 最后，后面的两行关闭 和 元素。 rss发布到web现在是时候把您的 RSS 文件上传到网上了。下面是具体的步骤： 1.为您的 RSS 命名。请注意文件必须有 .xml 的后缀。 验证您的 RSS 文件。（可以在 http://www.feedvalidator.org 找到很好的验证器）。 把 RSS 文件上传到您的 web 服务器上的 web 目录。 把这个小的橙色按钮 .eoutqhyfdcrf{zoom:80%;}或 .nivdsfcluukq{zoom:80%;}拷贝到您的 web 目录。 在你希望向外界提供 RSS 的页面上放置这个小按钮。然后向这个按钮添加一个指向 RSS 文件的链接。代码应该类似这样： 123&lt;a href=\"http://www.runoob.com/feed\"&gt;&lt;img loading=\"lazy\" src=\"http://www.runoob.com/images/rss.gif\" width=\"36\" height=\"14\"&gt;&lt;/a&gt; 把你的 RSS feed 提交到 RSS Feed 目录。要注意！feed 的 URL 不是你的页面，而是您的指向您的 feed 的 URL，比如 “http://www.runoob.com/feed\"。 此处提供一些免费的 RSS 聚合服务： Newsisfree: 点我注册 在重要的搜索引擎注册您的 feed ： WordPress Blogger Radio 更新您的 feed - 现在您已获得了来自 Google、Yahoo、以及 MSN 的 RSS feed 按钮。请您务必经常更新您的内容，并保持 RSS feed 的长期可用。 元素 元素 描述 category 可选的。为 feed 定义所属的一个或多个种类。 cloud 可选的。注册进程，以获得 feed 更新的立即通知。 copyright 可选。告知版权资料。 description 必需的。描述频道。 docs 可选的。规定指向当前 RSS 文件所用格式说明的 URL。 generator 可选的。规定用于生成 feed 的程序。 image 可选的。在聚合器呈现某个 feed 时，显示一个图像。 language 可选的。规定编写 feed 所用的语言。 lastBuildDate 可选的。定义 feed 内容的最后修改日期。 link 必需的。定义指向频道的超链接。 manageingEditor 可选的。定义 feed 内容编辑的电子邮件地址。 pubDate 可选的。为 feed 的内容定义最后发布日期。 可选的。feed 的 PICS 级别。 skipDays 可选的。规定忽略 feed 更新的天。 skipHours 可选的。规定忽略 feed 更新的小时。 textInput 可选的。规定应当与 feed 一同显示的文本输入域。 title 必需的。定义频道的标题。 ttl 可选的。指定从 feed 源更新此 feed 之前，feed 可被缓存的分钟数。 webMaster 可选的。定义此 feed 的 web 管理员的电子邮件地址。 元素 元素 描述 author 可选的。规定项目作者的电子邮件地址。 category 可选的。定义项目所属的一个或多个类别。 comments 可选的。允许项目连接到有关此项目的注释（文件）。 description 必需的。描述此项目。 enclosure 可选的。允许将一个媒体文件导入一个项中。 guid 可选的。为项目定义一个唯一的标识符。 link 必需的。定义指向此项目的超链接。 pubDate 可选的。定义此项目的最后发布日期。 source 可选的。为此项目指定一个第三方来源。 title 必需的。定义此项目的标题。","categories":[{"name":"rss","slug":"rss","permalink":"http://pistachio0812.github.io/categories/rss/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"rss","slug":"rss","permalink":"http://pistachio0812.github.io/tags/rss/"}],"author":"pistachio"},{"title":"jsDelivr+Github 使用方法","slug":"免费cdn","date":"2022-10-07T15:10:08.661Z","updated":"2022-10-07T15:39:37.552Z","comments":true,"path":"zh-CN/免费cdn/","permalink":"http://pistachio0812.github.io/zh-CN/%E5%85%8D%E8%B4%B9cdn/","excerpt":"","text":"参考博文： 1.免费CDN：jsDelivr+Github 使用方法 由于本人搭建博客需要换字体，又怕别人的链接失效，因此，想要把资源把握在自己的手里，通过查阅相关资料，找到上面的方法，尝试了一下，这里做下记录。 放在Github的资源在国内加载速度比较慢，因此需要使用CDN加速来优化网站打开速度，jsDelivr + Github便是免费且好用的CDN，非常适合博客网站使用。 新建github仓库.gscparesnzjs{zoom: 50%;} 克隆仓库到本地.fgxtkwkbtmfr{zoom:50%;} 在本地目录右键 Git Bash Here，执行以下命令： 1git clone git@github.com:pistachio0812/CDN.git 上传资源复制需要上传的资源到本地git仓库（注：jsDelivr不支持加载超过20M的资源），在本地git仓库目录下右键 Git Bash Here，执行以下命令： 1234git status //查看状态git add . //添加所有文件到暂存区git commit -m '第一次提交' //把文件提交到仓库git push //推送至远程仓库 .rqyphbqjgumc{zoom:50%;} 发布仓库点击release发布 .avrjiflwogbr{zoom:50%;} 自定义发布版号 .ccrsrxghqmnv{zoom:50%;} 通过jsDelivr引用资源使用方法：https://cdn.jsdelivr.net/gh/你的用户名/你的仓库名@发布的版本号/文件路径 1https://cdn.jsdelivr.net/gh/pistachio0812/CDN@0.1.0/cdn-fonts/MaShanZheng.woff2 注意：版本号不是必需的，是为了区分新旧资源，如果不使用版本号，将会直接引用最新资源，除此之外还可以使用某个范围内的版本，查看所有资源等，具体使用方法如下： 1234567891011121314151617// 加载任何Github发布、提交或分支https://cdn.jsdelivr.net/gh/user/repo@version/file// 加载 jQuery v3.2.1https://cdn.jsdelivr.net/gh/jquery/jquery@3.2.1/dist/jquery.min.js// 使用版本范围而不是特定版本https://cdn.jsdelivr.net/gh/jquery/jquery@3.2/dist/jquery.min.js https://cdn.jsdelivr.net/gh/jquery/jquery@3/dist/jquery.min.js // 完全省略该版本以获取最新版本https://cdn.jsdelivr.net/gh/jquery/jquery/dist/jquery.min.js // 将“.min”添加到任何JS/CSS文件中以获取缩小版本，如果不存在，将为会自动生成https://cdn.jsdelivr.net/gh/jquery/jquery@3.2.1/src/core.min.js // 在末尾添加 / 以获取资源目录列表https://cdn.jsdelivr.net/gh/jquery/jquery/","categories":[{"name":"github","slug":"github","permalink":"http://pistachio0812.github.io/categories/github/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"CDN","slug":"CDN","permalink":"http://pistachio0812.github.io/tags/CDN/"}],"author":"pistachio"},{"title":"typora数学公式教程","slug":"typora数学公式教程","date":"2022-09-27T03:16:53.509Z","updated":"2022-10-04T13:30:15.956Z","comments":true,"path":"zh-CN/typora数学公式教程/","permalink":"http://pistachio0812.github.io/zh-CN/typora%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%95%99%E7%A8%8B/","excerpt":"","text":"参考博文： 1.Typora数学公式输入指导手册 上面这个链接教程非常详细，有时间我会一个一个测试，有啥补充的可以私信我","categories":[{"name":"typora","slug":"typora","permalink":"http://pistachio0812.github.io/categories/typora/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"author":"coolboy"},{"title":"epoch和iteration的区别","slug":"epoch和iteration的区别","date":"2022-09-27T02:40:11.597Z","updated":"2022-10-04T13:09:36.966Z","comments":true,"path":"zh-CN/epoch和iteration的区别/","permalink":"http://pistachio0812.github.io/zh-CN/epoch%E5%92%8Citeration%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"参考博文： 1.epoch和iteration的区别 （1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；（2）iteration：1个iteration等于使用batchsize个样本训练一次；一个迭代 = 一个正向通过+一个反向通过（3）epoch：1个epoch等于使用训练集中的全部样本训练一次；一个epoch = 所有训练样本的一个正向传递和一个反向传递 举个例子，训练集有1000个样本，batchsize=10，那么：训练完整个样本集需要：100次iteration，1次epoch。 但是也有人说在有些文章中epoch和iteration是一个概念。 还有就是随机取batchsize，那么并不能保证所有的样本都被抽到，那么epoch的概念不明确。","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"基础概念","slug":"基础概念","permalink":"http://pistachio0812.github.io/tags/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"},{"name":"epoch&iteration","slug":"epoch-iteration","permalink":"http://pistachio0812.github.io/tags/epoch-iteration/"}],"author":"Daniel"},{"title":"开放图谱协议","slug":"开放图谱协议","date":"2022-09-26T12:21:00.458Z","updated":"2022-10-04T13:42:30.783Z","comments":true,"path":"zh-CN/开放图谱协议/","permalink":"http://pistachio0812.github.io/zh-CN/%E5%BC%80%E6%94%BE%E5%9B%BE%E8%B0%B1%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"参考博文： 1.什么是 Open Graph 标签？不懂你还做什么社交营销优化？ 2.https://smo.knowem.com/ 3.辅助函数（Helpers） | Hexo 今天在继续配置我的博客时，发现有一个open_graph不懂，结果一查才知道这是facebook推的一个开放图谱协议，用于分享网站的时候用，于是抱着学习的目的，继续深挖下去 按照第一个链接的方法，我查到了我网站的开放协议配置情况 .rkytgwgvobtr{} 和配置情况对比一下，是一样的 open graph protocol和_config.yml是一致的，twitter card info和主题volantis配置文件是一致的 1234# _config.volantis.yml文件open_graph: image: volantis-static/media/org.volantis/blog/favicon/android-chrome-192x192.png # https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/favicon/android-chrome-192x192.png twitter_card: summary # summary_large_image , summary 如果想配置里面的内容，打开第三个链接 .ygtwnovahoil{zoom:50%;} 原来的配置实在太简陋了，分享了网站估计也没人看，那就开始配置吧 123456open_graph: title: page.title url: url author: config.author image: # https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/favicon/android-chrome-192x192.png twitter_card: summary # summary_large_image , summary 结束了，以后如果还有改进再来补充","categories":[{"name":"Hexo博客搭建","slug":"Hexo博客搭建","permalink":"http://pistachio0812.github.io/categories/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://pistachio0812.github.io/tags/Hexo/"},{"name":"open_graph","slug":"open-graph","permalink":"http://pistachio0812.github.io/tags/open-graph/"}],"author":"Daniel"},{"title":"ubuntu软件安装的问题总结","slug":"ubuntu安装软件问题","date":"2022-09-26T01:12:36.010Z","updated":"2022-10-04T13:30:34.077Z","comments":true,"path":"zh-CN/ubuntu安装软件问题/","permalink":"http://pistachio0812.github.io/zh-CN/ubuntu%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E9%97%AE%E9%A2%98/","excerpt":"","text":"参考链接： 1.ubuntu安装软件时，status-code=409报错解决方案 2. 解决ubuntu软件商店无法安装软件提示snap问题 今天一大早上想去虚拟机上（Linux）看看昨天晚上没安装的软件，给重新安装一下，打开软件商店，结果遇到两个问题，昨天晚上我是用软件更新器安装了一会就退出关闭了 issue1: 安装火狐浏览器时，显示无法安装更新，status-code=409 解决方案： 12345(base) z@E580:~$ snap changesID Status Spawn Ready Summary6 Done 6 days ago, at 16:03 CST 6 days ago, at 16:04 CST 自动刷新 snap \"snapd\"7 Doing today at 16:40 CST - Install \"xmind\" snap from \"latest/stable\" channel 找到正在安装的软件，将其强行关闭 12(base) z@E580:~$ snap abort 7 重新安装 issue2: .jllieibohwew{} 原因是我们之前安装软件的时候没有安装完成就推出软件商店了，他已经安装了，只是没有安装完成而以， 解决办法： 1.查看安装详情： 1snap changes .pwnhocjnowbh{} ID7的进程是我之前安装失败的 2.清除当前安装，然后再重新安装，命令如下： 1snap abort 7 3.重新去软件商店安装 其实，还有另外一种办法，就是重新去软件更新器安装，继续完成更新，没有必要去软件商店更新，希望对你有所启发","categories":[{"name":"ubuntu","slug":"ubuntu","permalink":"http://pistachio0812.github.io/categories/ubuntu/"}],"tags":[{"name":"软件安装","slug":"软件安装","permalink":"http://pistachio0812.github.io/tags/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"}],"author":"coolboy"},{"title":"CABNet论文笔记","slug":"CABNet","date":"2022-09-25T02:28:30.021Z","updated":"2022-10-04T12:53:45.962Z","comments":true,"path":"zh-CN/CABNet/","permalink":"http://pistachio0812.github.io/zh-CN/CABNet/","excerpt":"","text":"参考博文： 1.Sci-Hub | Context-Aware Block Net for Small Object Detection | 10.1109/TCYB.2020.3004636 (et-fine.com) 今天看的这篇论文是吕培教授发表在IEEE TRANS上的Context-Aware Block Net for Small Object Detection,ok,开始进入正文 摘要CABNet能够捕捉基本的视觉模式，也可以捕捉小物体的语义信息。看到这个，其实我也有点懵，基本的视觉模式是啥，搞不懂，这个得好好查查，还有就是如何捕捉的呢，那得继续看论文喽，毕竟这只是摘要 引言文中引入文献，讲了几个近些年提升小目标检测性能的方法，第一种：用高分辨率特征图做预测，这样做是因为这些特征图能保留小目标的细节信息，如下图a所示，作者认为不可取，因为高分辨率特征图包含的上下文信息较少，会影响检测精度。第二种：如下图b所示，带有跳跃连接的自上而下的结构，能够在所有尺度上构建高级语义信息。这些引入了额外的上下文信息给高分辨率特征图，因此，提高了检测精度，然而，作者认为还是不可取，因为在训练和测试的时候计算开销很大，如果网络中使用了下采样，就会丢失小目标的信息，这是不可恢复的。 .nvobieffvcdi{} 有文献指出，对于小目标检测，高分辨率特征图的表征更适合准确定位目标，那为什么不保持更高分辨率的特征图（64×64）来检测小目标呢，如下图所示，主要的原因高分辨率特征图底层的神经元产生的感受野是有限的，也就是说，在特征图上包含的上下文信息也是受到限制的。 .kvsrsolvphkk{} 因此，如下图c所示，在CABNet上，只会在骨干网络（文中指VGG16)上下采样几次，目的就是保持小目标的空间信息（未完。。。） .jcwqifjctbft{zoom:80%;} 相关工作A目标检测 介绍了很多文献，总结下来就是小目标的检测性能之所以差劲，关键原因就是在一个深的网络里过度下采样了。 B小目标检测 由于内容太杂，简单说一下吧，里面介绍了前人的一些工作，比如，有人通过缩小小物体和大物体的表征差异来提升小目标的检测性能；有人通过简单的粘贴复制小目标多次提升性能；有人通过增加输入图片的大小来提升小目标的检测性能。 C空洞卷积 使用空洞卷积是能够丰富特征图的语义信息的，文章中提到了RFBNet使用空洞卷积保证了特征的可区分性和鲁棒性，受此启发，CAB通过不同扩张率的金字塔扩张卷积来包含多级上下文信息，如下图所示。 .ieojgbtyoarh{zoom:80%;} 思想上下文模块","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"}],"author":"coolboy"},{"title":"Warnings_unsample","slug":"warning_unsample","date":"2022-09-22T13:20:43.936Z","updated":"2022-10-04T13:34:23.332Z","comments":true,"path":"zh-CN/warning_unsample/","permalink":"http://pistachio0812.github.io/zh-CN/warning_unsample/","excerpt":"","text":"参考博文： 1.warnings.warn(“nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.” 刚刚想测试一下grad-cam的代码，结果模型碰到一个问题，就是nn.functional.unsample被抛弃了 问题： UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead. .xvylcvhrqqyd{zoom:50%;} 代码： 1feat1 = F.upsample_bilinear(self.fusion_layers[1](feat1), size=(38, 38)) 错误原因： python版本问题，python3.5 支持 upsample 函数，python3.6 不支持 upsample 函数 假如我们忽略这个警告，会导致实验效果降低，简单来说，这个警告一定要改 解决办法： 1feat1 = F.interpolate(self.fusion_layers[1](feat1), size=(38, 38), mode='bilinear', align_corners=True)","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://pistachio0812.github.io/categories/pytorch/"}],"tags":[{"name":"warnings","slug":"warnings","permalink":"http://pistachio0812.github.io/tags/warnings/"}],"author":"coolboy"},{"title":"Grad-cam论文笔记","slug":"grad-cam","date":"2022-09-22T11:37:18.237Z","updated":"2022-10-04T13:15:09.178Z","comments":true,"path":"zh-CN/grad-cam/","permalink":"http://pistachio0812.github.io/zh-CN/grad-cam/","excerpt":"","text":"参考博文： 1.Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization 2.jacobgil/pytorch-grad-cam: Advanced AI Explainability for computer vision. Support for CNNs, Vision Transformers, Classification, Object detection, Segmentation, Image similarity and more. (github.com) 3.一个库可视化类激活热力图Grad-CAM pytorch版本 4. libpng warning: iCCP: known incorrect sRGB profile 警告 下载源码从第二个参考博文里下载源码或者直接用pip安装，安装命令如下 1pip install grad-cam 解读源码现解读的是源码根目录下的cam.py文件，即它的使用代码，采取注释的方式解读： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151import argparseimport cv2import numpy as npimport torchfrom torchvision import models# 这里默认你了解它的各种变体，这里有11种变体，有时间我会各个解读一下，你们也可以自行了解from pytorch_grad_cam import GradCAM, \\ HiResCAM, \\ ScoreCAM, \\ GradCAMPlusPlus, \\ AblationCAM, \\ XGradCAM, \\ EigenCAM, \\ EigenGradCAM, \\ LayerCAM, \\ FullGrad, \\ GradCAMElementWise from pytorch_grad_cam import GuidedBackpropReLUModelfrom pytorch_grad_cam.utils.image import show_cam_on_image, \\ deprocess_image, \\ preprocess_imagefrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget# 这里是命令行参数，等下给出运行命令你就一下子明白了，最重要的就是选择method,必须得跟choices里的一样def get_args(): parser = argparse.ArgumentParser() parser.add_argument('--use-cuda', action='store_true', default=False, help='Use NVIDIA GPU acceleration') parser.add_argument( '--image-path', type=str, default='./examples/both.png', help='Input image path') parser.add_argument('--aug_smooth', action='store_true', help='Apply test time augmentation to smooth the CAM') parser.add_argument( '--eigen_smooth', action='store_true', help='Reduce noise by taking the first principle componenet' 'of cam_weights*activations') parser.add_argument('--method', type=str, default='gradcam', choices=['gradcam', 'hirescam', 'gradcam++', 'scorecam', 'xgradcam', 'ablationcam', 'eigencam', 'eigengradcam', 'layercam', 'fullgrad'], help='Can be gradcam/gradcam++/scorecam/xgradcam' '/ablationcam/eigencam/eigengradcam/layercam') args = parser.parse_args() args.use_cuda = args.use_cuda and torch.cuda.is_available() if args.use_cuda: print('Using GPU for acceleration') else: print('Using CPU for computation') return argsif __name__ == '__main__': \"\"\" python cam.py -image-path &lt;path_to_image&gt; Example usage of loading an image, and computing: 1. CAM 2. Guided Back Propagation 3. Combining both \"\"\" args = get_args() methods = \\ {\"gradcam\": GradCAM, \"hirescam\":HiResCAM, \"scorecam\": ScoreCAM, \"gradcam++\": GradCAMPlusPlus, \"ablationcam\": AblationCAM, \"xgradcam\": XGradCAM, \"eigencam\": EigenCAM, \"eigengradcam\": EigenGradCAM, \"layercam\": LayerCAM, \"fullgrad\": FullGrad, \"gradcamelementwise\": GradCAMElementWise} # 这里就可以替换你的模型了，用之前记得引入自己的模型 model = models.resnet50(pretrained=True) # Choose the target layer you want to compute the visualization for. # Usually this will be the last convolutional layer in the model. # Some common choices can be: # Resnet18 and 50: model.layer4 # VGG, densenet161: model.features[-1] # mnasnet1_0: model.layers[-1] # You can print the model to help chose the layer # You can pass a list with several target layers, # in that case the CAMs will be computed per layer and then aggregated. # You can also try selecting all layers of a certain type, with e.g: # from pytorch_grad_cam.utils.find_layers import find_layer_types_recursive # find_layer_types_recursive(model, [torch.nn.ReLU]) # 选择你想要可视化的特征层 target_layers = [model.layer4] # 读取图片 rgb_img = cv2.imread(args.image_path, 1)[:, :, ::-1] # 归一化 rgb_img = np.float32(rgb_img) / 255 # 预处理图片 input_tensor = preprocess_image(rgb_img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # We have to specify the target we want to generate # the Class Activation Maps for. # If targets is None, the highest scoring category (for every member in the batch) will be used. # You can target specific categories by # targets = [e.g ClassifierOutputTarget(281)] # 如果target设置为None,就会选择得分最高的类别 targets = None # Using the with statement ensures the context is freed, and you can # recreate different CAM objects in a loop. cam_algorithm = methods[args.method] with cam_algorithm(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam: # AblationCAM and ScoreCAM have batched implementations. # You can override the internal batch size for faster computation. cam.batch_size = 32 grayscale_cam = cam(input_tensor=input_tensor, targets=targets, aug_smooth=args.aug_smooth, eigen_smooth=args.eigen_smooth) # Here grayscale_cam has only one image in the batch grayscale_cam = grayscale_cam[0, :] # 展示图片 cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True) # cam_image is RGB encoded whereas \"cv2.imwrite\" requires BGR encoding. cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR) gb_model = GuidedBackpropReLUModel(model=model, use_cuda=args.use_cuda) gb = gb_model(input_tensor, target_category=None) cam_mask = cv2.merge([grayscale_cam, grayscale_cam, grayscale_cam]) cam_gb = deprocess_image(cam_mask * gb) gb = deprocess_image(gb) cv2.imwrite(f'{args.method}_cam.jpg', cam_image) cv2.imwrite(f'{args.method}_gb.jpg', gb) cv2.imwrite(f'{args.method}_cam_gb.jpg', cam_gb) 这里有很多类都是源码里写了的，可以看源码了解一下，将该代码放在你的模型下，另起一个文件，导入自己的模型即可 Grad-cam应用上述代码放在test.py文件里了 终端命令：python test.py --image-path zophie.png --method gradcam --use-cuda .msqixvzlbgjt{} issue: 1.可能有一两个包没用，比如HiResCAM,GradCAMElementWise 这很简单，就是这个代码人家一直在更新，你只要使用pip uninstall grad-cam卸载,然后重新使用pip install grad-cam命令安装一遍就行了 2.显示libpng warning: iCCP: known incorrect sRGB profile 放心，这只是一个警告啦，具体参考博文4 结果显示： 1.gradcam_cam.jpg .kwtdvplkbwgd{zoom:25%;} 2.gradcam_cam_gb.jpg .epcqobzpmsxe{zoom:25%;} 3.gradcam_gb.jpg .ldhwyovwmksr{zoom:25%;} 可以看出文件命名方式为类激活方法+预处理方法，有兴趣的可以去看看预处理方法，代码里有写","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"grad-cam","slug":"grad-cam","permalink":"http://pistachio0812.github.io/tags/grad-cam/"}],"author":"pistachio"},{"title":"visdom使用教程","slug":"visdom使用教程","date":"2022-09-20T13:25:26.535Z","updated":"2022-10-04T13:31:37.021Z","comments":true,"path":"zh-CN/visdom使用教程/","permalink":"http://pistachio0812.github.io/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/","excerpt":"","text":"参考博文： 1.PyTorch深度学习训练可视化工具visdom 在进行深度学习实验时，能够可视化地对训练过程和结果进行展示是非常有必要的。除了Torch版本的TensorBoard工具TensorBoardX之外，Torch官方也提供了一款非常好用的可视化神器——visdom。visdom是一款用于创建、组织和共享实时大量训练数据可视化的灵活工具。 .unppvznlypcs{} 深度学习模型训练通常放在远程的服务器上，服务器上训练的一个问题就在于不能方便地对训练进行可视化，相较于TensorFlow的可视化工具TensorBoard，visdom则是对应于PyTorch的可视化工具。 安装与启动 直接通过pip install visdom即可完成安装，之后在终端输入如下命令即可启动visdom服务： 1python -m visdom.server 启动服务后输入本地或者远程地址，端口号8097，即可打开visdom主页。 主要元素visdom界面简单，主要构成元素包括窗口(Windows)、环境(Environments)、状态(State)、过滤(Filter)、视图(Views)等。 环境：用于对可视化空间进行分区，比如在对训练进行可视化的时候我们可以在一个环境里对loss进行可视化，在另一个环境下对训练的输入输出进行可视化。 .cmgmijusluat{} 状态：visdom会自动缓存你创建的可视化内容，当页面关闭之后，重新加载便可恢复这些内容。 过滤：可用于筛选可视化窗口，快速查找。 .lbzpqkkwmdwh{} 视图：可以快速地对可视化窗口进行排列和管理。 应用示例 visdom将可以进行可视化的对象都放在基础模块中，包括单/多张图像、文本、语音、视频、svg矢量图、属性网格、matplotlib绘图对象、序列化状态对象等。基础图形由plotly提供，主要包括散点图、折线图、热图、茎叶图、柱形图、箱线图、表面图、等高线图、网格图等。 以matplotlib绘图对象为例进行展示。 .yhsjtziwywgm{} 具体到深度学习训练时，我们可以在torch训练代码下插入visdom的可视化模块： 123456if args.steps_plot &gt; 0 and step % args.steps_plot == 0: image = inputs[0].cpu().data vis.image(image,f'input (epoch: {epoch}, step: {step})') vis.image(outputs[0].cpu().max(0)[1].data, f'output (epoch: {epoch}, step: {step})') vis.image(targets[0].cpu().data, f'target (epoch: {epoch}, step: {step})') vis.image(loss, f'loss (epoch: {epoch}, step: {step})') 将上述模块插入到VOC 2012语义分割训练中，效果如下： .xdoenpqsrhgm{} 也可以监控训练过程中的loss变化： .wdrxawzmkffv{} 问题集合1.第一次安装后想运行试试看，结果出现了以下问题： 1234Traceback (most recent call last): File \"E:/code_set/ssd-v5/frs.py\", line 3, in &lt;module&gt; vis = visdom.Vidom(env='model_1')AttributeError: module 'visdom' has no attribute 'Vidom' 通过查阅相关资料，发现是自己导包导错了，写的时候并没有报错，很不容易发现 错误代码： 1234import visdomimport torchvis = visdom.Vidom(env='model_1')vis.text('Hello World', win='text1') 正确代码： 1234import visdomimport torchvis = visdom.Visdom(env='model_1')vis.text('Hello World', win='text1') 还可能是因为将文件命名成了visdom.py,导致代码内调包时引用了同级目录内的文件，具体参考：使用visdom时遇到问题 AttributeError: module ‘visdom’ has no attribute ‘Visdom’ 第一次成功应用visdom模块跑线性图 代码如下： 12345678910111213import visdomimport torchvis = visdom.Vidom(env='model_1')vis.text('Hello World', win='text1')vis2 = visdom.Visdom(env='test2')for ii in range(0,10): x = torch.Tensor([ii]) y = x vis2.line(X=x,Y=y,win='polynomial',update='append' if ii&gt;0 else None)x = torch.arange(0,9,0.1)y = (x ** 2)/9vis2.line(X=x,Y=y,win='polynomial',name = 'this is new trace',update='append') 结果： .dqmcsrhxpglc{} 记得选择环境test2 未完待续。。。。。。。。","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://pistachio0812.github.io/categories/pytorch/"}],"tags":[{"name":"可视化","slug":"可视化","permalink":"http://pistachio0812.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"}],"author":"pistachio"},{"title":"zetane的使用说明","slug":"zetane的使用","date":"2022-09-20T09:09:50.015Z","updated":"2022-10-04T13:36:17.373Z","comments":true,"path":"zh-CN/zetane的使用/","permalink":"http://pistachio0812.github.io/zh-CN/zetane%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"未完待续。。。","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"可视化","slug":"可视化","permalink":"http://pistachio0812.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"zetane","slug":"zetane","permalink":"http://pistachio0812.github.io/tags/zetane/"}],"author":"pistachio"},{"title":"matplotlib.use('agg')的作用机理","slug":"use_agg","date":"2022-09-17T13:26:58.004Z","updated":"2022-10-04T13:31:21.985Z","comments":true,"path":"zh-CN/use_agg/","permalink":"http://pistachio0812.github.io/zh-CN/use_agg/","excerpt":"","text":"博文链接： matplotlib.use(‘agg‘)“语句的作用机理 matplotlib中什么是后端 问题：在检查SSD代码时，在callback.py文件中发现出现了matplotlib.use('agg')的语句。PyCharm中不显示绘图。 相关示例： 123456789101112import numpy as npimport matplotlibmatplotlib.use('agg') import matplotlib.pyplot as plt # matplotlib.use('agg')必须在本句执行前运行x = np.arange(0, 2*np.pi, 0.001) y = np.sin(2 * np.pi * x) plt.clf() plt.plot(x,y) l = plt.axhline(linewidth=1, color='black') l = plt.axvline(linewidth=1, color='black') plt.show() 原理分析： 实际上，这样的理解是表面的，这个语句确实会使得在Pycharm运行时无法显示图，但是必须注意，这是其原理导致的，而这个语句并不是设置Pycharm不显示图的语句，其实前述的代码中，删掉 plt.show()，也不会显示图片。 matplotlib的use()命令其实是用来配置matplotlib的backend（后端）的命令。所谓后端，就是一个渲染器，用于将前端代码渲染成我们想要的图像。后端详细的解释可参考博客：matplotlib中什么是后端 对于用户接口，典型的渲染器是Agg，它是使用Anti-Grain Geometry C++库来产生光栅(像素)图。 那么为什么这样设置Pycharm会导致其不显示图片呢？ 可以查看一下目前的后端设置。方法是执行下面代码。 123&gt;&gt;&gt; import matplotlib&gt;&gt;&gt; matplotlib.get_backend()Qt5Agg 也就是说，Pycharm运行的时候，默认的后端是Qt5Agg。 实际上，Agg 渲染器是非交互式的后端，没有GUI界面，所以不显示图片，它是用来生成图像文件。Qt5Agg 是意思是Agg渲染器输出到Qt5绘图面板，它是交互式的后端，拥有在屏幕上展示的能力","categories":[{"name":"python","slug":"python","permalink":"http://pistachio0812.github.io/categories/python/"}],"tags":[{"name":"matplotlib","slug":"matplotlib","permalink":"http://pistachio0812.github.io/tags/matplotlib/"}],"author":"pistachio"},{"title":"wandb","slug":"wandb","date":"2022-09-16T11:41:25.886Z","updated":"2022-10-04T13:33:49.427Z","comments":true,"path":"zh-CN/wandb/","permalink":"http://pistachio0812.github.io/zh-CN/wandb/","excerpt":"","text":"未完待续，后面补充","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"可视化","slug":"可视化","permalink":"http://pistachio0812.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"}],"author":"pistachio"},{"title":"pyperclip模块","slug":"pyperclip","date":"2022-09-08T03:17:47.989Z","updated":"2022-10-04T13:25:23.374Z","comments":true,"path":"zh-CN/pyperclip/","permalink":"http://pistachio0812.github.io/zh-CN/pyperclip/","excerpt":"","text":"相关链接：python之Pyperclip pyperclip模块有copy()和paste()两个模块，你需要安装该模块 使用代码如下： 1234&gt;&gt;&gt; import pyperclip&gt;&gt;&gt; pyperclip.copy('Hello world!')&gt;&gt;&gt; pyperclip.paste()'Hello world!' 当然，如果你的程序之外的某个程序改变了剪贴板的内容，paste()函数就会返回它。 总结：这个模块在写程序时可以用来读取外面剪贴板的内容，或者读取你需要读取已经写入的某个内容，方便快捷","categories":[{"name":"python","slug":"python","permalink":"http://pistachio0812.github.io/categories/python/"}],"tags":[{"name":"函数库","slug":"函数库","permalink":"http://pistachio0812.github.io/tags/%E5%87%BD%E6%95%B0%E5%BA%93/"}],"author":"pistachio"},{"title":"np.newaxis详解","slug":"np_newaxis","date":"2022-09-07T09:21:34.917Z","updated":"2022-10-04T13:23:43.950Z","comments":true,"path":"zh-CN/np_newaxis/","permalink":"http://pistachio0812.github.io/zh-CN/np_newaxis/","excerpt":"","text":"原文链接：np.newaxis作用详解—-超简单理解方式，通透 np.newaxis的作用就是在这一位置增加一个一维，这一位置指的是np.newaxis所在的位置，举个例子如下。 12345678910111213x1 = np.array([1, 2, 3, 4, 5])# the shape of x1 is (5,)x1_new = x1[:, np.newaxis]# now, the shape of x1_new is (5, 1)# array([[1],# [2],# [3],# [4],# [5]])x1_new = x1[np.newaxis,:]# now, the shape of x1_new is (1, 5)# array([[1, 2, 3, 4, 5]])","categories":[{"name":"python","slug":"python","permalink":"http://pistachio0812.github.io/categories/python/"}],"tags":[{"name":"函数库","slug":"函数库","permalink":"http://pistachio0812.github.io/tags/%E5%87%BD%E6%95%B0%E5%BA%93/"},{"name":"newaxis","slug":"newaxis","permalink":"http://pistachio0812.github.io/tags/newaxis/"}],"author":"pistachio"},{"title":"CNN输入与输出的关系","slug":"CNN输入详解","date":"2022-09-06T09:01:53.315Z","updated":"2022-10-04T12:57:27.722Z","comments":true,"path":"zh-CN/CNN输入详解/","permalink":"http://pistachio0812.github.io/zh-CN/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"原文链接：卷积神经网络里输入图像大小何时是固定，何时是任意 相关论文链接： 1.Fully Convolutional Networks for Semantic Segmentation 2.Network In Network 3.SPP-Net: Deep absolute pose regression with synthetic views 卷积神经网络里输入图像大小何时是固定，何时是任意典型的CNN架构如下： .joiilygrqgrt{} 也就是卷积层（卷积+非线性激活）+池化层+全连接层+分类层。其中，卷积层、池化层、分类层其实都不在意图像大小，但是全连接层有问题。 一般而言，全连接层的一个神经元对应一个输入。换句话说，全连接层要求固定的输入维度。而不同大小的图像，卷积模块（卷积+非线性激活+池化）输出的特征映射维度是不一样的。因此，从这个意义上说，因为有全连接层存在，决定了输入的图像的大小必须是固定的。也就是你提到的： 如果神经网络里不仅仅只有卷积层，还有全连接层，那么输入的图像的大小必须是固定的。 既然是全连接层的限制，那么，如果我们去掉全连接层，岂不就可以支持任意大小的输入图像了？也就是你提到的： 如果一个神经网络里面只有卷积层，那么我输入的图像大小是可以任意的 这正是Jonathan Long提出的FCN（Fully Convolutional Networks，全卷积网络）背后的思路，用卷积层替换全连接层。 .vihkvdyltmij{} 当然，除了卷积层外，还可以用别的层替换全连接层。比如用全局平均池化（Global Average Pooling）层替换全连接层。 .njpsxmtvdefh{} 全局平均池化最早是由Min Lin等在网中网（Network In Network）里提出的， 其动机是为了缓解全连接层可能导致的过拟合问题。但因为全局平均池化将任意h * w * d的张量（特征映射）转换为1 * 1 * d的张量，因此碰巧可以自动适应不同尺寸的输入图像。之后的Inception-V3等架构借鉴了这一想法，用全局平均池化层代替全连接层，以适应不同尺寸的输入图像。 不过，也不是说只要有全连接层，输入的图像大小就一定要固定。之前提到过，固定输入图像大小的原因是：全连接层要求固定的输入维度。那么，除了像上面这样干脆用卷积层替换全连接层，还有一种思路，就是在卷积模块和全连接层之间加一个中间层，整理一下卷积模块的输出，保证不管输入图像大小怎么变，传给全连接层的始终是固定维数的输入。 Kaiming He等提出的SPP（Spatial Pyramid Pooling，空间金字塔池化）就是做这个的。 .funyafyjgupe{} 上图示意了引入SPP前后的不同流程。上为传统的CNN架构，图像（image）经过裁剪或拉伸（crop/warp）统一尺寸，再传给卷积层（conv layers）；下为应用了SPP层后的架构，图像直接传给卷积层，然后经过SPP处理，统一维度，再传给全连接层（fc layers）。 .mlsvgtekqeoo{zoom:50%;} 上为SPP层示意图，当中的大方框表示SPP层。SPP层的输入是卷积模块输出的任意尺寸的特征映射，SPP层的输出是固定长度的表示，也就是说，SPP层将固定维度的向量传给之后的全连接层。其中，SPP层使用很多spatial bins（空间箱）对特征映射应用池化操作（比如最大池化）。空间箱的个数是固定的，大小与特征映射的尺寸（对应输入图像的尺寸）成比例，这就保证了SPP层的输出向量的维数是固定的。 个人总结如下： 1.当卷积神经网络中出现了全连接时，由于网络模型固定，因此倒推过去输入尺寸必然固定 2.全局平均池化可以将任意h×w×c的张量变为1×1×c的张量，因此可以输入任意尺寸 3.SPPNet在卷积层和全连接层之间设置中间层，将全连接层前的输入设置为满足要求，因此输入可以任何尺寸 4.论文中较为常见的是第一种","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"卷积神经网络","slug":"计算机视觉/卷积神经网络","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"输入与输出","slug":"输入与输出","permalink":"http://pistachio0812.github.io/tags/%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BA/"}],"author":"pistachio"},{"title":"Remove Element","slug":"leetcode01","date":"2022-09-06T02:16:09.023Z","updated":"2022-10-04T13:18:34.535Z","comments":true,"path":"zh-CN/leetcode01/","permalink":"http://pistachio0812.github.io/zh-CN/leetcode01/","excerpt":"","text":"原文链接：Remove Element · LeetCode题解 题目：Given an array and a value, remove all instances of that &gt; value in place and return the new length. The order of elements can be changed. It doesn’t matter what you leave beyond the new length. 作为开胃菜，我当然选取了最容易的一道题目，在一个数组里面移除指定value，并且返回新的数组长度。这题唯一需要注意的地方在于in place，不能新建另一个数组。 方法很简单，使用两个游标i，j，遍历数组，如果碰到了value，使用j记录位置，同时递增i，直到下一个非value出现，将此时i对应的值复制到j的位置上，增加j，重复上述过程直到遍历结束。这时候j就是新的数组长度。 题解： 12345678910111213141516class Solution{public: int removeElement(int A[], int n, int elem){ int i=0; int j=0; for(i=0;i&lt;n;i++){ if(A[i]==elem){ continue; } A[j]=A[i]; j++; } return j; } }; 举一个最简单的例子，譬如数组为1，2，2，3，2，4，我们需要删除2，首先初始化i和j为0，指向第一个位置，因为第一个元素为1，所以A[0] = A[0]，i和j都加1，而第二个元素为2，我们递增i，直到碰到3，此时A[1] = A[3]，也就是3，递增i和j，这时候下一个元素又是2，递增i，直到碰到4，此时A[2] = A[5]，也就是4，再次递增i和j，这时候数组已经遍历完毕，结束。这时候j的值为3，刚好就是新的数组的长度。","categories":[{"name":"Leetcode","slug":"Leetcode","permalink":"http://pistachio0812.github.io/categories/Leetcode/"}],"tags":[{"name":"力扣题解","slug":"力扣题解","permalink":"http://pistachio0812.github.io/tags/%E5%8A%9B%E6%89%A3%E9%A2%98%E8%A7%A3/"}],"author":"pistachio"},{"title":"python知识补充","slug":"python知识补充","date":"2022-07-02T02:11:12.000Z","updated":"2022-10-04T13:25:57.547Z","comments":true,"path":"zh-CN/python知识补充/","permalink":"http://pistachio0812.github.io/zh-CN/python%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/","excerpt":"","text":"1.为源文件指定不同的字符编码 12345# 语法，python源文件默认使用UTF-8编码,特殊的编码注释必须在文件的第一行或者第二行定义# -*- coding: encoding -*-eg:# 使用windows-1252编码# -*- coding: cp-1252 -*- 2.交互模式中，最近一个表达式的值赋给变量_。我们可以把它当做一个桌面计算器，方便的用于连续计算。 12345678&gt;&gt;&gt; tax = 12.5 /100&gt;&gt;&gt; price = 100.5&gt;&gt;&gt; price * tax12.5625&gt;&gt;&gt; price + _113.0625&gt;&gt;&gt; round(_, 2)113.06 3.在交互式解释器中，输出的字符串会用引号引起来，特殊字符会用反斜杠转义，如果前面带有\\的字符被当做特殊字符，可以使用原始字符串，方法是在第一个引号前面加上一个r 12345&gt;&gt;&gt; print('c:\\user\\test')c:\\usertest&gt;&gt;&gt; print(r'c:\\user\\test')c:\\user\\test 4.python能够优雅的处理那些没有意义的切片：一个过大的索引值（即下标值大于字符串实际长度）将被字符串实际长度所代替，当上边界比下边界大时（即切片左值大于右值）就返回空字符串 12345# word = 'superman'&gt;&gt;&gt; word[4:42]'rman'&gt;&gt;&gt; word[42:]'' 5.序列的切片，一定要左边的数字小于右边的数字，否则返回空 12345678910&gt;&gt;&gt; lang'python'&gt;&gt;&gt; lst['python', 'java', 'c++']&gt;&gt;&gt; lang[-1:-3]''&gt;&gt;&gt; lang[-3:-1]'ho'&gt;&gt;&gt; lst[-3:-1]['python', 'java'] 6.反转 12345&gt;&gt;&gt; alst = [1, 2, 3, 4]&gt;&gt;&gt; alst[::-1][4, 3, 2, 1]&gt;&gt;&gt; list(reversed(alst))[4, 3, 2, 1] 7.append()和extend()是原地修改，没有返回值 12345&gt;&gt;&gt; one = ['good', 'good', 'study']&gt;&gt;&gt; another = one.extend(['day', 'day', 'up'])&gt;&gt;&gt; another #没有返回值&gt;&gt;&gt; one['good', 'good', 'study', 'day', 'day', 'up'] 8.extend()和append()区别 1234567&gt;&gt;&gt; lst = [1, 2, 3]&gt;&gt;&gt; lst.append(['quit', 'final'])&gt;&gt;&gt; lst[1, 2, 3, ['quit', 'final']]&gt;&gt;&gt; lst.extend(['test', 'different'])&gt;&gt;&gt; lst[1, 2, 3, ['quit', 'final'], 'test', 'different'] 9.如果元组只有一个元素时，应该在该元素后面加一个半角的英文逗号 123456&gt;&gt;&gt; a = (3)&gt;&gt;&gt; type(a)&lt;type 'int'&gt;&gt;&gt;&gt; b = (3, )&gt;&gt;&gt; type(b)&lt;type 'tuple'&gt; 10.","categories":[{"name":"python","slug":"python","permalink":"http://pistachio0812.github.io/categories/python/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"author":"pistachio"},{"title":"nn.relu和F.relu的区别","slug":"nn_relu和F_relu的区别","date":"2022-06-20T03:35:58.048Z","updated":"2022-10-04T13:22:14.815Z","comments":true,"path":"zh-CN/nn_relu和F_relu的区别/","permalink":"http://pistachio0812.github.io/zh-CN/nn_relu%E5%92%8CF_relu%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"参考博文： 1.PyTorch之nn.ReLU与F.ReLU的区别 示例代码： 1234567891011121314151617181920212223242526272829import torch.nn as nnimport torch.nn.functional as Fimport torch.nn as nn class AlexNet_1(nn.Module): def __init__(self, num_classes=n): super(AlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True), ) def forward(self, x): x = self.features(x) class AlexNet_2(nn.Module): def __init__(self, num_classes=n): super(AlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(64), ) def forward(self, x): x = self.features(x) x = F.ReLU(x) 在如上网络中，AlexNet_1与AlexNet_2实现的结果是一致的，但是可以看到将ReLU层添加到网络有两种不同的实现，即nn.ReLU和F.ReLU两种实现方法。其中nn.ReLU作为一个层结构，必须添加到nn.Module容器中才能使用，而F.ReLU则作为一个函数调用，看上去作为一个函数调用更方便更简洁。具体使用哪种方式，取决于编程风格。在PyTorch中,nn.X都有对应的函数版本F.X，但是并不是所有的F.X均可以用于forward或其它代码段中，因为当网络模型训练完毕时，在存储model时，在forward中的F.X函数中的参数是无法保存的。也就是说，在forward中，使用的F.X函数一般均没有状态参数，比如F.ReLU，F.avg_pool2d等，均没有参数，它们可以用在任何代码片段中。","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://pistachio0812.github.io/categories/pytorch/"}],"tags":[{"name":"函数库","slug":"函数库","permalink":"http://pistachio0812.github.io/tags/%E5%87%BD%E6%95%B0%E5%BA%93/"},{"name":"relu","slug":"relu","permalink":"http://pistachio0812.github.io/tags/relu/"}],"author":"pistachio"},{"title":"np.cumsum()函数","slug":"np_cumsum","date":"2022-06-12T09:29:40.260Z","updated":"2022-10-04T13:22:40.223Z","comments":true,"path":"zh-CN/np_cumsum/","permalink":"http://pistachio0812.github.io/zh-CN/np_cumsum/","excerpt":"","text":"参考博文： 1.numpy cumsum()函数简介 - zhengcixi - 博客园 (cnblogs.com) 2.numpy.cumsum — NumPy v1.10 Manual (scipy.org) 函数原型：numpy.``cumsum(a, axis=None, dtype=None, out=None) 可参考链接：https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.cumsum.html查看各个参数的含义。[](http://github.com/numpy/numpy/blob/v1.10.1/numpy/core/fromnumeric.py#L2038-L2106) 函数作用：求数组的所有元素的累计和，可通过参数axis指定求某个轴向的统计值。这里所说的轴可按照下图的含义理解： .evkshrgffuxc{} 下面举例进行说明： （1）不指定axis参数 1234561 &gt;&gt;&gt; a = np.array([[1, 2, 3], [4, 5, 6]])2 &gt;&gt;&gt; a3 array([[1, 2, 3],4 [4, 5, 6]])5 &gt;&gt;&gt; a.cumsum()6 array([ 1, 3, 6, 10, 15, 21], dtype=int32) 可以看出，不指定axis参数时，把二维数组当作了一维数组处理，进行累计求和运算。 （2）指定参数axis=0 1234567891 &gt;&gt;&gt; arr2 array([[0, 1, 2],3 [3, 4, 5],4 [6, 7, 8]])5 &gt;&gt;&gt; np.cumsum(arr, axis=0) 6 array([[ 0, 1, 2],7 [ 3, 5, 7],8 [ 9, 12, 15]], dtype=int32)9 &gt;&gt;&gt; np.cumsum(arr, axis=0)和arr.cumsum(axis=0)是一样的。可以看出，上述代码是按照轴0进行累计求和的。 （3）指定参数axis=1 123456781 &gt;&gt;&gt; arr2 array([[0, 1, 2],3 [3, 4, 5],4 [6, 7, 8]])5 &gt;&gt;&gt; arr.cumsum(axis=1)6 array([[ 0, 1, 3],7 [ 3, 7, 12],8 [ 6, 13, 21]], dtype=int32) 可以看出，上述代码是按照轴1进行累计求和的。 关于更高维的数组的运算就不测试了，暂时也用不上。","categories":[{"name":"python","slug":"python","permalink":"http://pistachio0812.github.io/categories/python/"}],"tags":[{"name":"cumsum","slug":"cumsum","permalink":"http://pistachio0812.github.io/tags/cumsum/"}],"author":"pistachio"},{"title":"pytorch中contiguous函数的用法","slug":"contiguous","date":"2022-06-12T07:41:31.468Z","updated":"2022-10-04T13:02:49.361Z","comments":true,"path":"zh-CN/contiguous/","permalink":"http://pistachio0812.github.io/zh-CN/contiguous/","excerpt":"","text":"参考博文： Pytorch之contiguous的用法 函数会使tensor变量在内存中的存储变得连续。 contiguous,()：view只能用在contiguous的variable上。如果在view之前用了transpose%2C permute等，需要用contiguous ()来返回一个contiguous copy。) contiguous tensor变量调用contiguous()函数会使tensor变量在内存中的存储变得连续。 contiguous()：view只能用在contiguous的variable上。如果在view之前用了transpose, permute等，需要用contiguous()来返回一个contiguous copy。 一种可能的解释是： 有些tensor并不是占用一整块内存，而是由不同的数据块组成，而tensor的view()操作依赖于内存是整块的，这时只需要执行contiguous()这个函数，把tensor变成在内存中连续分布的形式。 is_contiguous 判断是否contiguous用torch.Tensor.is_contiguous()函数。 12345import torchx = torch.ones(10, 10)x.is_contiguous() # Truex.transpose(0, 1).is_contiguous() # Falsex.transpose(0, 1).contiguous().is_contiguous() # True 在pytorch的最新版本0.4版本中，增加了torch.reshape(), 这与 numpy.reshape 的功能类似。它大致相当于 tensor.contiguous().view()","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://pistachio0812.github.io/categories/pytorch/"}],"tags":[{"name":"函数库","slug":"函数库","permalink":"http://pistachio0812.github.io/tags/%E5%87%BD%E6%95%B0%E5%BA%93/"},{"name":"contiguous","slug":"contiguous","permalink":"http://pistachio0812.github.io/tags/contiguous/"}],"author":"pistachio"},{"title":"RFBNet论文笔记","slug":"RFBNet","date":"2022-06-01T02:05:57.530Z","updated":"2022-10-04T13:28:53.442Z","comments":true,"path":"zh-CN/RFBNet/","permalink":"http://pistachio0812.github.io/zh-CN/RFBNet/","excerpt":"","text":"论文地址：Receptive Field Block Net for Accurate and Fast Object Detection (thecvf.com) 代码地址：https://github.com/ruinmessi/RFBNet Q1RFB模块是怎么考虑感受野尺寸和离心率之间的关系的？","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"RFBNet","slug":"RFBNet","permalink":"http://pistachio0812.github.io/tags/RFBNet/"}],"author":"pistachio"},{"title":"Darknet53","slug":"Darknet53","date":"2022-05-19T12:28:51.228Z","updated":"2022-10-04T13:06:41.654Z","comments":true,"path":"zh-CN/Darknet53/","permalink":"http://pistachio0812.github.io/zh-CN/Darknet53/","excerpt":"","text":"参考博文： 1.睿智的目标检测26——Pytorch搭建yolo3目标检测平台Bubbliiiing的博客-CSDN博客睿智的目标检测26 2.Darknet53网络各层参数详解 - 简书 (jianshu.com) 3.Darknet53网络结构及代码实现_Tc.小浩的博客-CSDN博客_darknet53 4.Yolov3算法详解 - 奥辰 - 博客园 (cnblogs.com) Darkenet53是Yolov3网络中的一部分（backbone），为了更加详细了解darknet53网络的结构，现将Darknet53各层输入与输出的形状列举下来，便于分析理解。 Darknet53的网络结构如图1所示，其中蓝色方块×1，x2，x8分别表示该模块重复1次、2次和8次，黄色方块是该模块的名字，Conv Block表示该模块是一个普通的卷积模块，Residual Bolck代表该模块是一个残差网络。 读者可以将图1和图2结合对比着看更容易理解Daeknet53网络三种不同的输出位置。 .wvbvfunmmong{zoom:50%;} 实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111import mathfrom collections import OrderedDictimport torch.nn as nn# ---------------------------------------------------------------------## 残差结构# 利用一个1x1卷积下降通道数，然后利用一个3x3卷积提取特征并且上升通道数# 最后接上一个残差边# ---------------------------------------------------------------------#class BasicBlock(nn.Module): def __init__(self, inplanes, planes): super(BasicBlock, self).__init__() # (kernel_size, stride, padding)=(1, 1, 0)保持了宽高不变 self.conv1 = nn.Conv2d( inplanes, planes[0], kernel_size=1, stride=1, padding=0, bias=False) self.bn1 = nn.BatchNorm2d(planes[0]) self.relu1 = nn.LeakyReLU(0.1) # (kernel_size, stride, padding)=(3, 1, 1)同样保持了宽高不变 self.conv2 = nn.Conv2d( planes[0], planes[1], kernel_size=3, stride=1, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(planes[1]) self.relu2 = nn.LeakyReLU(0.1) def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu1(out) out = self.conv2(out) out = self.bn2(out) out = self.relu2(out) out += residual return outclass DarkNet(nn.Module): def __init__(self, layers): super(DarkNet, self).__init__() self.inplanes = 32 # 416,416,3 -&gt; 416,416,32 self.conv1 = nn.Conv2d( 3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(self.inplanes) self.relu1 = nn.LeakyReLU(0.1) # 416,416,32 -&gt; 208,208,64 self.layer1 = self._make_layer([32, 64], layers[0]) # 208,208,64 -&gt; 104,104,128 self.layer2 = self._make_layer([64, 128], layers[1]) # 104,104,128 -&gt; 52,52,256 self.layer3 = self._make_layer([128, 256], layers[2]) # 52,52,256 -&gt; 26,26,512 self.layer4 = self._make_layer([256, 512], layers[3]) # 26,26,512 -&gt; 13,13,1024 self.layer5 = self._make_layer([512, 1024], layers[4]) self.layers_out_filters = [64, 128, 256, 512, 1024] # 进行权值初始化 for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() # ---------------------------------------------------------------------# # 在每一个layer里面，首先利用一个步长为2的3x3卷积进行下采样 # 然后进行残差结构的堆叠,使用了上面的BasicBlock模块 # planes = [last_layer_out_channels, this_layer_in_channels] # ---------------------------------------------------------------------# def _make_layer(self, planes, blocks): layers = [] # 下采样，步长为2，卷积核大小为3 layers.append((\"ds_conv\", nn.Conv2d( self.inplanes, planes[1], kernel_size=3, stride=2, padding=1, bias=False))) layers.append((\"ds_bn\", nn.BatchNorm2d(planes[1]))) layers.append((\"ds_relu\", nn.LeakyReLU(0.1))) # 加入残差结构 self.inplanes = planes[1] # 将Residual Block重复layer[i]次 for i in range(0, blocks): layers.append((\"residual_{}\".format( i), BasicBlock(self.inplanes, planes))) return nn.Sequential(OrderedDict(layers)) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu1(x) x = self.layer1(x) x = self.layer2(x) out3 = self.layer3(x) out4 = self.layer4(out3) out5 = self.layer5(out4) # 取出来了三层特征继续改进 return out3, out4, out5def darknet53(): model = DarkNet([1, 2, 8, 8, 4]) return model 说明： Darknet53中的53说的是卷积和全连接层数之和，53 = 1 + （1+2+8+8+4）*2 +5+1 看结构图就能明白，最后那个1表示全连接，图中没有画出来，得去看原论文，因为这里的代码是利用其做主干得到特征再进行改进，因此没有全连接。我更愿意说它是Darknet52,但根据习惯，还是叫它Darknet53。","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"yolov3","slug":"yolov3","permalink":"http://pistachio0812.github.io/tags/yolov3/"},{"name":"darknet","slug":"darknet","permalink":"http://pistachio0812.github.io/tags/darknet/"}],"author":"pistachio"},{"title":"FPS计算","slug":"FPS","date":"2022-05-19T00:13:15.890Z","updated":"2022-10-20T14:09:01.756Z","comments":true,"path":"zh-CN/FPS/","permalink":"http://pistachio0812.github.io/zh-CN/FPS/","excerpt":"","text":"参考博文： 1.帧率(FPS)计算的几种方法总结 (baidu.com) 帧率(FPS， frame per second)计算是游戏编程中常见的一个话题，因为表现在画面刷新与视觉感官上，所以相对而言，帧率非常影响用户体验。这也是很多大型3D游戏所要提升的重要点，意味着你要不断优化渲染速度与性能，不断提升画面质量。以下是几种计算帧率fps的方法。 固定时间帧数法其实这个方法的核心是1s内刷新了多少帧，完全不考虑其他设备，相对参照来计算帧率，最准的方法也可以叫做dps，即data per second. 帧率计算公式： 1fps = frame / elapsedTime 如果记录固定时间内的帧数，就可以计算出同步率。此种方法用得较多。 实现代码如下： 1234567891011121314151617int fps(){ static int fps = 0; static int startTime = getTime(); //ms static int frameCount = 0; ++frameCount; int curTime = getTime(); if(curTime - startTime &gt; 1000)//取固定时间为1s { fps = frameCount; frameCount = 0; startTime = curTime; } return fps;} 这个固定时间为1s，其实本文的获取方法是精度比较低，也就是没有采用高精度获取时间戳的方法，在一些要求数据比较高的方法中，最好采用高精度获取时间的方法。 另一种实现方式如下： 12345678910111213141516int fps(int deltatime){ static int fps = 0; static int timeLeft = 1000; //取固定时间间隔为1s static int frameCount = 0; ++frameCount; timeLeft -= deltaTime; if(timeLeft &lt; 0) { fps = frameCount; frameCount = 0; //重新计算 timeLeft = 1000; } return fps;} 固定帧数时间法帧率计算公式为： 1fps = frameNum / elapsedTime 如果每隔固定的帧数，计算帧数使用的时间，也可求出帧率。此种方法使用得较少。这个方法其实已经不能成为实时刷新的，因为帧率最好能够再1s内，如果说固定的帧数计算帧数使用的时间，那么我1s的帧率可能得等到10s后才能采集数据完毕计算出来结果。但是不可置否，这个帧率会比较稳定，变化跳动可能不会那么大。 实现代码如下： 1234567891011121314151617int fps(){ static int fps = 0; static int frameCount = 0; static int startTime = getTime();//ms ++frameCount; if(frameCount &gt;= 100)//取固定帧数为100帧 { int curTime = getTime(); fps = frameCount / (curTime - startTime) * 1000; startTime = curTime; frameCount = 0; } return fps;} 实时计算法实时计算法直接使用上一帧的时间间隔进行计算，结果具有实时性，但平滑性不好。这是刷新最快的，但是很明显这个帧率极其不稳定的，因为帧与帧之间的间隔总是不会那么稳定。 实现代码如下： 123456int fps(int deltaTime)//ms{ int fps = static_cast&lt;int&gt;(1.f / deltaTime * 1000); //别忘了先转换为浮点数，否则会有精度损失 return fps;} 总平均法总平均法使用全局帧数除以全局时间，以求出帧率。这个刷新就更慢了，这个方法的使用可能是放在一些对帧率这个参数要求没那么高的场景中。 实现代码如下： 12345678910111213int startTime = getTime();int fps(){ static int frameCount = 0; ++frameCount; int deltaTime = getTime() - startTime(); int fps = static_cast&lt;int&gt;(frameCount * 1.f / deltaTime * 1000); //别忘了先转换为浮点数，否则会有精度损失 return fps;} 精确采样法精确采样法采样前N个帧，然后计算平均值。此种方法需要额外的内存空间，所以不常用。一般而言，大都数实验场景中不会用如此复杂的方法。 实现代码如下： 123456789101112131415161718192021222324int fps(int deltaTime)//ms{ static std::queue&lt;int&gt; q; static int sumDuration = 0; //ms int fps = 0; if(q.size() &lt; 100)//设置样本数为100 { sumDuration += deltaTime; q.push(deltaTime); fps = statci_cast&lt;int&gt;(q.size() * 1.f / sumDuration * 1000.f); //别忘了先转换为浮点数，否则会有精度损失 } else { sumDuration -= q.front(); sumDuration += deltaTime; sumDuration.pop(); sumDuration.push(deltaTime); fps = static_cast&lt;int&gt;(100.f / sumDuration * 1000.f); //别忘了先转换为浮点数，否则会有精度损失 } return fps;} 平均采样法平均采样法利用上次的统计结果，克服了精确采样法需要使用额外空间的缺点。此种方法较常用。 实现代码如下： 1234567891011121314151617181920int fps(int deltaTime)//ms{ static float avgDuration = 0.f; static alpha = 1.f / 100.f;//采样数设置为100 static int frameCount = 0; ++frameCount; int fps = 0; if(1 == frameCount) { avgDuration = static_cast&lt;float&gt;(deltaTime); } else { avgDuration = avgDuration * (1 - alpha) + deltaTime * alpha; } fps = static_cast&lt;int&gt;(1.f / avgDuration * 1000); return fps;} 帧率的计算应该来说不能影响线程，进程中的性能，不要把这部分代码放在你的业务逻辑中，这样子其实又包含了你业务逻辑执行完毕后的运行时间，显得有点臃肿，并且代码不是那么优雅。最好是创建一个线程去计算帧率，这样会显得比较优雅。","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"基础概念","slug":"基础概念","permalink":"http://pistachio0812.github.io/tags/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"},{"name":"FPS","slug":"FPS","permalink":"http://pistachio0812.github.io/tags/FPS/"}],"author":"pistachio"},{"title":"用两个栈实现队列","slug":"用两个栈实现队列","date":"2022-05-17T12:32:30.848Z","updated":"2022-10-04T13:53:58.701Z","comments":true,"path":"zh-CN/用两个栈实现队列/","permalink":"http://pistachio0812.github.io/zh-CN/%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/","excerpt":"","text":"1.用两个栈实现队列 题目：剑指 Offer 09. 用两个栈实现队列 - 力扣（LeetCode） 题解：面试题09. 用两个栈实现队列（清晰图解） - 用两个栈实现队列 - 力扣（LeetCode） 2","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://pistachio0812.github.io/categories/leetcode/"}],"tags":[{"name":"力扣题解","slug":"力扣题解","permalink":"http://pistachio0812.github.io/tags/%E5%8A%9B%E6%89%A3%E9%A2%98%E8%A7%A3/"}],"author":"coolboy"},{"title":"Concat和add的区别","slug":"concat_add","date":"2022-05-08T09:05:28.481Z","updated":"2022-10-04T13:00:42.217Z","comments":true,"path":"zh-CN/concat_add/","permalink":"http://pistachio0812.github.io/zh-CN/concat_add/","excerpt":"","text":"参考博文： 1.https://openreview.net/pdf?id=q2ZaVU6bEsT 今天偶然看到了这篇论文，论文里有一部分讲到了concat,add还有自适应的关系，很感兴趣，记录下来。 .juyepgxmzlgb{zoom:50%;} 方法（a）和（c）分别是加权融合和连接操作。即在空间和通道维度上直接添加特征图。方法（b）是一种自适应融合方法。具体来说，假设输入的大小可以表示为（bs，C，H，W），我们可以通过卷积运算得到（bs，3，H，W）的空间自适应权重，连接和 Softmax。三个通道与三个输入一一对应，通过计算加权和可以将上下文信息聚合到输出。","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"Concat&add","slug":"Concat-add","permalink":"http://pistachio0812.github.io/tags/Concat-add/"}],"author":"pistachio"},{"title":"提醒幸福","slug":"提醒幸福","date":"2022-05-07T00:44:11.860Z","updated":"2022-10-04T13:57:04.040Z","comments":true,"path":"zh-CN/提醒幸福/","permalink":"http://pistachio0812.github.io/zh-CN/%E6%8F%90%E9%86%92%E5%B9%B8%E7%A6%8F/","excerpt":"","text":"我们从小就习惯了在提醒中过日子。天气刚有一丝风吹草动，妈妈就说，别忘了多穿衣服。才相识了一个朋友，爸爸就说，小心他是个骗子。你取得了一点成功，还没容得乐出声来，所有关切着你的人一起说，别骄傲！你沉浸在欢快中的时候，自己不停地对自己说：「千万不可太高兴，苦难也许马上就要降临……」我们已经习惯了在提醒中过日子。看得见的恐惧和看不见的恐惧始终像乌鸦盘旋在头顶。 在皓月当空的良宵，提醒会走出来对你说：注意风暴。于是我们忽略了皎洁的月光，急急忙忙做好风暴来临前的一切准备。当我们大睁着眼睛枕戈待旦之时，风暴却像迟归的羊群，不知在哪里徘徊。当我们实在忍受不了等待灾难的煎熬时，我们甚至会恶意地祈盼风暴早些到来。 风暴终于姗姗地来了。我们怅然发现，所做的准备多半是没有用的。事先能够抵御的风险毕竟有限，世上无法预计的灾难却是无限的。战胜灾难靠的更多的是临门一脚，先前的惴惴不安帮不上忙。 当风暴的尾巴终于远去，我们守住零乱的家园。气还没有喘匀，新的提醒又智慧地响起来，我们又开始对未来充满恐惧的期待。 人生总是有灾难。其实大多数人早已练就了对灾难的从容，我们只是还没有学会灾难间隙的快活。我们太多注重了自己警觉苦难，我们太忽视提醒幸福。请从此注意幸福！幸福也需要提醒吗？ 提醒注意跌倒……提醒注意路滑……提醒受骗上当……提醒荣辱不惊……先哲们提醒了我们一万零一次，却不提醒我们幸福。 也许他们认为幸福不提醒也跑不了的。也许他们以为好的东西你自会珍惜，犯不上谆谆告诫。也许他们太崇尚血与火，觉得幸福无足挂齿。他们总是站在危崖上，指点我们逃离未来的苦难。但避去苦难之后的时间是什么？ 那就是幸福啊！ 享受幸福是需要学习的，当幸福即将来临的时刻需要提醒。人可以自然而然地学会感官的享乐，人却无法天生地掌握幸福的韵律。灵魂的快意同器官的舒适像一对孪生兄弟，时而相傍相依，时而南辕北辙。幸福是一种心灵的振颤。它像会倾听音乐的耳朵一样，需要不断地训练。 简言之，幸福就是没有痛苦的时刻。它出现的频率并不像我们想象的那样少。 人们常常只是在幸福的金马车已经驶过去很远，捡起地上的金鬃毛说，原来我见过它。 人们喜爱回味幸福的标本，却忽略幸福披着露水散发清香的时刻。那时候我们往往步履匆匆，瞻前顾后不知在忙着什么。 世上有预报台风的，有预报蝗虫的，有预报瘟疫的，有预报地震的。没有人预报幸福。其实幸福和世界万物一样，有它的征兆。 幸福常常是朦胧的，很有节制地向我们喷洒甘霖。你不要总希冀轰轰烈烈的幸福，它多半只是悄悄地扑面而来。你也不要企图把水龙头拧得更大，使幸福很快地流失。而需静静地以平和之心，体验幸福的真谛。幸福绝大多数是朴素的。它不会像信号弹似的，在很高的天际闪烁红色的光芒。它披着本色外衣，亲切温暖地包裹起我们。 幸福不喜欢喧嚣浮华，常常在暗淡中降临。贫困中相濡以沫的一块糕饼，患难中心心相印的一个眼神，父亲一次粗糙的抚摸，女友一个温馨的字条……这都是千金难买的幸福啊。像一粒粒缀在旧绸子上的红宝石，在凄凉中愈发熠熠夺目。 幸福有时会同我们开一个玩笑，乔装打扮而来。机遇、友情、成功、团圆…… 它们都酷似幸福，但它们并不等同于幸福。幸福会借了它们的衣裙，袅袅婷婷而来，走得近了，揭去帏幔，才发觉它有钢铁般的内核。幸福有时会很短暂，不像苦难似的笼罩天空。如果把人生的苦难和幸福分置天平两端，苦难体积庞大，幸福可能只是一块小小的矿石。但指针一定要向幸福这一侧倾斜，因为它有生命的黄金。 幸福有梯形的切面，它可以扩大也可以缩小，就看你是否珍惜。 我们要提高对于幸福的警惕，当它到来的时刻，激情地享受每一分钟。据科学家研究，有意注意的结果比无意要好得多。 当春天来临的时候，我们要对自己说，这是春天啦！心里就会泛起茸茸的绿意。 幸福的时候，我们要对自己说，请记住这一刻！幸福就会长久地伴随我们。那我们岂不是拥有了更多的幸福！ 所以，丰收的季节，先不要去想可能的灾年，我们还有漫长的冬季来得及考虑这件事。我们要和朋友们跳舞唱歌，渲染喜悦。既然种子已经回报了汗水，我们就有权沉浸幸福。不要管以后的风霜雨雪，让我们先把麦子磨成面，烘一个香喷喷的面包。 所以，当我们从天涯海角相聚在一起的时候，请不要踌躇片刻后的别离。在今后漫长的岁月里，有无数孤寂的夜晚可以独自品尝愁绪。现在的每一分钟，都让它像纯净的酒精，燃烧成幸福的淡蓝色火焰，不留一丝渣滓。让我们一起举杯，说：我们幸福。 所以，当我们守候在年迈的父母膝下时，哪怕他们鬓发苍苍，哪怕他们垂垂老矣，你都要有勇气对自己说：我很幸福。因为天地无常，总有一天你会失去他们，会无限追悔此刻的时光。 幸福并不与财富地位声望婚姻同步，这只是你心灵的感觉。 所以，当我们一无所有的时候，我们也能够说：我很幸福。因为我们还有健康的身体。当我们不再享有健康的时候，那些最勇敢的人可以依然微笑着说：我很幸福。因为我还有一颗健康的心。甚至当我们连心也不再存在的时候，那些人类最优秀的分子仍旧可以对宇宙大声说：我很幸福。因为我曾经生活过。 常常提醒自己注意幸福，就像在寒冷的日子里经常看看太阳，心就不知不觉暖洋洋亮光光。","categories":[{"name":"文学黑洞","slug":"文学黑洞","permalink":"http://pistachio0812.github.io/categories/%E6%96%87%E5%AD%A6%E9%BB%91%E6%B4%9E/"}],"tags":[{"name":"今日故事","slug":"今日故事","permalink":"http://pistachio0812.github.io/tags/%E4%BB%8A%E6%97%A5%E6%95%85%E4%BA%8B/"}],"author":"pistachio"},{"title":"hexo标签插件测试","slug":"hexo标签插件","date":"2022-05-07T00:35:11.011Z","updated":"2022-10-04T13:16:33.234Z","comments":true,"path":"zh-CN/hexo标签插件/","permalink":"http://pistachio0812.github.io/zh-CN/hexo%E6%A0%87%E7%AD%BE%E6%8F%92%E4%BB%B6/","excerpt":"","text":"前言本文语法参考volantis4.0,博客正在使用的版本为4.3.1 问题总结如下： 1.在复选框checkbox下，选中状态比不选中状态框提前，对不齐。 2.button基础按钮那里本文写的参考有点问题，不知道哪里出了问题，但源码没错。 text带 下划线 的文本；带 着重号 的文本；带 波浪线 的文本；带 删除线 的文本 键盘样式的文本：⌘ + D 密码样式的文本：这里没有验证码 源码： 12345带 {% u 下划线 %} 的文本；带 {% emp 着重号 %} 的文本；带 {% wavy 波浪线 %} 的文本；带 {% del 删除线 %} 的文本键盘样式的文本：{% kbd ⌘ %} + {% kbd D %}密码样式的文本：{% psw 这里没有验证码 %} span语法： 1{% span 样式参数, 文本内容 %} 效果： 彩色文字在一段话中方便插入各种颜色的标签，包括：红色、黄色、绿色、青色、蓝色、灰色。 超大号文字文档「开始」页面中的标题部分就是超大号文字。 Volantis A Wonderful Theme for Hexo 源码: 12345678910#### 彩色文字在一段话中方便插入各种颜色的标签，包括：{% span red, 红色 %}、{% span yellow, 黄色 %}、{% span green, 绿色 %}、{% span cyan, 青色 %}、{% span blue, 蓝色 %}、{% span gray, 灰色 %}。#### 超大号文字文档「开始」页面中的标题部分就是超大号文字。{% span center logo large, Volantis %}{% span center small, A Wonderful Theme for Hexo %} 参数： 属性 可选值 字体 logo, code 颜色 red, yellow, green, cyan, blue, gray 大小 small, h4, h3, h2, h1, large, huge, ultra 对齐方向 left, center, right p语法： 1{% p 样式参数, 文本内容 %} 效果： 彩色文字在一段话中方便插入各种颜色的标签，包括：红色黄色绿色青色蓝色灰色 超大号文字文档「开始」页面中的标题部分就是超大号文字。 Volantis A Wonderful Theme for Hexo 源码： 12345678910#### 彩色文字在一段话中方便插入各种颜色的标签，包括：{% p red, 红色 %}{% p yellow, 黄色 %}{% p green, 绿色 %}{% p cyan, 青色 %}{% p blue, 蓝色 %}{% p gray, 灰色 %}#### 超大号文字文档「开始」页面中的标题部分就是超大号文字。{% p center logo large, Volantis %}{% p center small, A Wonderful Theme for Hexo %} 参数： 属性 可选值 字体 logo, code 颜色 red, yellow, green, cyan, blue, gray 大小 small, h4, h3, h2, h1, large, huge, ultra 对齐方向 left, center, right noteNoteBlock 是 Blockquote 的增强版，在左边显示图标，并且可以自定颜色。而 Note 是 NoteBlock 的简便写法。 语法： 1{% note 样式参数, 文本内容 %} 效果： 经典用法可以在配置文件中设置默认样式，为简单的一句话提供最的简便写法。 note quote 适合引用一段话 note info 默认主题色，适合中性的信息 note warning 默认黄色，适合警告性的信息 note error/danger 默认红色，适合危险性的信息 note done/success 默认绿色，适合正确操作的信息 #### 更多图标 这些都是默认样式，可以手动加上颜色： note radiation 默认样式 note radiation yellow 可以加上颜色 note bug red 说明还存在的一些故障 note link green 可以放置一些链接 note paperclip blue 放置一些附件链接 note todo 待办事项 note guide clear 可以加上一段向导 note download 可以放置下载链接 note message gray 一段消息 note up 可以说明如何进行更新 note undo light 可以说明如何撤销或者回退 源码： 123456789101112131415161718192021222324#### 经典用法{% note, 可以在配置文件中设置默认样式，为简单的一句话提供最的简便写法。 %}{% note quote, note quote 适合引用一段话 %}{% note info, note info 默认主题色，适合中性的信息 %}{% note warning, note warning 默认黄色，适合警告性的信息 %}{% note danger, note error/danger 默认红色，适合危险性的信息 %}{% note success, note done/success 默认绿色，适合正确操作的信息 %}#### 更多图标这些都是默认样式，可以手动加上颜色：{% note radiation, note radiation 默认样式 %}{% note radiation yellow, note radiation yellow 可以加上颜色 %}{% note bug red, note bug red 说明还存在的一些故障 %}{% note link green, note link green 可以放置一些链接 %}{% note paperclip blue, note paperclip blue 放置一些附件链接 %}{% note todo, note todo 待办事项 %}{% note guide clear, note guide clear 可以加上一段向导 %}{% note download, note download 可以放置下载链接 %}{% note message gray, note message gray 一段消息 %}{% note up, note up 可以说明如何进行更新 %}{% note undo light, note undo light 可以说明如何撤销或者回退 %} 参数: | 属性 | 可选值 | | ---- | :----------------------------------------------------------- | | 图标 | # 彩色的 quote, info, warning, done/success, error/danger # 灰色的，也可以指定颜色 radiation, bug, idea, link, paperclip, todo, message, guide, download, up, undo | | 颜色 | clear, light, gray, red, yellow, green, cyan, blue | ### noteblock NoteBlock 是 Blockquote 的增强版，在左边显示图标，并且可以自定颜色。而 Note 是 NoteBlock 的简便写法。 语法： 123{% noteblock 样式参数（可选）, 标题（可选） %}文本段落{% endnoteblock %} 演示效果： 标题（可选）Windows 10不是為所有人設計,而是為每個人設計嵌套测试： 请坐和放宽，我正在帮你搞定一切… Folding 测试： 点击查看更多 不要说我们没有警告过你我们都有不顺利的时候 源码： 1234567891011121314151617{% noteblock, 标题（可选） %}Windows 10不是為所有人設計,而是為每個人設計{% noteblock done %}嵌套测试： 请坐和放宽，我正在帮你搞定一切...{% endnoteblock %}{% folding yellow, Folding 测试： 点击查看更多 %}{% note warning, 不要说我们没有警告过你 %}{% noteblock bug red %}我们都有不顺利的时候{% endnoteblock %}{% endfolding %}{% endnoteblock %} 参数： 属性 可选值 图标 # 彩色的 quote, info, warning, done/success, error/danger # 灰色的，也可以指定颜色 radiation, bug, idea, link, paperclip, todo, message, guide, download, up, undo 颜色 clear, light, gray, red, yellow, green, cyan, blue checkbox语法： 1{% checkbox 样式参数（可选）, 文本（支持简单md） %} 演示效果： 纯文本测试 支持简单的 markdown 语法 支持自定义颜色 绿色 + 默认选中 黄色 + 默认选中 青色 + 默认选中 蓝色 + 默认选中 增加 减少 叉 源码： 1234567891011# 复选框{% checkbox 纯文本测试 %}{% checkbox checked, 支持简单的 [markdown](https://guides.github.com/features/mastering-markdown/) 语法 %}{% checkbox red, 支持自定义颜色 %}{% checkbox green checked, 绿色 + 默认选中 %}{% checkbox yellow checked, 黄色 + 默认选中 %}{% checkbox cyan checked, 青色 + 默认选中 %}{% checkbox blue checked, 蓝色 + 默认选中 %}{% checkbox plus green checked, 增加 %}{% checkbox minus yellow checked, 减少 %}{% checkbox times red checked, 叉 %} 参数： 属性 可选值 颜色 red, yellow, green, cyan, blue 样式 plus, minus, times 选中状态 checked radio语法： 12# 单选框{% radio 样式参数（可选）, 文本（支持简单md） %} 演示效果： 纯文本测试 支持简单的 markdown 语法 支持自定义颜色 绿色 黄色 青色 蓝色 源码： 1234567{% radio 纯文本测试 %}{% radio checked, 支持简单的 [markdown](https://guides.github.com/features/mastering-markdown/) 语法 %}{% radio red, 支持自定义颜色 %}{% radio green, 绿色 %}{% radio yellow, 黄色 %}{% radio cyan, 青色 %}{% radio blue, 蓝色 %} 参数： 属性 可选值 颜色 red, yellow, green, cyan, blue 选中状态 checked timeline语法： 123456789101112131415{% timeline 时间线标题（可选） %}{% timenode 时间节点（标题） %}正文内容{% endtimenode %}{% timenode 时间节点（标题） %}正文内容{% endtimenode %}{% endtimeline %} 效果： 2020-07-24 2.6.6 -&gt; 3.0 如果有 hexo-lazyload-image 插件，需要删除并重新安装最新版本，设置 lazyload.isSPA: true。2.x 版本的 css 和 js 不适用于 3.x 版本，如果使用了 use_cdn: true 则需要删除。2.x 版本的 fancybox 标签在 3.x 版本中被重命名为 gallery 。2.x 版本的置顶 top: true 改为了 pin: true，并且同样适用于 layout: page 的页面。如果使用了 hexo-offline 插件，建议卸载，3.0 版本默认开启了 pjax 服务。 2020-05-15 2.6.3 -&gt; 2.6.6 不需要额外处理。 2020-04-20 2.6.2 -&gt; 2.6.3 全局搜索 seotitle 并替换为 seo_title。group 组件的索引规则有变，使用 group 组件的文章内，group: group_name 对应的组件名必须是 group_name。group 组件的列表名优先显示文章的 short_title 其次是 title。 源码： 123456789101112131415161718192021222324252627{% timeline %}{% timenode 2020-07-24 [2.6.6 -&gt; 3.0](https://github.com/volantis-x/hexo-theme-volantis/releases) %}1. 如果有 `hexo-lazyload-image` 插件，需要删除并重新安装最新版本，设置 `lazyload.isSPA: true`。2. 2.x 版本的 css 和 js 不适用于 3.x 版本，如果使用了 `use_cdn: true` 则需要删除。3. 2.x 版本的 fancybox 标签在 3.x 版本中被重命名为 gallery 。4. 2.x 版本的置顶 `top: true` 改为了 `pin: true`，并且同样适用于 `layout: page` 的页面。5. 如果使用了 `hexo-offline` 插件，建议卸载，3.0 版本默认开启了 pjax 服务。{% endtimenode %}{% timenode 2020-05-15 [2.6.3 -&gt; 2.6.6](https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.6) %}不需要额外处理。{% endtimenode %}{% timenode 2020-04-20 [2.6.2 -&gt; 2.6.3](https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.3) %}1. 全局搜索 `seotitle` 并替换为 `seo_title`。2. group 组件的索引规则有变，使用 group 组件的文章内，`group: group_name` 对应的组件名必须是 `group_name`。2. group 组件的列表名优先显示文章的 `short_title` 其次是 `title`。{% endtimenode %}{% endtimeline %} link语法： 1{% link 标题, 链接, 图片链接（可选） %} 演示效果： 如何参与项目https://volantis.js.org/contributors/ 源码： 1{% link 如何参与项目, https://volantis.js.org/contributors/, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets@master/logo/256/safari.png %} button基础按钮语法：`标题` 参数如下： 样式参数： `regular, large, center` 图标： 第1个或者第2个参数包含 `fa-` 的那个被识别为图标。 效果： 不设置任何参数的 按钮 适合融入段落中。 regular 按钮适合独立于段落之外： 示例博客 large 按钮更具有强调作用，建议搭配 center 使用： 开始使用 源码： 123456789不设置任何参数的 {% btn 按钮, / %} 适合融入段落中。regular 按钮适合独立于段落之外：{% btn regular, 示例博客, https://xaoxuu.com, fas fa-play-circle %}large 按钮更具有强调作用，建议搭配 center 使用：{% btn center large, 开始使用, https://volantis.js.org/v3/getting-started/, fas fa-download %} #### 富文本按钮 语法： 1234{% btns 样式参数 %}{% cell 标题, 链接, 图片或者图标 %}{% cell 标题, 链接, 图片或者图标 %}{% endbtns %} 参数列表： 样式参数位置可以写图片样式、布局方式，多个样式参数用空格隔开。 圆角样式： 12# 默认为方形rounded, circle 布局方式： 默认为自动宽度，适合视野内只有一两个的情况。 | 参数 | 含义 | | ------ | ---------------------------------------- | | wide | 宽一点的按钮 | | fill | 填充布局，自动铺满至少一行，多了会换行。 | | center | 居中，按钮之间是固定间距。 | | around | 居中分散 | | grid2 | 等宽最多2列，屏幕变窄会适当减少列数。 | | grid3 | 等宽最多3列，屏幕变窄会适当减少列数。 | | grid4 | 等宽最多4列，屏幕变窄会适当减少列数。 | | grid5 | 等宽最多5列，屏幕变窄会适当减少列数。 | 增加文字样式： 可以在容器内增加 `标题` 和 `描述文字` 效果： 如果需要显示类似「团队成员」之类的一组含有头像的链接： xaoxuu xaoxuu xaoxuu xaoxuu xaoxuu 或者含有图标的按钮： 下载源码 查看文档 圆形图标 + 标题 + 描述 + 图片 + 网格5列 + 居中 心率管家 专业版 心率管家 免费版 源码： 12345678910111213141516171819202122232425262728# 如果需要显示类似「团队成员」之类的一组含有头像的链接：{% btns circle grid5 %}{% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %}{% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %}{% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %}{% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %}{% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %}{% endbtns %}# 或者含有图标的按钮：{% btns rounded grid5 %}{% cell 下载源码, /, fas fa-download %}{% cell 查看文档, /, fas fa-book-open %}{% endbtns %}# 圆形图标 + 标题 + 描述 + 图片 + 网格5列 + 居中{% btns circle center grid5 %}&lt;a href='https://apps.apple.com/cn/app/heart-mate-pro-hrm-utility/id1463348922?ls=1'&gt; &lt;i class='fab fa-apple'&gt;&lt;/i&gt; &lt;b&gt;心率管家&lt;/b&gt; {% p red, 专业版 %} &lt;img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_pro.png'&gt;&lt;/a&gt;&lt;a href='https://apps.apple.com/cn/app/heart-mate-lite-hrm-utility/id1475747930?ls=1'&gt; &lt;i class='fab fa-apple'&gt;&lt;/i&gt; &lt;b&gt;心率管家&lt;/b&gt; {% p green, 免费版 %} &lt;img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_lite.png'&gt;&lt;/a&gt;{% endbtns %} ghcard样式： 12{% ghcard 用户名, 其它参数（可选） %}{% ghcard 用户名/仓库, 其它参数（可选） %} 效果示例： 如下所示，其实就是一个4×2的表格，只不过每个格子里放的是用户信息卡片 源码： 123456# 用户信息卡片| {% ghcard pistachio0812 %} | {% ghcard pistacho0812, theme=vue %} || -- | -- || {% ghcard pistachio0812, theme=buefy %} | {% ghcard pistachio0812, theme=solarized-light %} || {% ghcard pistachio0812, theme=onedark %} | {% ghcard pistachio0812, theme=solarized-dark %} || {% ghcard pistachio0812, theme=algolia %} | {% ghcard pistachio0812, theme=calm %} | 效果示例： 源码： 123456# 仓库信息卡片| {% ghcard volantis-x/hexo-theme-volantis %} | {% ghcard volantis-x/hexo-theme-volantis, theme=vue %} || -- | -- || {% ghcard volantis-x/hexo-theme-volantis, theme=buefy %} | {% ghcard volantis-x/hexo-theme-volantis, theme=solarized-light %} || {% ghcard volantis-x/hexo-theme-volantis, theme=onedark %} | {% ghcard volantis-x/hexo-theme-volantis, theme=solarized-dark %} || {% ghcard volantis-x/hexo-theme-volantis, theme=algolia %} | {% ghcard volantis-x/hexo-theme-volantis, theme=calm %} | 更多参数选择： 1 GitHub卡片API参数https://github.com/anuraghazra/github-readme-stats site网站卡片可以显示网站截图、logo、标题、描述，使用方法和友链标签一模一样，唯一的区别是数据文件名称为 sites.yml，可以和友链数据混用，通过分组过滤实现不一样的效果。 样式： 1{% sites only:community_team %} dropmenu语法容器： 123{% menu 前缀（可省略）, 标题, 后缀（可省略） %}菜单内容{% endmenu %} 菜单内容： 1.菜单项 1{% menuitem 文本, 链接, 图标 %} 2.分割线 1{% menuitem hr %} 3.子菜单 123{% submenu 嵌套菜单, 图标 %}菜单内容{% endsubmenu %} 示例效果示例1 下拉菜单 主题源码 更新日志 有疑问？ 看 FAQ 看 本站源码 提 Issue 示例2 这个是 下拉菜单 主题源码 更新日志 有疑问？ 看 FAQ 看 本站源码 示例3 这个是 下拉菜单 主题源码 更新日志 有疑问？ 看 FAQ 看 本站源码 的示例效果。 源码12345678910111213141516171819示例1{% menu 下拉菜单 %}{% menuitem 主题源码, https://github.com/volantis-x/hexo-theme-volantis/, fas fa-file-code %}{% menuitem 更新日志, https://github.com/volantis-x/hexo-theme-volantis/releases/, fas fa-clipboard-list %}{% menuitem hr %}{% submenu 有疑问？, fas fa-question-circle %}{% menuitem 看 FAQ, /faqs/ %}{% menuitem 看 本站源码, https://github.com/volantis-x/volantis-docs/ %}{% menuitem 提 Issue, https://github.com/volantis-x/hexo-theme-volantis/issues/ %}{% endsubmenu %}{% endmenu %}示例2{% menu 这个是, 下拉菜单 %}（同上）{% endmenu %}示例3{% menu 这个是, 下拉菜单, 的示例效果。 %}（同上）{% endmenu %} tab此插件移植自Next 语法： 12345678{% tabs 页面内不重复的ID %}&lt;!-- tab 栏目1 --&gt;内容&lt;!-- endtab --&gt;&lt;!-- tab 栏目2 --&gt;内容&lt;!-- endtab --&gt;{% endtabs %} 演示效果： 栏目1栏目2这是栏目1这是栏目2 源码： 123456789101112131415{% tabs tab-id %}&lt;!-- tab 栏目1 --&gt;这是栏目1&lt;!-- endtab --&gt;&lt;!-- tab 栏目2 --&gt;这是栏目2&lt;!-- endtab --&gt;{% endtabs %}","categories":[{"name":"Hexo博客搭建","slug":"Hexo博客搭建","permalink":"http://pistachio0812.github.io/categories/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://pistachio0812.github.io/tags/Hexo/"},{"name":"volantis","slug":"volantis","permalink":"http://pistachio0812.github.io/tags/volantis/"}],"author":"coolboy"},{"title":"SSD损失函数详解","slug":"SSD损失函数","date":"2022-05-06T11:52:57.287Z","updated":"2022-10-04T13:29:19.037Z","comments":true,"path":"zh-CN/SSD损失函数/","permalink":"http://pistachio0812.github.io/zh-CN/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/","excerpt":"","text":"参考博文： 1.目标识别：SSD pytorch代码学习笔记 2.SSD论文 损失函数SSD损失函数包括两部分：位置损失函数L_{loc}和置信度损失函数L_{conf} 整个损失函数为： .sgpqmyrlnwll{zoom:80%;} 说明： ·N是先验框的正样本数量 ·c是类别置信度预测值 ·l为先验框所对应边界框的位置预测值 ·g为ground truth的位置参数 位置损失函数针对所有正样本，采用Smooth L1损失，位置信息都是编码之后的信息。 .xuktwhqhnifr{zoom:50%;} 代码如下： 12345def _l1_smooth_loss(self, y_true, y_pred): abs_loss = torch.abs(y_true - y_pred) sq_loss = 0.5 * (y_true - y_pred)**2 l1_loss = torch.where(abs_loss &lt; 1.0, sq_loss, abs_loss - 0.5) return torch.sum(l1_loss, -1) 置信度损失函数首先需要使用 hard negative mining 将正负样本按照 1:3 的比例把负样本抽样出来，抽样的方法是： 思想： 针对所有batch的confidence，按照置信度误差进行降序排列，取出前top_k个负样本。 编程: ·reshape所有batch中的conf batch_conf = conf_data.view(-1, self.num_classes) ·置信度误差越大，实际上就是预测背景的置信度越小 ·把所有置信度进行log_softmax处理（均为负值），预测的置信度越小，则log_softmax越小，取绝对值，则|log_softmax|越大，降序排列后，取前top_k的负样本。 softmax代码如下： 12345def _softmax_loss(self, y_true, y_pred): y_pred = torch.clamp(y_pred, min=1e-7) softmax_loss = -torch.sum(y_true * torch.log(y_pred), axis=-1) return softmax_loss 源代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186import mathfrom functools import partialimport torchimport torch.nn as nnclass MultiboxLoss(nn.Module): def __init__(self, num_classes, alpha=1.0, neg_pos_ratio=3.0, background_label_id=0, negatives_for_hard=100.0): self.num_classes = num_classes self.alpha = alpha self.neg_pos_ratio = neg_pos_ratio if background_label_id != 0: raise Exception('Only 0 as background label id is supported') self.background_label_id = background_label_id self.negatives_for_hard = torch.FloatTensor([negatives_for_hard])[0] def _l1_smooth_loss(self, y_true, y_pred): abs_loss = torch.abs(y_true - y_pred) sq_loss = 0.5 * (y_true - y_pred)**2 l1_loss = torch.where(abs_loss &lt; 1.0, sq_loss, abs_loss - 0.5) return torch.sum(l1_loss, -1) def _softmax_loss(self, y_true, y_pred): y_pred = torch.clamp(y_pred, min=1e-7) softmax_loss = -torch.sum(y_true * torch.log(y_pred), axis=-1) return softmax_loss def forward(self, y_true, y_pred): # --------------------------------------------- # # y_true batch_size, 8732, 4 + self.num_classes + 1 # y_pred batch_size, 8732, 4 + self.num_classes # --------------------------------------------- # num_boxes = y_true.size()[1] y_pred = torch.cat([y_pred[0], nn.Softmax(-1)(y_pred[1])], dim=-1) # --------------------------------------------- # # 分类的loss # batch_size,8732,21 -&gt; batch_size,8732 # --------------------------------------------- # conf_loss = self._softmax_loss(y_true[:, :, 4:-1], y_pred[:, :, 4:]) # --------------------------------------------- # # 框的位置的loss # batch_size,8732,4 -&gt; batch_size,8732 # --------------------------------------------- # loc_loss = self._l1_smooth_loss(y_true[:, :, :4], y_pred[:, :, :4]) # --------------------------------------------- # # 获取所有的正标签的loss # --------------------------------------------- # pos_loc_loss = torch.sum(loc_loss * y_true[:, :, -1], axis=1) pos_conf_loss = torch.sum(conf_loss * y_true[:, :, -1], axis=1) # --------------------------------------------- # # 每一张图的正样本的个数 # num_pos [batch_size,] # --------------------------------------------- # num_pos = torch.sum(y_true[:, :, -1], axis=-1) # --------------------------------------------- # # 每一张图的负样本的个数 # num_neg [batch_size,] # --------------------------------------------- # num_neg = torch.min(self.neg_pos_ratio * num_pos, num_boxes - num_pos) # 找到了哪些值是大于0的 pos_num_neg_mask = num_neg &gt; 0 # --------------------------------------------- # # 如果所有的图，正样本的数量均为0 # 那么则默认选取100个先验框作为负样本 # --------------------------------------------- # has_min = torch.sum(pos_num_neg_mask) # --------------------------------------------- # # 从这里往后，与视频中看到的代码有些许不同。 # 由于以前的负样本选取方式存在一些问题， # 我对该部分代码进行重构。 # 求整个batch应该的负样本数量总和 # --------------------------------------------- # num_neg_batch = torch.sum( num_neg) if has_min &gt; 0 else self.negatives_for_hard # --------------------------------------------- # # 对预测结果进行判断，如果该先验框没有包含物体 # 那么它的不属于背景的预测概率过大的话 # 就是难分类样本 # --------------------------------------------- # confs_start = 4 + self.background_label_id + 1 confs_end = confs_start + self.num_classes - 1 # --------------------------------------------- # # batch_size,8732 # 把不是背景的概率求和，求和后的概率越大 # 代表越难分类。 # --------------------------------------------- # max_confs = torch.sum(y_pred[:, :, confs_start:confs_end], dim=2) # --------------------------------------------------- # # 只有没有包含物体的先验框才得到保留 # 我们在整个batch里面选取最难分类的num_neg_batch个 # 先验框作为负样本。 # --------------------------------------------------- # max_confs = (max_confs * (1 - y_true[:, :, -1])).view([-1]) _, indices = torch.topk(max_confs, k=int( num_neg_batch.cpu().numpy().tolist())) neg_conf_loss = torch.gather(conf_loss.view([-1]), 0, indices) # 进行归一化 num_pos = torch.where(num_pos != 0, num_pos, torch.ones_like(num_pos)) total_loss = torch.sum( pos_conf_loss) + torch.sum(neg_conf_loss) + torch.sum(self.alpha * pos_loc_loss) total_loss = total_loss / torch.sum(num_pos) return total_lossdef weights_init(net, init_type='normal', init_gain=0.02): def init_func(m): classname = m.__class__.__name__ if hasattr(m, 'weight') and classname.find('Conv') != -1: if init_type == 'normal': torch.nn.init.normal_(m.weight.data, 0.0, init_gain) elif init_type == 'xavier': torch.nn.init.xavier_normal_(m.weight.data, gain=init_gain) elif init_type == 'kaiming': torch.nn.init.kaiming_normal_( m.weight.data, a=0, mode='fan_in') elif init_type == 'orthogonal': torch.nn.init.orthogonal_(m.weight.data, gain=init_gain) else: raise NotImplementedError( 'initialization method [%s] is not implemented' % init_type) elif classname.find('BatchNorm2d') != -1: torch.nn.init.normal_(m.weight.data, 1.0, 0.02) torch.nn.init.constant_(m.bias.data, 0.0) print('initialize network with %s type' % init_type) net.apply(init_func)def get_lr_scheduler(lr_decay_type, lr, min_lr, total_iters, warmup_iters_ratio=0.1, warmup_lr_ratio=0.1, no_aug_iter_ratio=0.3, step_num=10): def yolox_warm_cos_lr(lr, min_lr, total_iters, warmup_total_iters, warmup_lr_start, no_aug_iter, iters): if iters &lt;= warmup_total_iters: # lr = (lr - warmup_lr_start) * iters / float(warmup_total_iters) + warmup_lr_start lr = (lr - warmup_lr_start) * pow(iters / float(warmup_total_iters), 2) + warmup_lr_start elif iters &gt;= total_iters - no_aug_iter: lr = min_lr else: lr = min_lr + 0.5 * (lr - min_lr) * ( 1.0 + math.cos(math.pi * (iters - warmup_total_iters) / (total_iters - warmup_total_iters - no_aug_iter)) ) return lr def step_lr(lr, decay_rate, step_size, iters): if step_size &lt; 1: raise ValueError(\"step_size must above 1.\") n = iters // step_size out_lr = lr * decay_rate ** n return out_lr if lr_decay_type == \"cos\": warmup_total_iters = min(max(warmup_iters_ratio * total_iters, 1), 3) warmup_lr_start = max(warmup_lr_ratio * lr, 1e-6) no_aug_iter = min(max(no_aug_iter_ratio * total_iters, 1), 15) func = partial(yolox_warm_cos_lr, lr, min_lr, total_iters, warmup_total_iters, warmup_lr_start, no_aug_iter) else: decay_rate = (min_lr / lr) ** (1 / (step_num - 1)) step_size = total_iters / step_num func = partial(step_lr, lr, decay_rate, step_size) return funcdef set_optimizer_lr(optimizer, lr_scheduler_func, epoch): lr = lr_scheduler_func(epoch) for param_group in optimizer.param_groups: param_group['lr'] = lr","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"SSD","slug":"SSD","permalink":"http://pistachio0812.github.io/tags/SSD/"},{"name":"损失函数","slug":"损失函数","permalink":"http://pistachio0812.github.io/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"}],"author":"coolboy"},{"title":"YOLOV5论文笔记","slug":"YOLOV5","date":"2022-05-05T03:00:10.013Z","updated":"2022-10-04T13:35:38.630Z","comments":true,"path":"zh-CN/YOLOV5/","permalink":"http://pistachio0812.github.io/zh-CN/YOLOV5/","excerpt":"","text":"参考博文： 1.pytorch搭建yolov5 2.yolov5-pytorch 3.以YOLOV4为例详解anchor_based目标检测训练过程 整体结构解析.nmwyxpabnldb{zoom:50%;} 在学习YoloV5之前，我们需要对YoloV5所作的工作有一定的了解，这有助于我们后面去了解网络的细节。 和之前版本的Yolo类似，整个YoloV5可以依然可以分为三个部分，分别是Backbone，FPN以及Yolo Head。 Backbone可以被称作YoloV5的主干特征提取网络，根据它的结构以及之前Yolo主干的叫法，我一般叫它CSPDarknet，输入的图片首先会在CSPDarknet里面进行特征提取，提取到的特征可以被称作特征层，是输入图片的特征集合。在主干部分，我们获取了三个特征层进行下一步网络的构建，这三个特征层我称它为有效特征层。 FPN可以被称作YoloV5的加强特征提取网络，在主干部分获得的三个有效特征层会在这一部分进行特征融合，特征融合的目的是结合不同尺度的特征信息。在FPN部分，已经获得的有效特征层被用于继续提取特征。在YoloV5里依然使用到了Panet的结构，我们不仅会对特征进行上采样实现特征融合，还会对特征再次进行下采样实现特征融合。 Yolo Head是YoloV5的分类器与回归器，通过CSPDarknet和FPN，我们已经可以获得三个加强过的有效特征层。每一个特征层都有宽、高和通道数，此时我们可以将特征图看作一个又一个特征点的集合，每一个特征点都有通道数个特征。Yolo Head实际上所做的工作就是对特征点进行判断，判断特征点是否有物体与其对应。与以前版本的Yolo一样，YoloV5所用的解耦头是一起的，也就是分类和回归在一个1X1卷积里实现。 因此，整个YoloV5网络所作的工作就是 特征提取-特征加强-预测特征点对应的物体情况。 网络结构解析主干网络backbone.tuctdgvpcexe{zoom:50%;} 这部分在nets/CSPdarknet.py文件下，YoloV5所使用的主干特征提取网络为CSPDarknet，接下来就是各个组件实现: Conv2D_BN_SiLU在代码里把这卷积、正则化和激活进行封装，封装成类Conv 1234567891011121314151617181920212223class SiLU(nn.Module): @staticmethod def forward(x): return x * torch.sigmoid(x)# 自动填充,padding=1/2*kernel_size,根据计算卷积计算宽高公式自然懂的def autopad(k, p=None): if p is None: p = k // 2 if isinstance(k, int) else [x // 2 for x in k] return pclass Conv(nn.Module): def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): super(Conv, self).__init__() self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) self.bn = nn.BatchNorm2d(c2, eps=0.001, momentum=0.03) self.act = SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity()) def forward(self, x): return self.act(self.bn(self.conv(x))) def fuseforward(self, x): return self.act(self.conv(x)) 残差网络使用了残差网络Residual，CSPDarknet中的残差卷积可以分为两个部分，主干部分是一次1X1的卷积和一次3X3的卷积；残差边部分不做任何处理，直接将主干的输入与输出结合。整个YoloV5的主干部分都由残差卷积构成： 如下图所示： .chkuwwygdnot{zoom:50%;} 代码如下： 1234567891011class Bottleneck(nn.Module): # Standard bottleneck def __init__(self, c1, c2, shortcut=True, g=1, e=0.5): # 参数分别代表ch_in, ch_out, shortcut, groups, expansion super(Bottleneck, self).__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c_, c2, 3, 1, g=g) self.add = shortcut and c1 == c2 def forward(self, x): return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x)) 残差网络的特点是容易优化，并且能够通过增加相当的深度来提高准确率。其内部的残差块使用了跳跃连接，缓解了在深度神经网络中增加深度带来的梯度消失问题。 CSPnet使用CSPnet网络结构，CSPnet结构并不算复杂，就是将原来的残差块的堆叠进行了一个拆分，拆成左右两部分：主干部分继续进行原来的残差块的堆叠；另一部分则像一个残差边一样，经过少量处理直接连接到最后。因此可以认为CSP中存在一个大的残差边。 如下图所示： .qjhwgvfkxxhl{zoom:50%;} 代码实现： 123456789101112131415# 这里是backbone里CSPLayer的实现class C3(nn.Module): # CSP Bottleneck with 3 convolutions def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5): # ch_in, ch_out, number, shortcut, groups, expansion super(C3, self).__init__() c_ = int(c2 * e) # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c1, c_, 1, 1) self.cv3 = Conv(2 * c_, c2, 1) # act=FReLU(c2) self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)]) # self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)]) def forward(self, x): return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1)) Focus网络结构使用了Focus网络结构，这个网络结构是在YoloV5里面使用到比较有趣的网络结构，具体操作是在一张图片中每隔一个像素拿到一个值，这个时候获得了四个独立的特征层，然后将四个独立的特征层进行堆叠，此时宽高信息就集中到了通道信息，输入通道扩充了四倍。拼接起来的特征层相对于原先的三通道变成了十二个通道，下图很好的展示了Focus结构，一看就能明白。 .mqmobmqseawj{} 12345678910111213class Focus(nn.Module): def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True): # ch_in, ch_out, kernel, stride, padding, groups super(Focus, self).__init__() self.conv = Conv(c1 * 4, c2, k, s, p, g, act) def forward(self, x): return self.conv(torch.cat( [x[..., ::2, ::2], # 1 x[..., 1::2, ::2], # 2 x[..., ::2, 1::2], # 3 x[..., 1::2, 1::2] # 4 ], 1)) SiLU函数使用了SiLU激活函数，SiLU是Sigmoid和ReLU的改进版。SiLU具备无上界有下界、平滑、非单调的特性。SiLU在深层模型上的效果优于 ReLU。可以看做是平滑的ReLU激活函数。 .fketdturnptf{zoom: 80%;} .cnbhrupvyttq{zoom:50%;} 12345class SiLU(nn.Module): @staticmethod def forward(x): return x * torch.sigmoid(x) SPP使用了SPP结构，通过不同池化核大小的最大池化进行特征提取，提高网络的感受野。在YoloV4中，SPP是用在FPN里面的，在YoloV5中，SPP模块被用在了主干特征提取网络中。 如下图所示： .yldnlhqfjsfc{zoom:50%;} 代码如下： 12345678910111213class SPP(nn.Module): # Spatial pyramid pooling layer used in YOLOv3-SPP def __init__(self, c1, c2, k=(5, 9, 13)): super(SPP, self).__init__() c_ = c1 // 2 # hidden channels self.cv1 = Conv(c1, c_, 1, 1) self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1) self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k]) def forward(self, x): x = self.cv1(x) return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1)) CSPDarknet在完成各个部件后，整个的CSPDarknet实现如下（左边部分）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class CSPDarknet(nn.Module): def __init__(self, base_channels, base_depth): super().__init__() #-----------------------------------------------# # 输入图片是640, 640, 3 # 初始的基本通道是64 #-----------------------------------------------# #-----------------------------------------------# # 利用focus网络结构进行特征提取 # 640, 640, 3 -&gt; 320, 320, 12 -&gt; 320, 320, 64 #-----------------------------------------------# self.stem = Focus(3, base_channels, k=3) #-----------------------------------------------# # 完成卷积之后，320, 320, 64 -&gt; 160, 160, 128 # 完成CSPlayer之后，160, 160, 128 -&gt; 160, 160, 128 #-----------------------------------------------# self.dark2 = nn.Sequential( Conv(base_channels, base_channels * 2, 3, 2), C3(base_channels * 2, base_channels * 2, base_depth), ) #-----------------------------------------------# # 完成卷积之后，160, 160, 128 -&gt; 80, 80, 256 # 完成CSPlayer之后，80, 80, 256 -&gt; 80, 80, 256 #-----------------------------------------------# self.dark3 = nn.Sequential( Conv(base_channels * 2, base_channels * 4, 3, 2), C3(base_channels * 4, base_channels * 4, base_depth * 3), ) #-----------------------------------------------# # 完成卷积之后，80, 80, 256 -&gt; 40, 40, 512 # 完成CSPlayer之后，40, 40, 512 -&gt; 40, 40, 512 #-----------------------------------------------# self.dark4 = nn.Sequential( Conv(base_channels * 4, base_channels * 8, 3, 2), C3(base_channels * 8, base_channels * 8, base_depth * 3), ) #-----------------------------------------------# # 完成卷积之后，40, 40, 512 -&gt; 20, 20, 1024 # 完成SPP之后，20, 20, 1024 -&gt; 20, 20, 1024 # 完成CSPlayer之后，20, 20, 1024 -&gt; 20, 20, 1024 #-----------------------------------------------# self.dark5 = nn.Sequential( Conv(base_channels * 8, base_channels * 16, 3, 2), SPP(base_channels * 16, base_channels * 16), C3(base_channels * 16, base_channels * 16, base_depth, shortcut=False), ) # feat1, feat2, feat3是用来目标检测的三个特征层，后续还要进行FPN层的构建，也就是中间那部分 def forward(self, x): x = self.stem(x) x = self.dark2(x) #-----------------------------------------------# # dark3的输出为80, 80, 256，是一个有效特征层 #-----------------------------------------------# x = self.dark3(x) feat1 = x #-----------------------------------------------# # dark4的输出为40, 40, 512，是一个有效特征层 #-----------------------------------------------# x = self.dark4(x) feat2 = x #-----------------------------------------------# # dark5的输出为20, 20, 1024，是一个有效特征层 #-----------------------------------------------# x = self.dark5(x) feat3 = x return feat1, feat2, feat3 好了，我们现在已经提取到了构建FPN层的三个特征层，接下来就是构建FPN 构建FPN加强特征提取.shfrxvljswgn{zoom:50%;} 再次回到这幅图，现在我们需要进行中间部分的实现了,这部分在yolo.py文件。 在特征利用部分，YoloV5提取多特征层进行目标检测，一共提取三个特征层。三个特征层位于主干部分CSPdarknet的不同位置，分别位于中间层，中下层，底层，当输入为(640,640,3)的时候，三个特征层的shape分别为feat1=(80,80,256)、feat2=(40,40,512)、feat3=(20,20,1024)。 在获得三个有效特征层后，我们利用这三个有效特征层进行FPN层的构建，构建方式为： 1.feat3=(20,20,1024)的特征层进行1次1X1卷积调整通道后获得P5，P5进行上采样UmSampling2d后与feat2=(40,40,512)特征层进行结合，然后使用CSPLayer进行特征提取获得P5_upsample，此时获得的特征层为(40,40,512)。 2.P5_upsample=(40,40,512)的特征层进行1次1X1卷积调整通道后获得P4，P4进行上采样UmSampling2d后与feat1=(80,80,256)特征层进行结合，然后使用CSPLayer进行特征提取P3_out，此时获得的特征层为(80,80,256)。 3.P3_out=(80,80,256)的特征层进行一次3x3卷积进行下采样，下采样后与P4堆叠，然后使用CSPLayer进行特征提取P4_out，此时获得的特征层为(40,40,512)。 4.P4_out=(40,40,512)的特征层进行一次3x3卷积进行下采样，下采样后与P5堆叠，然后使用CSPLayer进行特征提取P5_out，此时获得的特征层为(20,20,1024)。 注：p3_out, p4_out,p5_out即为经过FPN输出的三个特征层，用于检测。 特征金字塔可以将不同shape的特征层进行特征融合，有利于提取出更好的特征。 代码实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136import torchimport torch.nn as nnfrom nets.ConvNext import ConvNeXt_Small, ConvNeXt_Tinyfrom nets.CSPdarknet import C3, Conv, CSPDarknetfrom nets.Swin_transformer import Swin_transformer_Tiny#---------------------------------------------------## yolo_body#---------------------------------------------------#class YoloBody(nn.Module): def __init__(self, anchors_mask, num_classes, phi, backbone='cspdarknet', pretrained=False, input_shape=[640, 640]): super(YoloBody, self).__init__() depth_dict = {'s' : 0.33, 'm' : 0.67, 'l' : 1.00, 'x' : 1.33,} width_dict = {'s' : 0.50, 'm' : 0.75, 'l' : 1.00, 'x' : 1.25,} dep_mul, wid_mul = depth_dict[phi], width_dict[phi] base_channels = int(wid_mul * 64) # 64 base_depth = max(round(dep_mul * 3), 1) # 3 #-----------------------------------------------# # 输入图片是640, 640, 3 # 初始的基本通道是64 #-----------------------------------------------# self.backbone_name = backbone if backbone == \"cspdarknet\": #---------------------------------------------------# # 生成CSPdarknet53的主干模型 # 获得三个有效特征层，他们的shape分别是： # 80,80,256 # 40,40,512 # 20,20,1024 #---------------------------------------------------# self.backbone = CSPDarknet(base_channels, base_depth, phi, pretrained) else: #---------------------------------------------------# # 如果输入不为cspdarknet，则调整通道数 # 使其符合YoloV5的格式 #---------------------------------------------------# self.backbone = { 'convnext_tiny' : ConvNeXt_Tiny, 'convnext_small' : ConvNeXt_Small, 'swin_transfomer_tiny' : Swin_transformer_Tiny, }[backbone](pretrained=pretrained, input_shape=input_shape) in_channels = { 'convnext_tiny' : [192, 384, 768], 'convnext_small' : [192, 384, 768], 'swin_transfomer_tiny' : [192, 384, 768], }[backbone] feat1_c, feat2_c, feat3_c = in_channels self.conv_1x1_feat1 = Conv(feat1_c, base_channels * 4, 1, 1) self.conv_1x1_feat2 = Conv(feat2_c, base_channels * 8, 1, 1) self.conv_1x1_feat3 = Conv(feat3_c, base_channels * 16, 1, 1) # 上采样操作，采用最近邻插值法 self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\") # Conv1×1,获得p5 self.conv_for_feat3 = Conv(base_channels * 16, base_channels * 8, 1, 1) # CSPLayer操作，进行特征提取，获得p5_unsample self.conv3_for_upsample1 = C3(base_channels * 16, base_channels * 8, base_depth, shortcut=False) self.conv_for_feat2 = Conv(base_channels * 8, base_channels * 4, 1, 1) self.conv3_for_upsample2 = C3(base_channels * 8, base_channels * 4, base_depth, shortcut=False) # 下采样操作，采用卷积步长为2的方法，通道不变，宽高压缩一半 self.down_sample1 = Conv(base_channels * 4, base_channels * 4, 3, 2) self.conv3_for_downsample1 = C3(base_channels * 8, base_channels * 8, base_depth, shortcut=False) self.down_sample2 = Conv(base_channels * 8, base_channels * 8, 3, 2) self.conv3_for_downsample2 = C3(base_channels * 16, base_channels * 16, base_depth, shortcut=False) # 80, 80, 256 =&gt; 80, 80, 3 * (5 + num_classes) =&gt; 80, 80, 3 * (4 + 1 + num_classes) self.yolo_head_P3 = nn.Conv2d(base_channels * 4, len(anchors_mask[2]) * (5 + num_classes), 1) # 40, 40, 512 =&gt; 40, 40, 3 * (5 + num_classes) =&gt; 40, 40, 3 * (4 + 1 + num_classes) self.yolo_head_P4 = nn.Conv2d(base_channels * 8, len(anchors_mask[1]) * (5 + num_classes), 1) # 20, 20, 1024 =&gt; 20, 20, 3 * (5 + num_classes) =&gt; 20, 20, 3 * (4 + 1 + num_classes) self.yolo_head_P5 = nn.Conv2d(base_channels * 16, len(anchors_mask[0]) * (5 + num_classes), 1) def forward(self, x): # backbone feat1, feat2, feat3 = self.backbone(x) if self.backbone_name != \"cspdarknet\": feat1 = self.conv_1x1_feat1(feat1) feat2 = self.conv_1x1_feat2(feat2) feat3 = self.conv_1x1_feat3(feat3) # 20, 20, 1024 -&gt; 20, 20, 512 P5 = self.conv_for_feat3(feat3) # 20, 20, 512 -&gt; 40, 40, 512 P5_upsample = self.upsample(P5) # 40, 40, 512 -&gt; 40, 40, 1024 P4 = torch.cat([P5_upsample, feat2], 1) # 40, 40, 1024 -&gt; 40, 40, 512 P4 = self.conv3_for_upsample1(P4) # 40, 40, 512 -&gt; 40, 40, 256 P4 = self.conv_for_feat2(P4) # 40, 40, 256 -&gt; 80, 80, 256 P4_upsample = self.upsample(P4) # 80, 80, 256 cat 80, 80, 256 -&gt; 80, 80, 512 P3 = torch.cat([P4_upsample, feat1], 1) # 80, 80, 512 -&gt; 80, 80, 256 P3 = self.conv3_for_upsample2(P3) # 80, 80, 256 -&gt; 40, 40, 256 P3_downsample = self.down_sample1(P3) # 40, 40, 256 cat 40, 40, 256 -&gt; 40, 40, 512 P4 = torch.cat([P3_downsample, P4], 1) # 40, 40, 512 -&gt; 40, 40, 512 P4 = self.conv3_for_downsample1(P4) # 40, 40, 512 -&gt; 20, 20, 512 P4_downsample = self.down_sample2(P4) # 20, 20, 512 cat 20, 20, 512 -&gt; 20, 20, 1024 P5 = torch.cat([P4_downsample, P5], 1) # 20, 20, 1024 -&gt; 20, 20, 1024 P5 = self.conv3_for_downsample2(P5) #---------------------------------------------------# # 第三个特征层 # y3=(batch_size,75,80,80) #---------------------------------------------------# out2 = self.yolo_head_P3(P3) #---------------------------------------------------# # 第二个特征层 # y2=(batch_size,75,40,40) #---------------------------------------------------# out1 = self.yolo_head_P4(P4) #---------------------------------------------------# # 第一个特征层 # y1=(batch_size,75,20,20) #---------------------------------------------------# out0 = self.yolo_head_P5(P5) return out0, out1, out2 注：特征金字塔输出来的是p3,p4,p5。代码里输出的是yolohead输出来的结果，即out0,out1,out2 利用YoloHead获得预测结果.gilsflqdmtnv{zoom:50%;} 利用FPN特征金字塔，我们可以获得三个加强特征，这三个加强特征的shape分别为(20,20,1024)、(40,40,512)、(80,80,256)，然后我们利用这三个shape的特征层传入Yolo Head获得预测结果。 对于每一个特征层，我们可以获得利用一个卷积调整通道数，最终的通道数和需要区分的种类个数相关，在YoloV5里，每一个特征层上每一个特征点存在3个先验框。 如果使用的是voc训练集，类则为20种，最后的维度应该为75 = 3x25，三个特征层的shape为(20,20,75)，(40,40,75)，(80,80,75)。最后的75可以拆分成3个25，对应3个先验框的25个参数，25可以拆分成4+1+20。前4个参数用于判断每一个特征点的回归参数，回归参数调整后可以获得预测框；第5个参数用于判断每一个特征点是否包含物体；最后20个参数用于判断每一个特征点所包含的物体种类。 如果使用的是coco训练集，类则为80种，最后的维度应该为255 = 3x85，三个特征层的shape为(20,20,255)，(40,40,255)，(80,80,255)最后的255可以拆分成3个85，对应3个先验框的85个参数，85可以拆分成4+1+80。前4个参数用于判断每一个特征点的回归参数，回归参数调整后可以获得预测框；第5个参数用于判断每一个特征点是否包含物体；最后80个参数用于判断每一个特征点所包含的物体种类。 在上一部分其实已经写到了预测部分的代码，现在我们把它单独拿出来看看,或者直接看上面FPN那里的代码。 1234567891011121314151617181920212223242526class YoloBody(nn.Module): def __init__(self, anchor_mask, num_classes, phi): super(YoloBody, self).__init__() # 80, 80, 256 =&gt; 80, 80, 3 * (5 + num_classes) =&gt; 80, 80, 3 * (4 + 1 + num_classes) self.yolo_head_P3 = nn.Conv2d(base_channels * 4, len(anchors_mask[2]) * (5 + num_classes), 1) # 40, 40, 512 =&gt; 40, 40, 3 * (5 + num_classes) =&gt; 40, 40, 3 * (4 + 1 + num_classes) self.yolo_head_P4 = nn.Conv2d(base_channels * 8, len(anchors_mask[1]) * (5 + num_classes), 1) # 20, 20, 1024 =&gt; 20, 20, 3 * (5 + num_classes) =&gt; 20, 20, 3 * (4 + 1 + num_classes) self.yolo_head_P5 = nn.Conv2d(base_channels * 16, len(anchors_mask[0]) * (5 + num_classes), 1) def forward(self, x): #---------------------------------------------------# # 第三个特征层 # y3=(batch_size,75,80,80) #---------------------------------------------------# out2 = self.yolo_head_P3(P3) #---------------------------------------------------# # 第二个特征层 # y2=(batch_size,75,40,40) #---------------------------------------------------# out1 = self.yolo_head_P4(P4) #---------------------------------------------------# # 第一个特征层 # y1=(batch_size,75,20,20) #---------------------------------------------------# out0 = self.yolo_head_P5(P5) return out0, out1, out2 注： anchor_mask=[[6, 7, 8], [3, 4, 5], [0, 1, 2]], 表示三个特征图的9个先验框，因此每一个特征图上每一个特点上存在3个先验框。 预测获得预测框和得分假设我们使用coco数据集进行训练，由第二步我们可以获得三个特征层的预测结果，shape分别为(N,20,20,255)，(N,40,40,255)，(N,80,80,255)的数据。 但是这个预测结果并不对应着最终的预测框在图片上的位置，还需要解码才可以完成。在YoloV5里，每一个特征层上每一个特征点存在3个先验框。 每个特征层最后的255可以拆分成3个85，对应3个先验框的85个参数，我们先将其reshape一下，其结果为(N,20,20,3,85)，(N,40.40,3,85)，(N,80,80,3,85)。 其中的85可以拆分成4+1+80。前4个参数用于判断每一个特征点的回归参数，回归参数调整后可以获得预测框；第5个参数用于判断每一个特征点是否包含物体；最后80个参数用于判断每一个特征点所包含的物体种类。 以(N,20,20,3,85)这个特征层为例，该特征层相当于将图像划分成20x20个特征点，如果某个特征点落在物体的对应框内，就用于预测该物体。 如图所示，蓝色的点为20x20的特征点，此时我们对左图黑色点的三个先验框进行解码操作演示：1、进行中心预测点的计算，利用Regression预测结果前两个序号的内容对特征点的三个先验框中心坐标进行偏移，偏移后是右图红色的三个点；2、进行预测框宽高的计算，利用Regression预测结果后两个序号的内容求指数后获得预测框的宽高；3、此时获得的预测框就可以绘制在图片上了。 .tpkwwepexzsr{zoom:50%;} 除去这样的解码操作，还有非极大抑制的操作需要进行，防止同一种类的框的堆积。 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394# utils/utils_bbox.pydef decode_box(self, inputs): outputs = [] for i, input in enumerate(inputs): #-----------------------------------------------# # 输入的input一共有三个，他们的shape分别是 # batch_size, 255, 20, 20 # batch_size, 255, 40, 40 # batch_size, 255, 80, 80 #-----------------------------------------------# batch_size = input.size(0) input_height = input.size(2) input_width = input.size(3) #-----------------------------------------------# # 输入为416x416时 # stride_h = stride_w = 32、16、8 #-----------------------------------------------# stride_h = self.input_shape[0] / input_height stride_w = self.input_shape[1] / input_width #-------------------------------------------------# # 此时获得的scaled_anchors大小是相对于特征层的 #-------------------------------------------------# scaled_anchors = [(anchor_width / stride_w, anchor_height / stride_h) for anchor_width, anchor_height in self.anchors[self.anchors_mask[i]]] #-----------------------------------------------# # 输入的input一共有三个，他们的shape分别是 # batch_size, 3, 20, 20, 85 # batch_size, 3, 40, 40, 85 # batch_size, 3, 80, 80, 85 #-----------------------------------------------# prediction = input.view(batch_size, len(self.anchors_mask[i]), self.bbox_attrs, input_height, input_width).permute(0, 1, 3, 4, 2).contiguous() #-----------------------------------------------# # 先验框的中心位置的调整参数 #-----------------------------------------------# x = torch.sigmoid(prediction[..., 0]) y = torch.sigmoid(prediction[..., 1]) #-----------------------------------------------# # 先验框的宽高调整参数 #-----------------------------------------------# w = torch.sigmoid(prediction[..., 2]) h = torch.sigmoid(prediction[..., 3]) #-----------------------------------------------# # 获得置信度，是否有物体 #-----------------------------------------------# conf = torch.sigmoid(prediction[..., 4]) #-----------------------------------------------# # 种类置信度 #-----------------------------------------------# pred_cls = torch.sigmoid(prediction[..., 5:]) FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor #----------------------------------------------------------# # 生成网格，先验框中心，网格左上角 # batch_size,3,20,20 #----------------------------------------------------------# grid_x = torch.linspace(0, input_width - 1, input_width).repeat(input_height, 1).repeat( batch_size * len(self.anchors_mask[i]), 1, 1).view(x.shape).type(FloatTensor) grid_y = torch.linspace(0, input_height - 1, input_height).repeat(input_width, 1).t().repeat( batch_size * len(self.anchors_mask[i]), 1, 1).view(y.shape).type(FloatTensor) #----------------------------------------------------------# # 按照网格格式生成先验框的宽高 # batch_size,3,20,20 #----------------------------------------------------------# anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0])) anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1])) anchor_w = anchor_w.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(w.shape) anchor_h = anchor_h.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(h.shape) #----------------------------------------------------------# # 利用预测结果对先验框进行调整 # 首先调整先验框的中心，从先验框中心向右下角偏移 # 再调整先验框的宽高。 #----------------------------------------------------------# pred_boxes = FloatTensor(prediction[..., :4].shape) pred_boxes[..., 0] = x.data * 2. - 0.5 + grid_x pred_boxes[..., 1] = y.data * 2. - 0.5 + grid_y pred_boxes[..., 2] = (w.data * 2) ** 2 * anchor_w pred_boxes[..., 3] = (h.data * 2) ** 2 * anchor_h #----------------------------------------------------------# # 将输出结果归一化成小数的形式 #----------------------------------------------------------# _scale = torch.Tensor([input_width, input_height, input_width, input_height]).type(FloatTensor) output = torch.cat((pred_boxes.view(batch_size, -1, 4) / _scale, conf.view(batch_size, -1, 1), pred_cls.view(batch_size, -1, self.num_classes)), -1) outputs.append(output.data) return outputs 得分筛选和非极大值抑制得到最终的预测结果后还要进行得分排序与非极大抑制筛选。 得分筛选就是筛选出得分满足confidence置信度的预测框。非极大抑制就是筛选出一定区域内属于同一种类得分最大的框。 得分筛选与非极大抑制的过程可以概括如下：1、找出该图片中得分大于门限函数的框。在进行重合框筛选前就进行得分的筛选可以大幅度减少框的数量。2、对种类进行循环，非极大抑制的作用是筛选出一定区域内属于同一种类得分最大的框，对种类进行循环可以帮助我们对每一个类分别进行非极大抑制。3、根据得分对该种类进行从大到小排序。4、每次取出得分最大的框，计算其与其它所有预测框的重合程度，重合程度过大的则剔除。 得分筛选与非极大抑制后的结果就可以用于绘制预测框了。 下图是经过非极大抑制的： .wmjtyzrunrdr{zoom:50%;} 下图是未经过非极大值抑制的： .isfkjyssftdw{zoom:50%;} 实现代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889def non_max_suppression(self, prediction, num_classes, input_shape, image_shape, letterbox_image, conf_thres=0.5, nms_thres=0.4): #----------------------------------------------------------# # 将预测结果的格式转换成左上角右下角的格式。 # prediction [batch_size, num_anchors, 85] #----------------------------------------------------------# box_corner = prediction.new(prediction.shape) box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2 box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2 box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2 box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2 prediction[:, :, :4] = box_corner[:, :, :4] output = [None for _ in range(len(prediction))] for i, image_pred in enumerate(prediction): #----------------------------------------------------------# # 对种类预测部分取max。 # class_conf [num_anchors, 1] 种类置信度 # class_pred [num_anchors, 1] 种类 #----------------------------------------------------------# class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1, keepdim=True) #----------------------------------------------------------# # 利用置信度进行第一轮筛选 #----------------------------------------------------------# conf_mask = (image_pred[:, 4] * class_conf[:, 0] &gt;= conf_thres).squeeze() #----------------------------------------------------------# # 根据置信度进行预测结果的筛选 #----------------------------------------------------------# image_pred = image_pred[conf_mask] class_conf = class_conf[conf_mask] class_pred = class_pred[conf_mask] if not image_pred.size(0): continue #-------------------------------------------------------------------------# # detections [num_anchors, 7] # 7的内容为：x1, y1, x2, y2, obj_conf, class_conf, class_pred #-------------------------------------------------------------------------# detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1) #------------------------------------------# # 获得预测结果中包含的所有种类 #------------------------------------------# unique_labels = detections[:, -1].cpu().unique() if prediction.is_cuda: unique_labels = unique_labels.cuda() detections = detections.cuda() for c in unique_labels: #------------------------------------------# # 获得某一类得分筛选后全部的预测结果 #------------------------------------------# detections_class = detections[detections[:, -1] == c] #------------------------------------------# # 使用官方自带的非极大抑制会速度更快一些！ #------------------------------------------# keep = nms( detections_class[:, :4], detections_class[:, 4] * detections_class[:, 5], nms_thres ) max_detections = detections_class[keep] # # 按照存在物体的置信度排序 # _, conf_sort_index = torch.sort(detections_class[:, 4]*detections_class[:, 5], descending=True) # detections_class = detections_class[conf_sort_index] # # 进行非极大抑制 # max_detections = [] # while detections_class.size(0): # # 取出这一类置信度最高的，一步一步往下判断，判断重合程度是否大于nms_thres，如果是则去除掉 # max_detections.append(detections_class[0].unsqueeze(0)) # if len(detections_class) == 1: # break # ious = bbox_iou(max_detections[-1], detections_class[1:]) # detections_class = detections_class[1:][ious &lt; nms_thres] # # 堆叠 # max_detections = torch.cat(max_detections).data # Add max detections to outputs output[i] = max_detections if output[i] is None else torch.cat((output[i], max_detections)) if output[i] is not None: output[i] = output[i].cpu().numpy() box_xy, box_wh = (output[i][:, 0:2] + output[i][:, 2:4])/2, output[i][:, 2:4] - output[i][:, 0:2] output[i][:, :4] = self.yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image) return output 训练部分loss组成计算loss实际上是网络的预测结果和网络的真实结果的对比。和网络的预测结果一样，网络的损失也由三个部分组成，分别是Reg部分、Obj部分、Cls部分。Reg部分是特征点的回归参数判断、Obj部分是特征点是否包含物体判断、Cls部分是特征点包含的物体的种类。 正样本的匹配过程在YoloV5中，训练时正样本的匹配过程可以分为两部分。a、匹配先验框。b、匹配特征点。 所谓正样本匹配，就是寻找哪些先验框被认为有对应的真实框，并且负责这个真实框的预测。 匹配先验框在YoloV5网络中，一共设计了9个不同大小的先验框。每个输出的特征层对应3个先验框。 对于任何一个真实框gt，YoloV5不再使用iou进行正样本的匹配，而是直接采用高宽比进行匹配，即使用真实框和9个不同大小的先验框计算宽高比。 如果真实框与某个先验框的宽高比例大于设定阈值，则说明该真实框和该先验框匹配度不够，将该先验框认为是负样本。 比如此时有一个真实框，它的宽高为[200, 200]，是一个正方形。YoloV5默认设置的9个先验框为[10,13], [16,30], [33,23], [30,61], [62,45], [59,119], [116,90], [156,198], [373,326]。设定阈值门限为4。 此时我们需要计算该真实框和9个先验框的宽高比例。比较宽高时存在两个情况，一个是真实框的宽高比先验框大，一个是先验框的宽高比真实框大。因此我们需要同时计算：真实框的宽高/先验框的宽高；先验框的宽高/真实框的宽高。然后在这其中选取最大值。 下个列表就是比较结果，这是一个shape为[9, 4]的矩阵，9代表9个先验框，4代表真实框的宽高/先验框的宽高；先验框的宽高/真实框的宽高。 123456789[[20. 15.38461538 0.05 0.065 ] [12.5 6.66666667 0.08 0.15 ] [ 6.06060606 8.69565217 0.165 0.115 ] [ 6.66666667 3.27868852 0.15 0.305 ] [ 3.22580645 4.44444444 0.31 0.225 ] [ 3.38983051 1.68067227 0.295 0.595 ] [ 1.72413793 2.22222222 0.58 0.45 ] [ 1.28205128 1.01010101 0.78 0.99 ] [ 0.53619303 0.61349693 1.865 1.63 ]] 然后对每个先验框的比较结果取最大值。获得下述矩阵： 12[20. 12.5 8.69565217 6.66666667 4.44444444 3.38983051 2.22222222 1.28205128 1.865 ] 之后我们判断，哪些先验框的比较结果的值小于门限。可以知道[59,119], [116,90], [156,198], [373,326]四个先验框均满足需求。 [116,90], [156,198], [373,326]属于20,20的特征层。[59,119]属于40,40的特征层。 此时我们已经可以判断哪些大小的先验框可用于该真实框的预测。 匹配特征点在过去的Yolo系列中，每个真实框由其中心点所在的网格内的左上角特征点来负责预测。 对于被选中的特征层，首先计算真实框落在哪个网格内，此时该网格左上角特征点便是一个负责预测的特征点。 同时利用四舍五入规则，找出最近的两个网格，将这三个网格都认为是负责预测该真实框的。 .bhyntxnfkblw{zoom:50%;} 红色点表示该真实框的中心，除了当前所处的网格外，其2个最近的邻域网格也被选中。从这里就可以发现预测框的XY轴偏移部分的取值范围不再是0-1，而是0.5-1.5。 找到对应特征点后，对应特征点在a中被选中的先验框负责该真实框的预测。 计算loss由第一部分可知，YoloV5的损失由三个部分组成：1、Reg部分，由第2部分可知道每个真实框对应的先验框，获取到每个框对应的先验框后，取出该先验框对应的预测框，利用真实框和预测框计算CIOU损失，作为Reg部分的Loss组成。2、Obj部分，由第2部分可知道每个真实框对应的先验框，所有真实框对应的先验框都是正样本，剩余的先验框均为负样本，根据正负样本和特征点的是否包含物体的预测结果计算交叉熵损失，作为Obj部分的Loss组成。3、Cls部分，由第三部分可知道每个真实框对应的先验框，获取到每个框对应的先验框后，取出该先验框的种类预测结果，根据真实框的种类和先验框的种类预测结果计算交叉熵损失，作为Cls部分的Loss组成。实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353import torchimport torch.nn as nnimport mathimport numpy as npclass YOLOLoss(nn.Module): def __init__(self, anchors, num_classes, input_shape, cuda, anchors_mask = [[6,7,8], [3,4,5], [0,1,2]], label_smoothing = 0): super(YOLOLoss, self).__init__() #-----------------------------------------------------------# # 13x13的特征层对应的anchor是[142, 110],[192, 243],[459, 401] # 26x26的特征层对应的anchor是[36, 75],[76, 55],[72, 146] # 52x52的特征层对应的anchor是[12, 16],[19, 36],[40, 28] #-----------------------------------------------------------# self.anchors = anchors self.num_classes = num_classes self.bbox_attrs = 5 + num_classes self.input_shape = input_shape self.anchors_mask = anchors_mask self.label_smoothing = label_smoothing self.threshold = 4 self.balance = [0.4, 1.0, 4] self.box_ratio = 5 self.cls_ratio = 0.5 self.obj_ratio = 1 self.cuda = cuda def clip_by_tensor(self, t, t_min, t_max): t = t.float() result = (t &gt;= t_min).float() * t + (t &lt; t_min).float() * t_min result = (result &lt;= t_max).float() * result + (result &gt; t_max).float() * t_max return result def MSELoss(self, pred, target): return torch.pow(pred - target, 2) def BCELoss(self, pred, target): epsilon = 1e-7 pred = self.clip_by_tensor(pred, epsilon, 1.0 - epsilon) output = - target * torch.log(pred) - (1.0 - target) * torch.log(1.0 - pred) return output def box_giou(self, b1, b2): \"\"\" 输入为： ---------- b1: tensor, shape=(batch, feat_w, feat_h, anchor_num, 4), xywh b2: tensor, shape=(batch, feat_w, feat_h, anchor_num, 4), xywh 返回为： ------- giou: tensor, shape=(batch, feat_w, feat_h, anchor_num, 1) \"\"\" #----------------------------------------------------# # 求出预测框左上角右下角 #----------------------------------------------------# b1_xy = b1[..., :2] b1_wh = b1[..., 2:4] b1_wh_half = b1_wh/2. b1_mins = b1_xy - b1_wh_half b1_maxes = b1_xy + b1_wh_half #----------------------------------------------------# # 求出真实框左上角右下角 #----------------------------------------------------# b2_xy = b2[..., :2] b2_wh = b2[..., 2:4] b2_wh_half = b2_wh/2. b2_mins = b2_xy - b2_wh_half b2_maxes = b2_xy + b2_wh_half #----------------------------------------------------# # 求真实框和预测框所有的iou #----------------------------------------------------# intersect_mins = torch.max(b1_mins, b2_mins) intersect_maxes = torch.min(b1_maxes, b2_maxes) intersect_wh = torch.max(intersect_maxes - intersect_mins, torch.zeros_like(intersect_maxes)) intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1] b1_area = b1_wh[..., 0] * b1_wh[..., 1] b2_area = b2_wh[..., 0] * b2_wh[..., 1] union_area = b1_area + b2_area - intersect_area iou = intersect_area / union_area #----------------------------------------------------# # 找到包裹两个框的最小框的左上角和右下角 #----------------------------------------------------# enclose_mins = torch.min(b1_mins, b2_mins) enclose_maxes = torch.max(b1_maxes, b2_maxes) enclose_wh = torch.max(enclose_maxes - enclose_mins, torch.zeros_like(intersect_maxes)) #----------------------------------------------------# # 计算对角线距离 #----------------------------------------------------# enclose_area = enclose_wh[..., 0] * enclose_wh[..., 1] giou = iou - (enclose_area - union_area) / enclose_area return giou #---------------------------------------------------# # 平滑标签 #---------------------------------------------------# def smooth_labels(self, y_true, label_smoothing, num_classes): return y_true * (1.0 - label_smoothing) + label_smoothing / num_classes def forward(self, l, input, targets=None): #----------------------------------------------------# # l 代表使用的是第几个有效特征层 # input的shape为 bs, 3*(5+num_classes), 13, 13 # bs, 3*(5+num_classes), 26, 26 # bs, 3*(5+num_classes), 52, 52 # targets 真实框的标签情况 [batch_size, num_gt, 5] #----------------------------------------------------# #--------------------------------# # 获得图片数量，特征层的高和宽 #--------------------------------# bs = input.size(0) in_h = input.size(2) in_w = input.size(3) #-----------------------------------------------------------------------# # 计算步长 # 每一个特征点对应原来的图片上多少个像素点 # # 如果特征层为13x13的话，一个特征点就对应原来的图片上的32个像素点 # 如果特征层为26x26的话，一个特征点就对应原来的图片上的16个像素点 # 如果特征层为52x52的话，一个特征点就对应原来的图片上的8个像素点 # stride_h = stride_w = 32、16、8 #-----------------------------------------------------------------------# stride_h = self.input_shape[0] / in_h stride_w = self.input_shape[1] / in_w #-------------------------------------------------# # 此时获得的scaled_anchors大小是相对于特征层的 #-------------------------------------------------# scaled_anchors = [(a_w / stride_w, a_h / stride_h) for a_w, a_h in self.anchors] #-----------------------------------------------# # 输入的input一共有三个，他们的shape分别是 # bs, 3 * (5+num_classes), 13, 13 =&gt; bs, 3, 5 + num_classes, 13, 13 =&gt; batch_size, 3, 13, 13, 5 + num_classes # batch_size, 3, 13, 13, 5 + num_classes # batch_size, 3, 26, 26, 5 + num_classes # batch_size, 3, 52, 52, 5 + num_classes #-----------------------------------------------# prediction = input.view(bs, len(self.anchors_mask[l]), self.bbox_attrs, in_h, in_w).permute(0, 1, 3, 4, 2).contiguous() #-----------------------------------------------# # 先验框的中心位置的调整参数 #-----------------------------------------------# x = torch.sigmoid(prediction[..., 0]) y = torch.sigmoid(prediction[..., 1]) #-----------------------------------------------# # 先验框的宽高调整参数 #-----------------------------------------------# w = torch.sigmoid(prediction[..., 2]) h = torch.sigmoid(prediction[..., 3]) #-----------------------------------------------# # 获得置信度，是否有物体 #-----------------------------------------------# conf = torch.sigmoid(prediction[..., 4]) #-----------------------------------------------# # 种类置信度 #-----------------------------------------------# pred_cls = torch.sigmoid(prediction[..., 5:]) #-----------------------------------------------# # 获得网络应该有的预测结果 #-----------------------------------------------# y_true, noobj_mask, box_loss_scale = self.get_target(l, targets, scaled_anchors, in_h, in_w) #---------------------------------------------------------------# # 将预测结果进行解码，判断预测结果和真实值的重合程度 # 如果重合程度过大则忽略，因为这些特征点属于预测比较准确的特征点 # 作为负样本不合适 #----------------------------------------------------------------# pred_boxes = self.get_pred_boxes(l, x, y, h, w, targets, scaled_anchors, in_h, in_w) if self.cuda: y_true = y_true.cuda() noobj_mask = noobj_mask.cuda() box_loss_scale = box_loss_scale.cuda() #-----------------------------------------------------------# # reshape_y_true[...,2:3]和reshape_y_true[...,3:4] # 表示真实框的宽高，二者均在0-1之间 # 真实框越大，比重越小，小框的比重更大。 #-----------------------------------------------------------# box_loss_scale = 2 - box_loss_scale #---------------------------------------------------------------# # 计算预测结果和真实结果的giou #----------------------------------------------------------------# giou = self.box_giou(pred_boxes[y_true[..., 4] == 1], y_true[..., :4][y_true[..., 4] == 1]) loss_loc = torch.sum((1 - giou) * box_loss_scale[y_true[..., 4] == 1]) #-----------------------------------------------------------# # 计算置信度的loss #-----------------------------------------------------------# loss_conf = torch.sum(self.BCELoss(conf[y_true[..., 4] == 1], giou.detach().clamp(0))) + \\ torch.sum(self.BCELoss(conf, y_true[..., 4]) * noobj_mask) loss_cls = torch.sum(self.BCELoss(pred_cls[y_true[..., 4] == 1], self.smooth_labels(y_true[..., 5:][y_true[..., 4] == 1], self.label_smoothing, self.num_classes))) loss = loss_loc * self.box_ratio + loss_conf * self.balance[l] * self.obj_ratio + loss_cls * self.cls_ratio num_pos = torch.sum(y_true[..., 4]) num_pos = torch.max(num_pos, torch.ones_like(num_pos)) return loss, num_pos def get_near_points(self, x, y, i, j): sub_x = x - i sub_y = y - j if sub_x &gt; 0.5 and sub_y &gt; 0.5: return [[0, 0], [1, 0], [0, 1]] elif sub_x &lt; 0.5 and sub_y &gt; 0.5: return [[0, 0], [-1, 0], [0, 1]] elif sub_x &lt; 0.5 and sub_y &lt; 0.5: return [[0, 0], [-1, 0], [0, -1]] else: return [[0, 0], [1, 0], [0, -1]] def get_target(self, l, targets, anchors, in_h, in_w): #-----------------------------------------------------# # 计算一共有多少张图片 #-----------------------------------------------------# bs = len(targets) #-----------------------------------------------------# # 用于选取哪些先验框不包含物体 #-----------------------------------------------------# noobj_mask = torch.ones(bs, len(self.anchors_mask[l]), in_h, in_w, requires_grad = False) #-----------------------------------------------------# # 让网络更加去关注小目标 #-----------------------------------------------------# box_loss_scale = torch.zeros(bs, len(self.anchors_mask[l]), in_h, in_w, requires_grad = False) #-----------------------------------------------------# # anchors_best_ratio #-----------------------------------------------------# box_best_ratio = torch.zeros(bs, len(self.anchors_mask[l]), in_h, in_w, requires_grad = False) #-----------------------------------------------------# # batch_size, 3, 13, 13, 5 + num_classes #-----------------------------------------------------# y_true = torch.zeros(bs, len(self.anchors_mask[l]), in_h, in_w, self.bbox_attrs, requires_grad = False) for b in range(bs): if len(targets[b])==0: continue batch_target = torch.zeros_like(targets[b]) #-------------------------------------------------------# # 计算出正样本在特征层上的中心点 #-------------------------------------------------------# batch_target[:, [0,2]] = targets[b][:, [0,2]] * in_w batch_target[:, [1,3]] = targets[b][:, [1,3]] * in_h batch_target[:, 4] = targets[b][:, 4] batch_target = batch_target.cpu() #-------------------------------------------------------# # batch_target : num_true_box, 4 # anchors : 9, 2 # # ratios_of_gt_anchors : num_true_box, 9, 2 # ratios_of_anchors_gt : num_true_box, 9, 2 # # ratios : num_true_box, 9, 4 # max_ratios : num_true_box, 9 #-------------------------------------------------------# ratios_of_gt_anchors = torch.unsqueeze(batch_target[:, 2:4], 1) / torch.unsqueeze(torch.FloatTensor(anchors), 0) ratios_of_anchors_gt = torch.unsqueeze(torch.FloatTensor(anchors), 0) / torch.unsqueeze(batch_target[:, 2:4], 1) ratios = torch.cat([ratios_of_gt_anchors, ratios_of_anchors_gt], dim = -1) max_ratios, _ = torch.max(ratios, dim = -1) for t, ratio in enumerate(max_ratios): #-------------------------------------------------------# # ratio : 9 #-------------------------------------------------------# over_threshold = ratio &lt; self.threshold over_threshold[torch.argmin(ratio)] = True for k, mask in enumerate(self.anchors_mask[l]): if not over_threshold[mask]: continue #----------------------------------------# # 获得真实框属于哪个网格点 #----------------------------------------# i = torch.floor(batch_target[t, 0]).long() j = torch.floor(batch_target[t, 1]).long() offsets = self.get_near_points(batch_target[t, 0], batch_target[t, 1], i, j) for offset in offsets: local_i = i + offset[0] local_j = j + offset[1] if local_i &gt;= in_w or local_i &lt; 0 or local_j &gt;= in_h or local_j &lt; 0: continue if box_best_ratio[b, k, local_j, local_i] != 0: if box_best_ratio[b, k, local_j, local_i] &gt; ratio[mask]: y_true[b, k, local_j, local_i, :] = 0 else: continue #----------------------------------------# # 取出真实框的种类 #----------------------------------------# c = batch_target[t, 4].long() #----------------------------------------# # noobj_mask代表无目标的特征点 #----------------------------------------# noobj_mask[b, k, local_j, local_i] = 0 #----------------------------------------# # tx、ty代表中心调整参数的真实值 #----------------------------------------# y_true[b, k, local_j, local_i, 0] = batch_target[t, 0] y_true[b, k, local_j, local_i, 1] = batch_target[t, 1] y_true[b, k, local_j, local_i, 2] = batch_target[t, 2] y_true[b, k, local_j, local_i, 3] = batch_target[t, 3] y_true[b, k, local_j, local_i, 4] = 1 y_true[b, k, local_j, local_i, c + 5] = 1 #----------------------------------------# # 用于获得xywh的比例 # 大目标loss权重小，小目标loss权重大 #----------------------------------------# box_loss_scale[b, k, local_j, local_i] = batch_target[t, 2] * batch_target[t, 3] / in_w / in_h #----------------------------------------# # 获得当前先验框最好的比例 #----------------------------------------# box_best_ratio[b, k, local_j, local_i] = ratio[mask] return y_true, noobj_mask, box_loss_scale def get_pred_boxes(self, l, x, y, h, w, targets, scaled_anchors, in_h, in_w): #-----------------------------------------------------# # 计算一共有多少张图片 #-----------------------------------------------------# bs = len(targets) FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor #-----------------------------------------------------# # 生成网格，先验框中心，网格左上角 #-----------------------------------------------------# grid_x = torch.linspace(0, in_w - 1, in_w).repeat(in_h, 1).repeat( int(bs * len(self.anchors_mask[l])), 1, 1).view(x.shape).type(FloatTensor) grid_y = torch.linspace(0, in_h - 1, in_h).repeat(in_w, 1).t().repeat( int(bs * len(self.anchors_mask[l])), 1, 1).view(y.shape).type(FloatTensor) # 生成先验框的宽高 scaled_anchors_l = np.array(scaled_anchors)[self.anchors_mask[l]] anchor_w = FloatTensor(scaled_anchors_l).index_select(1, LongTensor([0])) anchor_h = FloatTensor(scaled_anchors_l).index_select(1, LongTensor([1])) anchor_w = anchor_w.repeat(bs, 1).repeat(1, 1, in_h * in_w).view(w.shape) anchor_h = anchor_h.repeat(bs, 1).repeat(1, 1, in_h * in_w).view(h.shape) #-------------------------------------------------------# # 计算调整后的先验框中心与宽高 #-------------------------------------------------------# pred_boxes_x = torch.unsqueeze(x * 2. - 0.5 + grid_x, -1) pred_boxes_y = torch.unsqueeze(y * 2. - 0.5 + grid_y, -1) pred_boxes_w = torch.unsqueeze((w * 2) ** 2 * anchor_w, -1) pred_boxes_h = torch.unsqueeze((h * 2) ** 2 * anchor_h, -1) pred_boxes = torch.cat([pred_boxes_x, pred_boxes_y, pred_boxes_w, pred_boxes_h], dim = -1) return pred_boxes","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"yolov5","slug":"yolov5","permalink":"http://pistachio0812.github.io/tags/yolov5/"}],"author":"coolboy"},{"title":"Resnet论文笔记","slug":"Resnet","date":"2022-05-03T10:49:34.707Z","updated":"2022-10-04T13:27:17.178Z","comments":true,"path":"zh-CN/Resnet/","permalink":"http://pistachio0812.github.io/zh-CN/Resnet/","excerpt":"","text":"参考博文： 1.faster-rcnn-pytorch代码 2.Deep Residual Learning for Image Recognition (thecvf.com) 3.ResNet50结构 4.Pytorch搭建Faster R-CNN目标检测平台 网络结构该网络是从原论文搬过来的，也就是参考博文2里的内容。有兴趣的可以读读原论文。 .piqtdhmekwcn{} 代码复现具体网络结构： 1.Bottleneck .krdfyygcixin{zoom:50%;} 2.Resnet50中50的含义：（3+4+6+3)*3+2=50（卷积全连接层数之和） 2.resnet101同理，只要把model = ResNet(Bottleneck, [3, 4, 6, 3])改成model=ResNet(Bottleneck, [3, 4, 23, 3]),101=(3+4+23+3)*3+2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153import mathimport torch.nn as nnfrom torch.hub import load_state_dict_from_url# c0表示输入特征图通道数，c1表示输出特征图通道数，由此可见，该类作用为通道扩张4倍class Bottleneck(nn.Module): expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None): super(Bottleneck, self).__init__() # (N, C0, H, W)-&gt;(N, C1, H, W) self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) # (N, C1, H, W)-&gt;(N, C1, H, W) self.bn1 = nn.BatchNorm2d(planes) # (N, C1, H, W)-&gt;(N, C1, H, W) self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # (N, C1, H, W)-&gt;(N, C1, H, W) self.bn2 = nn.BatchNorm2d(planes) # (N, C1, H, W)-&gt;(N, 4*C1, H, W) self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False) # (N, 4*C1, H, W)-&gt;(N, 4*C1, H, W) self.bn3 = nn.BatchNorm2d(planes * 4) # (N, 4*C1, H, W)-&gt;(N, 4*C1, H, W) self.relu = nn.ReLU(inplace=True) # defalt is None self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return outclass ResNet(nn.Module): def __init__(self, block, layers, num_classes=1000): #-----------------------------------# # 假设输入进来的图片是600,600,3 #-----------------------------------# self.inplanes = 64 super(ResNet, self).__init__() # 600,600,3 -&gt; 300,300,64 self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU(inplace=True) # 300,300,64 -&gt; 150,150,64 self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True) # 150,150,64 -&gt; 150,150,256 # 此时stride=1,但通道和原来一样，进行下采样，通道变为4*64=256 self.layer1 = self._make_layer(block, 64, layers[0]) # 150,150,256 -&gt; 75,75,512 # 此时stride=2,进行下采样，宽高变为原来的1/2，通道变为4*128=512 self.layer2 = self._make_layer(block, 128, layers[1], stride=2) # 75,75,512 -&gt; 38,38,1024 到这里可以获得一个38,38,1024的共享特征层 self.layer3 = self._make_layer(block, 256, layers[2], stride=2) # self.layer4被用在classifier模型中 # 38,38,1024 -&gt; 19,19,2048 self.layer4 = self._make_layer(block, 512, layers[3], stride=2) # 19,19,2048 -&gt; 2,2,2048 self.avgpool = nn.AvgPool2d(7) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() # 定义每个层的操作，比如在resnet50里conv2_x重复了三次，那么就是block=Bottleck,blocks=3 # 其实conv3_x,conv4_x,conv5_x中block都是Bottlenecck,就是重复的次数不一样 def _make_layer(self, block, planes, blocks, stride=1): downsample = None #-------------------------------------------------------------------# # 当模型需要进行高和宽的压缩的时候，或者通道不为原来的4倍，就需要用到残差边的downsample # 个人认为这里通道变化不大（相比于2倍上层特征图通道数）也需要downsample是因为通道变化不 # 大，信息不够细致 #-------------------------------------------------------------------# if stride != 1 or self.inplanes != planes * block.expansion: # (N, C0, H, W)-&gt;(N, 4*C1, H, W) downsample = nn.Sequential( nn.Conv2d(self.inplanes, planes * block.expansion,kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes * block.expansion), ) # layers记录块结果 layers = [] layers.append(block(self.inplanes, planes, stride, downsample)) self.inplanes = planes * block.expansion for i in range(1, blocks): layers.append(block(self.inplanes, planes)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) # 接上面假设，此时x=(N, 2, 2, 2048) x = self.avgpool(x) # (N, 2, 2, 2048)-&gt;(N, 4096) x = x.view(x.size(0), -1) # (N,4096)-&gt;(N, 1000) x = self.fc(x) return xdef resnet50(pretrained = False): model = ResNet(Bottleneck, [3, 4, 6, 3]) if pretrained: state_dict = load_state_dict_from_url(\"https://download.pytorch.org/models/resnet50-19c8e357.pth\", model_dir=\"./model_data\") model.load_state_dict(state_dict) #----------------------------------------------------------------------------# # 获取特征提取部分，从conv1到model.layer3，最终获得一个38,38,1024的特征层 #----------------------------------------------------------------------------# features = list([model.conv1, model.bn1, model.relu, model.maxpool, model.layer1, model.layer2, model.layer3]) #----------------------------------------------------------------------------# # 获取分类部分，从model.layer4到model.avgpool #----------------------------------------------------------------------------# classifier = list([model.layer4, model.avgpool]) features = nn.Sequential(*features) classifier = nn.Sequential(*classifier) return features, classifier","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"resnet","slug":"resnet","permalink":"http://pistachio0812.github.io/tags/resnet/"}]},{"title":"fork me on github","slug":"Fork_github","date":"2022-05-02T13:46:32.496Z","updated":"2022-10-31T01:40:23.867Z","comments":true,"path":"zh-CN/Fork_github/","permalink":"http://pistachio0812.github.io/zh-CN/Fork_github/","excerpt":"","text":"参考博文： 1.如何在博客园添加 Fork me on GitHub 彩带效果 2.GitHub Ribbons | The GitHub Blog 3.Hexo添加Follow me on CSDN效果 博客园添加fork进入博客园后台进入博客园的管理界面，依次点击 管理 -&gt; 设置，进入到设置页面后，将页面拖动到最下面，您会看到：页首Html代码一栏 .yudzxhsektef{zoom:50%;} 添加代码在页面Html代码框中输入如下代码： 12345&lt;a href=\"https://github.com/pistachio\"&gt; &lt;img style=\"position: fixed; top: 0; right: 0; border: 0; z-index:9999;\" src=\"https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png\" alt=\"Fork me on GitHub\"&gt;&lt;/a&gt; 注意，您需要将 &lt;a href=\"\"&gt; 中的链接换成您自己的 GitHub 主页地址。 保存后，随意打开一篇您自己的博客，就可以看见和教程开头展示的效果一样了。大功告成！ 更换彩带颜色我上面使用的是红色的彩带，如果您需要更换成其他颜色，只需将 &lt;img&gt; 标签中的 src 地址更换成您想要颜色的地址即可。 1234567891011# 绿色https://github.blog/wp-content/uploads/2008/12/forkme_right_green_007200.png# 黑色https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png# 橘黄色https://github.blog/wp-content/uploads/2008/12/forkme_right_orange_ff7600.png# 灰色https://github.blog/wp-content/uploads/2008/12/forkme_right_gray_6d6d6d.png# 白色 https://github.blog/wp-content/uploads/2008/12/forkme_right_white_ffffff.png 从上面挑选一款您喜欢的样式颜色吧！！ hexo添加fork实现方法粘贴复制如下的代码到themes\\hexo-theme-next\\layout\\layout.ejs文件中(放在&lt;div id=\"l_body\"&gt;&lt;/div&gt;的下面 如图)，并把href改为你的csdn主页,换成github同上，记得换上自己的链接。 123456789# 黑色版本# 个人认为position属性值改为fixed好一点~ &lt;!--Follow me on CSDN--&gt; &lt;a href=\"https://blog.csdn.net/qq_38452951\"&gt; &lt;img loading=\"lazy\" width=\"149\" height=\"149\" style=\"position: absolute; top: 0; right: 0; border: 0;\" src=\"https://img-blog.csdnimg.cn/abe3797b7d77419b81ecc02dd1bf8c34.png\" class=\"attachment-full size-full\" alt=\"Fork me on GitHub\" data-recalc-dims=\"1\"&gt;&lt;/a&gt;# 白色版本 &lt;!--Follow me on CSDN--&gt; &lt;a href=\"https://blog.csdn.net/qq_38452951\"&gt;&lt;img loading=\"lazy\" width=\"149\" height=\"149\" style=\"position: absolute; top: 0; right: 0; border: 0;\" src=\"https://img-blog.csdnimg.cn/1f8e1ef9be9f4f7db01fe3a2d57829de.png\" class=\"attachment-full size-full\" alt=\"Fork me on GitHub\" data-recalc-dims=\"1\"&gt;&lt;/a&gt; .poketggshnql{zoom:50%;} 效果图详情请见： 1.追风赶月的少年 - 博客园 (cnblogs.com) 2.相思似海深旧事如天远 (pistachio0812.github.io)","categories":[{"name":"Hexo博客搭建","slug":"Hexo博客搭建","permalink":"http://pistachio0812.github.io/categories/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"博客提升","slug":"博客提升","permalink":"http://pistachio0812.github.io/tags/%E5%8D%9A%E5%AE%A2%E6%8F%90%E5%8D%87/"},{"name":"fork","slug":"fork","permalink":"http://pistachio0812.github.io/tags/fork/"}],"author":"pistachio"},{"title":"Sanderson the Man 1","slug":"The_story_of_a_great_schoolmaster","date":"2022-05-01T08:13:09.288Z","updated":"2022-10-04T13:29:53.244Z","comments":true,"path":"zh-CN/The_story_of_a_great_schoolmaster/","permalink":"http://pistachio0812.github.io/zh-CN/The_story_of_a_great_schoolmaster/","excerpt":"","text":"Of all the men I have met—and I have now had a fairly long and active life and have met a very great variety of interesting people—one only has stirred me to a biographical effort. This one exception is F. W. Sanderson, for many years the headmaster of Oundle School. I think him beyond question the greatest man I have ever known with any degree of intimacy1, and it is in the hope of conveying to others something of my sense not merely of his importance, but of his peculiar2 genius and the rich humanity of his character, that I am setting out to write this book. He was in himself a very delightful[Pg 2] mixture of subtlety3 and simplicity4, generosity5, adventurousness6, imagination and steadfast7 purpose, and he approached the general life of our time at such an angle as to reflect the most curious and profitable lights upon it. To tell his story is to reflect upon all the main educational ideas of the last half-century, and to revise our conception of the process and purpose of the modern community in relation to education. For Sanderson had a mind like an octopus8, it seemed always to have a tentacle9 free to reach out beyond what was already held, and his tentacles10 grew and radiated farther and farther. Before his end he had come to a vision of the school as a centre for the complete reorganisation of civilised life.","categories":[{"name":"novel","slug":"novel","permalink":"http://pistachio0812.github.io/categories/novel/"}],"tags":[],"author":"coolboy"},{"title":"感受野的计算","slug":"感受野计算","date":"2022-04-27T13:24:03.069Z","updated":"2022-10-04T13:38:34.319Z","comments":true,"path":"zh-CN/感受野计算/","permalink":"http://pistachio0812.github.io/zh-CN/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/","excerpt":"","text":"参考博文： 1.感受野的计算 2.感受野 | 机器之心 计算机视觉中常常出现感受野的概念，我是在修改SSD网络中的过程中，发现增强感受野是非常有必要的。SSD的主干网络是VGG16,其中有个结论：低层特征图的感受野较小，高层特征图的感受野较大。 感受野定义神经网络中每一层输出特征图上的像素点在输入图片上的映射的区域大小，也就是特征图上的每一个点对应的输入图片的区域。 计算公式.jdnpokjuwnsm{zoom:50%;} 示例在VGG16中：pool5中pool5:RF=2conv5_3 ：RF=(2-1)1+3=4conv5_2 : RF=(4-1)1+3=6conv5_2: RF=(6-1)1+3=8conv5_1: RF=(8-1)2+2=16pool4 : RF=(8-1)2+2=16conv4_3: RF=(16-1)1+3=18conv4_2: RF=(18-1)1+3=20conv4_1 : RF=(20-1)1+3=22pool3: RF=(22-1)2+2=44conv3_3: RF=(44-1)1+3=46conv3_2: RF=(46-1)1+3=48conv3_1: RF=(48-1)1+3=50pool2: RF=(50-1)2+2=100conv2_2: RF=(150-1)1+3=152conv2_1: RF=（152-1)1+3=154pool1: RF=(154-1)2+2=208conv1_2: RF=(208-1)1+3=210conv1_1: RF=(210-1)1+3=212计算结果为：pool5输出的特征图在输入图片上的感受野为212* 结果如下图所示： .mqjdcuysruof{}","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"感受野","slug":"感受野","permalink":"http://pistachio0812.github.io/tags/%E6%84%9F%E5%8F%97%E9%87%8E/"}],"author":"pistachio"},{"title":"InceptionV3论文笔记","slug":"InceptionV3","date":"2022-04-22T01:38:19.564Z","updated":"2022-10-04T13:17:51.125Z","comments":true,"path":"zh-CN/InceptionV3/","permalink":"http://pistachio0812.github.io/zh-CN/InceptionV3/","excerpt":"","text":"参考博文： 论文地址：Inceptionv3 源码地址：InceptionV3_TensorFlow: Inception v3 文章引用出处1：https://blog.csdn.net/weixin_44791964/article/details/102802866 文章引用出处2：InceptionV3 网络模型 InceptionV3模型InceptionV3模型是谷歌Inception系列里面的第三代模型，其模型结构与InceptionV2模型放在了同一篇论文里，其实二者模型结构差距不大，相比于其它神经网络模型，Inception网络最大的特点在于将神经网络层与层之间的卷积运算进行了拓展。如VGG，AlexNet网络，它就是一直卷积下来的，一层接着一层；ResNet则是创新性的引入了残差网络的概念，使得靠前若干层的某一层数据输出直接跳过多层引入到后面数据层的输入部分，后面的特征层的内容会有一部分由其前面的某一层线性贡献。而Inception网络则是采用不同大小的卷积核，使得存在不同大小的感受野，最后实现拼接达到不同尺度特征的融合。对于InceptionV3而言，其网络中存在着如下的结构。这个结构使用不同大小的卷积核对输入进行卷积（这个结构主要在代码中的block1使用）。 .fenomnymgxlc{} 还存在着这样的结构，利用1x7的卷积和7x1的卷积代替7x7的卷积，这样可以只使用约（1x7 + 7x1) / (7x7) = 28.6%的计算开销；利用1x3的卷积和3x1的卷积代替3x3的卷积，这样可以只使用约（1x3 + 3x1) / (3x3) = 67%的计算开销。下图利用1x7的卷积和7x1的卷积代替7x7的卷积（这个结构主要在代码中的block2使用）。 .cfamatcnsjhv{} 下图利用1x3的卷积和3x1的卷积代替3x3的卷积（这个结构主要在代码中的block3使用）。 .osqfuleogbtc{} 网络部分实现代码一共将InceptionV3划分为3个block，对应着35x35、17x17，8x8维度大小的图像。每个block中间有许多的part，对应着不同的特征层深度，用于特征提取。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253#-------------------------------------------------------------## InceptionV3的网络部分#-------------------------------------------------------------#from __future__ import print_functionfrom __future__ import absolute_importimport warningsimport numpy as npfrom keras.models import Modelfrom keras import layersfrom keras.layers import Activation,Dense,Input,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2Dfrom keras.layers import GlobalAveragePooling2D,GlobalMaxPooling2Dfrom keras.engine.topology import get_source_inputsfrom keras.utils.layer_utils import convert_all_kernels_in_modelfrom keras.utils.data_utils import get_filefrom keras import backend as Kfrom keras.applications.imagenet_utils import decode_predictionsfrom keras.preprocessing import imagedef conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(1, 1), name=None): if name is not None: bn_name = name + '_bn' conv_name = name + '_conv' else: bn_name = None conv_name = None x = Conv2D( filters, (num_row, num_col), strides=strides, padding=padding, use_bias=False, name=conv_name)(x) x = BatchNormalization(scale=False, name=bn_name)(x) x = Activation('relu', name=name)(x) return xdef InceptionV3(input_shape=[299,299,3], classes=1000): img_input = Input(shape=input_shape) x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding='valid') x = conv2d_bn(x, 32, 3, 3, padding='valid') x = conv2d_bn(x, 64, 3, 3) x = MaxPooling2D((3, 3), strides=(2, 2))(x) x = conv2d_bn(x, 80, 1, 1, padding='valid') x = conv2d_bn(x, 192, 3, 3, padding='valid') x = MaxPooling2D((3, 3), strides=(2, 2))(x) #--------------------------------# # Block1 35x35 #--------------------------------# # Block1 part1 # 35 x 35 x 192 -&gt; 35 x 35 x 256 branch1x1 = conv2d_bn(x, 64, 1, 1) branch5x5 = conv2d_bn(x, 48, 1, 1) branch5x5 = conv2d_bn(branch5x5, 64, 5, 5) branch3x3dbl = conv2d_bn(x, 64, 1, 1) branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3) branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3) branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x) branch_pool = conv2d_bn(branch_pool, 32, 1, 1) x = layers.concatenate( [branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=3, name='mixed0') # Block1 part2 # 35 x 35 x 256 -&gt; 35 x 35 x 288 branch1x1 = conv2d_bn(x, 64, 1, 1) branch5x5 = conv2d_bn(x, 48, 1, 1) branch5x5 = conv2d_bn(branch5x5, 64, 5, 5) branch3x3dbl = conv2d_bn(x, 64, 1, 1) branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3) branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3) branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x) branch_pool = conv2d_bn(branch_pool, 64, 1, 1) x = layers.concatenate( [branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=3, name='mixed1') # Block1 part3 # 35 x 35 x 288 -&gt; 35 x 35 x 288 branch1x1 = conv2d_bn(x, 64, 1, 1) branch5x5 = conv2d_bn(x, 48, 1, 1) branch5x5 = conv2d_bn(branch5x5, 64, 5, 5) branch3x3dbl = conv2d_bn(x, 64, 1, 1) branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3) branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3) branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x) branch_pool = conv2d_bn(branch_pool, 64, 1, 1) x = layers.concatenate( [branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=3, name='mixed2') #--------------------------------# # Block2 17x17 #--------------------------------# # Block2 part1 # 35 x 35 x 288 -&gt; 17 x 17 x 768 branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding='valid') branch3x3dbl = conv2d_bn(x, 64, 1, 1) branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3) branch3x3dbl = conv2d_bn( branch3x3dbl, 96, 3, 3, strides=(2, 2), padding='valid') branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x) x = layers.concatenate( [branch3x3, branch3x3dbl, branch_pool], axis=3, name='mixed3') # Block2 part2 # 17 x 17 x 768 -&gt; 17 x 17 x 768 branch1x1 = conv2d_bn(x, 192, 1, 1) branch7x7 = conv2d_bn(x, 128, 1, 1) branch7x7 = conv2d_bn(branch7x7, 128, 1, 7) branch7x7 = conv2d_bn(branch7x7, 192, 7, 1) branch7x7dbl = conv2d_bn(x, 128, 1, 1) branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1) branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7) branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1) branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7) branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x) branch_pool = conv2d_bn(branch_pool, 192, 1, 1) x = layers.concatenate( [branch1x1, branch7x7, branch7x7dbl, branch_pool], axis=3, name='mixed4') # Block2 part3 and part4 # 17 x 17 x 768 -&gt; 17 x 17 x 768 -&gt; 17 x 17 x 768 for i in range(2): branch1x1 = conv2d_bn(x, 192, 1, 1) branch7x7 = conv2d_bn(x, 160, 1, 1) branch7x7 = conv2d_bn(branch7x7, 160, 1, 7) branch7x7 = conv2d_bn(branch7x7, 192, 7, 1) branch7x7dbl = conv2d_bn(x, 160, 1, 1) branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1) branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7) branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1) branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7) branch_pool = AveragePooling2D( (3, 3), strides=(1, 1), padding='same')(x) branch_pool = conv2d_bn(branch_pool, 192, 1, 1) x = layers.concatenate( [branch1x1, branch7x7, branch7x7dbl, branch_pool], axis=3, name='mixed' + str(5 + i)) # Block2 part5 # 17 x 17 x 768 -&gt; 17 x 17 x 768 branch1x1 = conv2d_bn(x, 192, 1, 1) branch7x7 = conv2d_bn(x, 192, 1, 1) branch7x7 = conv2d_bn(branch7x7, 192, 1, 7) branch7x7 = conv2d_bn(branch7x7, 192, 7, 1) branch7x7dbl = conv2d_bn(x, 192, 1, 1) branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1) branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7) branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1) branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7) branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x) branch_pool = conv2d_bn(branch_pool, 192, 1, 1) x = layers.concatenate( [branch1x1, branch7x7, branch7x7dbl, branch_pool], axis=3, name='mixed7') #--------------------------------# # Block3 8x8 #--------------------------------# # Block3 part1 # 17 x 17 x 768 -&gt; 8 x 8 x 1280 branch3x3 = conv2d_bn(x, 192, 1, 1) branch3x3 = conv2d_bn(branch3x3, 320, 3, 3, strides=(2, 2), padding='valid') branch7x7x3 = conv2d_bn(x, 192, 1, 1) branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7) branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1) branch7x7x3 = conv2d_bn( branch7x7x3, 192, 3, 3, strides=(2, 2), padding='valid') branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x) x = layers.concatenate( [branch3x3, branch7x7x3, branch_pool], axis=3, name='mixed8') # Block3 part2 part3 # 8 x 8 x 1280 -&gt; 8 x 8 x 2048 -&gt; 8 x 8 x 2048 for i in range(2): branch1x1 = conv2d_bn(x, 320, 1, 1) branch3x3 = conv2d_bn(x, 384, 1, 1) branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3) branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1) branch3x3 = layers.concatenate( [branch3x3_1, branch3x3_2], axis=3, name='mixed9_' + str(i)) branch3x3dbl = conv2d_bn(x, 448, 1, 1) branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3) branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3) branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1) branch3x3dbl = layers.concatenate( [branch3x3dbl_1, branch3x3dbl_2], axis=3) branch_pool = AveragePooling2D( (3, 3), strides=(1, 1), padding='same')(x) branch_pool = conv2d_bn(branch_pool, 192, 1, 1) x = layers.concatenate( [branch1x1, branch3x3, branch3x3dbl, branch_pool], axis=3, name='mixed' + str(9 + i)) # 平均池化后全连接。 x = GlobalAveragePooling2D(name='avg_pool')(x) x = Dense(classes, activation='softmax', name='predictions')(x) inputs = img_input model = Model(inputs, x, name='inception_v3') return model 预测部分建立网络后就可以进行预测了，实现代码如下： 12345678910111213141516171819202122def preprocess_input(x): x /= 255. x -= 0.5 x *= 2. return xif __name__ == '__main__': model = InceptionV3() model.load_weights(\"inception_v3_weights_tf_dim_ordering_tf_kernels.h5\") img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(299, 299)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) preds = model.predict(x) print('Predicted:', decode_predictions(preds)) 预测所需的已经训练好的InceptionV3模型可以在https://github.com/fchollet/deep-learning-models/releases下载。非常方便。预测结果为： 12Predicted: [[('n02504458', 'African_elephant', 0.50874853), ('n01871265', 'tusker', 0.19524273), ('n02504013', 'Indian_elephant', 0.1566972), ('n01917289', 'brain_coral', 0.0008956835), ('n01695060', 'Komodo_dragon', 0.0008260256)]]","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"Inception","slug":"Inception","permalink":"http://pistachio0812.github.io/tags/Inception/"}],"author":"pistachio"},{"title":"未婚妻","slug":"未婚妻","date":"2022-04-22T01:06:39.876Z","updated":"2022-10-04T13:46:11.729Z","comments":true,"path":"zh-CN/未婚妻/","permalink":"http://pistachio0812.github.io/zh-CN/%E6%9C%AA%E5%A9%9A%E5%A6%BB/","excerpt":"","text":"度过了几天假期之后，我要回巴黎了。 当我走进车站，火车已挤满了旅客。大多数的车门前，都站着一个男人或一个妇女，好像是在拦阻后来的旅客。 尽管如此，我还是踮起脚尖向每一个车厢内部观看，希望能找到一个座位。我发现靠近车门坐着的旅客旁边，有一个空座位，但上面放着两个大篮子，里面的鸡子和鸭子把头伸在篮子外面。 我犹豫了好一会之后，决定走进车厢。我说很对不起了，让我来把篮子移开。可是一位穿着工作服的农民对我说：「小姐，请等一等，我就来把它们从这里拿开。」 当我把放在农民膝上的水果篮子提在手中时，他轻轻地把两篮家禽塞在凳子下面。 我们都听到鸭子叫喊，表示不高兴。母鸡却低下头，像是受委屈的样子。农民的妻子一面喊着鸭子鸡子的名字，一面对它们说着话。 我坐下以后，鸭子也安静下来了。这时，坐在我对面的旅客问农民是否他把家禽带到市场上去卖。 农民回答说：「先生，不是送上市场的。后天，我的儿子就要结婚，我把鸡鸭带来送给儿子。」 他脸上显出幸福愉快的神情。他看了看周围的人，仿佛要向所有的人们都表达他自己的快乐。另外的旅客都留心倾听，他们似乎听了之后感到很高兴。只有一个老媪是例外，她占了两人的座位，枕着三个枕头，正在叱骂拥塞在车厢中的农民。 火车开动了。刚才说话的旅客开始阅读报纸，这时农民对他说：「我的儿子在巴黎，他是一家商店的职员，将和一位小姐结婚，她也是一家商店的职员。」 这个旅客把已经打开的报纸放在他膝上，同时移动身子坐在凳的边沿。他问道：「未婚妻美丽吗？」 农民说：「我不知道，我还没有见过她。」 这个旅客有些惊讶，又说：「真的吗？假如她长得丑，使你不喜欢，将怎样办？」 农民回答：「这种事情可能发生，但我相信，她会使我们喜欢，因为我们的儿子很爱我们，他不会娶一个难看的妻子。」 农民的妻子又补上一句：「再说，既然她使我们的儿子腓利普喜欢，她也会使我们喜欢的。」 农民的妻子转过身来向着我，我看到她一双柔和的眼睛肿充满着微笑。她的面容娇小玲珑，非常可爱，我不能相信她就是一个已达结婚年龄的儿子的母亲。 她想知道我是否也去巴黎，当我回答说也是去巴黎时，这个旅客就开玩笑了。他说：「我打赌：这位小姐就是未婚妻，她是来迎接她的公公婆婆而没有介绍自己使他们认识。」 所有的眼睛都向我注视，我羞得面红耳赤，这时农民夫妇同声说：「嗳！真是这样的话，我们将非常高兴。」 我向他们说明这完全是误会。可是这个旅客提醒他们，说我曾沿着火车走过两次，好像我是尽力找认什么人；又说我在登上车厢前是多么犹豫迟疑。 所有的人都笑起来，我在困窘中解释说，这个座位是我能够找到的惟一的座位。 农民的妻子说：「这没有什么关系，你非常使我喜欢。假使我们的媳妇能像你那样，我将多么高兴。」 农民接着说：「是啊，我们的媳妇最好能像你。」 这个旅客对于他自己的这番笑话感到很得意，他带着开玩笑的样子看了我一眼后，对农民夫妇说：「你们相信我没有弄错。当你们到达巴黎时，你们的儿子会对你们说：『这位就是我的未婚妻。』」 他说完后，放声大笑一阵，便往凳子里边一坐，开始专心读他的报纸了。 一会儿以后，农民的妻子完全转身向着我；她在她带来的篮子底层找寻一会儿，便拿出一块煎饼。她一面把煎饼请我吃，一面对我说，这煎饼是她今天早晨亲手做的。 我不知怎样辞谢才好，只得采用夸大的方法，把伤风说成发烧，她才把这块煎饼放在篮子的底层。 接着，她又请我吃一串葡萄，我不得不接受了。 当火车在一个站停下时，我很难阻止农民下车为我购买一杯热的饮料。 我看到这一对好心人一心只想爱他们儿子选中的未婚妻时，自己因不是他们的媳妇而感到遗憾。他们的爱情使自己觉得多么温暖。我是孤女，从未见过父母的慈容；而和我一起生活的人，谁都对我漠不关心。 我惊异地看到他们的眼光时时注视在我身上，好像他们是在爱抚我那样。 到达巴黎时，我帮助他们把篮子从车上搬下来，并领他们向出口处走去。 当我看到一个身材高高的青年向他们扑过来，用双臂抱着他们时，我就稍稍离开他们远一些。他热情地吻着他父亲，又吻着他母亲。 父母只管笑眯眯地接受儿子的亲吻，连服务员推着的行李车快要撞他们时所发出的警铃声，他们都听不到；急于赶路的旅客把臂肘撞在他们身上，他们好像也没有感觉到。 他们在前面走着，我在后面跟着。儿子的一只手臂挽着鸭篮子，另一只手臂抱着他妈妈的肩膀，他微微弯着身躯靠向妈妈，笑嘻嘻地倾听母亲说话。 他像他父亲，一双眼睛鲜明快乐，笑声爽快而响亮。 外面，天几乎全黑了。我撑起大衣的领子。我落在他们后面，稍离开他们几步路。这时，他们的儿子去雇一辆车子。 农民爱抚着一只染有各种颜色、很美丽的花母鸡的头时，对他妻子说：「假如我早知道她不是我们媳妇，那么我早就把这只花母鸡送给她了。」 「是啊！假如我早知道……」 农民的妻子向着已经走出车站的长长人群做手势，眼睛望着远处说：「她已随着人群走了。」 正在这时，他们的儿子已雇到一辆车子回来了。他尽可能好地把他的父母安顿在车上，他自己却在赶车人的旁边坐下，而且侧转身子，以免遮了他父母的视线。 他看起来长得身强力壮，性情温和，我想他的未婚妻一定是很幸福的。 他们的车子消失在黑暗中了，于是我沿着每一条街道慢吞吞地走去。孤零零的我不由自主地回到了自己的房间。 我已二十岁了，还没有一个人来向我谈过爱情。","categories":[{"name":"文学黑洞","slug":"文学黑洞","permalink":"http://pistachio0812.github.io/categories/%E6%96%87%E5%AD%A6%E9%BB%91%E6%B4%9E/"}],"tags":[{"name":"今日故事","slug":"今日故事","permalink":"http://pistachio0812.github.io/tags/%E4%BB%8A%E6%97%A5%E6%95%85%E4%BA%8B/"}],"author":"pistachio"},{"title":"激活函数","slug":"激活函数合集","date":"2022-04-21T13:58:33.980Z","updated":"2022-10-04T13:39:57.062Z","comments":true,"path":"zh-CN/激活函数合集/","permalink":"http://pistachio0812.github.io/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/","excerpt":"","text":"参考博文： 1.激活函数介绍 2.pytorch激活函数官方文档) 什么是激活函数活函数（Activation functions）对于神经网络模型学习与理解复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。 如果网络中不使用激活函数，网络每一层的输出都是上层输入的线性组合，无论神经网络有多少层，输出都是输入的线性组合。 如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，此时神经网络就可以应用到各类非线性场景当中了。 常见的激活函数如sigmoid、tanh、relu等，它们的输入输出映射均为非线性，这样才可以给网络赋予非线性逼近能力。 常用的激活函数sigmoidSigmoid函数是一个在生物学中常见的S型函数，它能够把输入的连续实值变换为0和1之间的输出，如果输入是特别小的负数，则输出为0，如果输入是特别大的正数，则输出为1。即将输入量映射到0到1之间。 .mgsooyjfquhu{zoom:50%;} Sigmoid可以作为非线性激活函数赋予网络非线性区分能力，也可以用来做二分类。其计算公式为： .heryygbqtcjf{} 优点： 1.曲线过渡平滑，处处可导；缺点： 1.幂函数运算较慢，激活函数计算量大；2.求取反向梯度时，Sigmoid的梯度在饱和区域非常平缓，很容易造称梯度消失的问题，减缓收敛速度。 TanhTanh是一个奇函数，它能够把输入的连续实值变换为-1和1之间的输出，如果输入是特别小的负数，则输出为-1，如果输入是特别大的正数，则输出为1；解决了Sigmoid函数的不是0均值的问题。 .edgopafgtqro{zoom:50%;} Tanh可以作为非线性激活函数赋予网络非线性区分能力。其计算公式为： .igtngdedihsf{} 优点： 1.曲线过渡平滑，处处可导；2.具有良好的对称性，网络是0均值的。缺点： 1.与Sigmoid类似，幂函数运算较慢，激活函数计算量大；2.与Sigmoid类似，求取反向梯度时，Tanh的梯度在饱和区域非常平缓，很容易造称梯度消失的问题，减缓收敛速度。 Relu线性整流函数（Rectified Linear Unit, ReLU），是一种深度神经网络中常用的激活函数，整个函数可以分为两部分，在小于0的部分，激活函数的输出为0；在大于0的部分，激活函数的输出为输入。 .pbqnzhkprsfi{zoom:50%;} 计算公式为： .rwtwmcdryfmu{} 优点： 1.收敛速度快，不存在饱和区间，在大于0的部分梯度固定为1，有效解决了Sigmoid中存在的梯度消失的问题；2.计算速度快，ReLU只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的指数运算，具有类生物性质。缺点： 1.它在训练时可能会“死掉”。如果一个非常大的梯度经过一个ReLU神经元，更新过参数之后，这个神经元的的值都小于0，此时ReLU再也不会对任何数据有激活现象了。如果这种情况发生，那么从此所有流过这个神经元的梯度将都变成 0。合理设置学习率，会降低这种情况的发生概率。 SwishSwish是Sigmoid和ReLU的改进版，类似于ReLU和Sigmoid的结合，β是个常数或可训练的参数。Swish 具备无上界有下界、平滑、非单调的特性。Swish 在深层模型上的效果优于 ReLU。 .lrsuipcjeulu{zoom:50%;} 计算公式为： .qssjgvvfkqky{} 优点： Swish具有一定ReLU函数的优点； Swish具有一定Sigmoid函数的优点； Swish函数可以看做是介于线性函数与ReLU函数之间的平滑函数。 缺点： 运算复杂，速度较慢。 MishMish与Swish激活函数类似，Mish具备无上界有下界、平滑、非单调的特性。Mish在深层模型上的效果优于 ReLU。无上边界可以避免由于激活值过大而导致的函数饱和。 .ytckxgdklvyv{zoom:50%;} 计算公式： .awyrtumbocqn{} 优点： Mish具有一定ReLU函数的优点，收敛快速； Mish具有一定Sigmoid函数的优点，函数平滑； Mish函数可以看做是介于线性函数与ReLU函数之间的平滑函数。 缺点： 运算复杂，速度较慢。 Swish和Mish的梯度对比.hdjyxlzvdkck{zoom:50%;} 绘制代码pytorch官方文档中的激活函数基本都在这里 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import matplotlib.pyplot as pltimport numpy as npdef Sigmoid(x): y = np.exp(x) / (np.exp(x) + 1) return ydef Tanh(x): y = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) # y = np.tanh(x) return ydef ReLU(x): y = np.where(x &lt; 0, 0, x) return ydef LeakyReLU(x, a): # LeakyReLU的a参数不可训练，人为指定。 y = np.where(x &lt; 0, a * x, x) return ydef PReLU(x, a): # PReLU的a参数可训练 y = np.where(x &lt; 0, a * x, x) return ydef ReLU6(x): y = np.minimum(np.maximum(x, 0), 6) return ydef Swish(x, b): y = x * (np.exp(b*x) / (np.exp(b*x) + 1)) return ydef Mish(x): # 这里的Mish已经经过e和ln的约运算 temp = 1 + np.exp(x) y = x * ((temp*temp-1) / (temp*temp+1)) return ydef Grad_Swish(x, b): y_grad = np.exp(b*x)/(1+np.exp(b*x)) + x * (b*np.exp(b*x) / ((1+np.exp(b*x))*(1+np.exp(b*x)))) return y_graddef Grad_Mish(x): temp = 1 + np.exp(x) y_grad = (temp*temp-1) / (temp*temp+1) + x*(4*temp*(temp-1)) / ((temp*temp+1)*(temp*temp+1)) return y_gradif __name__ == '__main__': x = np.arange(-10, 10, 0.01) plt.plot(x, Sigmoid(x)) plt.title(\"Sigmoid\") plt.grid() plt.show() plt.plot(x, Tanh(x)) plt.title(\"Tanh\") plt.grid() plt.show() plt.plot(x, ReLU(x)) plt.title(\"ReLU\") plt.grid() plt.show() plt.plot(x, LeakyReLU(x, 0.1)) plt.title(\"LeakyReLU\") plt.grid() plt.show() plt.plot(x, PReLU(x, 0.25)) plt.title(\"PReLU\") plt.grid() plt.show() plt.plot(x, ReLU6(x)) plt.title(\"ReLU6\") plt.grid() plt.show() plt.plot(x, Swish(x, 1)) plt.title(\"Swish\") plt.grid() plt.show() plt.plot(x, Mish(x)) plt.title(\"Mish\") plt.grid() plt.show() plt.plot(x, Grad_Mish(x)) plt.plot(x, Grad_Swish(x, 1)) plt.title(\"Gradient of Mish and Swish\") plt.legend(['Mish', 'Swish']) plt.grid() plt.show()","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://pistachio0812.github.io/categories/pytorch/"}],"tags":[{"name":"函数库","slug":"函数库","permalink":"http://pistachio0812.github.io/tags/%E5%87%BD%E6%95%B0%E5%BA%93/"},{"name":"激活函数","slug":"激活函数","permalink":"http://pistachio0812.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"}],"author":"coolboy"},{"title":"CNN参数计算","slug":"CNN参数量计算","date":"2022-04-21T13:07:28.008Z","updated":"2022-11-03T12:02:08.079Z","comments":true,"path":"zh-CN/CNN参数量计算/","permalink":"http://pistachio0812.github.io/zh-CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/","excerpt":"","text":"在CNN的前向传播过程中，我们通常需要参数量，那么参数量的计算又当如何计算呢，不急，听我慢慢道来。 实例： 1.定义一个简单网络 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 该文件为test.pyimport torchimport torch.nn as nnimport mathdef conv_out_size_same(size, stride): return int(math.ceil(float(size) / float(stride)))class discriminator(nn.Module): def __init__(self, d=128, input_shape=[64, 64]): super(discriminator, self).__init__() s_h, s_w = input_shape[0], input_shape[1] s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) self.s_h16, self.s_w16 = conv_out_size_same( s_h8, 2), conv_out_size_same(s_w8, 2) # 64,64,3 -&gt; 32,32,128 self.conv1 = nn.Conv2d(3, d, 4, 2, 1) # 32,32,128 -&gt; 16,16,256 self.conv2 = nn.Conv2d(d, d * 2, 4, 2, 1) self.conv2_bn = nn.BatchNorm2d(d * 2) # 16,16,256 -&gt; 8,8,512 self.conv3 = nn.Conv2d(d * 2, d * 4, 4, 2, 1) self.conv3_bn = nn.BatchNorm2d(d * 4) # 8,8,512 -&gt; 4,4,1024 self.conv4 = nn.Conv2d(d * 4, d * 8, 4, 2, 1) self.conv4_bn = nn.BatchNorm2d(d * 8) # 4,4,1024 -&gt; 1 self.linear = nn.Linear(self.s_h16 * self.s_w16 * d * 8, 1) self.leaky_relu = nn.LeakyReLU(negative_slope=0.2) self.sigmoid = nn.Sigmoid() def weight_init(self): for m in self.modules(): if isinstance(m, nn.Conv2d): m.weight.data.normal_(0.0, 0.02) elif isinstance(m, nn.BatchNorm2d): m.weight.data.normal_(0.1, 0.02) m.bias.data.fill_(0) elif isinstance(m, nn.Linear): m.weight.data.normal_(0.0, 0.02) m.bias.data.fill_(0) def forward(self, x): bs, _, _, _ = x.size() # (3, 64, 64)-&gt;(128, 32, 32) x = self.leaky_relu(self.conv1(x)) # (128, 32, 32)-&gt;(256, 16, 16) x = self.leaky_relu(self.conv2_bn(self.conv2(x))) # (256, 16, 16)-&gt;(512, 8, 8) x = self.leaky_relu(self.conv3_bn(self.conv3(x))) # (512, 8, 8)-&gt;(1024, 4, 4) x = self.leaky_relu(self.conv4_bn(self.conv4(x))) # (1024, 4, 4)-&gt;(bs, 16*1024) x = x.view([bs, -1]) # (bs, 16*1024)-&gt;(bs, 1) x = self.sigmoid(self.linear(x)) return x.squeeze() 2.调用summary()函数计算参数以及输出 1234567from test import discriminatorfrom torchsummary import summaryimport torchif __name__ == '__main__': device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') m = discriminator(d=128, input_shape=[64, 64]).to(device) summary(m, input_size=(3, 64, 64)) 结果如下： 1234567891011121314151617181920212223242526272829---------------------------------------------------------------- Layer (type) Output Shape Param #================================================================ Conv2d-1 [-1, 128, 32, 32] 6,272 LeakyReLU-2 [-1, 128, 32, 32] 0 Conv2d-3 [-1, 256, 16, 16] 524,544 BatchNorm2d-4 [-1, 256, 16, 16] 512 LeakyReLU-5 [-1, 256, 16, 16] 0 Conv2d-6 [-1, 512, 8, 8] 2,097,664 BatchNorm2d-7 [-1, 512, 8, 8] 1,024 LeakyReLU-8 [-1, 512, 8, 8] 0 Conv2d-9 [-1, 1024, 4, 4] 8,389,632 BatchNorm2d-10 [-1, 1024, 4, 4] 2,048 LeakyReLU-11 [-1, 1024, 4, 4] 0 Linear-12 [-1, 1] 16,385 Sigmoid-13 [-1, 1] 0================================================================Total params: 11,038,081Trainable params: 11,038,081Non-trainable params: 0----------------------------------------------------------------Input size (MB): 0.05Forward/backward pass size (MB): 4.63Params size (MB): 42.11Estimated Total Size (MB): 46.78----------------------------------------------------------------Process finished with exit code 0 1.卷积（Conv) 以Conv2d-1为例： 变化：（3, 64, 64)-&gt;(128, 32, 32) 计算方法： .xpqlbfwwjbgf{} 其中bias等于输出通道数 因此：params=128×(4×4×3)+128=6272 2.激活函数（Activation) 不产生参数 3.正则化（BN) 以BatchNorm2d-4为例： （256，16， 16)-&gt;(256， 16， 16 ) 计算方法： .unqfvqdlcqxl{} 因此：params=256×2=512 4.全连接（FC) 以Linear-12为例： （1024， 4， 4）-&gt;(1) 计算方法： .uwzgybcrcbsf{} 其中：bias等于输出通道数 因此：params=1024×4×4×1+1=16385 5.池化层（pooling) 不产生参数","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"卷积神经网络","slug":"计算机视觉/卷积神经网络","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"参数计算","slug":"参数计算","permalink":"http://pistachio0812.github.io/tags/%E5%8F%82%E6%95%B0%E8%AE%A1%E7%AE%97/"}],"author":"coolboy"},{"title":"DCGAN论文笔记","slug":"DCGAN","date":"2022-04-21T12:01:50.891Z","updated":"2022-10-04T13:08:09.582Z","comments":true,"path":"zh-CN/DCGAN/","permalink":"http://pistachio0812.github.io/zh-CN/DCGAN/","excerpt":"","text":"参考博文： 1.pytorch搭建DCGAN 论文地址：https://arxiv.53yu.com/pdf/1511.06434.pdf 论文源码：略 文章引用源码：https://github.com/bubbliiiing/dcgan-pytorch 网络构建DCGANDCGAN的全称是Deep Convolutional Generative Adversarial Networks，即深度卷积对抗生成网络。 它是由Alec Radford在论文Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks中提出的。 实际上它就是在GAN的基础上增加深度卷积网络结构。 论文中给出的DCGAN结构如图所示。其使用反卷积将特征层的高宽不断扩大，整体结构看起来像普通神经网络的逆过程。 .qmxxulprdsob{} 生成网络的构建对于生成网络来讲，它的目的是生成假图片，它的输入是正态分布随机数。输出是假图片。 在GAN当中，我们将这个正态分布随机数长度定义为100，在经过处理后，我们会得到一个(64,64,3)的假图片。 在处理过程中，我们会使用到反卷积，反卷积的概念是相对于正常卷积的，在正常卷积下，我们的特征层的高宽会不断被压缩；在反卷积下，我们的特征层的高宽会不断变大。 .xjdyiwholhsh{} 在DCGAN的生成网络中，我们首先利用一个全连接，将输入长条全连接到16,384（4x4x1024）这样一个长度上，这样我们才可以对这个全连接的结果进行reshape，使它变成(4,4,1024)的特征层。 在获得这个特征层之后，我们就可以利用反卷积进行上采样了。 在每次反卷积后，特征层的高和宽会变为原来的两倍，在四次反卷积后，我们特征层的shape变化是这样的：( 4 , 4 , 1024 ) − &gt; ( 8 , 8 , 512 ) − &gt; ( 16 , 16 , 256 ) − &gt; ( 32 , 32 , 128 ) − &gt; ( 64 , 64 , 3 ) 此时我们再进行一次tanh激活函数，我们就可以获得一张假图片了。实现代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import mathimport torchimport torch.nn as nn# 如果stride=2,就是宽高减半，下采样操作def conv_out_size_same(size, stride): return int(math.ceil(float(size) / float(stride)))# 反卷积公式：H_out=(H_in −1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1class generator(nn.Module): def __init__(self, d=128, input_shape=[64, 64]): super(generator, self).__init__() # 64, 64 s_h, s_w = input_shape[0], input_shape[1] # 32, 32 s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) # 16, 16 s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) # 8, 8 s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) # 4, 4 self.s_h16, self.s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # (bs, 100)-&gt; (bs, 4*4*128*8) self.linear = nn.Linear(100, self.s_h16 * self.s_w16 * d * 8) self.linear_bn = nn.BatchNorm2d(d * 8) # (bs, 1024, 4, 4)-&gt;(bs, 512, 8, 8) self.deconv1 = nn.ConvTranspose2d(d * 8, d * 4, 4, 2, 1) self.deconv1_bn = nn.BatchNorm2d(d * 4) # (bs, 512, 8, 8)-&gt;(bs, 256, 16, 16) self.deconv2 = nn.ConvTranspose2d(d * 4, d * 2, 4, 2, 1) self.deconv2_bn = nn.BatchNorm2d(d * 2) # (bs, 256, 16, 16)-&gt;(bs, 128, 32, 32) self.deconv3 = nn.ConvTranspose2d(d * 2, d, 4, 2, 1) self.deconv3_bn = nn.BatchNorm2d(d) # (bs, 128, 8, 8)-&gt;(bs, 3, 64, 64) self.deconv4 = nn.ConvTranspose2d(d, 3, 4, 2, 1) self.relu = nn.ReLU() def weight_init(self): for m in self.modules(): if isinstance(m, nn.ConvTranspose2d): m.weight.data.normal_(0.0, 0.02) m.bias.data.fill_(0) elif isinstance(m, nn.BatchNorm2d): m.weight.data.normal_(0.1, 0.02) m.bias.data.fill_(0) elif isinstance(m, nn.Linear): m.weight.data.normal_(0.0, 0.02) m.bias.data.fill_(0) def forward(self, x): # (bs, 100) bs, _ = x.size() # (bs, 16*1024) x = self.linear(x) # (bs, 1024, 4, 4) x = x.view([bs, -1, self.s_h16, self.s_w16]) x = self.relu(self.linear_bn(x)) # (bs, 1024, 4, 4)-&gt;(bs, 512, 8, 8) x = self.relu(self.deconv1_bn(self.deconv1(x))) # (bs, 512, 8, 8)-&gt;(bs, 256, 16, 16) x = self.relu(self.deconv2_bn(self.deconv2(x))) # (bs, 256, 16, 16)-&gt;(bs, 128, 32, 32) x = self.relu(self.deconv3_bn(self.deconv3(x))) # (bs, 128, 32, 32)-&gt;(bs, 3, 64, 64) x = torch.tanh(self.deconv4(x)) return x 判别网络的构建对于生成网络来讲，它的目的是生成假图片，它的输入是正态分布随机数。输出是假图片。 对于判别网络来讲，它的目的是判断输入图片的真假，它的输入是图片，输出是判断结果。 判断结果处于0-1之间，利用接近1代表判断为真图片，接近0代表判断为假图片。 判别网络的构建和普通卷积网络差距不大，都是不断的卷积对图片进行下采用，在多次卷积后，最终接一次全连接判断结果。 实现代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import torchimport torch.nn as nnimport mathdef conv_out_size_same(size, stride): return int(math.ceil(float(size) / float(stride)))class discriminator(nn.Module): def __init__(self, d=128, input_shape=[64, 64]): super(discriminator, self).__init__() s_h, s_w = input_shape[0], input_shape[1] s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) self.s_h16, self.s_w16 = conv_out_size_same( s_h8, 2), conv_out_size_same(s_w8, 2) # 64,64,3 -&gt; 32,32,128 self.conv1 = nn.Conv2d(3, d, 4, 2, 1) # 32,32,128 -&gt; 16,16,256 self.conv2 = nn.Conv2d(d, d * 2, 4, 2, 1) self.conv2_bn = nn.BatchNorm2d(d * 2) # 16,16,256 -&gt; 8,8,512 self.conv3 = nn.Conv2d(d * 2, d * 4, 4, 2, 1) self.conv3_bn = nn.BatchNorm2d(d * 4) # 8,8,512 -&gt; 4,4,1024 self.conv4 = nn.Conv2d(d * 4, d * 8, 4, 2, 1) self.conv4_bn = nn.BatchNorm2d(d * 8) # 4,4,1024 -&gt; 1 self.linear = nn.Linear(self.s_h16 * self.s_w16 * d * 8, 1) self.leaky_relu = nn.LeakyReLU(negative_slope=0.2) self.sigmoid = nn.Sigmoid() def weight_init(self): for m in self.modules(): if isinstance(m, nn.Conv2d): m.weight.data.normal_(0.0, 0.02) elif isinstance(m, nn.BatchNorm2d): m.weight.data.normal_(0.1, 0.02) m.bias.data.fill_(0) elif isinstance(m, nn.Linear): m.weight.data.normal_(0.0, 0.02) m.bias.data.fill_(0) def forward(self, x): bs, _, _, _ = x.size() # (3, 64, 64)-&gt;(128, 32, 32) x = self.leaky_relu(self.conv1(x)) # (128, 32, 32)-&gt;(256, 16, 16) x = self.leaky_relu(self.conv2_bn(self.conv2(x))) # (256, 16, 16)-&gt;(512, 8, 8) x = self.leaky_relu(self.conv3_bn(self.conv3(x))) # (512, 8, 8)-&gt;(1024, 4, 4) x = self.leaky_relu(self.conv4_bn(self.conv4(x))) # (1024, 4, 4)-&gt;(bs, 16*1024) x = x.view([bs, -1]) # (bs, 16*1024)-&gt;(bs, 1) x = self.sigmoid(self.linear(x)) return x.squeeze() 训练思路DCGAN的训练可以分为生成器训练和判别器训练，每一个step中一般先训练判别器，然后训练生成器 判别器的训练在训练判别器的时候我们希望判别器可以判断输入图片的真伪，因此我们的输入就是真图片、假图片和它们对应的标签。 因此判别器的训练步骤如下： 1、随机选取batch_size个真实的图片。2、随机生成batch_size个N维向量，传入到Generator中生成batch_size个虚假图片。3、真实图片的label为1，虚假图片的label为0，将真实图片和虚假图片当作训练集传入到Discriminator中进行训练。 .rxsmxkydlhyd{zoom: 25%;} 生成器训练在训练生成器的时候我们希望生成器可以生成极为真实的假图片。因此我们在训练生成器需要知道判别器认为什么图片是真图片。 因此生成器的训练步骤如下： 1、随机生成batch_size个N维向量，传入到Generator中生成batch_size个虚假图片。2、将虚假图片的Discriminator预测结果与1的对比作为loss对Generator进行训练（与1对比的意思是，让生成器根据判别器判别的结果进行训练） .nswdrvnyducb{zoom: 50%;} 利用DCGAN生成图片详情见源码和参考博文","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"GAN","slug":"GAN","permalink":"http://pistachio0812.github.io/tags/GAN/"}],"author":"pistachio"},{"title":"C语言基础学习","slug":"C语言","date":"2022-04-21T03:18:26.880Z","updated":"2022-10-04T13:05:06.411Z","comments":true,"path":"zh-CN/C语言/","permalink":"http://pistachio0812.github.io/zh-CN/C%E8%AF%AD%E8%A8%80/","excerpt":"","text":"参考博文1.菜鸟教程C语言教程 编译/执行C程序实例123456789#include &lt;stdio.h&gt; int main(){ /* 我的第一个 C 程序 */ printf(\"Hello, World! \\n\"); return 0;} 实例解析： 所有的 C 语言程序都需要包含 main() 函数。 代码从 main() 函数开始执行。 /* … */ 用于注释说明。 printf() 用于格式化输出到屏幕。printf() 函数在 “stdio.h” 头文件中声明。 stdio.h 是一个头文件 (标准输入输出头文件) , #include 是一个预处理命令，用来引入头文件。 当编译器遇到 printf() 函数时，如果没有找到 stdio.h 头文件，会发生编译错误。 return 0; 语句用于表示退出程序。 C简介C 语言是一种通用的高级语言，最初是由丹尼斯·里奇在贝尔实验室为开发 UNIX 操作系统而设计的。C 语言最开始是于 1972 年在 DEC PDP-11 计算机上被首次实现。 在 1978 年，布莱恩·柯林汉（Brian Kernighan）和丹尼斯·里奇（Dennis Ritchie）制作了 C 的第一个公开可用的描述，现在被称为 K&amp;R 标准。 UNIX 操作系统，C编译器，和几乎所有的 UNIX 应用程序都是用 C 语言编写的。由于各种原因，C 语言现在已经成为一种广泛使用的专业语言。 易于学习。 结构化语言。 它产生高效率的程序。 它可以处理底层的活动。 关于C C 语言是为了编写 UNIX 操作系统而被发明的。 C 语言是以 B 语言为基础的，B 语言大概是在 1970 年被引进的。 C 语言标准是于 1988 年由美国国家标准协会（ANSI，全称 American National Standard Institute）制定的。 截至 1973 年，UNIX 操作系统完全使用 C 语言编写。 目前，C 语言是最广泛使用的系统程序设计语言。 大多数先进的软件都是使用 C 语言实现的。 当今最流行的 Linux 操作系统和 RDBMS（Relational Database Management System：关系数据库管理系统） MySQL 都是使用 C 语言编写的。 为什么要使用C C 语言最初是用于系统开发工作，特别是组成操作系统的程序。由于 C 语言所产生的代码运行速度与汇编语言编写的代码运行速度几乎一样，所以采用 C 语言作为系统开发语言。下面列举几个使用 C 的实例：操作系统、语言编译器、汇编器、文本编辑器、打印机、网络驱动器、现代程序、数据库、语言解释器、实体工具。 C程序一个 C 语言程序，可以是 3 行，也可以是数百万行，它可以写在一个或多个扩展名为 “.c” 的文本文件中，例如，hello.c。您可以使用 “vi”、“vim” 或任何其他文本编辑器来编写您的 C 语言程序。 本教程假定您已经知道如何编辑一个文本文件，以及如何在程序文件中编写源代码。 C11C11（也被称为C1X）指ISO标准ISO/IEC 9899:2011，是当前最新的C语言标准。在它之前的C语言标准为C99。 新特性 对齐处理（Alignment）的标准化（包括_Alignas标志符，alignof运算符，aligned_alloc函数以及头文件）。 Noreturn 函数标记，类似于 gcc 的 _attribute((noreturn))。 _Generic 关键字。 多线程（Multithreading）支持，包括：_Thread_local存储类型标识符，头文件，里面包含了线程的创建和管理函数。_Atomic类型修饰符和头文件。 增强的Unicode的支持。基于C Unicode技术报告ISO/IEC TR 19769:2004，增强了对Unicode的支持。包括为UTF-16/UTF-32编码增加了char16_t和char32_t数据类型，提供了包含unicode字符串转换函数的头文件。 删除了 gets() 函数，使用一个新的更安全的函数gets_s()替代。 增加了边界检查函数接口，定义了新的安全的函数，例如 fopen_s()，strcat_s() 等等。 增加了更多浮点处理宏(宏)。 匿名结构体/联合体支持。这个在gcc早已存在，C11将其引入标准。 静态断言（Static assertions），_Static_assert()，在解释 #if 和 #error 之后被处理。 新的 fopen() 模式，(“…x”)。类似 POSIX 中的 O_CREAT|O_EXCL，在文件锁中比较常用。 新增 quick_exit() 函数作为第三种终止程序的方式。当 exit()失败时可以做最少的清理工作。 C环境设置#","categories":[{"name":"C语言","slug":"C语言","permalink":"http://pistachio0812.github.io/categories/C%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"函数库","slug":"函数库","permalink":"http://pistachio0812.github.io/tags/%E5%87%BD%E6%95%B0%E5%BA%93/"}],"author":"pistachio"},{"title":"机器学习","slug":"机器学习","date":"2022-04-20T13:58:07.370Z","updated":"2022-10-04T13:39:15.996Z","comments":true,"path":"zh-CN/机器学习/","permalink":"http://pistachio0812.github.io/zh-CN/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"常见概念TP、TN、FP和FN概念 P(分类器认为是正样本) N(分类器认为是负样本) T(正确分类) TP TN F(错误分类) FP FN 注： TP（True Positives）意思就是被分为了正样本，而且分对了。（样本是正样本）TN（True Negatives）意思就是被分为了负样本，而且分对了。(样本是负样本)FP（False Positives）意思就是被分为了正样本，但是分错了。（样本是负样本）FN（False Negatives）意思就是被分为了负样本，但是分错了。（样本是正样本） 正确率正确率是我们最常见的评价指标，通常来说，正确率越高，分类器越好。TP是分类器认为是正样本而且确实是正样本的样本数，FP是分类器认为是正样本但实际上不是正样本的样本数，Precision就是“分类器认为是正类并且确实是正类的部分占所有分类器认为是正类的比例”。 .pmglsmgvdlbv{zoom:50%;} 召回率TP是分类器认为是正样本而且确实是正样本的样本数，FN是分类器认为是负样本但实际上不是负样本的样本数，Recall就是“分类器认为是正类并且确实是正类的部分占所有确实是正类的比例”。 .tcjjsfycgxlb{zoom:50%;}","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://pistachio0812.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"author":"coolboy"},{"title":"可视化技术","slug":"可视化技术","date":"2022-04-20T02:13:47.463Z","updated":"2022-10-04T13:42:57.504Z","comments":true,"path":"zh-CN/可视化技术/","permalink":"http://pistachio0812.github.io/zh-CN/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8A%80%E6%9C%AF/","excerpt":"","text":"参考博文： 1.CNN模型的可视化 CNNVis清华大学视觉分析组做的一个网站，目的是为了更好地分析深度卷积神经网络。大家可以在训练的时候采取不同的卷积核尺寸和个数对照来看训练的中间过程。 PlotNeuralNet1.安装texlive ubuntu 12345678910(1)下载texlive镜像 https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/(2)使用图形化安装界面，需要安装perl的tk组件 sudo apt-get install perl-tk(3)加载镜像文件安装sudo mount -o loop texlive.iso /mntcd /mnt sudo ./install-tl -gui (4)安装texlive-latex-extrasudo apt-get install texlive-latex-extra windows 12(1)下载并安装[MikTex](https://miktex.org/download)(2)下载并安装bash, 详情见 Git bash(https://git-scm.com/download/win)or Cygwin(https://www.cygwin.com/)","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"可视化","slug":"可视化","permalink":"http://pistachio0812.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"}],"author":"pistachio"},{"title":"数据集介绍","slug":"数据集","date":"2022-04-19T03:12:24.034Z","updated":"2022-10-04T13:44:56.964Z","comments":true,"path":"zh-CN/数据集/","permalink":"http://pistachio0812.github.io/zh-CN/%E6%95%B0%E6%8D%AE%E9%9B%86/","excerpt":"","text":"参考博文1.针对 VOC2007和VOC2012 的具体用法 2.Pascal Voc（07+12）联合训练并在07上测试 VOC2007和VOC2012用法目前广大研究者们普遍使用的是 VOC2007和VOC2012数据集，因为二者是互斥的，不相容的。 论文中针对 VOC2007和VOC2012 的具体用法有以下几种： 1.只用VOC2007的trainval 训练，使用VOC2007的test测试。2.只用VOC2012的trainval 训练，使用VOC2012的test测试，这种用法很少使用，因为大家都会结合VOC2007使用。3.（推荐）使用 VOC2007 的 train+val 和 VOC2012的 train+val 训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12 ，研究者可以自己测试在VOC2007上的结果，因为VOC2007的test是公开的。4.使用 VOC2007 的 train+val+test 和 VOC2012的 train+val训练，然后使用 VOC2012的test测试，这个用法是论文中经常看到的 07++12 ，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。5.先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val、 VOC2012的 train+val 微调训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12+COCO 。6.先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val+test 、 VOC2012的 train+val 微调训练，然后使用 VOC2012的test测试 ，这个用法是论文中经常看到的 07++12+COCO，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。 VOC07+12联合训练并在07上测试数据分布对于分类/检测任务而言，完成07 + 12数据集合并后，共得到如下数据： 1234# 5011+11540=16551, 12608+27450=40058训练数据：16551张图像，共40058个目标# 全部来自voc07_test测试数据：4952张图像，共12032个目标 组成如下所示： 训练数据： 1.VOC2007的训练集提供了： 123训练数据：2501张图像，共6301个目标验证数据：2510张图像，共6307个目标训练+验证数据：5011张图像，共12608个目标 2.VOC2012的训练集提供了： 123训练数据：5717张图像，共13609个目标验证数据：5823张图像，共13841个目标训练+验证数据：11540张图像，共27450个目标 测试数据： 1.VOC2007的测试集提供了： 1测试数据：4952张图像，共12032个目标 各种数据集介绍Pascal VOC官网地址：Pascal VOC Dataset Mirror (pjreddie.com) MS COCOILSVRC","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"数据集","slug":"数据集","permalink":"http://pistachio0812.github.io/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"}],"author":"Daniel"},{"title":"RetinaNet论文笔记","slug":"RetinaNet","date":"2022-04-18T12:41:22.906Z","updated":"2022-10-04T13:28:09.387Z","comments":true,"path":"zh-CN/RetinaNet/","permalink":"http://pistachio0812.github.io/zh-CN/RetinaNet/","excerpt":"","text":"论文地址：Focal loss for dense object detection 源码地址：RetinaNet 文章引用代码地址：https://github.com/bubbliiiing/retinanet-pytorch 文章出处：https://blog.csdn.net/weixin_44791964/article/details/108319189 实现思路Retinanet是在何凯明大神提出Focal loss同时提出的一种新的目标检测方案，来验证Focal Loss的有效性。 One-Stage目标检测方法常常使用先验框提高预测性能，一张图像可能生成成千上万的候选框，但是其中只有很少一部分是包含目标的的，有目标的就是正样本，没有目标的就是负样本。这种情况造成了One-Stage目标检测方法的正负样本不平衡，也使得One-Stage目标检测方法的检测效果比不上Two-Stage目标检测方法。 Focal Loss是一种新的用于平衡One-Stage目标检测方法正负样本的Loss方案。 Retinane的结构非常简单，但是其存在非常多的先验框，以输入600x600x3的图片为例，就存在着67995个先验框，这些先验框里面大多包含的是背景，存在非常多的负样本。以Focal Loss训练的Retinanet可以有效的平衡正负样本，实现有效的训练。 预测部分主干网络.geljjekrmwuz{} 假设输入的图片大小为600x600x3。 ResNet50有两个基本的块，分别名为Conv Block和Identity Block，其中Conv Block输入和输出的维度是不一样的，所以不能连续串联，它的作用是改变网络的维度；Identity Block输入维度和输出维度相同，可以串联，用于加深网络的。 .yyiriuisdawj{} 当输入的图片为600x600x3的时候，shape变化与总的网络结构如下： .xgqyunylxkov{zoom:50%;} 我们取出长宽压缩了三次、四次、五次的结果来进行网络金字塔结构的构造 实现代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.autograd import Variableimport mathimport torch.utils.model_zoo as model_zooimport pdbmodel_urls = {'resnet18': 'https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth','resnet34': 'https://s3.amazonaws.com/pytorch/models/resnet34-333f7ec4.pth','resnet50': 'https://s3.amazonaws.com/pytorch/models/resnet50-19c8e357.pth','resnet101': 'https://s3.amazonaws.com/pytorch/models/resnet101-5d3b4d8f.pth','resnet152': 'https://s3.amazonaws.com/pytorch/models/resnet152-b121ed2d.pth',}def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1): \"\"\"3x3 convolution with padding\"\"\" return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)def conv1x1(in_planes, out_planes, stride=1): \"\"\"1x1 convolution\"\"\" return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)class BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None): super(BasicBlock, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d if groups != 1 or base_width != 64: raise ValueError('BasicBlock only supports groups=1 and base_width=64') if dilation &gt; 1: raise NotImplementedError(\"Dilation &gt; 1 not supported in BasicBlock\") # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = norm_layer(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = norm_layer(planes) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return outclass Bottleneck(nn.Module): expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None): super(Bottleneck, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d width = int(planes * (base_width / 64.)) * groups # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv1x1(inplanes, width) self.bn1 = norm_layer(width) self.conv2 = conv3x3(width, width, stride, groups, dilation) self.bn2 = norm_layer(width) self.conv3 = conv1x1(width, planes * self.expansion) self.bn3 = norm_layer(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return outclass ResNet(nn.Module): def __init__(self, block, layers, num_classes=1000): self.inplanes = 64 super(ResNet, self).__init__() self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True) # change self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2) self.layer3 = self._make_layer(block, 256, layers[2], stride=2) self.layer4 = self._make_layer(block, 512, layers[3], stride=2) self.avgpool = nn.AvgPool2d(7) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() def _make_layer(self, block, planes, blocks, stride=1): downsample = None if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample)) self.inplanes = planes * block.expansion for i in range(1, blocks): layers.append(block(self.inplanes, planes)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) x = x.view(x.size(0), -1) x = self.fc(x) return xdef resnet18(pretrained=False, **kwargs): \"\"\"Constructs a ResNet-18 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet \"\"\" model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls['resnet18'], model_dir='model_data'), strict=False) return modeldef resnet34(pretrained=False, **kwargs): \"\"\"Constructs a ResNet-34 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet \"\"\" model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls['resnet34'], model_dir='model_data'), strict=False) return modeldef resnet50(pretrained=False, **kwargs): \"\"\"Constructs a ResNet-50 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet \"\"\" model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls['resnet50'], model_dir='model_data'), strict=False) return modeldef resnet101(pretrained=False, **kwargs): \"\"\"Constructs a ResNet-101 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet \"\"\" model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls['resnet101'], model_dir='model_data'), strict=False) return modeldef resnet152(pretrained=False, **kwargs): \"\"\"Constructs a ResNet-152 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet \"\"\" model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls['resnet152'], model_dir='model_data'), strict=False) return model 从特征获取预测结果.gwdjzowrqhsu{} 由抽象的结构图可知，获得到的特征还需要经过图像金字塔的处理，这样的结构可以融合多尺度的特征，实现更有效的预测。 图像金字塔的具体结构如下： .mbfekqdwepei{} 通过图像金字塔我们可以获得五个有效的特征层，分别是P3、P4、P5、P6、P7，为了和普通特征层区分，我们称之为有效特征层，将这五个有效的特征层传输过class+box subnets就可以获得预测结果了。 class subnet采用4次256通道的卷积和1次num_anchors x num_classes的卷积，num_anchors指的是该特征层所拥有的先验框数量，num_classes指的是网络一共对多少类的目标进行检测。 box subnet采用4次256通道的卷积和1次num_anchors x 4的卷积，num_anchors指的是该特征层所拥有的先验框数量，4指的是先验框的调整情况。 需要注意的是，每个特征层所用的class subnet是同一个class subnet；每个特征层所用的box subnet是同一个box subnet。 .kfwkyccpxiei{} 其中：1.num_anchors x 4的卷积 用于预测 该特征层上 每一个网格点上 每一个先验框的变化情况。（为什么说是变化情况呢，这是因为ssd的预测结果需要结合先验框获得预测框，预测结果就是先验框的变化情况。） 2.num_anchors x num_classes的卷积 用于预测 该特征层上 每一个网格点上 每一个预测框对应的种类。实现代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213import torch.nn as nnimport torch.nn.functional as F import torchimport mathfrom nets.resnet import resnet18,resnet34,resnet50,resnet101,resnet152from utils.anchors import Anchorsclass PyramidFeatures(nn.Module): def __init__(self, C3_size, C4_size, C5_size, feature_size=256): super(PyramidFeatures, self).__init__() self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0) self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0) self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0) self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1) self.P7_1 = nn.ReLU() self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1) def forward(self, inputs): C3, C4, C5 = inputs _, _, h4, w4 = C4.size() _, _, h3, w3 = C3.size() P5_x = self.P5_1(C5) P5_upsampled_x = F.interpolate(P5_x, size=(h4, w4)) P5_x = self.P5_2(P5_x) P4_x = self.P4_1(C4) P4_x = P5_upsampled_x + P4_x P4_upsampled_x = F.interpolate(P4_x, size=(h3, w3)) P4_x = self.P4_2(P4_x) P3_x = self.P3_1(C3) P3_x = P3_x + P4_upsampled_x P3_x = self.P3_2(P3_x) P6_x = self.P6(C5) P7_x = self.P7_1(P6_x) P7_x = self.P7_2(P7_x) return [P3_x, P4_x, P5_x, P6_x, P7_x]class RegressionModel(nn.Module): def __init__(self, num_features_in, num_anchors=9, feature_size=256): super(RegressionModel, self).__init__() self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1) self.act1 = nn.ReLU() self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act2 = nn.ReLU() self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act3 = nn.ReLU() self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act4 = nn.ReLU() self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, padding=1) def forward(self, x): out = self.conv1(x) out = self.act1(out) out = self.conv2(out) out = self.act2(out) out = self.conv3(out) out = self.act3(out) out = self.conv4(out) out = self.act4(out) out = self.output(out) # out is B x C x W x H, with C = 4*num_anchors out = out.permute(0, 2, 3, 1) return out.contiguous().view(out.shape[0], -1, 4)class ClassificationModel(nn.Module): def __init__(self, num_features_in, num_anchors=9, num_classes=80, anchor=0.01, feature_size=256): super(ClassificationModel, self).__init__() self.num_classes = num_classes self.num_anchors = num_anchors self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1) self.act1 = nn.ReLU() self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act2 = nn.ReLU() self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act3 = nn.ReLU() self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act4 = nn.ReLU() self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, padding=1) self.output_act = nn.Sigmoid() def forward(self, x): out = self.conv1(x) out = self.act1(out) out = self.conv2(out) out = self.act2(out) out = self.conv3(out) out = self.act3(out) out = self.conv4(out) out = self.act4(out) out = self.output(out) out = self.output_act(out) # out is B x C x W x H, with C = n_classes + n_anchors out1 = out.permute(0, 2, 3, 1) batch_size, width, height, channels = out1.shape out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes) return out2.contiguous().view(x.shape[0], -1, self.num_classes)class Resnet(nn.Module): def __init__(self, phi, load_weights=False): super(Resnet, self).__init__() self.edition = [resnet18,resnet34,resnet50,resnet101,resnet152] model = self.edition[phi](load_weights) del model.avgpool del model.fc self.model = model def forward(self, x): x = self.model.conv1(x) x = self.model.bn1(x) x = self.model.relu(x) x = self.model.maxpool(x) x = self.model.layer1(x) feat1 = self.model.layer2(x) feat2 = self.model.layer3(feat1) feat3 = self.model.layer4(feat2) return [feat1,feat2,feat3]class Retinanet(nn.Module): def __init__(self, num_classes, phi, pretrain_weights=False): super(Retinanet, self).__init__() self.pretrain_weights = pretrain_weights self.backbone_net = Resnet(phi,pretrain_weights) fpn_sizes = { 0: [128, 256, 512], 1: [128, 256, 512], 2: [512, 1024, 2048], 3: [512, 1024, 2048], 4: [512, 1024, 2048], }[phi] self.fpn = PyramidFeatures(fpn_sizes[0], fpn_sizes[1], fpn_sizes[2]) self.regressionModel = RegressionModel(256) self.classificationModel = ClassificationModel(256, num_classes=num_classes) self.anchors = Anchors() self._init_weights() def _init_weights(self): if not self.pretrain_weights: print(\"_init_weights\") for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() print(\"_init_classificationModel\") anchor = 0.01 self.classificationModel.output.weight.data.fill_(0) self.classificationModel.output.bias.data.fill_(-math.log((1.0 - anchor) / anchor)) print(\"_init_regressionModel\") self.regressionModel.output.weight.data.fill_(0) self.regressionModel.output.bias.data.fill_(0) def forward(self, inputs): p3, p4, p5 = self.backbone_net(inputs) features = self.fpn([p3, p4, p5]) regression = torch.cat([self.regressionModel(feature) for feature in features], dim=1) classification = torch.cat([self.classificationModel(feature) for feature in features], dim=1) anchors = self.anchors(features) return features, regression, classification, anchors 预测结果的解码我们通过对每一个特征层的处理，可以获得三个内容，分别是： num_anchors x 4的卷积 用于预测 该特征层上 每一个网格点上 每一个先验框的变化情况。 num_anchors x num_classes的卷积 用于预测 该特征层上 每一个网格点上 每一个预测框对应的种类。 每一个有效特征层对应的先验框对应着该特征层上 每一个网格点上 预先设定好的9个框。 我们利用 num_anchors x 4的卷积 与 每一个有效特征层对应的先验框 获得框的真实位置。 每一个有效特征层对应的先验框就是，如图所示的作用：每一个有效特征层将整个图片分成与其长宽对应的网格，如P3的特征层就是将整个图像分成75x75个网格；然后从每个网格中心建立9个先验框，一共75x75x9个，50625个先验框 .mtfporuineqt{} 先验框虽然可以代表一定的框的位置信息与框的大小信息，但是其是有限的，无法表示任意情况，因此还需要调整，Retinanet利用4次256通道的卷积+num_anchors x 4的卷积的结果对先验框进行调整。 num_anchors x 4中的num_anchors表示了这个网格点所包含的先验框数量，其中的4表示了框的左上角xy轴，右下角xy的调整情况。 Retinanet解码过程就是将对应的先验框的左上角和右下角进行位置的调整，调整完的结果就是预测框的位置了。 当然得到最终的预测结构后还要进行得分排序与非极大抑制筛选这一部分基本上是所有目标检测通用的部分。1、取出每一类得分大于confidence_threshold的框和得分。2、利用框的位置和得分进行非极大抑制。实现代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191def decodebox(regression, anchors, img): dtype = regression.dtype anchors = anchors.to(dtype) y_centers_a = (anchors[..., 0] + anchors[..., 2]) / 2 x_centers_a = (anchors[..., 1] + anchors[..., 3]) / 2 ha = anchors[..., 2] - anchors[..., 0] wa = anchors[..., 3] - anchors[..., 1] w = regression[..., 3].exp() * wa h = regression[..., 2].exp() * ha y_centers = regression[..., 0] * ha + y_centers_a x_centers = regression[..., 1] * wa + x_centers_a ymin = y_centers - h / 2. xmin = x_centers - w / 2. ymax = y_centers + h / 2. xmax = x_centers + w / 2. boxes = torch.stack([xmin, ymin, xmax, ymax], dim=2) _, _, height, width = np.shape(img) boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0) boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0) boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width - 1) boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height - 1) # fig = plt.figure() # ax = fig.add_subplot(121) # grid_x = x_centers_a[0,-4*4*9:] # grid_y = y_centers_a[0,-4*4*9:] # plt.ylim(-600,1200) # plt.xlim(-600,1200) # plt.gca().invert_yaxis() # plt.scatter(grid_x.cpu(),grid_y.cpu()) # anchor_left = anchors[0,-4*4*9:,1] # anchor_top = anchors[0,-4*4*9:,0] # anchor_w = wa[0,-4*4*9:] # anchor_h = ha[0,-4*4*9:] # for i in range(9,18): # rect1 = plt.Rectangle([anchor_left[i],anchor_top[i]],anchor_w[i],anchor_h[i],color=\"r\",fill=False) # ax.add_patch(rect1) # ax = fig.add_subplot(122) # grid_x = x_centers_a[0,-4*4*9:] # grid_y = y_centers_a[0,-4*4*9:] # plt.scatter(grid_x.cpu(),grid_y.cpu()) # plt.ylim(-600,1200) # plt.xlim(-600,1200) # plt.gca().invert_yaxis() # y_centers = y_centers[0,-4*4*9:] # x_centers = x_centers[0,-4*4*9:] # pre_left = xmin[0,-4*4*9:] # pre_top = ymin[0,-4*4*9:] # pre_w = xmax[0,-4*4*9:]-xmin[0,-4*4*9:] # pre_h = ymax[0,-4*4*9:]-ymin[0,-4*4*9:] # for i in range(9,18): # plt.scatter(x_centers[i].cpu(),y_centers[i].cpu(),c='r') # rect1 = plt.Rectangle([pre_left[i],pre_top[i]],pre_w[i],pre_h[i],color=\"r\",fill=False) # ax.add_patch(rect1) # plt.show() return boxesdef retinanet_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image): #-----------------------------------------------------------------# # 把y轴放前面是因为方便预测框和图像的宽高进行相乘 #-----------------------------------------------------------------# box_yx = box_xy[..., ::-1] box_hw = box_wh[..., ::-1] input_shape = np.array(input_shape) image_shape = np.array(image_shape) if letterbox_image: #-----------------------------------------------------------------# # 这里求出来的offset是图像有效区域相对于图像左上角的偏移情况 # new_shape指的是宽高缩放情况 #-----------------------------------------------------------------# new_shape = np.round(image_shape * np.min(input_shape/image_shape)) offset = (input_shape - new_shape)/2./input_shape scale = input_shape/new_shape box_yx = (box_yx - offset) * scale box_hw *= scale box_mins = box_yx - (box_hw / 2.) box_maxes = box_yx + (box_hw / 2.) boxes = np.concatenate([box_mins[..., 0:1], box_mins[..., 1:2], box_maxes[..., 0:1], box_maxes[..., 1:2]], axis=-1) boxes *= np.concatenate([image_shape, image_shape], axis=-1) return boxesdef non_max_suppression(prediction, input_shape, image_shape, letterbox_image, conf_thres=0.5, nms_thres=0.4): output = [None for _ in range(len(prediction))] #----------------------------------------------------------# # 预测只用一张图片，只会进行一次 #----------------------------------------------------------# for i, image_pred in enumerate(prediction): #----------------------------------------------------------# # 对种类预测部分取max。 # class_conf [num_anchors, 1] 种类置信度 # class_pred [num_anchors, 1] 种类 #----------------------------------------------------------# class_conf, class_pred = torch.max(image_pred[:, 4:], 1, keepdim=True) #----------------------------------------------------------# # 利用置信度进行第一轮筛选 #----------------------------------------------------------# conf_mask = (class_conf[:, 0] &gt;= conf_thres).squeeze() #----------------------------------------------------------# # 根据置信度进行预测结果的筛选 #----------------------------------------------------------# image_pred = image_pred[conf_mask] class_conf = class_conf[conf_mask] class_pred = class_pred[conf_mask] if not image_pred.size(0): continue #-------------------------------------------------------------------------# # detections [num_anchors, 6] # 6的内容为：x1, y1, x2, y2, class_conf, class_pred #-------------------------------------------------------------------------# detections = torch.cat((image_pred[:, :4], class_conf.float(), class_pred.float()), 1) #------------------------------------------# # 获得预测结果中包含的所有种类 #------------------------------------------# unique_labels = detections[:, -1].cpu().unique() if prediction.is_cuda: unique_labels = unique_labels.cuda() detections = detections.cuda() for c in unique_labels: #------------------------------------------# # 获得某一类得分筛选后全部的预测结果 #------------------------------------------# detections_class = detections[detections[:, -1] == c] #------------------------------------------# # 使用官方自带的非极大抑制会速度更快一些！ #------------------------------------------# keep = nms( detections_class[:, :4], detections_class[:, 4], nms_thres ) max_detections = detections_class[keep] # #------------------------------------------# # # 按照存在物体的置信度排序 # #------------------------------------------# # _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True) # detections_class = detections_class[conf_sort_index] # #------------------------------------------# # # 进行非极大抑制 # #------------------------------------------# # max_detections = [] # while detections_class.size(0): # #---------------------------------------------------# # # 取出这一类置信度最高的，一步一步往下判断。 # # 判断重合程度是否大于nms_thres，如果是则去除掉 # #---------------------------------------------------# # max_detections.append(detections_class[0].unsqueeze(0)) # if len(detections_class) == 1: # break # ious = bbox_iou(max_detections[-1], detections_class[1:]) # detections_class = detections_class[1:][ious &lt; nms_thres] # #------------------------------------------# # # 堆叠 # #------------------------------------------# # max_detections = torch.cat(max_detections).data output[i] = max_detections if output[i] is None else torch.cat((output[i], max_detections)) if output[i] is not None: output[i] = output[i].cpu().numpy() box_xy, box_wh = (output[i][:, 0:2] + output[i][:, 2:4])/2, output[i][:, 2:4] - output[i][:, 0:2] output[i][:, :4] = retinanet_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image) return output 原图上进行绘制通过第三步，我们可以获得预测框在原图上的位置，而且这些预测框都是经过筛选的。这些筛选后的框可以直接绘制在图片上，就可以获得结果了。 训练部分真实框的处理从预测部分我们知道，每个特征层的预测结果，num_anchors x 4的卷积 用于预测 该特征层上 每一个网格点上 每一个先验框的变化情况。 也就是说，我们直接利用retinanet网络预测到的结果，并不是预测框在图片上的真实位置，需要解码才能得到真实位置。 而在训练的时候，我们需要计算loss函数，这个loss函数是相对于Retinanet网络的预测结果的。我们需要把图片输入到当前的Retinanet网络中，得到预测结果；同时还需要把真实框的信息，进行编码，这个编码是把真实框的位置信息格式转化为Retinanet预测结果的格式信息。 也就是，我们需要找到 每一张用于训练的图片的每一个真实框对应的先验框，并求出如果想要得到这样一个真实框，我们的预测结果应该是怎么样的。 从预测结果获得真实框的过程被称作解码，而从真实框获得预测结果的过程就是编码的过程。 因此我们只需要将解码过程逆过来就是编码过程了。 在进行编码的时候，我们需要找到每一个真实框对应的先验框，我们把和真实框重合程度在0.5以上的作为正样本，在0.4以下的作为负样本，在0.4和0.5之间的作为忽略样本。实现代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def get_target(anchor, bbox_annotation, classification, cuda): IoU = calc_iou(anchor[:, :], bbox_annotation[:, :4]) IoU_max, IoU_argmax = torch.max(IoU, dim=1) # compute the loss for classification targets = torch.ones_like(classification) * -1 if cuda: targets = targets.cuda() targets[torch.lt(IoU_max, 0.4), :] = 0 positive_indices = torch.ge(IoU_max, 0.5) num_positive_anchors = positive_indices.sum() assigned_annotations = bbox_annotation[IoU_argmax, :] targets[positive_indices, :] = 0 targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1 return targets, num_positive_anchors, positive_indices, assigned_annotations def encode_bbox(assigned_annotations, positive_indices, anchor_widths, anchor_heights, anchor_ctr_x, anchor_ctr_y): assigned_annotations = assigned_annotations[positive_indices, :] anchor_widths_pi = anchor_widths[positive_indices] anchor_heights_pi = anchor_heights[positive_indices] anchor_ctr_x_pi = anchor_ctr_x[positive_indices] anchor_ctr_y_pi = anchor_ctr_y[positive_indices] gt_widths = assigned_annotations[:, 2] - assigned_annotations[:, 0] gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1] gt_ctr_x = assigned_annotations[:, 0] + 0.5 * gt_widths gt_ctr_y = assigned_annotations[:, 1] + 0.5 * gt_heights # efficientdet style gt_widths = torch.clamp(gt_widths, min=1) gt_heights = torch.clamp(gt_heights, min=1) targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi targets_dw = torch.log(gt_widths / anchor_widths_pi) targets_dh = torch.log(gt_heights / anchor_heights_pi) targets = torch.stack((targets_dy, targets_dx, targets_dh, targets_dw)) targets = targets.t() return targets loss计算loss的计算分为两个部分：1、Smooth Loss：获取所有正标签的框的预测结果的回归loss。2、Focal Loss：获取所有未被忽略的种类的预测结果的交叉熵loss。 由于在Retinanet的训练过程中，正负样本极其不平衡，即 存在对应真实框的先验框可能只有若干个，但是不存在对应真实框的负样本却有上万个，这就会导致负样本的loss值极大，因此引入了Focal Loss进行正负样本的平衡。 Focal loss是何恺明大神提出的一种新的loss计算方案。其具有两个重要的特点。 a)控制正负样本的权重控制容易分类和难分类样本的权重正负样本的概念如下：一张图像可能生成成千上万的候选框，但是其中只有很少一部分是包含目标的的，有目标的就是正样本，没有目标的就是负样本。 容易分类和难分类样本的概念如下：假设存在一个二分类，样本1属于类别1的pt=0.9，样本2属于类别1的pt=0.6，显然前者更可能是类别1，其就是容易分类的样本；后者有可能是类别1，所以其为难分类样本。 如何实现权重控制呢：以二分类为例，常用交叉熵loss: .aydxggbxordz{zoom:50%;} 利用pt简化交叉熵损失： .sdntoqmfcnsp{zoom:50%;} 因此得到： .zflqgdzrrbyb{zoom:50%;} 想要降低负样本的影响，可以在常规的损失函数前增加一个系数αt。与Pt类似，当label=1的时候，αt=α；当label=otherwise的时候，αt=1 - α，a的范围也是0到1。此时我们便可以通过设置α实现控制正负样本对loss的贡献.fhpqwfwwnltm{zoom:50%;} 其中： .pdokhjsbkevo{zoom:50%;} 分解： .jyoxrfuwxhjs{zoom:50%;} b)控制容易分类和难分类样本的权重 按照刚才的思路，一个二分类，样本1属于类别1的pt=0.9，样本2属于类别1的pt=0.6，也就是 是某个类的概率越大，其越容易分类 所以利用1-Pt就可以计算出其属于容易分类或者难分类。具体实现方式如下。 .vpxkgrdeqcfm{zoom:50%;} 其中调制系数为： .fkopykiirtzw{zoom:50%;} 1、当pt趋于0的时候，调制系数趋于1，对于总的loss的贡献很大。当pt趋于1的时候，调制系数趋于0，也就是对于总的loss的贡献很小。2、当γ=0的时候，focal loss就是传统的交叉熵损失，可以通过调整γ实现调制系数的改变。 c）两种权重控制方法合并通过如下公式就可以实现控制正负样本的权重和控制容易分类和难分类样本的权重。 .wvtfwnjuejoj{zoom:50%;} 实现代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990class FocalLoss(nn.Module): def __init__(self): super(FocalLoss, self).__init__() def forward(self, classifications, regressions, anchors, annotations, alpha = 0.25, gamma = 2.0, cuda = True): # 设置 dtype = regressions.dtype batch_size = classifications.shape[0] classification_losses = [] regression_losses = [] # 获得先验框，将先验框转换成中心宽高的形势 anchor = anchors[0, :, :].to(dtype) # 转换成中心，宽高的形式 anchor_widths = anchor[:, 3] - anchor[:, 1] anchor_heights = anchor[:, 2] - anchor[:, 0] anchor_ctr_x = anchor[:, 1] + 0.5 * anchor_widths anchor_ctr_y = anchor[:, 0] + 0.5 * anchor_heights for j in range(batch_size): # 取出真实框 bbox_annotation = annotations[j] # 获得每张图片的分类结果和回归预测结果 classification = classifications[j, :, :] regression = regressions[j, :, :] # 平滑标签 classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4) if len(bbox_annotation) == 0: alpha_factor = torch.ones_like(classification) * alpha if cuda: alpha_factor = alpha_factor.cuda() alpha_factor = 1. - alpha_factor focal_weight = classification focal_weight = alpha_factor * torch.pow(focal_weight, gamma) bce = -(torch.log(1.0 - classification)) cls_loss = focal_weight * bce if cuda: regression_losses.append(torch.tensor(0).to(dtype).cuda()) else: regression_losses.append(torch.tensor(0).to(dtype)) classification_losses.append(cls_loss.sum()) continue # 获得目标预测结果 targets, num_positive_anchors, positive_indices, assigned_annotations = get_target(anchor, bbox_annotation, classification, cuda) alpha_factor = torch.ones_like(targets) * alpha if cuda: alpha_factor = alpha_factor.cuda() alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor) focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification) focal_weight = alpha_factor * torch.pow(focal_weight, gamma) bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification)) cls_loss = focal_weight * bce zeros = torch.zeros_like(cls_loss) if cuda: zeros = zeros.cuda() cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, zeros) classification_losses.append(cls_loss.sum() / torch.clamp(num_positive_anchors.to(dtype), min=1.0)) # smoooth_l1 if positive_indices.sum() &gt; 0: targets = encode_bbox(assigned_annotations, positive_indices, anchor_widths, anchor_heights, anchor_ctr_x, anchor_ctr_y) regression_diff = torch.abs(targets - regression[positive_indices, :]) regression_loss = torch.where( torch.le(regression_diff, 1.0 / 9.0), 0.5 * 9.0 * torch.pow(regression_diff, 2), regression_diff - 0.5 / 9.0 ) regression_losses.append(regression_loss.mean()) else: if cuda: regression_losses.append(torch.tensor(0).to(dtype).cuda()) else: regression_losses.append(torch.tensor(0).to(dtype)) c_loss = torch.stack(classification_losses).mean() r_loss = torch.stack(regression_losses).mean() loss = c_loss + r_loss return loss, c_loss, r_loss","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"retinanet","slug":"retinanet","permalink":"http://pistachio0812.github.io/tags/retinanet/"}],"author":"coolboy"},{"title":"CenterNet论文笔记","slug":"centernet","date":"2022-04-18T12:14:53.445Z","updated":"2022-10-04T12:55:56.684Z","comments":true,"path":"zh-CN/centernet/","permalink":"http://pistachio0812.github.io/zh-CN/centernet/","excerpt":"","text":"论文地址：CenterNet: Keypoint Triplets for Object Detection (thecvf.com) 源码地址： CenterNet: Keypoint Triplets for Object Detection 文章引用代码地址：https://github.com/bubbliiiing/centernet-pytorch 文章出处：Pytorch搭建自己的Centernet目标检测平台 主干网络主干网络选用resnet,ResNet50有两个基本的块，分别名为Conv Block和Identity Block，其中Conv Block输入和输出的维度是不一样的，所以不能连续串联，它的作用是改变网络的维度；Identity Block输入维度和输出维度相同，可以串联，用于加深网络的。","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"}],"author":"pistachio"},{"title":"vision transformer论文笔记","slug":"VIT","date":"2022-04-18T01:03:00.568Z","updated":"2022-10-04T13:32:47.108Z","comments":true,"path":"zh-CN/VIT/","permalink":"http://pistachio0812.github.io/zh-CN/VIT/","excerpt":"","text":"论文地址：https://arxiv.org/pdf/2010.11929.pdf 源码地址：google-research/vision_transformer (github.com) 文章引用源码：https://github.com/bubbliiiing/classification-pytorch 文章出处：https://blog.csdn.net/weixin_44791964/article/details/122637701 实现思路Vision Transformer是Transformer的视觉版本，Transformer基本上已经成为了自然语言处理的标配，但是在视觉中的运用还受到限制。 Vision Transformer打破了这种NLP与CV的隔离，将Transformer应用于图像图块（patch）序列上，进一步完成图像分类任务。简单来理解，Vision Transformer就是将输入进来的图片，每隔一定的区域大小划分图片块。然后将划分后的图片块组合成序列，将组合后的结果传入Transformer特有的Multi-head Self-attention进行特征提取。最后利用Cls Token进行分类。 整体架构.xpsfmbsnxdcm{} 与寻常的分类网络类似，整个Vision Transformer可以分为两部分，一部分是特征提取部分，另一部分是分类部分。 在特征提取部分，VIT所做的工作是特征提取。特征提取部分在图片中的对应区域是Patch+Position Embedding和Transformer Encoder。Patch+Position Embedding的作用主要是对输入进来的图片进行分块处理，每隔一定的区域大小划分图片块。然后将划分后的图片块组合成序列。在获得序列信息后，传入Transformer Encoder进行特征提取，这是Transformer特有的Multi-head Self-attention结构，通过自注意力机制，关注每个图片块的重要程度。 在分类部分，VIT所做的工作是利用提取到的特征进行分类。在进行特征提取的时候，我们会在图片序列中添加上Cls Token，该Token会作为一个单位的序列信息一起进行特征提取，提取的过程中，该Cls Token会与其它的特征进行特征交互，融合其它图片序列的特征。最终，我们利用Multi-head Self-attention结构提取特征后的Cls Token进行全连接分类。 网络结构详解特征提取部分a）Patch+Position Embedding .yqadgtschonr{} 该部分作用：对输入进来的图片进行分块处理，每隔一定的区域大小划分图片块。然后将划分后的图片块组合成序列。 该部分首先对输入进来的图片进行分块处理，处理方式其实很简单，使用的是现成的卷积。由于卷积使用的是滑动窗口的思想，我们只需要设定特定的步长，就可以输入进来的图片进行分块处理了。 在VIT中，我们常设置这个卷积的卷积核大小为16x16，步长也为16x16，此时卷积就会每隔16个像素点进行一次特征提取，由于卷积核大小为16x16，两个图片区域的特征提取过程就不会有重叠。当我们输入的图片是[224, 224, 3]的时候，我们可以获得一个[14, 14, 768]的特征层。 .wwjqlhravfku{} 下一步就是将这个特征层组合成序列，组合的方式非常简单，就是将高宽维度进行平铺，[14, 14, 768]在高宽维度平铺后，获得一个196, 768的特征层。平铺完成后，我们会在图片序列中添加上Cls Token，该Token会作为一个单位的序列信息一起进行特征提取，图中的这个0*就是Cls Token，我们此时获得一个197, 768的特征层。 .mrjryudrzyll{} 添加完成Cls Token后，再为所有特征添加上位置信息，这样网络才有区分不同区域的能力。添加方式其实也非常简单，我们生成一个197, 768的参数矩阵，这个参数矩阵是可训练的，把这个矩阵加上197, 768的特征层即可。 到这里，Patch+Position Embedding就构建完成了，构建代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# [224, 224, 3]-&gt;[14, 14, 768]class PatchEmbed(nn.Module): def __init__(self, input_shape=[224, 224], patch_size=16, in_chans=3, num_features=768, norm_layer=None, flatten=True): super().__init__() # 196 = 14 * 14 self.num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size) self.flatten = flatten self.proj = nn.Conv2d(in_chans, num_features, kernel_size=patch_size, stride=patch_size) self.norm = norm_layer(num_features) if norm_layer else nn.Identity() def forward(self, x): x = self.proj(x) if self.flatten: x = x.flatten(2).transpose(1, 2) # BCHW -&gt; BNC x = self.norm(x) # x = [b, 196, 768] return xclass VisionTransformer(nn.Module): def __init__( self, input_shape=[224, 224], patch_size=16, in_chans=3, num_classes=1000, num_features=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0.1, attn_drop_rate=0.1, drop_path_rate=0.1, norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=GELU ): super().__init__() #-----------------------------------------------# # 224, 224, 3 -&gt; 196, 768 #-----------------------------------------------# self.patch_embed = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features) num_patches = (224 // patch_size) * (224 // patch_size) self.num_features = num_features self.new_feature_shape = [int(input_shape[0] // patch_size), int(input_shape[1] // patch_size)] self.old_feature_shape = [int(224 // patch_size), int(224 // patch_size)] #--------------------------------------------------------------------------------------------------------------------# # classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。 # # 在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。 # 此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。 # 在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。 #--------------------------------------------------------------------------------------------------------------------# # 196, 768 -&gt; 197, 768 self.cls_token = nn.Parameter(torch.zeros(1, 1, num_features)) #--------------------------------------------------------------------------------------------------------------------# # 为网络提取到的特征添加上位置信息。 # 以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768 # 此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。 #--------------------------------------------------------------------------------------------------------------------# # 197, 768 -&gt; 197, 768 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, num_features)) def forward_features(self, x): # x = [b, 196, 768] x = self.patch_embed(x) # cls_token = [b, 1, 768] cls_token = self.cls_token.expand(x.shape[0], -1, -1) # x = [b, 197, 768] x = torch.cat((cls_token, x), dim=1) # [1, 1, 768] cls_token_pe = self.pos_embed[:, 0:1, :] # [1, 196, 768] img_token_pe = self.pos_embed[:, 1: , :] # [1, 196, 768]-&gt;[1, 14, 14, 768]-&gt;[1, 768, 14, 14] img_token_pe = img_token_pe.view(1, *self.old_feature_shape, -1).permute(0, 3, 1, 2) # 做插值，以防输入图片不是224*224 img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode='bicubic', align_corners=False) # [1, 768, 14, 14]-&gt;[1, 14, 14, 768]-&gt;[1, 196, 768] img_token_pe = img_token_pe.permute(0, 2, 3, 1).flatten(1, 2) # [1, 197, 768] pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=1) x = self.pos_drop(x + pos_embed) b）transformer encoder .qedpislnwwqf{} 在上一步获得shape为197, 768的序列信息后，将序列信息传入Transformer Encoder进行特征提取，这是Transformer特有的Multi-head Self-attention结构，通过自注意力机制，关注每个图片块的重要程度。 1)self-attention结构解析 看懂Self-attention结构，其实看懂下面这个动图就可以了，动图中存在一个序列的三个单位输入，每一个序列单位的输入都可以通过三个处理（比如全连接）获得Query、Key、Value，Query是查询向量、Key是键向量、Value值向量。 .hdhivghwgqpz{} 如果我们想要获得input-1的输出，那么我们进行如下几步：1、利用input-1的查询向量，分别乘上input-1、input-2、input-3的键向量，此时我们获得了三个score。2、然后对这三个score取softmax，获得了input-1、input-2、input-3各自的重要程度。3、然后将这个重要程度乘上input-1、input-2、input-3的值向量，求和。4、此时我们获得了input-1的输出。 如图所示，我们进行如下几步：1、input-1的查询向量为[1, 0, 2]，分别乘上input-1、input-2、input-3的键向量，获得三个score为2，4，4。2、然后对这三个score取softmax，获得了input-1、input-2、input-3各自的重要程度，获得三个重要程度为0.0，0.5，0.5。3、然后将这个重要程度乘上input-1、input-2、input-3的值向量，求和，即0.0 ∗ [ 1 , 2 , 3 ] + 0.5 ∗ [ 2 , 8 , 0 ] + 0.5 ∗ [ 2 , 6 , 3 ] = [ 2.0 , 7.0 , 1.5 ]4、此时我们获得了input-1的输出 [2.0, 7.0, 1.5]。 上述的例子中，序列长度仅为3，每个单位序列的特征长度仅为3，在VIT的Transformer Encoder中，序列长度为197，每个单位序列的特征长度为768 // num_heads。但计算过程是一样的。在实际运算时，我们采用矩阵进行运算。2)self-attention的矩阵运算 实际的矩阵运算过程如下图所示。我以实际矩阵为例子给大家解析： .buwclqvllsnq{} 输入的Query、Key、Value如下图所示： .cwsmrefhdyqz{} 首先利用 查询向量query 叉乘 转置后的键向量key，这一步可以通俗的理解为，利用查询向量去查询序列的特征，获得序列每个部分的重要程度score。 输出的每一行，都代表input-1、input-2、input-3，对当前input的贡献，我们对这个贡献值取一个softmax。 .ynepjhduqoik{} 然后利用 score 叉乘 value，这一步可以通俗的理解为，将序列每个部分的重要程度重新施加到序列的值上去。 .otfuuafiniyi{} 矩阵代码运算如下： 1234567891011121314151617181920212223242526272829303132import numpy as npdef soft_max(z): t = np.exp(z) a = np.exp(z) / np.expand_dims(np.sum(t, axis=1), 1) return aQuery = np.array([ [1,0,2], [2,2,2], [2,1,3]])Key = np.array([ [0,1,1], [4,4,0], [2,3,1]])Value = np.array([ [1,2,3], [2,8,0], [2,6,3]])scores = Query @ Key.Tprint(scores)scores = soft_max(scores)print(scores)out = scores @ Valueprint(out) 3）Multihead多头注意力机制 多头注意力机制的示意图如图所示： .xmayyhooelkw{} 这幅图给人的感觉略显迷茫，我们跳脱出这个图，直接从矩阵的shape入手会清晰很多。 在第一步进行图像的分割后，我们获得的特征层为197, 768。 在施加多头的时候，我们直接对196, 768的最后一维度进行分割，比如我们想分割成12个头，那么矩阵的shape就变成了196, 12, 64。 然后我们将196, 12, 64进行转置，将12放到前面去，获得的特征层为12, 196, 64。之后我们忽略这个12，把它和batch维度同等对待，只对196, 64进行处理，其实也就是上面的注意力机制的过程了。 .fkyrfxepyiee{zoom:50%;} 123456789101112131415161718192021222324252627282930313233343536#--------------------------------------------------------------------------## Attention机制# 将输入的特征qkv特征进行划分，首先生成query, key, value。query是查询向量、key是键向量、v是值向量。# 然后利用 查询向量query 叉乘 转置后的键向量key，这一步可以通俗的理解为，利用查询向量去查询序列的特征，获得序列每个部分的重要程度score。# 然后利用 score 叉乘 value，这一步可以通俗的理解为，将序列每个部分的重要程度重新施加到序列的值上去。#--------------------------------------------------------------------------#class Attention(nn.Module): def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.): super().__init__() self.num_heads = num_heads self.scale = (dim // num_heads) ** -0.5 # 768-&gt;768*3 self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_drop) def forward(self, x): # batch, 196, 768 B, N, C = x.shape # batch, 196, 768 -&gt; batch, 196, 768*3 -&gt; batch, 196, 3, 8, 768/8=96 -&gt; 3, batch, 8, 196, 96 qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # 3 * 1, batch, 8, 196, 96 -&gt; q, k, v = batch: 16, head: 8, patch: 196, each_head_attention_channels: 96 q, k, v = qkv[0], qkv[1], qkv[2] # batch, 8, 196, 96 @ batch, 8, 96, 196 -&gt; batch, 8, 196, 196 attn = (q @ k.transpose(-2, -1)) * self.scale attn = attn.softmax(dim=-1) attn = self.attn_drop(attn) # batch, 8, 196, 196 @ batch, 8, 196, 96 -&gt; batch, 8, 196, 96 -&gt; batch, 196, 8, 96 -&gt; batch, 196, 768 x = (attn @ v).transpose(1, 2).reshape(B, N, C) # batch, 196, 768 -&gt; batch, 196, 768 x = self.proj(x) # Dropout(batch, 196, 768) x = self.proj_drop(x) return x 4）TransformerBlock的构建 .vwcrxghodtgq{} 在完成MultiHeadSelfAttention的构建后，我们需要在其后加上两个全连接。就构建了整个TransformerBlock block流程见下图： .covlvyxttkhl{zoom: 80%;} 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Mlp(nn.Module): \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks \"\"\" def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=GELU, drop=0.): super().__init__() out_features = out_features or in_features hidden_features = hidden_features or in_features drop_probs = (drop, drop) self.fc1 = nn.Linear(in_features, hidden_features) self.act = act_layer() self.drop1 = nn.Dropout(drop_probs[0]) self.fc2 = nn.Linear(hidden_features, out_features) self.drop2 = nn.Dropout(drop_probs[1]) def forward(self, x): # batch, 196, 768 -&gt; batch, 196, 768 x = self.fc1(x) # batch, 196, 768 -&gt; batch, 196, 768 x = self.act(x) # batch, 196, 768 -&gt; batch, 196, 768 x = self.drop1(x) # batch, 196, 768 -&gt; batch, 196, 768 x = self.fc2(x) # batch, 196, 768 -&gt; batch, 196, 768 x = self.drop2(x) return x# a transoformer encoder blockclass Block(nn.Module): def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., drop_path=0., act_layer=GELU, norm_layer=nn.LayerNorm): super().__init__() self.norm1 = norm_layer(dim) self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop) self.norm2 = norm_layer(dim) self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop) self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity() def forward(self, x): x = x + self.drop_path(self.attn(self.norm1(x))) x = x + self.drop_path(self.mlp(self.norm2(x))) return x c）VIT模型构建 .pidgftpwvoir{} 整个VIT模型由一个Patch+Position Embedding加上多个TransformerBlock组成。典型的TransforerBlock的数量为12个 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100class VisionTransformer(nn.Module): def __init__( self, input_shape=[224, 224], patch_size=16, in_chans=3, num_classes=1000, num_features=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0.1, attn_drop_rate=0.1, drop_path_rate=0.1, norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=GELU ): super(VisionTransformer, self).__init__() #-----------------------------------------------# # 224, 224, 3 -&gt; batch, 196, 768 #-----------------------------------------------# self.patch_embed = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features) num_patches = (224 // patch_size) * (224 // patch_size) self.num_features = num_features self.new_feature_shape = [int(input_shape[0] // patch_size), int(input_shape[1] // patch_size)] self.old_feature_shape = [int(224 // patch_size), int(224 // patch_size)] #--------------------------------------------------------------------------------------------------------------------# # classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。 # # 在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。 # 此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。 # 在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。 #--------------------------------------------------------------------------------------------------------------------# # 1, 1, 768 self.cls_token = nn.Parameter(torch.zeros(1, 1, num_features)) #--------------------------------------------------------------------------------------------------------------------# # 为网络提取到的特征添加上位置信息。 # 以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768 # 此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。 #--------------------------------------------------------------------------------------------------------------------# # 1, 197, 768 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, num_features)) # 1, 197, 768 self.pos_drop = nn.Dropout(p=drop_rate) #-----------------------------------------------# # 197, 768 -&gt; 197, 768 12次 #-----------------------------------------------# dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)] self.blocks = nn.Sequential( *[ Block( dim = num_features, num_heads = num_heads, mlp_ratio = mlp_ratio, qkv_bias = qkv_bias, drop = drop_rate, attn_drop = attn_drop_rate, drop_path = dpr[i], norm_layer = norm_layer, act_layer = act_layer )for i in range(depth) ] ) self.norm = norm_layer(num_features) self.head = nn.Linear(num_features, num_classes) if num_classes &gt; 0 else nn.Identity() def forward_features(self, x): x = self.patch_embed(x) cls_token = self.cls_token.expand(x.shape[0], -1, -1) x = torch.cat((cls_token, x), dim=1) cls_token_pe = self.pos_embed[:, 0:1, :] img_token_pe = self.pos_embed[:, 1: , :] img_token_pe = img_token_pe.view(1, *self.old_feature_shape, -1).permute(0, 3, 1, 2) img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode='bicubic', align_corners=False) img_token_pe = img_token_pe.permute(0, 2, 3, 1).flatten(1, 2) pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=1) x = self.pos_drop(x + pos_embed) x = self.blocks(x) x = self.norm(x) return x[:, 0] def forward(self, x): # # 整个Transformer Encoder = batch, 768 x = self.forward_features(x) # 最后的MLP Header = batch, 768 -&gt; 768 -&gt; 1000 -&gt; batch, 1000 x = self.head(x) return x def freeze_backbone(self): backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]] for module in backbone: try: for param in module.parameters(): param.requires_grad = False except: module.requires_grad = False def Unfreeze_backbone(self): backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]] for module in backbone: try: for param in module.parameters(): param.requires_grad = True except: module.requires_grad = True 分类部分.qdiwjtkaqfnx{} 在分类部分，VIT所做的工作是利用提取到的特征进行分类。 在进行特征提取的时候，我们会在图片序列中添加上Cls Token，该Token会作为一个单位的序列信息一起进行特征提取，提取的过程中，该Cls Token会与其它的特征进行特征交互，融合其它图片序列的特征。 最终，我们利用Multi-head Self-attention结构提取特征后的Cls Token进行全连接分类。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798class VisionTransformer(nn.Module): def __init__( self, input_shape=[224, 224], patch_size=16, in_chans=3, num_classes=1000, num_features=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0.1, attn_drop_rate=0.1, drop_path_rate=0.1, norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=GELU ): super().__init__() #-----------------------------------------------# # 224, 224, 3 -&gt; 196, 768 #-----------------------------------------------# self.patch_embed = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features) num_patches = (224 // patch_size) * (224 // patch_size) self.num_features = num_features self.new_feature_shape = [int(input_shape[0] // patch_size), int(input_shape[1] // patch_size)] self.old_feature_shape = [int(224 // patch_size), int(224 // patch_size)] #--------------------------------------------------------------------------------------------------------------------# # classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。 # # 在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。 # 此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。 # 在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。 #--------------------------------------------------------------------------------------------------------------------# # 196, 768 -&gt; 197, 768 self.cls_token = nn.Parameter(torch.zeros(1, 1, num_features)) #--------------------------------------------------------------------------------------------------------------------# # 为网络提取到的特征添加上位置信息。 # 以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768 # 此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。 #--------------------------------------------------------------------------------------------------------------------# # 197, 768 -&gt; 197, 768 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, num_features)) self.pos_drop = nn.Dropout(p=drop_rate) #-----------------------------------------------# # 197, 768 -&gt; 197, 768 12次 #-----------------------------------------------# dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)] self.blocks = nn.Sequential( *[ Block( dim = num_features, num_heads = num_heads, mlp_ratio = mlp_ratio, qkv_bias = qkv_bias, drop = drop_rate, attn_drop = attn_drop_rate, drop_path = dpr[i], norm_layer = norm_layer, act_layer = act_layer )for i in range(depth) ] ) self.norm = norm_layer(num_features) self.head = nn.Linear(num_features, num_classes) if num_classes &gt; 0 else nn.Identity() def forward_features(self, x): x = self.patch_embed(x) cls_token = self.cls_token.expand(x.shape[0], -1, -1) x = torch.cat((cls_token, x), dim=1) cls_token_pe = self.pos_embed[:, 0:1, :] img_token_pe = self.pos_embed[:, 1: , :] img_token_pe = img_token_pe.view(1, *self.old_feature_shape, -1).permute(0, 3, 1, 2) img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode='bicubic', align_corners=False) img_token_pe = img_token_pe.permute(0, 2, 3, 1).flatten(1, 2) pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=1) x = self.pos_drop(x + pos_embed) x = self.blocks(x) x = self.norm(x) return x[:, 0] def forward(self, x): x = self.forward_features(x) x = self.head(x) return x def freeze_backbone(self): backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]] for module in backbone: try: for param in module.parameters(): param.requires_grad = False except: module.requires_grad = False def Unfreeze_backbone(self): backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]] for module in backbone: try: for param in module.parameters(): param.requires_grad = True except: module.requires_grad = True VIT构建代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230import mathfrom collections import OrderedDictfrom functools import partialimport numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as F#--------------------------------------## Gelu激活函数的实现# 利用近似的数学公式#--------------------------------------#class GELU(nn.Module): def __init__(self): super(GELU, self).__init__() def forward(self, x): return 0.5 * x * (1 + F.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * torch.pow(x,3))))def drop_path(x, drop_prob: float = 0., training: bool = False): if drop_prob == 0. or not training: return x keep_prob = 1 - drop_prob shape = (x.shape[0],) + (1,) * (x.ndim - 1) random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device) random_tensor.floor_() output = x.div(keep_prob) * random_tensor return outputclass DropPath(nn.Module): def __init__(self, drop_prob=None): super(DropPath, self).__init__() self.drop_prob = drop_prob def forward(self, x): return drop_path(x, self.drop_prob, self.training)class PatchEmbed(nn.Module): def __init__(self, input_shape=[224, 224], patch_size=16, in_chans=3, num_features=768, norm_layer=None, flatten=True): super().__init__() self.num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size) self.flatten = flatten self.proj = nn.Conv2d(in_chans, num_features, kernel_size=patch_size, stride=patch_size) self.norm = norm_layer(num_features) if norm_layer else nn.Identity() def forward(self, x): x = self.proj(x) if self.flatten: x = x.flatten(2).transpose(1, 2) # BCHW -&gt; BNC x = self.norm(x) return x#--------------------------------------------------------------------------------------------------------------------## Attention机制# 将输入的特征qkv特征进行划分，首先生成query, key, value。query是查询向量、key是键向量、v是值向量。# 然后利用 查询向量query 叉乘 转置后的键向量key，这一步可以通俗的理解为，利用查询向量去查询序列的特征，获得序列每个部分的重要程度score。# 然后利用 score 叉乘 value，这一步可以通俗的理解为，将序列每个部分的重要程度重新施加到序列的值上去。#--------------------------------------------------------------------------------------------------------------------#class Attention(nn.Module): def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.): super().__init__() self.num_heads = num_heads self.scale = (dim // num_heads) ** -0.5 self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_drop) def forward(self, x): B, N, C = x.shape qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) q, k, v = qkv[0], qkv[1], qkv[2] attn = (q @ k.transpose(-2, -1)) * self.scale attn = attn.softmax(dim=-1) attn = self.attn_drop(attn) x = (attn @ v).transpose(1, 2).reshape(B, N, C) x = self.proj(x) x = self.proj_drop(x) return xclass Mlp(nn.Module): \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks \"\"\" def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=GELU, drop=0.): super().__init__() out_features = out_features or in_features hidden_features = hidden_features or in_features drop_probs = (drop, drop) self.fc1 = nn.Linear(in_features, hidden_features) self.act = act_layer() self.drop1 = nn.Dropout(drop_probs[0]) self.fc2 = nn.Linear(hidden_features, out_features) self.drop2 = nn.Dropout(drop_probs[1]) def forward(self, x): x = self.fc1(x) x = self.act(x) x = self.drop1(x) x = self.fc2(x) x = self.drop2(x) return xclass Block(nn.Module): def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., drop_path=0., act_layer=GELU, norm_layer=nn.LayerNorm): super().__init__() self.norm1 = norm_layer(dim) self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop) self.norm2 = norm_layer(dim) self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop) self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity() def forward(self, x): x = x + self.drop_path(self.attn(self.norm1(x))) x = x + self.drop_path(self.mlp(self.norm2(x))) return x class VisionTransformer(nn.Module): def __init__( self, input_shape=[224, 224], patch_size=16, in_chans=3, num_classes=1000, num_features=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0.1, attn_drop_rate=0.1, drop_path_rate=0.1, norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=GELU ): super().__init__() #-----------------------------------------------# # 224, 224, 3 -&gt; 196, 768 #-----------------------------------------------# self.patch_embed = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features) num_patches = (224 // patch_size) * (224 // patch_size) self.num_features = num_features self.new_feature_shape = [int(input_shape[0] // patch_size), int(input_shape[1] // patch_size)] self.old_feature_shape = [int(224 // patch_size), int(224 // patch_size)] #--------------------------------------------------------------------------------------------------------------------# # classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。 # # 在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。 # 此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。 # 在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。 #--------------------------------------------------------------------------------------------------------------------# # 196, 768 -&gt; 197, 768 self.cls_token = nn.Parameter(torch.zeros(1, 1, num_features)) #--------------------------------------------------------------------------------------------------------------------# # 为网络提取到的特征添加上位置信息。 # 以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768 # 此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。 #--------------------------------------------------------------------------------------------------------------------# # 197, 768 -&gt; 197, 768 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, num_features)) self.pos_drop = nn.Dropout(p=drop_rate) #-----------------------------------------------# # 197, 768 -&gt; 197, 768 12次 #-----------------------------------------------# dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)] self.blocks = nn.Sequential( *[ Block( dim = num_features, num_heads = num_heads, mlp_ratio = mlp_ratio, qkv_bias = qkv_bias, drop = drop_rate, attn_drop = attn_drop_rate, drop_path = dpr[i], norm_layer = norm_layer, act_layer = act_layer )for i in range(depth) ] ) self.norm = norm_layer(num_features) self.head = nn.Linear(num_features, num_classes) if num_classes &gt; 0 else nn.Identity() def forward_features(self, x): x = self.patch_embed(x) cls_token = self.cls_token.expand(x.shape[0], -1, -1) x = torch.cat((cls_token, x), dim=1) cls_token_pe = self.pos_embed[:, 0:1, :] img_token_pe = self.pos_embed[:, 1: , :] img_token_pe = img_token_pe.view(1, *self.old_feature_shape, -1).permute(0, 3, 1, 2) img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode='bicubic', align_corners=False) img_token_pe = img_token_pe.permute(0, 2, 3, 1).flatten(1, 2) pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=1) x = self.pos_drop(x + pos_embed) x = self.blocks(x) x = self.norm(x) return x[:, 0] def forward(self, x): x = self.forward_features(x) x = self.head(x) return x def freeze_backbone(self): backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]] for module in backbone: try: for param in module.parameters(): param.requires_grad = False except: module.requires_grad = False def Unfreeze_backbone(self): backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]] for module in backbone: try: for param in module.parameters(): param.requires_grad = True except: module.requires_grad = True def vit(input_shape=[224, 224], pretrained=False, num_classes=1000): model = VisionTransformer(input_shape) if pretrained: model.load_state_dict(torch.load(\"model_data/vit-patch_16.pth\")) if num_classes!=1000: model.head = nn.Linear(model.num_features, num_classes) return model","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"transformer","slug":"transformer","permalink":"http://pistachio0812.github.io/tags/transformer/"}],"author":"pistachio"},{"title":"马裤先生","slug":"马裤先生","date":"2022-04-18T00:36:02.708Z","updated":"2022-10-04T13:43:38.194Z","comments":true,"path":"zh-CN/马裤先生/","permalink":"http://pistachio0812.github.io/zh-CN/%E9%A9%AC%E8%A3%A4%E5%85%88%E7%94%9F/","excerpt":"","text":"火车在北平东站还没开，同屋那位睡上铺的穿马裤，戴平光的眼镜，青缎子洋服上身，胸袋插着小楷羊毫，足登青绒快靴的先生发了问：「你也是从北平上车？」很和气的。 火车还没动呢，不从北平上车，由哪儿呢？我只好反攻了：「你从哪儿上车？」 他没言语。看了看铺位，用尽全身的力气喊了声，「茶房！」 茶房跑来了。 「拿毯子！」马裤先生喊。 「请少待一会儿，先生。」茶房很和气的说， 「拿枕头！」 「先生，您等我忙过这会儿去，毯子和枕头就一齐全到。」茶房说的很快，可依然是很和气。 马裤先生没任何的表示。茶房故意地笑了笑，表示歉意。然后搭讪着慢慢地转身，腿刚预备好要走，背后打了个霹雳：「茶房！」 茶房不是假装没听见，便是耳朵已经震聋，竟自没回头，一直地快步走开。 「茶房！茶房！茶房！」马裤先生连喊，一声比一声高：站台上送客的跑过一群来，以为车上失了火，要不然便是出了人命。茶房始终没回头。马裤先生又挖了鼻孔一下，坐在我的床上。 茶房从门前走过。 「茶房！拿手巾把！」 「等等。」茶房似乎下了抵抗的决心。 马裤先生把领带解开，摘下领子来，分别挂在铁钩上：所有的钩子都被占了，他的帽子，大衣，已占了两个。车开了，他爬上了上铺，在我的头上脱靴子，并且击打靴底上的土。枕着个手提箱，车还没到永定门，他睡着了。我心中安坦了许多。 到了丰台，车还没站住，上面出了声：「茶房！」 没等茶房答应，他又睡着了；大概这次是梦话。 过了丰台，大概还没到廊房，上面又打了雷：「茶房！」 茶房来了，眉毛拧得好像要把谁吃了才痛快。 「干吗？先——生——」 「拿茶！」 「好吧！」茶房的眉毛拧得直往下落毛。 马裤先生又入了梦乡，呼声只比「茶房」小一点。有时呼声稍低一点，用咬牙来补上。 到了天津。又上来些旅客。车好容易又从天津开走。刚一开车，茶房给马裤先生拿来头一份毯子枕头和手巾把。马裤先生用手巾把耳鼻孔全钻得到家，这一把手巾擦了至少有一刻钟，最后用手巾擦了擦手提箱上的土。我给他数着，从老站到总站的十来分钟之间，他又喊了四五十声茶房。茶房只来了一次，他的问题是火车向哪面走呢？茶房的回答是不知道；于是又引起他的建议，车上总该有人知道，茶房应当负责去问。茶房说，连驶车的也不晓得东西南北。于是他几乎变了颜色，万一车走迷了路？！ 茶房没再回答，可是又掉了几根眉毛。 他又睡了，这次是在头上摔了摔袜子，可是一口痰并没往下唾，而是照顾了车顶。 我的目的地是德州，天将亮就到了。谢天谢地！ 我雇好车，进了城，还清清楚楚地听见「茶房！」 一个多礼拜了，我还惦记着茶房的眉毛呢。","categories":[{"name":"文学黑洞","slug":"文学黑洞","permalink":"http://pistachio0812.github.io/categories/%E6%96%87%E5%AD%A6%E9%BB%91%E6%B4%9E/"}],"tags":[{"name":"今日故事","slug":"今日故事","permalink":"http://pistachio0812.github.io/tags/%E4%BB%8A%E6%97%A5%E6%95%85%E4%BA%8B/"}],"author":"Deniel"},{"title":"烟之外","slug":"烟之外","date":"2022-04-17T02:29:57.291Z","updated":"2022-10-04T13:53:13.678Z","comments":true,"path":"zh-CN/烟之外/","permalink":"http://pistachio0812.github.io/zh-CN/%E7%83%9F%E4%B9%8B%E5%A4%96/","excerpt":"","text":"在涛声中唤你的名字，而你的名字已在千帆之外潮来潮去左边的鞋印才下午右边的鞋印已黄昏了六月原是一本很感伤的书结局如此之凄美——落日西沉 你依然凝视那人眼中展示的一片纯白他跪向你，向昨日那朵美了整个下午的云海哟，为何在众灯之中独点亮那一盏茫然还能抓住什么呢？你那曾被称为云的眸子现有人叫作烟","categories":[{"name":"文学黑洞","slug":"文学黑洞","permalink":"http://pistachio0812.github.io/categories/%E6%96%87%E5%AD%A6%E9%BB%91%E6%B4%9E/"}],"tags":[{"name":"今日故事","slug":"今日故事","permalink":"http://pistachio0812.github.io/tags/%E4%BB%8A%E6%97%A5%E6%95%85%E4%BA%8B/"}],"author":"Daniel"},{"title":"Faster-RCNN论文笔记","slug":"Faster-rcnn","date":"2022-04-16T13:50:10.247Z","updated":"2022-10-04T13:10:22.584Z","comments":true,"path":"zh-CN/Faster-rcnn/","permalink":"http://pistachio0812.github.io/zh-CN/Faster-rcnn/","excerpt":"","text":"论文地址：Faster R-CNN 源码地址：ShaoqingRen/faster_rcnn: Faster R-CNN (github.com) 文章引用源码：https://github.com/bubbliiiing/faster-rcnn-pytorch 文章出处：https://blog.csdn.net/weixin_44791964/article/details/105739918 实现思路预测部分主干网络.yboarkpprzld{}","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"faster-rcnn","slug":"faster-rcnn","permalink":"http://pistachio0812.github.io/tags/faster-rcnn/"}],"author":"pistachio"},{"title":"yolov3论文笔记","slug":"YOLOV3","date":"2022-04-16T07:40:57.513Z","updated":"2022-10-04T13:35:06.698Z","comments":true,"path":"zh-CN/YOLOV3/","permalink":"http://pistachio0812.github.io/zh-CN/YOLOV3/","excerpt":"","text":"论文地址：https://arxiv.org/pdf/1804.02767.pdf 源码地址：ultralytics/yolov3 文章引用源码：https://github.com/bubbliiiing/yolo3-pytorch 文章出处：https://blog.csdn.net/weixin_44791964/article/details/105310627 实现思路预测部分主干网络Darknet53.xxxpwjiifrpd{} YoloV3所使用的主干特征提取网络为Darknet53，它具有两个重要特点：1、Darknet53具有一个重要特点是使用了残差网络Residual，Darknet53中的残差卷积就是首先进行一次卷积核大小为3X3、步长为2的卷积，该卷积会压缩输入进来的特征层的宽和高，此时我们可以获得一个特征层，我们将该特征层命名为layer。之后我们再对该特征层进行一次1X1的卷积和一次3X3的卷积，并把这个结果加上layer，此时我们便构成了残差结构。通过不断的1X1卷积和3X3卷积以及残差边的叠加，我们便大幅度的加深了网络。残差网络的特点是容易优化，并且能够通过增加相当的深度来提高准确率。其内部的残差块使用了跳跃连接，缓解了在深度神经网络中增加深度带来的梯度消失问题。 2、Darknet53的每一个卷积部分使用了特有的DarknetConv2D结构，每一次卷积的时候进行l2正则化，完成卷积后进行BatchNormalization标准化与LeakyReLU。普通的ReLU是将所有的负值都设为零，Leaky ReLU则是给所有负值赋予一个非零斜率。以数学的方式我们可以表示为： .fbdsdkaveumf{} 实现代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596python# 详情见nets/darknet.py#---------------------------------------------------------------------## 残差结构# 利用一个1x1卷积下降通道数，然后利用一个3x3卷积提取特征并且上升通道数# 最后接上一个残差边#---------------------------------------------------------------------#class BasicBlock(nn.Module): def __init__(self, inplanes, planes): super(BasicBlock, self).__init__() self.conv1 = nn.Conv2d(inplanes, planes[0], kernel_size=1, stride=1, padding=0, bias=False) self.bn1 = nn.BatchNorm2d(planes[0]) self.relu1 = nn.LeakyReLU(0.1) self.conv2 = nn.Conv2d(planes[0], planes[1], kernel_size=3, stride=1, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(planes[1]) self.relu2 = nn.LeakyReLU(0.1) def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu1(out) out = self.conv2(out) out = self.bn2(out) out = self.relu2(out) out += residual return outclass DarkNet(nn.Module): def __init__(self, layers): super(DarkNet, self).__init__() self.inplanes = 32 # 416,416,3 -&gt; 416,416,32 self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(self.inplanes) self.relu1 = nn.LeakyReLU(0.1) # 416,416,32 -&gt; 208,208,64 self.layer1 = self._make_layer([32, 64], layers[0]) # 208,208,64 -&gt; 104,104,128 self.layer2 = self._make_layer([64, 128], layers[1]) # 104,104,128 -&gt; 52,52,256 self.layer3 = self._make_layer([128, 256], layers[2]) # 52,52,256 -&gt; 26,26,512 self.layer4 = self._make_layer([256, 512], layers[3]) # 26,26,512 -&gt; 13,13,1024 self.layer5 = self._make_layer([512, 1024], layers[4]) self.layers_out_filters = [64, 128, 256, 512, 1024] # 进行权值初始化 for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() #---------------------------------------------------------------------# # 在每一个layer里面，首先利用一个步长为2的3x3卷积进行下采样 # 然后进行残差结构的堆叠 #---------------------------------------------------------------------# def _make_layer(self, planes, blocks): layers = [] # 下采样，步长为2，卷积核大小为3 layers.append((\"ds_conv\", nn.Conv2d(self.inplanes, planes[1], kernel_size=3, stride=2, padding=1, bias=False))) layers.append((\"ds_bn\", nn.BatchNorm2d(planes[1]))) layers.append((\"ds_relu\", nn.LeakyReLU(0.1))) # 加入残差结构 self.inplanes = planes[1] for i in range(0, blocks): layers.append((\"residual_{}\".format(i), BasicBlock(self.inplanes, planes))) return nn.Sequential(OrderedDict(layers)) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu1(x) x = self.layer1(x) x = self.layer2(x) out3 = self.layer3(x) out4 = self.layer4(out3) out5 = self.layer5(out4) return out3, out4, out5def darknet53(): model = DarkNet([1, 2, 8, 8, 4]) return model 从特征层获取预测结果.yofwjjhbpodg{} 从特征获取预测结果的过程可以分为两个部分，分别是： ·构建FPN特征金字塔进行加强特征提取。·利用Yolo Head对三个有效特征层进行预测。a）构建FPN特征金字塔进行加强特征提取在特征利用部分，YoloV3提取多特征层进行目标检测，一共提取三个特征层。三个特征层位于主干部分Darknet53的不同位置，分别位于中间层，中下层，底层，三个特征层的shape分别为(52,52,256)、(26,26,512)、(13,13,1024)。 在获得三个有效特征层后，我们利用这三个有效特征层进行FPN层的构建，构建方式为： ·13x13x1024的特征层进行5次卷积处理，处理完后利用YoloHead获得预测结果，一部分用于进行上采样UmSampling2d后与26x26x512特征层进行结合，结合特征层的shape为(26,26,768)。·结合特征层再次进行5次卷积处理，处理完后利用YoloHead获得预测结果，一部分用于进行上采样UmSampling2d后与52x52x256特征层进行结合，结合特征层的shape为(52,52,384)。·结合特征层再次进行5次卷积处理，处理完后利用YoloHead获得预测结果。特征金字塔可以将不同shape的特征层进行特征融合，有利于提取出更好的特征。 b）利用Yolo Head获得预测结果利用FPN特征金字塔，我们可以获得三个加强特征，这三个加强特征的shape分别为(13,13,512)、(26,26,256)、(52,52,128)，然后我们利用这三个shape的特征层传入Yolo Head获得预测结果。 Yolo Head本质上是一次3x3卷积加上一次1x1卷积，3x3卷积的作用是特征整合，1x1卷积的作用是调整通道数。 对三个特征层分别进行处理，假设我们预测是的VOC数据集，我们的输出层的shape分别为(13,13,75)，(26,26,75)，(52,52,75)，最后一个维度为75是因为该图是基于voc数据集的，它的类为20种，YoloV3针对每一个特征层的每一个特征点存在3个先验框，所以预测结果的通道数为3x25；如果使用的是coco训练集，类则为80种，最后的维度应该为255 = 3x85，三个特征层的shape为(13,13,255)，(26,26,255)，(52,52,255) 其实际情况就是，输入N张416x416的图片，在经过多层的运算后，会输出三个shape分别为(N,13,13,255)，(N,26,26,255)，(N,52,52,255)的数据，对应每个图分为13x13、26x26、52x52的网格上3个先验框的位置。 实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899# 详情见nets/yolo.pydef conv2d(filter_in, filter_out, kernel_size): pad = (kernel_size - 1) // 2 if kernel_size else 0 return nn.Sequential(OrderedDict([ (\"conv\", nn.Conv2d(filter_in, filter_out, kernel_size=kernel_size, stride=1, padding=pad, bias=False)), (\"bn\", nn.BatchNorm2d(filter_out)), (\"relu\", nn.LeakyReLU(0.1)), ]))#------------------------------------------------------------------------## make_last_layers里面一共有七个卷积，前五个用于提取特征。# 后两个用于获得yolo网络的预测结果#------------------------------------------------------------------------#def make_last_layers(filters_list, in_filters, out_filter): m = nn.Sequential( conv2d(in_filters, filters_list[0], 1), conv2d(filters_list[0], filters_list[1], 3), conv2d(filters_list[1], filters_list[0], 1), conv2d(filters_list[0], filters_list[1], 3), conv2d(filters_list[1], filters_list[0], 1), conv2d(filters_list[0], filters_list[1], 3), nn.Conv2d(filters_list[1], out_filter, kernel_size=1, stride=1, padding=0, bias=True) ) return mclass YoloBody(nn.Module): def __init__(self, anchors_mask, num_classes): super(YoloBody, self).__init__() #---------------------------------------------------# # 生成darknet53的主干模型 # 获得三个有效特征层，他们的shape分别是： # 52,52,256 # 26,26,512 # 13,13,1024 #---------------------------------------------------# self.backbone = darknet53() #---------------------------------------------------# # out_filters : [64, 128, 256, 512, 1024] #---------------------------------------------------# out_filters = self.backbone.layers_out_filters #------------------------------------------------------------------------# # 计算yolo_head的输出通道数，对于voc数据集而言 # final_out_filter0 = final_out_filter1 = final_out_filter2 = 75 #------------------------------------------------------------------------# self.last_layer0 = make_last_layers([512, 1024], out_filters[-1], len(anchors_mask[0]) * (num_classes + 5)) self.last_layer1_conv = conv2d(512, 256, 1) self.last_layer1_upsample = nn.Upsample(scale_factor=2, mode='nearest') self.last_layer1 = make_last_layers([256, 512], out_filters[-2] + 256, len(anchors_mask[1]) * (num_classes + 5)) self.last_layer2_conv = conv2d(256, 128, 1) self.last_layer2_upsample = nn.Upsample(scale_factor=2, mode='nearest') self.last_layer2 = make_last_layers([128, 256], out_filters[-3] + 128, len(anchors_mask[2]) * (num_classes + 5)) def forward(self, x): #---------------------------------------------------# # 获得三个有效特征层，他们的shape分别是： # 52,52,256；26,26,512；13,13,1024 #---------------------------------------------------# x2, x1, x0 = self.backbone(x) #---------------------------------------------------# # 第一个特征层 # out0 = (batch_size,255,13,13) #---------------------------------------------------# # 13,13,1024 -&gt; 13,13,512 -&gt; 13,13,1024 -&gt; 13,13,512 -&gt; 13,13,1024 -&gt; 13,13,512 out0_branch = self.last_layer0[:5](x0) out0 = self.last_layer0[5:](out0_branch) # 13,13,512 -&gt; 13,13,256 -&gt; 26,26,256 x1_in = self.last_layer1_conv(out0_branch) x1_in = self.last_layer1_upsample(x1_in) # 26,26,256 + 26,26,512 -&gt; 26,26,768 x1_in = torch.cat([x1_in, x1], 1) #---------------------------------------------------# # 第二个特征层 # out1 = (batch_size,255,26,26) #---------------------------------------------------# # 26,26,768 -&gt; 26,26,256 -&gt; 26,26,512 -&gt; 26,26,256 -&gt; 26,26,512 -&gt; 26,26,256 out1_branch = self.last_layer1[:5](x1_in) out1 = self.last_layer1[5:](out1_branch) # 26,26,256 -&gt; 26,26,128 -&gt; 52,52,128 x2_in = self.last_layer2_conv(out1_branch) x2_in = self.last_layer2_upsample(x2_in) # 52,52,128 + 52,52,256 -&gt; 52,52,384 x2_in = torch.cat([x2_in, x2], 1) #---------------------------------------------------# # 第一个特征层 # out3 = (batch_size,255,52,52) #---------------------------------------------------# # 52,52,384 -&gt; 52,52,128 -&gt; 52,52,256 -&gt; 52,52,128 -&gt; 52,52,256 -&gt; 52,52,128 out2 = self.last_layer2(x2_in) return out0, out1, out2 预测结果的解码由第二步我们可以获得三个特征层的预测结果，shape分别为： ·(N,13,13,255)·(N,26,26,255)·(N,52,52,255)在这里我们简单了解一下每个有效特征层到底做了什么：每一个有效特征层将整个图片分成与其长宽对应的网格，如(N,13,13,255)的特征层就是将整个图像分成13x13个网格；然后从每个网格中心建立多个先验框，这些框是网络预先设定好的框，网络的预测结果会判断这些框内是否包含物体，以及这个物体的种类。 由于每一个网格点都具有三个先验框，所以上述的预测结果可以reshape为： (N,13,13,3,85)(N,26,26,3,85)(N,52,52,3,85)其中的85可以拆分为4+1+80，其中的4代表先验框的调整参数，1代表先验框内是否包含物体，80代表的是这个先验框的种类，由于coco分了80类，所以这里是80。如果YoloV3只检测两类物体，那么这个85就变为了4+1+2 = 7。 即85包含了4+1+80，分别代表x_offset、y_offset、h和w、置信度、分类结果。 但是这个预测结果并不对应着最终的预测框在图片上的位置，还需要解码才可以完成。 YoloV3的解码过程分为两步： ·先将每个网格点加上它对应的x_offset和y_offset，加完后的结果就是预测框的中心。·然后再利用 先验框和h、w结合 计算出预测框的宽高。这样就能得到整个预测框的位置了。 得到最终的预测结果后还要进行得分排序与非极大抑制筛选。 这一部分基本上是所有目标检测通用的部分。其对于每一个类进行判别：1、取出每一类得分大于self.obj_threshold的框和得分。2、利用框的位置和得分进行非极大抑制。 实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224# 详情见utils/utils_bbox.pyclass DecodeBox(): def __init__(self, anchors, num_classes, input_shape, anchors_mask = [[6,7,8], [3,4,5], [0,1,2]]): super(DecodeBox, self).__init__() self.anchors = anchors self.num_classes = num_classes self.bbox_attrs = 5 + num_classes self.input_shape = input_shape #-----------------------------------------------------------# # 13x13的特征层对应的anchor是[116,90],[156,198],[373,326] # 26x26的特征层对应的anchor是[30,61],[62,45],[59,119] # 52x52的特征层对应的anchor是[10,13],[16,30],[33,23] #-----------------------------------------------------------# self.anchors_mask = anchors_mask def decode_box(self, inputs): outputs = [] for i, input in enumerate(inputs): #-----------------------------------------------# # 输入的input一共有三个，他们的shape分别是 # batch_size, 255, 13, 13 # batch_size, 255, 26, 26 # batch_size, 255, 52, 52 #-----------------------------------------------# batch_size = input.size(0) input_height = input.size(2) input_width = input.size(3) #-----------------------------------------------# # 输入为416x416时 # stride_h = stride_w = 32、16、8 #-----------------------------------------------# stride_h = self.input_shape[0] / input_height stride_w = self.input_shape[1] / input_width #-------------------------------------------------# # 此时获得的scaled_anchors大小是相对于特征层的 #-------------------------------------------------# scaled_anchors = [(anchor_width / stride_w, anchor_height / stride_h) for anchor_width, anchor_height in self.anchors[self.anchors_mask[i]]] #-----------------------------------------------# # 输入的input一共有三个，他们的shape分别是 # batch_size, 3, 13, 13, 85 # batch_size, 3, 26, 26, 85 # batch_size, 3, 52, 52, 85 #-----------------------------------------------# prediction = input.view(batch_size, len(self.anchors_mask[i]), self.bbox_attrs, input_height, input_width).permute(0, 1, 3, 4, 2).contiguous() #-----------------------------------------------# # 先验框的中心位置的调整参数 #-----------------------------------------------# x = torch.sigmoid(prediction[..., 0]) y = torch.sigmoid(prediction[..., 1]) #-----------------------------------------------# # 先验框的宽高调整参数 #-----------------------------------------------# w = prediction[..., 2] h = prediction[..., 3] #-----------------------------------------------# # 获得置信度，是否有物体 #-----------------------------------------------# conf = torch.sigmoid(prediction[..., 4]) #-----------------------------------------------# # 种类置信度 #-----------------------------------------------# pred_cls = torch.sigmoid(prediction[..., 5:]) FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor #----------------------------------------------------------# # 生成网格，先验框中心，网格左上角 # batch_size,3,13,13 #----------------------------------------------------------# grid_x = torch.linspace(0, input_width - 1, input_width).repeat(input_height, 1).repeat( batch_size * len(self.anchors_mask[i]), 1, 1).view(x.shape).type(FloatTensor) grid_y = torch.linspace(0, input_height - 1, input_height).repeat(input_width, 1).t().repeat( batch_size * len(self.anchors_mask[i]), 1, 1).view(y.shape).type(FloatTensor) #----------------------------------------------------------# # 按照网格格式生成先验框的宽高 # batch_size,3,13,13 #----------------------------------------------------------# anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0])) anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1])) anchor_w = anchor_w.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(w.shape) anchor_h = anchor_h.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(h.shape) #----------------------------------------------------------# # 利用预测结果对先验框进行调整 # 首先调整先验框的中心，从先验框中心向右下角偏移 # 再调整先验框的宽高。 #----------------------------------------------------------# pred_boxes = FloatTensor(prediction[..., :4].shape) pred_boxes[..., 0] = x.data + grid_x pred_boxes[..., 1] = y.data + grid_y pred_boxes[..., 2] = torch.exp(w.data) * anchor_w pred_boxes[..., 3] = torch.exp(h.data) * anchor_h #----------------------------------------------------------# # 将输出结果归一化成小数的形式 #----------------------------------------------------------# _scale = torch.Tensor([input_width, input_height, input_width, input_height]).type(FloatTensor) output = torch.cat((pred_boxes.view(batch_size, -1, 4) / _scale, conf.view(batch_size, -1, 1), pred_cls.view(batch_size, -1, self.num_classes)), -1) outputs.append(output.data) return outputs def yolo_correct_boxes(self, box_xy, box_wh, input_shape, image_shape, letterbox_image): #-----------------------------------------------------------------# # 把y轴放前面是因为方便预测框和图像的宽高进行相乘 #-----------------------------------------------------------------# box_yx = box_xy[..., ::-1] box_hw = box_wh[..., ::-1] input_shape = np.array(input_shape) image_shape = np.array(image_shape) if letterbox_image: #-----------------------------------------------------------------# # 这里求出来的offset是图像有效区域相对于图像左上角的偏移情况 # new_shape指的是宽高缩放情况 #-----------------------------------------------------------------# new_shape = np.round(image_shape * np.min(input_shape/image_shape)) offset = (input_shape - new_shape)/2./input_shape scale = input_shape/new_shape box_yx = (box_yx - offset) * scale box_hw *= scale box_mins = box_yx - (box_hw / 2.) box_maxes = box_yx + (box_hw / 2.) boxes = np.concatenate([box_mins[..., 0:1], box_mins[..., 1:2], box_maxes[..., 0:1], box_maxes[..., 1:2]], axis=-1) boxes *= np.concatenate([image_shape, image_shape], axis=-1) return boxes def non_max_suppression(self, prediction, num_classes, input_shape, image_shape, letterbox_image, conf_thres=0.5, nms_thres=0.4): #----------------------------------------------------------# # 将预测结果的格式转换成左上角右下角的格式。 # prediction [batch_size, num_anchors, 85] #----------------------------------------------------------# box_corner = prediction.new(prediction.shape) box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2 box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2 box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2 box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2 prediction[:, :, :4] = box_corner[:, :, :4] output = [None for _ in range(len(prediction))] for i, image_pred in enumerate(prediction): #----------------------------------------------------------# # 对种类预测部分取max。 # class_conf [num_anchors, 1] 种类置信度 # class_pred [num_anchors, 1] 种类 #----------------------------------------------------------# class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1, keepdim=True) #----------------------------------------------------------# # 利用置信度进行第一轮筛选 #----------------------------------------------------------# conf_mask = (image_pred[:, 4] * class_conf[:, 0] &gt;= conf_thres).squeeze() #----------------------------------------------------------# # 根据置信度进行预测结果的筛选 #----------------------------------------------------------# image_pred = image_pred[conf_mask] class_conf = class_conf[conf_mask] class_pred = class_pred[conf_mask] if not image_pred.size(0): continue #-------------------------------------------------------------------------# # detections [num_anchors, 7] # 7的内容为：x1, y1, x2, y2, obj_conf, class_conf, class_pred #-------------------------------------------------------------------------# detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1) #------------------------------------------# # 获得预测结果中包含的所有种类 #------------------------------------------# unique_labels = detections[:, -1].cpu().unique() if prediction.is_cuda: unique_labels = unique_labels.cuda() detections = detections.cuda() for c in unique_labels: #------------------------------------------# # 获得某一类得分筛选后全部的预测结果 #------------------------------------------# detections_class = detections[detections[:, -1] == c] #------------------------------------------# # 使用官方自带的非极大抑制会速度更快一些！ #------------------------------------------# keep = nms( detections_class[:, :4], detections_class[:, 4] * detections_class[:, 5], nms_thres ) max_detections = detections_class[keep] # # 按照存在物体的置信度排序 # _, conf_sort_index = torch.sort(detections_class[:, 4]*detections_class[:, 5], descending=True) # detections_class = detections_class[conf_sort_index] # # 进行非极大抑制 # max_detections = [] # while detections_class.size(0): # # 取出这一类置信度最高的，一步一步往下判断，判断重合程度是否大于nms_thres，如果是则去除掉 # max_detections.append(detections_class[0].unsqueeze(0)) # if len(detections_class) == 1: # break # ious = bbox_iou(max_detections[-1], detections_class[1:]) # detections_class = detections_class[1:][ious &lt; nms_thres] # # 堆叠 # max_detections = torch.cat(max_detections).data # Add max detections to outputs output[i] = max_detections if output[i] is None else torch.cat((output[i], max_detections)) if output[i] is not None: output[i] = output[i].cpu().numpy() box_xy, box_wh = (output[i][:, 0:2] + output[i][:, 2:4])/2, output[i][:, 2:4] - output[i][:, 0:2] output[i][:, :4] = self.yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image) return output 原图上进行绘制通过第三步，我们可以获得预测框在原图上的位置，而且这些预测框都是经过筛选的。这些筛选后的框可以直接绘制在图片上，就可以获得结果了。 训练部分计算loss所需参数在计算loss的时候，实际上是pred和target之间的对比：pred就是网络的预测结果。target就是网络的真实框情况。 pred是什么对于yolo3的模型来说，网络最后输出的内容就是三个特征层每个网格点对应的预测框及其种类，即三个特征层分别对应着图片被分为不同size的网格后，每个网格点上三个先验框对应的位置、置信度及其种类。 输出层的shape分别为(13,13,75)，(26,26,75)，(52,52,75)，最后一个维度为75是因为是基于voc数据集的，它的类为20种，yolo3只有针对每一个特征层存在3个先验框，所以最后维度为3x25；如果使用的是coco训练集，类则为80种，最后的维度应该为255 = 3x85，三个特征层的shape为(13,13,255)，(26,26,255)，(52,52,255) 现在的y_pre还是没有解码的，解码了之后才是真实图像上的情况。 target是什么。target就是一个真实图像中，真实框的情况。第一个维度是batch_size，第二个维度是每一张图片里面真实框的数量，第三个维度内部是真实框的信息，包括位置以及种类。 loss的计算过程拿到pred和target后，不可以简单的减一下作为对比，需要进行如下步骤。 判断真实框在图片中的位置，判断其属于哪一个网格点去检测。判断真实框和这个特征点的哪个先验框重合程度最高。计算该网格点应该有怎么样的预测结果才能获得真实框，与真实框重合度最高的先验框被用于作为正样本。根据网络的预测结果获得预测框，计算预测框和所有真实框的重合程度，如果重合程度大于一定门限，则将该预测框对应的先验框忽略。其余作为负样本。最终损失由三个部分组成：a、正样本，编码后的长宽与xy轴偏移量与预测值的差距。b、正样本，预测结果中置信度的值与1对比；负样本，预测结果中置信度的值与0对比。c、实际存在的框，种类预测结果与实际结果的对比。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327# 详情见nets/yolo_training.pyclass YOLOLoss(nn.Module): def __init__(self, anchors, num_classes, input_shape, cuda, anchors_mask = [[6,7,8], [3,4,5], [0,1,2]]): super(YOLOLoss, self).__init__() #-----------------------------------------------------------# # 13x13的特征层对应的anchor是[116,90],[156,198],[373,326] # 26x26的特征层对应的anchor是[30,61],[62,45],[59,119] # 52x52的特征层对应的anchor是[10,13],[16,30],[33,23] #-----------------------------------------------------------# self.anchors = anchors self.num_classes = num_classes self.bbox_attrs = 5 + num_classes self.input_shape = input_shape self.anchors_mask = anchors_mask self.ignore_threshold = 0.7 self.cuda = cuda def clip_by_tensor(self, t, t_min, t_max): t = t.float() result = (t &gt;= t_min).float() * t + (t &lt; t_min).float() * t_min result = (result &lt;= t_max).float() * result + (result &gt; t_max).float() * t_max return result def MSELoss(self, pred, target): return torch.pow(pred - target, 2) def BCELoss(self, pred, target): epsilon = 1e-7 pred = self.clip_by_tensor(pred, epsilon, 1.0 - epsilon) output = - target * torch.log(pred) - (1.0 - target) * torch.log(1.0 - pred) return output def forward(self, l, input, targets=None): #----------------------------------------------------# # l代表的是，当前输入进来的有效特征层，是第几个有效特征层 # input的shape为 bs, 3*(5+num_classes), 13, 13 # bs, 3*(5+num_classes), 26, 26 # bs, 3*(5+num_classes), 52, 52 # targets代表的是真实框。 #----------------------------------------------------# #--------------------------------# # 获得图片数量，特征层的高和宽 # 13和13 #--------------------------------# bs = input.size(0) in_h = input.size(2) in_w = input.size(3) #-----------------------------------------------------------------------# # 计算步长 # 每一个特征点对应原来的图片上多少个像素点 # 如果特征层为13x13的话，一个特征点就对应原来的图片上的32个像素点 # 如果特征层为26x26的话，一个特征点就对应原来的图片上的16个像素点 # 如果特征层为52x52的话，一个特征点就对应原来的图片上的8个像素点 # stride_h = stride_w = 32、16、8 # stride_h和stride_w都是32。 #-----------------------------------------------------------------------# stride_h = self.input_shape[0] / in_h stride_w = self.input_shape[1] / in_w #-------------------------------------------------# # 此时获得的scaled_anchors大小是相对于特征层的 #-------------------------------------------------# scaled_anchors = [(a_w / stride_w, a_h / stride_h) for a_w, a_h in self.anchors] #-----------------------------------------------# # 输入的input一共有三个，他们的shape分别是 # bs, 3*(5+num_classes), 13, 13 =&gt; batch_size, 3, 13, 13, 5 + num_classes # batch_size, 3, 26, 26, 5 + num_classes # batch_size, 3, 52, 52, 5 + num_classes #-----------------------------------------------# prediction = input.view(bs, len(self.anchors_mask[l]), self.bbox_attrs, in_h, in_w).permute(0, 1, 3, 4, 2).contiguous() #-----------------------------------------------# # 先验框的中心位置的调整参数 #-----------------------------------------------# x = torch.sigmoid(prediction[..., 0]) y = torch.sigmoid(prediction[..., 1]) #-----------------------------------------------# # 先验框的宽高调整参数 #-----------------------------------------------# w = prediction[..., 2] h = prediction[..., 3] #-----------------------------------------------# # 获得置信度，是否有物体 #-----------------------------------------------# conf = torch.sigmoid(prediction[..., 4]) #-----------------------------------------------# # 种类置信度 #-----------------------------------------------# pred_cls = torch.sigmoid(prediction[..., 5:]) #-----------------------------------------------# # 获得网络应该有的预测结果 #-----------------------------------------------# y_true, noobj_mask, box_loss_scale = self.get_target(l, targets, scaled_anchors, in_h, in_w) #---------------------------------------------------------------# # 将预测结果进行解码，判断预测结果和真实值的重合程度 # 如果重合程度过大则忽略，因为这些特征点属于预测比较准确的特征点 # 作为负样本不合适 #----------------------------------------------------------------# noobj_mask = self.get_ignore(l, x, y, h, w, targets, scaled_anchors, in_h, in_w, noobj_mask) if self.cuda: y_true = y_true.cuda() noobj_mask = noobj_mask.cuda() box_loss_scale = box_loss_scale.cuda() #-----------------------------------------------------------# # reshape_y_true[...,2:3]和reshape_y_true[...,3:4] # 表示真实框的宽高，二者均在0-1之间 # 真实框越大，比重越小，小框的比重更大。 #-----------------------------------------------------------# box_loss_scale = 2 - box_loss_scale #-----------------------------------------------------------# # 计算中心偏移情况的loss，使用BCELoss效果好一些 #-----------------------------------------------------------# loss_x = torch.sum(self.BCELoss(x, y_true[..., 0]) * box_loss_scale * y_true[..., 4]) loss_y = torch.sum(self.BCELoss(y, y_true[..., 1]) * box_loss_scale * y_true[..., 4]) #-----------------------------------------------------------# # 计算宽高调整值的loss #-----------------------------------------------------------# loss_w = torch.sum(self.MSELoss(w, y_true[..., 2]) * 0.5 * box_loss_scale * y_true[..., 4]) loss_h = torch.sum(self.MSELoss(h, y_true[..., 3]) * 0.5 * box_loss_scale * y_true[..., 4]) #-----------------------------------------------------------# # 计算置信度的loss #-----------------------------------------------------------# loss_conf = torch.sum(self.BCELoss(conf, y_true[..., 4]) * y_true[..., 4]) + \\ torch.sum(self.BCELoss(conf, y_true[..., 4]) * noobj_mask) loss_cls = torch.sum(self.BCELoss(pred_cls[y_true[..., 4] == 1], y_true[..., 5:][y_true[..., 4] == 1])) loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls num_pos = torch.sum(y_true[..., 4]) num_pos = torch.max(num_pos, torch.ones_like(num_pos)) return loss, num_pos def calculate_iou(self, _box_a, _box_b): #-----------------------------------------------------------# # 计算真实框的左上角和右下角 #-----------------------------------------------------------# b1_x1, b1_x2 = _box_a[:, 0] - _box_a[:, 2] / 2, _box_a[:, 0] + _box_a[:, 2] / 2 b1_y1, b1_y2 = _box_a[:, 1] - _box_a[:, 3] / 2, _box_a[:, 1] + _box_a[:, 3] / 2 #-----------------------------------------------------------# # 计算先验框获得的预测框的左上角和右下角 #-----------------------------------------------------------# b2_x1, b2_x2 = _box_b[:, 0] - _box_b[:, 2] / 2, _box_b[:, 0] + _box_b[:, 2] / 2 b2_y1, b2_y2 = _box_b[:, 1] - _box_b[:, 3] / 2, _box_b[:, 1] + _box_b[:, 3] / 2 #-----------------------------------------------------------# # 将真实框和预测框都转化成左上角右下角的形式 #-----------------------------------------------------------# box_a = torch.zeros_like(_box_a) box_b = torch.zeros_like(_box_b) box_a[:, 0], box_a[:, 1], box_a[:, 2], box_a[:, 3] = b1_x1, b1_y1, b1_x2, b1_y2 box_b[:, 0], box_b[:, 1], box_b[:, 2], box_b[:, 3] = b2_x1, b2_y1, b2_x2, b2_y2 #-----------------------------------------------------------# # A为真实框的数量，B为先验框的数量 #-----------------------------------------------------------# A = box_a.size(0) B = box_b.size(0) #-----------------------------------------------------------# # 计算交的面积 #-----------------------------------------------------------# max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2), box_b[:, 2:].unsqueeze(0).expand(A, B, 2)) min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2), box_b[:, :2].unsqueeze(0).expand(A, B, 2)) inter = torch.clamp((max_xy - min_xy), min=0) inter = inter[:, :, 0] * inter[:, :, 1] #-----------------------------------------------------------# # 计算预测框和真实框各自的面积 #-----------------------------------------------------------# area_a = ((box_a[:, 2]-box_a[:, 0]) * (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter) # [A,B] area_b = ((box_b[:, 2]-box_b[:, 0]) * (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter) # [A,B] #-----------------------------------------------------------# # 求IOU #-----------------------------------------------------------# union = area_a + area_b - inter return inter / union # [A,B] def get_target(self, l, targets, anchors, in_h, in_w): #-----------------------------------------------------# # 计算一共有多少张图片 #-----------------------------------------------------# bs = len(targets) #-----------------------------------------------------# # 用于选取哪些先验框不包含物体 #-----------------------------------------------------# noobj_mask = torch.ones(bs, len(self.anchors_mask[l]), in_h, in_w, requires_grad = False) #-----------------------------------------------------# # 让网络更加去关注小目标 #-----------------------------------------------------# box_loss_scale = torch.zeros(bs, len(self.anchors_mask[l]), in_h, in_w, requires_grad = False) #-----------------------------------------------------# # batch_size, 3, 13, 13, 5 + num_classes #-----------------------------------------------------# y_true = torch.zeros(bs, len(self.anchors_mask[l]), in_h, in_w, self.bbox_attrs, requires_grad = False) for b in range(bs): if len(targets[b])==0: continue batch_target = torch.zeros_like(targets[b]) #-------------------------------------------------------# # 计算出正样本在特征层上的中心点 #-------------------------------------------------------# batch_target[:, [0,2]] = targets[b][:, [0,2]] * in_w batch_target[:, [1,3]] = targets[b][:, [1,3]] * in_h batch_target[:, 4] = targets[b][:, 4] batch_target = batch_target.cpu() #-------------------------------------------------------# # 将真实框转换一个形式 # num_true_box, 4 #-------------------------------------------------------# gt_box = torch.FloatTensor(torch.cat((torch.zeros((batch_target.size(0), 2)), batch_target[:, 2:4]), 1)) #-------------------------------------------------------# # 将先验框转换一个形式 # 9, 4 #-------------------------------------------------------# anchor_shapes = torch.FloatTensor(torch.cat((torch.zeros((len(anchors), 2)), torch.FloatTensor(anchors)), 1)) #-------------------------------------------------------# # 计算交并比 # self.calculate_iou(gt_box, anchor_shapes) = [num_true_box, 9]每一个真实框和9个先验框的重合情况 # best_ns: # [每个真实框最大的重合度max_iou, 每一个真实框最重合的先验框的序号] #-------------------------------------------------------# best_ns = torch.argmax(self.calculate_iou(gt_box, anchor_shapes), dim=-1) for t, best_n in enumerate(best_ns): if best_n not in self.anchors_mask[l]: continue #----------------------------------------# # 判断这个先验框是当前特征点的哪一个先验框 #----------------------------------------# k = self.anchors_mask[l].index(best_n) #----------------------------------------# # 获得真实框属于哪个网格点 #----------------------------------------# i = torch.floor(batch_target[t, 0]).long() j = torch.floor(batch_target[t, 1]).long() #----------------------------------------# # 取出真实框的种类 #----------------------------------------# c = batch_target[t, 4].long() #----------------------------------------# # noobj_mask代表无目标的特征点 #----------------------------------------# noobj_mask[b, k, j, i] = 0 #----------------------------------------# # tx、ty代表中心调整参数的真实值 #----------------------------------------# y_true[b, k, j, i, 0] = batch_target[t, 0] - i.float() y_true[b, k, j, i, 1] = batch_target[t, 1] - j.float() y_true[b, k, j, i, 2] = math.log(batch_target[t, 2] / anchors[best_n][0]) y_true[b, k, j, i, 3] = math.log(batch_target[t, 3] / anchors[best_n][1]) y_true[b, k, j, i, 4] = 1 y_true[b, k, j, i, c + 5] = 1 #----------------------------------------# # 用于获得xywh的比例 # 大目标loss权重小，小目标loss权重大 #----------------------------------------# box_loss_scale[b, k, j, i] = batch_target[t, 2] * batch_target[t, 3] / in_w / in_h return y_true, noobj_mask, box_loss_scale def get_ignore(self, l, x, y, h, w, targets, scaled_anchors, in_h, in_w, noobj_mask): #-----------------------------------------------------# # 计算一共有多少张图片 #-----------------------------------------------------# bs = len(targets) FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor #-----------------------------------------------------# # 生成网格，先验框中心，网格左上角 #-----------------------------------------------------# grid_x = torch.linspace(0, in_w - 1, in_w).repeat(in_h, 1).repeat( int(bs * len(self.anchors_mask[l])), 1, 1).view(x.shape).type(FloatTensor) grid_y = torch.linspace(0, in_h - 1, in_h).repeat(in_w, 1).t().repeat( int(bs * len(self.anchors_mask[l])), 1, 1).view(y.shape).type(FloatTensor) # 生成先验框的宽高 scaled_anchors_l = np.array(scaled_anchors)[self.anchors_mask[l]] anchor_w = FloatTensor(scaled_anchors_l).index_select(1, LongTensor([0])) anchor_h = FloatTensor(scaled_anchors_l).index_select(1, LongTensor([1])) anchor_w = anchor_w.repeat(bs, 1).repeat(1, 1, in_h * in_w).view(w.shape) anchor_h = anchor_h.repeat(bs, 1).repeat(1, 1, in_h * in_w).view(h.shape) #-------------------------------------------------------# # 计算调整后的先验框中心与宽高 #-------------------------------------------------------# pred_boxes_x = torch.unsqueeze(x.data + grid_x, -1) pred_boxes_y = torch.unsqueeze(y.data + grid_y, -1) pred_boxes_w = torch.unsqueeze(torch.exp(w.data) * anchor_w, -1) pred_boxes_h = torch.unsqueeze(torch.exp(h.data) * anchor_h, -1) pred_boxes = torch.cat([pred_boxes_x, pred_boxes_y, pred_boxes_w, pred_boxes_h], dim = -1) for b in range(bs): #-------------------------------------------------------# # 将预测结果转换一个形式 # pred_boxes_for_ignore num_anchors, 4 #-------------------------------------------------------# pred_boxes_for_ignore = pred_boxes[b].view(-1, 4) #-------------------------------------------------------# # 计算真实框，并把真实框转换成相对于特征层的大小 # gt_box num_true_box, 4 #-------------------------------------------------------# if len(targets[b]) &gt; 0: batch_target = torch.zeros_like(targets[b]) #-------------------------------------------------------# # 计算出正样本在特征层上的中心点 #-------------------------------------------------------# batch_target[:, [0,2]] = targets[b][:, [0,2]] * in_w batch_target[:, [1,3]] = targets[b][:, [1,3]] * in_h batch_target = batch_target[:, :4] #-------------------------------------------------------# # 计算交并比 # anch_ious num_true_box, num_anchors #-------------------------------------------------------# anch_ious = self.calculate_iou(batch_target, pred_boxes_for_ignore) #-------------------------------------------------------# # 每个先验框对应真实框的最大重合度 # anch_ious_max num_anchors #-------------------------------------------------------# anch_ious_max, _ = torch.max(anch_ious, dim = 0) anch_ious_max = anch_ious_max.view(pred_boxes[b].size()[:3]) noobj_mask[b][anch_ious_max &gt; self.ignore_threshold] = 0 return noobj_mask","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"yolov3","slug":"yolov3","permalink":"http://pistachio0812.github.io/tags/yolov3/"}],"author":"coolboy"},{"title":"扁平时代的写作·节选","slug":"扁平时代的写作","date":"2022-04-16T07:28:40.697Z","updated":"2022-10-04T13:38:07.941Z","comments":true,"path":"zh-CN/扁平时代的写作/","permalink":"http://pistachio0812.github.io/zh-CN/%E6%89%81%E5%B9%B3%E6%97%B6%E4%BB%A3%E7%9A%84%E5%86%99%E4%BD%9C/","excerpt":"","text":"​ 一个「扁平」的世界里众声喧沸。从原则上说，由编辑、审查、批准一类关卡所组成的文化权力体系几近瓦解，每一个IP地址自由发声，都可能成为强大的文化媒体。英才惨遭埋没的可能，伪学与赝品一手遮天的可能，在传统意义上都会减少。全民批评权的运用，也是一种有益的破坏性检验。不过问题的另一面，是胡说比深思容易，粗品比精品多产，优秀者至少没有数量上的优势。一旦优劣平权成了优劣俱放，文化产量中庸质与恶质的占比肯定大大攀升，低端文化产能不仅无法淘汰，还可能日益滚大和坐大。一些优秀作品即使生产出来，也可能在过量的文化淹没中，在受众们暴饮暴食式的阅读之后，在食欲不振的这些快餐者们那里，出现影响力的严重折扣。一旦肠胃已经吃坏了，再多的良药也都无济于事。 ​ 一个「扁平」的世界里多数为王。在一般的情况下，有些潮流可以修复民众良知，是真理的脱颖而出；有些潮流泯灭民众良知，是泡沫和垃圾的霸道横行。但不管是哪种情况，多数人的理解力构成潮流的边界，那么大众型和通俗化的真理尚有机会，而冷门的、偏僻的、艰险的、高难的——又常常是重要的文化探索，则可能缺氧。进一步说，市场总是嗅觉灵敏地跟踪多数，跟踪购买力的所在，以实现利润最大化。它们必然就低不就高，随众不随寡，视高深、高难、高雅为营销毒药，并有足够的本领使舆论、奖项、教育、权力等资源向低端集中，打造出泡沫霸权和垃圾霸权。一种品质趋下的文化诱导机制，在这种情况下几乎难以避免。 ​ 一个「扁平」的世界还有易破难立的特点。特别是自18世纪启蒙运动以来，敬畏感随着上帝一同消失。叛逆比服从更流行，权利比责任更动心，无论左右翼都造反成癖，在获得解构主义一番学术装备后更是见立必破打倒一切。这一过程削弱了上帝与王权，清算了教条与伪善，其功绩不可低估；但无政府式的激进狂飚若无解药，其结局必是相对性等同虚无性，民主化等同民粹化，任何共识难以成，真理永远缺位。真理也许还是有的，但在很多时候只剩下每个人那里「我」的真理，即自恋、自闭、自利的各种强辞，甚至是专职扒粪的哄客四起——这不过是社会沦入一片「原子化」散沙的文化表征。圣人、先知、导师一类从此不再，文化成了一地碎片和自由落体。一个个公权政府在这样的逐利时代也更像个总务处，无心也无力充当精神旗帜，无心也无力实施有效的社会调控。避骂自保的公关活动已够他们忙的了，讨好票源和收买民意已够他们累的了，他们哪还有建构民族与人类精神的远大抱负和坚定行动？ ​ 越来越多的迹象表明，一旦失去文化的约束和引导机制，一个扁平的世界就是没有方向的世界，是无深度和无高度的世界。即使有成打的托翁和莎翁再世，他们通常也形同刺猬而不是狮子，是暗燃而不是火炬——在生态、经济、政治等重大危机逼近之前，在民众的真理渴求大增之前，情况大体如此。 ​ 这个时代当然还有文化，有文化运动与文化冲突，也不乏轮番登台的文化偶像。不过，与传统意义上的圣人、先知、导师不同，很多现代文化偶像形式大于内容，迎合多于独行，公关造势优于埋头苦干，成功获利重于大道担当。这些人不过是营构一种虚假的方向，在无方向时代满足一种偶像消费，其中既包括对偶像的适时狂拜，也包括对偶像的适时狂毁。在这里，狂拜或狂毁只在一念，不需深思熟虑和身体力行，因此所需偶像不必经久耐用，隔数月或隔几天就更换一个，实为摊档上的寻常。正因为如此，很多偶像不得不焦灼难安，不得不到处奔走，拼命保持公众能见度成了他们的殊死搏斗，也成了他们与以往大师的明显区别之一。一个个豪华大片就这样火了，又冷了；一个个惊世的主义就这样火了，又冷了；一个个让人开心的狂生或浪女就这样火了，又冷了——到后来，很多人参与围观纯粹是为了有权开骂，争相点击只是赢来讥嘲和自秀高明的资格，于是火就是为了冷，或者说火本身就是冷。 ​ 中国互联网络信息中心2008年的统计报告显示，高达47％左右的公众已经不信任或不太信任网络。美国佩尤研究中心2004年的调查统计显示，媒体公信力一直下滑，比如对CNN信任值已跌至32％，即大多数人持怀疑态度。有意思的是，这一类文化产业不正是公众用高点击率、高收视率、高票房额等热心喂养起来的么？不都是文化市场上的成功典范么？时值二十一世纪，人类有了前所未有的文化自由选择权，但为什么从这时起人类倒变得如此犹疑不定、六神无主、手足无措、茫然无计，竟找不到自己真正信赖和需要的东西？如果人类长期处于这样一种文化消费中的自我分裂和自我对抗，那么这种所好即所疑、所乐即所耻、所爱即所憎的左右两难，是不是一种文化狂欢之下的精神死机状态？ ​ 也许需要重新启动，重新确定一个方向。 ​ 一个重建精神价值的方向。 ​ 这需要很多人的共同努力，重建一种非权力化和非利益化的文化核心、级差以及组织，即文明教化的正常体系。是的，在这里我愿意重新使用「教化」这样一个词，在人类几百年来钟情于「自由」一词以后，在有效教化与宽幅自由互为条件的奇诡历史之中。","categories":[{"name":"文学黑洞","slug":"文学黑洞","permalink":"http://pistachio0812.github.io/categories/%E6%96%87%E5%AD%A6%E9%BB%91%E6%B4%9E/"}],"tags":[{"name":"今日故事","slug":"今日故事","permalink":"http://pistachio0812.github.io/tags/%E4%BB%8A%E6%97%A5%E6%95%85%E4%BA%8B/"}],"author":"Daniel"},{"title":"学习网站集合","slug":"学习网站集合","date":"2022-04-16T07:26:49.125Z","updated":"2022-10-04T13:52:41.154Z","comments":true,"path":"zh-CN/学习网站集合/","permalink":"http://pistachio0812.github.io/zh-CN/%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99%E9%9B%86%E5%90%88/","excerpt":"","text":"万门好课万门好课是一家提供多品类原创精品课程的在线教育平台 ，课程覆盖IT与互联网类、职业成长类、经济金融类、本科学习类等领域 。 整体课程定位侧重于“用户刚需”类课程，如语言版块的出国英语考试类课程、小语种培训课程，本科学习版块的各学科基础大课，以及特色的万门通识课程包括PS、化妆等。 网易云课堂网易云课堂立足于实用性的要求，网易云课堂与多家教育、培训机构建立合作，课程数量已达4100+，课时总数超50000,涵盖实用软件、IT与互联网、外语学习、生活家居、兴趣爱好、职场技能、金融管理、考试认证、中小学、亲子教育等十余大门类。 网易公开课网易公开课首批1200集课程上线，其中有200多集配有中文字幕。用户可以在线免费观看来自于哈佛大学等世界级名校的公开课课程，可汗学院，TED等教育性组织的精彩视频，内容涵盖人文、社会、艺术、科学、金融等领域。 力求为爱学习的网友创造一个公开的免费课程平台，借此向外界公开招聘兼职字幕翻译。 爱课程网爱课程网利用现代信息技术和网络技术， 面向高校师生和社会大众。提供优质教育资源共享和个性化教学资源服务，具有资源浏览、搜索、重组、评价、课程包的导入导出、发布、互动参与和“教”“学”兼备等功能。 粉笔网粉笔网是一个互联网教育平台，业务包含：公务员考试，考研、教师资格、事业单位、英语、建造、财会等技能培训；利用技术手段实现智能批改功能，并提供免费题库，供用户查阅学习，利用网络直播，进行线上授课，同时提供实物图书、试卷以及客户服务。 魔方英语声同小语种论坛","categories":[{"name":"科研推荐","slug":"科研推荐","permalink":"http://pistachio0812.github.io/categories/%E7%A7%91%E7%A0%94%E6%8E%A8%E8%8D%90/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"author":"coolboy"},{"title":"tensorflow学习笔记","slug":"学习笔记","date":"2022-04-16T07:26:49.094Z","updated":"2022-10-04T13:51:36.177Z","comments":true,"path":"zh-CN/学习笔记/","permalink":"http://pistachio0812.github.io/zh-CN/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"tf中矩阵量存在形式常用有三种，具体如下：1.tf.Variable()表示神经网络中可变化的量（可以通过trainable=False设置成不可变),可在运行中赋值，可以通过constant或者其他方式进行初始化。2.tf.constant()可以通过numpy中的array或者list,还有给定的shape和数值进行赋值3.tf.placeholder()相当于占位符，也是有shape的量，因为训练过程中需要不断赋值和替换值，而整体计算的结构是不变的。代码：//导包 import tensorflow as tf //定义变量A = tf.Variable(tf.ones([4,4])) //变量初始化import numpy as np cst = tf.constant(np.ones([4,4]),dtype=tf.float32) 需要指定类型dtype=tf.float32,tf中不能隐式转换浮点和整型cst = tf.constant(1.0,shape=[4,4],dtype=tf.float32)也是可以的A = tf.Variable(cst) //定义placeholderX = tf.placeholder(dtype=tf.float32,shape=[4,1]) //矩阵相乘C = tf.matmul(A,X) //定义Sessionsess = tf.Session() //初始化变量init = tf.global_variables_initializer() //执行初始化sess.run(init) //运行矩阵相乘sess.run(C,feed_dict={X:[[1],[1],[1],[1]]}) //获取变量值A_val = A.value() Avalue = sess.run(A_val) //完整矩形相乘代码import tensorflow as tf import numpy as np X = tf.placeholder(dtype=tf.float32,shape=[4,1]) A = tf.Variable(tf.zeros([4,4])) C = tf.matmul(A,X) sess = tf.session() init = tf.global_variables_initializer() sess.run(init) print(sess.run(A)) //为了使计算图更加清晰，可以使用variable_scope()//定义变量名称with tf.variable_scope(\"first-nn-layer\"): W = tf.Variable(tf.zeros([784,10]),name=\"W\") b = tf.Variable(tf.zeros([10]),name=\"b\") y = tf.matmul(x,W)+b variable_summaries(W) //标识不同的变量//不同作用域下的同名变量with tf.variable_scope(\"first-nn-layer\"): W = tf.Variable(tf.zeros([784,10]),name=\"W\") b = tf.Variable(tf.zeros([10]),name=\"b\") W1 = tf.Variable(tf.zeros([784,10]),name=\"W\") print(W.name) print(W1.name) w、w1虽然name一样，但是计算中依然当成不同的变量，让同一个scope的同一变量可以通过get_variable()函数//获取变量with tf.variable_scope(\"first-nn-layer\") as scope: W = tf.get_variable(\"W\",[784, 10]) b = tf.get_variable(\"b\",[10]) scope.reuse_variables()#缺少则会报错 W1 = tf.get_variables(\"W\",shape=[784,10]) print(W.name) print(W1.name) w、w1属于同一个变量注：若此时缺少了scope.reuse_variables()则会报错，因为同时引用了同一个变量，对于不同层的变量，可以利用variable_scope进行区分，在再次引入相关变量时，需要加上reuse=True,否则依然会报错。如果变量不存在时加上reuse=True,依然会报错，因为该变量不存在 with tf.variable_scope(\"first-nn-layer\") as scope: W = tf.get_variable(\"W\",[784, 10]) b = tf.get_variable(\"b\",[10]) with tf.variable_scope(\"second-nn-layer\") as scope: W = tf.get_variable(\"W\",[784, 10]) b = tf.get_variable(\"b\",[10]) with tf.variable_scope(\"second-nn-layer\", reuse=True) as scope: W3 = tf.get_variable(\"W\",[784, 10]) b3 = tf.get_variable(\"b\",[10]) print(W.name) print(W3.name) //保存模型//定义saversaver = tf.train.Saver() //在训练过程中进行保存，保存为训练过程中的变量//变量保存for itr in range(1000): ... saver.save(sess,\"model/al\",global_step=itr) //加载计算//变量载入saver.restore(sess,\"model/v2-200\") 3.4构建计算图//前面在描述计算图，这里观察所建立的计算图//定义summarytrain_writer = tf.summary.FileWriter(\"logdir\",sess.graph) 注：sess.graph就是描绘的计算图，”logdir”是log的存储文件夹。在Shell中运行Tensorboard,在浏览器中输入localhost:6006,然后点击graph就可以看到设计的网络模型了。 //问题：描绘的计算图非常杂乱无章，变量命名的可读性很差，需要进行整理。//变量命名x = tf.placeholder(tf.float32,[None,784],name=\"input_x\") label = tf.placeholder(tf.float32,[None,10],name=\"input_label\") W = tf.Variable(tf.zeros([874,10]),name=\"W\") b = tf.Variable(tf.zeros([10]),name=\"b\") //问题：依然不够清楚，可以将输入层的x和label归为一类//定义作用域with tf.variable_scope(\"input\"): x = tf.placeholder(tf.float32,[None,784],name=\"input_x\") label = tf.placeholder(tf.float32,[None,10],name=\"input_label\") with tf.variable_scope(\"first-nn-layer\"): W = tf.Variable(tf.zeros([784,10]), name=\"W\") b = tf.Variable(tf.zeros([10]),name=\"b\") y = tf.matmul(x,W)+b with tf.variable_scope(\"loss\"): loss = tf.reduce_mean(tf.square(y-label)) //同一作用域下的同名变量是相同的，涉及到变量复用的问题，以及后续变量的获取，为了观察变量的变化，需要观察的变量加入summary函数//定义summary函数def variable_summaries(var): with tf.name_scope('summaries'): mean = tf.reduce_mean(var) tf.summary.scalar('mean',mean) with tf.name_scope('stddev'): stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean))) tf.summary.scalar('stddev',stddev) tf.summary.scalar('max',tf.reduce_max(var)) tf.summary.scalar('min',tf.reduce_min(var)) tf.summary.histogram('histogram',var) //若要观测W的相关情况，调用summary函数//调用summary函数variable_summaries(W) //再用merge_all函数收集summary信息//获取summary信息merged = tf.summary.merge_all() //summary保存summary = sess.run(merged, feed_dict={x:batch_xs,label:batch_ys}) train_writer.add_summary(summary,itr) 注：此时可以在网页中访问，观察变量随迭代变化的情况，可以通过不同的方式对变量进行观测，比如时序统计、histogram,这些统计信息对于分析训练过程是非常重要的 3.5全连接网络构建//tf官方手写识别版本的简化版本//单层全连接网络 引入库from tensorflow.examples.tutorials.mnist import input_data#产生数据，手写识别的图片和标签 import tensorflow as tf 获取数据mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True) 构建网络模型x,label分别为图形数据和标签数据x = tf.placeholder(tf.float32,[None,784]) label = tf.placeholder(tf.float32,[None,10]) 构建单层网络中的权值和偏置W = tf.Variable(tf.zeros([784,10])) b = tf.Variable(tf.zeros([10]) 本例中无非线性激活函数y = tf.matmul(x,W)+b 定义损失函数为欧氏距离，但这并不是最好的，多分类问题通常使用交叉熵loss = tf.reduce_mean(tf.square(y-label)) 若使用交叉熵损失函数soft_max = tf.nn.softmax(logit, axis=1) loss = tf.reduce_mean(-label*tf.log(soft_max)) 用梯度迭代算法train_step = tf.train.GradientDescentOptimizer(0.005).minimize(loss) 用于验证correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(label,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float.32)) 定义会话sess = tf.Session() 初始化所有变量sess.run(tf.global_variable_initializer()) 迭代过程for itr in range(3000): batch_xs,batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict={x:batch_xs,label:batch_ys}) if itr%10==0: print(\"step:%6d accuracy:\"%iter, sess.run(accuracy, feed_dict={x:mnist.test.images, label:mnist.test.labels})) 获取W取值W_value = sess.run(W.value()) //定义一个单层全连接函数def full_layer(input_tensor, out_dim, name=\"full\"): with tf.variable_scope(name): shape = input_tensor.get_shape()as_list() W = tf.get_variable('W',(shape[1],out_dim),dtype=tf.float32, initalizer=tf.truncated_normal_initializer(stddev=0.1)) b = tf.get_variable('b',[out_dim], dtype=tf.float32, initializer=tf.constant_initializer(0)) out = tf.matmul(input_tensor, W)+b return tf.nn.sigmoid(nn) 3.6CNN构建//CNN手写识别 预读取MNIST手写字库from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True) import tensorflow as tf 用正态分布随机数初始化变量，本例中仅作为权值def weight_variable(shape): initial=tf.truncated_normal(shape,stddev=0.1) #正态分布 `return tf.Variable(initial)` 用常量方式初始化偏置def bias_variable(shape): initial=tf.constant(0.1,shape=shape) #常数分布 `return tf.Variable(initial)` 定义二维卷积的过程def conv2d(x,W): return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=’SAME’) 定义池化层，简单地说就是选个最大的数，进一步降低自由参数的个数def max_pool_2x2(x): return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME') x = tf.placeholder(tf.float32,shape=[100,784]) y = tf.placeholder(tf.float32,shape=[100,10]) W_conv1 = weight_variable([5,5,1,32]) b_conv1 = bias_variable([32]) x_image = tf.reshape(x,[-1,28,28,1]) y_conv1 = tf.nn.relu(conv2d(x_iamge,W_conv1)+b_conv1) y_pool1 = max_pool_2x2(y_conv1) W_conv2 = weight_variable([5,5,32,64]) b_conv2 = weight_variable([64]) y_conv2 = tf.nn.relu(conv2d(y_pool1,W_conv2)+b_conv2) y_pool2 = max_pool_2x2(y_conv2) y_fc_flat = tf.reshape(y_pool2,[-1,7*7*64]) W_fc1 = weight_variable([7*7*64,10]) b_fc1 = bias_variable([10]) y_fc1 = tf.nn.relu(tf.matmul(y_fc_flat,W_fc1)+b_fc1) cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_fc1)) train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) sess = tf.Session() init = tf.global_variables_initializer() sess.run(init) for i in range(1000): bx,by = mnist.train.next_batch(100) sess.run(train_step,feed_dict={x:bx,y:by}) import numpy as np import matplotlib.pyplot as plt 设置输出风格，为画图美观import matplotlib as mpl mpl.style.use('seaborn-darkgrid') val = W_conv1.value() convVal = np.array(sess.run(val)) convVal = np.reshape(convVal,[5,5,32]) plt.imshow(convVal[:,:,6]) plt.show() 3.8多架构运行//GPU使用GPU可以加速深度学习作业的训练速度，如果服务器有多个GPU,那么tensorflow默认使用全部使用部分GPU: python程序启动时调用：CUDA_VISIBLE_DEVICES=0.2.3 python script.py python代码内进行调用：import os os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" //配置GPU显存某些情况下，多作业或者共享GPU的场景中，可以控制tf使用GPU显存大小gpuOptions = tf.GPUOptions(per_process_gpu_memory_fraction=0.8) sess = tf.Session(config=tf.ConfigProto(gpu_options=gpuOptions)) //GPU运行代码 将变量的定义和分配定义到GPU上进行with tf.device('/gpu:0'): W = tf.get_variable('W',(in_dim,out_dim),dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.1)) b = tf.get_variable('b'),[out_dim],dtype=tf.float32,initializer=tf.constant_initializer(0)) net = tf.matmul(input_tensor,W)+b 在CPU上计算激活函数with tf.device('/cpu:0'): net = tf.nn.sigmoid(net) //多CPU使用，多设备计算//利用标号简单的区分并运行 在CPU0上计算with tf.device('/cpu:0') ... net = tf.nn.sigmoid(net) 在CPU1上计算with tf.device('/cpu:1') ... net = tf.nn.sigmoid(net) //在集群上运行，需要在多个主机上准备多份代码，代码前面部分相同，后续有所不同//定义多主机运行参数 这里的地址形式为IP:Portcluster = tf.train.ClusterSpectf.train.ClusterSpec({ \"worker\":[ \"xx.xx.xx.xx:2222\", #/job:worker/task:0 \"xx.xx.xx.xx:2222\", #这里job的名称为自定义 \"xx.xx.xx.xx:2222\" #task编号同样需在Server中定义 ], \"ps\":[ \"xx.xx.xx.xx:2222\", \"xx.xx.xx.xx:2222\" ]}) server = tf.train.Server(cluster, job_name=\"worker\", task_index=0) //定义第二个主机参数 这里的地址形式为IP:Portcluster = tf.train.ClusterSpectf.train.ClusterSpec({ \"worker\":[ \"xx.xx.xx.xx:2222\", #/job:worker/task:0 \"xx.xx.xx.xx:2222\", #这里job的名称为自定义 \"xx.xx.xx.xx:2222\" #task编号同样需在Server中定义 ], \"ps\":[ \"xx.xx.xx.xx:2222\", \"xx.xx.xx.xx:2222\" ]}) server = tf.train.Server(cluster, job_name=\"worker\", task_index=1) //不同设备的运行代码with tf.device('/job:worker/task:0/cpu:0'): ... //将不同的任务分配到不同的计算节点上//分配计算任务with tf.device(tf.train.replica_device_setter( worker_device=\"/job:worker/task:%d\" %task_index,cluster=cluster) //函数replica_device_setter会将变量参数的定义部分自动定义到ps服务中，后续需要定义Session,用于执行这个过程//多主机运行 定义句柄，迭代多少步后停止迭代hooks = [tf.train.StopAtStepHook(last_step=1000000)] MonitoredTrainingSession函数会完成会话初始化工作保存checkpoint,恢复checkpoint,异常判断等这里需要定义master主机，定义保存、控制操作的masterwith tf.train.MonitroedTrainingSession( master=server.target, is_chief=(task_index==0), checkpoint_dir=\"dir/to/cp\", hooks=hooks) as mon_sess: ... 注：在程序运行过程中，需要认为将程序分配到各个主机上，依次运行各个主机 //队列用于数据读取和处理，队列可以是先进先出队列，也可以是随机队列，用于随机化输出//tf中队列的操作是对于训练前的过程而言的，有以下作用1.多线程数据预处理并将其推入队列2.在执行过程中，队列不断提供训练数据//简单实例说明队列使用def simple_shuffle_batch(source,capacity,batch_size=10): #定义随机序列 `queue = tf.RandomShuffleQueue( capacity=capacity, min_after_dequeue=int(0.9*capacity), shapes=source.shape, dtypes=source.dtype)` #定义enqueue过程 `enqueue = queue.enqueue(source)` #定义执行进程个数 `num_threads = 4 qr = tf.train.QueueRunner(queue,[enqueue]*num_threads)` #声明Queue runner,使得其可以被执行 `tf.train.add_queue_runner(qr)` #获取数据 `return queue.dequeue_many(batch_size)` 产生测试数据input = tf.constant(list(range(100))) input = tf.data.Dataset.from_tensor_slices(input) input = input.make_one_shot_iterator().get_next() 定义函数get_batch = simple_shuffle_batch(input,capacity=20) 定义sessionwith tf.train.MonitoredSession() as sess: while not sess.should_stop(): print(sess.run(get_batch)) 注：队列操作可以使得数据读取过程得到并行的优化，这对于提升程序的运行速度是很有利的。 //tf相关扩展4.2.1 tf Layers//全连接网络 layers定义全连接网络net = tf.layers.dense(inputs=net, units=units, activation=tf.nn.relu) 卷积网络net = tf.layers.conv2d( inputs=net, #输入 filters=n_features, #输出特征数 kernel-size=[5, 5], #卷积核心大小 padding=\"same\", #边界 activation=tf.nn.relu #激活函数 ) //前馈神经网络函数 二维最大池化net = tf.layers.max_pooling2d(...) 二维平均池化net = tf.layers.average_pooling2d(...) 二维卷积net = tf.layers.conv2d(...) dropoutnet = tf.layers.dropout(...) 展开net = tf.layers.flatten(...) BNnet = tf.layers.batch_normalization(...) 4.2.2 tf Slim 卷积函数def conv2d_layer(input_tensor, size=1, feature=128, name='conv1d'): with tf.variable_scope(name): shape = input_tensor.get_shape.as_list() kernel = tf.get_variable('kernel', (size, size, shape[-1], feature), dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1)) b = tf.get_variable('b', [feature], dtype=tf.float32, initializer=tf.constant_initializer(0)) out = tf.nn.conv2d(input_tensor, kernel, strides=[1,2,2,1],padding='SAME')+b return tf.nn.relu(out) 全连接函数def full_layer(input_tensor, out_dim, name='full'): with tf.variable_scope(name): shape = input_tensor.get_shape.as_list() W = tf.get_variable('W', (shape[1], out_dim), dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1)) b = tf.get_variabel('b', [out_dim], dtype=tf.float32, initializer=tf.constant_initializer(0)) out = tf.matmul(input_tensor, W)+b return out //slim实现卷积，tfv2取消该库 引入slim库import tensorflow.contrib.slim as slim 定义卷积层net = slim.conv2d(inputs, 16, 4, strides=2, activation_fn=tf.nn.relu, scope='conv1') 加入池化层net = tf.nn.max_pool(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') net = slim.conv2d(net, 32, 4, strides=2, activation_fn=tf.nn.relu, scope='conv2') net = tf.nn.max_pool(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') flatten层，用于将三维的图形数据展开成一维数据，用于全连接层net = slim.flatten(net) 全连接层y = slim.fully_connected(net, 10, activation_fn=line, scope='full', reuse=False) 4.2.3 tfLearn//tflearn抽象层次更高，代码可读性更好,其是一个完整的生态//基础网络架构 全连接net = tflearn.fully_connected(...) 卷积net = tflearn.conv_2d(...) LSTMnet = tflearn.lstm(...) dropoutnet = tflearn.dropout(...) //输入函数network = tflearn.input_data(shape=[None, 28, 28, 1], name='input') //优化部分 定义优化过程network = tflearn.layers.estimator.regression( network, optimizer='adam', #优化方法 learning_rate=0.01, #学习率 loss='categorical_crossentropy', #损失函数 name='target') //利用tflearn完成手写数字的识别任务import tflearn from tflearn.layers.core import input_data,dropout, fully_connected from tflearn.layers.conv import conv_2d, , max_pool_2d from tflearn.layers.normalization import local_response_normalization from tflearn.layers.estimator import regression 载入并处理数据import tflearn.datasets.mnist as mnist X, Y, testX, testY = mnist.load_data(one_hot=True) 转换为二维图形X = X.reshape([-1, 28, 28, 1]) testX = testX.reshape([-1, 28, 28, 1]) 建立神经网络network = tflearn.input_data(shape=[None, 28, 28, 1], name='input') network = conv_2d(network, 32, 3, activation='relu', regularizer='L2') network = max_pool_2d(network) network = local_response_normalization(network) network = fully_connected(network, 128, activation='tanh') network = dropout(network, 0.8) network = fully_connected(network, 256, activation='tanh') network = dropout(network, 0.8) network = fully_connected(network, 10, activation='softmax') 定义优化过程network = regression( network, optimizer='adam', learning_rate=0.01, loss='categorical_crossentropy', name='target') 训练过程model = tflearn.DNN(network, tensorboard_verbose=0) model.fit({'input':X}, {'target':Y}, n_epoch=20, validation_set=({'input':testX}, {'target':testY}), snapshot_step=100, show_metric=True, run_id='convnet_mnist') //Keras代码可读性好，并且横跨多个机器学习框架，但其扩展性较差//引入库from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D //Keras直接顺序加入模型，无需通过数据方式进行传递//基础网络层from keras.models import Sequential model = Sequential() 加入卷积层model.add(Conv2D(...)) 加入池化层model.add(MaxPooling2D(...)) 加入全连接层model.add(Dense(...)) dropoutmodel.add(Dropout(0.25)) //定义model后可直接加入多种层进行操作，同样其需要定义训练函数//定义优化过程from keras.optimizers import SGD 定义迭代算法sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd) 训练过程model.fit(x_train, y_train, batch_szie=32, epochs=10) 评估训练效果score = model.evaluate(x_test, y_test, batch_size=32) //完整代码import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D from keras.optimizers import SGD 这里utils为自己定义的库函数，用于载入数据import utils X, Y, testX, testY = utils.load_data(one_hot=True) model = Sequential() 定义神经网络过程model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3))) model.add(Conv2D(32, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(Conv2D(64, (3, 3), activatin='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) 展开为一维数据用于全连接层model.add(Flatten()) model.add(Dense(256, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) 梯度迭代算法sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd) 训练过程model.fit(x_train, y_train, batch_size=32, epochs=10) 效果评估score = model.evaluate(x_test, y_test, batch_size=32) 4.3 Tensorboard与问题监控//tensorboard最重要的作用就在于观察训练过程中的各种问题并改善，包括梯度消失、过拟合等问题//获取所有可训练的参数var_list_w = [var for var in tf.trainable_variables() if 'w' in var.name] var_list_b = [var for var in tf.trainable_variables() if 'b' in var.name] //利用定义的梯度算法来计算梯度gradient_w = optimizer.compute_gradients(loss, var_list=var_list_w) gradient_b = optimizer.compute_gradients(loss, var_list=var_list_b) //返回的梯度是一个列表，可对其进行各种列表操作//加入summary操作for idx, itr_g in enumerate(gradient_w): variable_summaries(itr_g[0], 'layer%d-w-grad'%idx) for idx, itr_g in enumerate(gradient_b): variable_summaries(itr_g[0], 'layer%d-b-grad'%idx for idx, itr_g in enumerate(var_list_w): variable_summaries(itr_g, 'layer%d-w-grad'%idx) for idx, itr_g in enumerate(var_list_b): variable_summaries(itr_g, 'layer%d-b-grad'%idx) 4.4改善深度神经网络//出现梯度消失一种最有效的方式就是进行BN操作//batchnorm层net = tf.contrib.layers.batch_norm(net) //加入BN层的神经网络 对于sigmoid激活函数来讲，BN操作效果可能不理想net = slim.fully_connected(x, 4, activation_fn=tf.nn.sigmoid, scope='full1', reuse=False) net = tf.contrib.layers.batch_norm(net) net = slim.fully_connected(net, 8, activation_fn=tf.nn.sigmoid, scope='full2', reuse=False) net = tf.contrib.layers.batch_norm(net) net = slim.fully_connected(net, 8, activation_fn=tf.nn.sigmoid, scope='full3', reuse=False) net = tf.contrib.layers.batch_norm(net) net = slim.fully_connected(net, 4, activation_fn=tf.nn.sigmoid, scope='full4', reuse=False) net = tf.contrib.layers.batch_norm(net) net = slim.fully_connected(net, 3, activation_fn=tf.nn.sigmoid, scope='full5', reuse=False) loss = tf.reduce_mean(tf.square(y-label)) 4.5性能优化建议//训练前的优化技巧1.网络结构优化Relu和BN能够有效加快神经网络训练速度卷积核心的选取可以从大的卷积核心修改为多个小的卷积核心将nxn修改为nx1+1xn，减少参数量，不同的输出内容之间可以进行concat引入跨层支路解决梯度问题（ResNet)2.初始值的选取不好的初始值对训练的影响非常大，有效的初始化方法包括xavier初始化方法和He初始化方法3.数据预处理包括去均值和方差均衡//训练过程中的优化技巧1）优化算法的选择Adam2）学习率的选取从大的步长开始进行迭代，逐步减少学习率3）Batchsize选择4）model ensembles使用不同初始值同时训练多个模型，预测过程中将多个模型输出结果做平均，有效提升结果精度5)dropout选择从0.5附近进行调整，调整步长为0.05左右 //物体检测1.传统检测方法2001年，基于Haar特征和Adaboost检测方法引起轰动2012年之前，三方面不断创新与优化：特征的设计更新、检测窗口的选择、分类器的设计更新 2.深度学习的物体检测1）基于分类的物体检测处理过程：图像被分解成多个小区域，每个小区域将运行一个分类算法以确定区域是否包含待检测物体，之后再在这个小区域的周围确认物体的边界框。代表算法：R-CNN、Fast-RCNN、Faster-RCNN2) 基于回归的物体检测将问题建模为回归问题，通过深度神经网络直接预测出边界框和所属类别的置信度。代表算法：SSD、YOLO模型 //YOLO模型官网：https://pjreddie.com/darknet/yolo///选讲tiny YOLO v1模型，由9个卷积层和3个全连接层组成，每个卷积层都由卷积层、LeakyRelu和Max Pooling操作组成，前9个卷积层可被理解为特征提取器，最后三个全连接层可被理解为预测边界框的回归器。参考论文：You Only Look Once:Unified, Real-Time Object Detection参考实例：https://github.com/xslittlegrass/CarND-Vehicle-Detection模型参数：45089374深度学习框架：Keras 1.2.2 //构建YOLO模型网络结构import keras from keras.models import Sequential from keras.layers.convolutional import Convlution2D, MaxPooling2D from keras.layers.advanced_activations import LeakyReLU from keras.layers.core import Flatten, Dense, Activation, Reshape from utils import load_weights, Box, yolo_net_out_to_car_boxes, draw_box def construct_yolo_model(): keras.backend.set_image_dim_ordering('th') model = Sequential() model.add(Convolution2D(16, 3, 3, input_shape=(3, 448, 448), border_mode='same', subsample=(1, 1))) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Convolution2D(32, 3, 3, border_mode='same')) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid')) model.add(Convolution2D(32, 3, 3, border_mode='same')) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid')) model.add(Convolution2D(64, 3, 3, border_mode='same')) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid')) model.add(Convolution2D(128, 3, 3, border_mode='same')) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid')) model.add(Convolution2D(256, 3, 3, border_mode='same')) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid')) model.add(Convolution2D(512, 3, 3, border_mode='same')) model.add(LeakyReLU(alpha=0.1)) model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid')) model.add(Convolution2D(1024, 3, 3, border_mode='same')) model.add(LeakyReLU(alpha=0.1)) model.add(Convolution2D(1024, 3, 3, border_mode='same')) model.add(LeakyReLU(alpha=0.1)) model.add(Convolution2D(1024, 3, 3, border_mode='same')) model.add(LeakyReLU(alpha=0.1 model.add(Flatten()) model.add(Dense(256)) model.add(Dense(4096)) model.add(LeakyReLU(alpha=0.1)) model.add(Dense(1470)) model.summary() return model 注：网络的输入是形状为（3,448,448)的图像，其输出是一个1470维度的向量，它包含预测边界框、物体类别信息。1470矢量被分成三个部分，分别给出了所属类别概率、置信度和边框坐标。这三个部分进一步划分为49个小区域，与每个单元的预测相对应。输出向量信息组织方式：probability:49*20=980判断类别，20个类别confidence:49*2=98是否包含物体（0,1）box coordinates:49*8=392 (x_min,y_min,x_max,y_max),(c_x,c_y,w,h) 8.4.3车辆图像数据探索1.车辆视频数据预处理//预处理及可视化图像def visualize_images(): imagePath = './test_images/test1.jpg' image = plt.imread(imagePath) #去除顶部和底部图片 `image_crop = image[300:650,500:,:]` #将图片转换为模型所需要的输入格式 `resized = cv2.resize(image_crop, (448, 448)) f1,(ax11,ax22,ax33) = plt.subplot(1, 3, figsize=(16, 6)) ax11.imshow(image) ax22.imshow(image_crop) ax33.imshow(resized) pylab.show() return resized` 8.4.5迁移学习通过迁移学习加载使用Pre-trained YOLO模型进行行车检测。具体做法是将pre-trained模型中的权重加载进之前构造的模型结构中，官网提供的权重，可以通过脚本解析成Keras能够加载的格式。//加载YOLO模型权重`def load_model_weights(model): #预训练权重网址：https://pjreddie.com/darknet/yolo/ load_weights(model, './yolo-tiny.weights')` //加载模型权重的具体逻辑def load_weights(model, yolo_weight_file): data = np.fromfile(yolo_weight_file, np.float32) data = data[4:] index = 0 for layer in model.layers: shape = [w.shape for w in layer.get_weights()] if shape !=[]: kshape, bshape = shape bia = data[index:index+np.prod(bshape)].reshape(bshape) index += np.prod(bshape) ker = data[index:index:index+np.prod(kshape)].reshape(kshape) index += np.prod(kshape) layer.set_weights([ker, bia]) //模型推断//使用模型进行在线推断，预测出车辆区域`def inference_image(model, resized): #转置 batch = np.transpose(resized, (2, 0, 1)) #将像素值变换到-1~1 batch = 2*(batch/255.) - 1 #将一张图片转为数组 batch = np.expand_dims(batch, axis=0） out = model.predict(batch) return out` //绘制检测结果//将上述的预测结果转换为边框坐标，同时基于阈值进行预测th = 0.17 boxes = yolo_net_to_out_to_car_boxes(out[0], threshold=th) //定义box边框对象，判断是否保留预测的边框结果通过c,在图像上绘制车辆位置通过对象中的坐标信息 定义box类，存储边框信息和物体检测类别等信息`class Box:def init(self): #x, y轴坐标 self.x, self.y = float(), float() #边框宽度和长度 self.w, self.h = float(), float() #置信度 self.c = float() #所属类别概率 self.prob = float()` //通过yolo_net_to_out_to_car_boxes方法，将预测出的Vector转换为Box对象信息。其核心逻辑是解析模型预测输出向量中的坐标、类别和置信度信息//置信度大于阈值边界框则进行保留class_num = 6 #yolo模型可以预测多种类别，6为车辆所属类别 p = probs[grid, :] *bx.c if p[class_num]&gt;=threshold: bx.prob = p[class_num] boxes.append(bx) //将结果绘制在图像上def visualize_image_car_detection(boxes): imagePath = './test_images/test1.jpg' image = plt.imread(imagePath) f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6)) ax1.imshow(image) ax2.imshow(draw_box(boxes, plt.imread(imagePath), [[500, 1280],[300,650]])) pylab.show() //将边框绘制在图像上def draw_box(boxes, im, crop_dim): imgcv = im [xmin, xmax] = crop_dim[0] [ymin, ymax] = crop_dim[1] for b in boxes: h, w, _ = imgcv.shape left = int((b.x-b.w/2.)*w) right = int((b.x+b.w/2.)*w) top = int((b.y-b.h/2.)*h) bot = int((b.y+b.h/2.)*h) left = int(left*(xmax-xmin)/w+xmin) right = int(right*(xmax-xmin)/w+xmin) top = int(top*(ymax-ymin)/h+ymin) bot = int(bot*(ymax-ymin)/h+ymin) if left&lt;0 : left=0 if right&gt;w-1 : right=w-1 if top&lt;0 : top=0 if bot&gt;h-1 : bot=h-1 thick = int((h+w)//150) cv2.rectangle(imgcv, (left, top), (right, bot), (255,0,0), thick) return imgcv 8.5.1英伟达End to End模型End to End的好处：通过缩减人工预处理和后续处理，尽可能使模型从原始输入到输出，使得其根据网络模型能够有足够多的空间进行自动调节，从而减少基于规则的复杂变化。缺点：可解释性较差，准确度和精度不容易受控制。//构建英伟达模型import tensorflow as tf import keras from keras.models import Sequential from keras.layers import Dense, Activation, Flatten, Lambda from keras.layers import Conv2D, Dropout from keras import losses def nvida_model(): model = Sequential() model.add(Lambda(lambda x: x/127.5-1., input_shape=(img_height, img_width, img_channels))) model.add(Conv2D(24, kernel_size=(5, 5), strides=(2, 2), padding='valid', kernel_initializer='he_normal', activation='elu')) model.add(Conv2D(36, kernel_size=(5, 5), strides=(2, 2), padding='valid', kernel_initializer='he_normal', activation='elu')) model.add(Conv2D(48, kernel_size=(5, 5), strides=(2, 2), padding='valid', kernel_initializer='he_normal', activation='elu')) model.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='valid', kernel_initializer='he_normal', activation='elu')) model.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='valid', kernel_initializer='he_normal', activation='elu model.add(Flatten()) model.add(Dense(1164, kernel_initializer='he_normal', activation='elu')) model.add(Dense(100, kernel_initializer='he_normal', activation='elu')) model.add(Dense(50, kernel_initializer='he_normal', activation='elu')) model.add(Dense(10, kernel_initializer='he_normal', activation='elu')) model.add(Dense(1, kernel_initializer='he_normal')) model.compile(loss='mse', optimizer='Adadelta') return model //8.5.3数据分析1）转向控制数据分布 绘制转向分布def steering_distribution(): wheel_sig = pd.read_csv(params.data_dir+'/epoch01_steering.csv') wheel_sig.head() wheel_sig.wheel.hist(bins=50) 2)数据变化幅度 绘制转向变化幅度def angel_visualize(): wheel_sig = pd.read_csv(params.data_dir+'/epoch01_steering.csv') wheel_sig.plot(x='frame', y='wheel') plt.show() 8.5.4读入视频，并处理图像//使用OpenCV从视频中提取图像，以及与其对应的转向角度并返回 提取图像并处理`imgs = []wheels = []epochs = [10]for epoch in epochs: vid_path = utils.join_dir(params.data_dir, ‘epoch{:0&gt;2}_front.mp4’.format(epoch)) assert os.path.isfile(vid_path) frame_count = frame_count_func(vid_path) cap = cv2.VideoCapture(vid_path) for frame_id in range(frame_count): while True: #通过OpenCV中的VideoCapture进行视频中图像的提取 ret, img = cap.read() if not ret: break #用户可以自定义对图像的处理、扩展和增强操作 img = a_image_convert.img_preprocess(img, color_mode, flip=False) imgs.append(img) csv.path = os.path.join(data_dir, 'epoch{:0&gt;2}_steering.csv'.format(epoch)) rows = pd.read_csv(csv.path) yy = rows['wheel'].values wheels.extend(yy) cap.release() imgs = np.array(imgs) wheels = np.array(wheels) wheels = np.reshape(wheels, (len(wheels), 1) return imgs, wheels` 8.5.5深度学习模型构建与训练//训练模型`def training(model, X_train_RGB, y_train_RGB): RGB_model = model time_start = time.time() #fit the model RGB_history = RGB_model.fit(X_train_RGB, y_train_RGB, epochs=30, batch_size=batch_size) return RGB_model, RGB_history` //可视化结果 将训练过程中的loss误差进行可视化def visualize_label(RGB_history): print(RGB_history.history['loss'] plt.figure(figsize=(9, 6)) plt.plot(RGB_history.history['loss']) plt.title('model loss') plt.ylabel('Loss', fontsize=12) plt.xlabel('Epoch', fontsize=12) plt.legend(['train RGB'], loc='upper right') plt.grid() plt.show() //可视化//数据的绘图过程就是将前面所得到的一系列数据，通过静态、动态的二维、三维图形进行展示1.Matplotlib//绘制y=sinx图像import numpy as np import matplotlib.pyplot as plt x = np.linspace(0, 4*np.pi, 1000) y = np.sin(x) plt.plot(x,y) //利用API,绘制更加审美要求的图像`import numpy as npimport matplotlib.pyplot as pltimport matplotlib as mpl 设置图片风格mpl.style.use(‘seaborn-darkgrid’) 定义曲线x = np.linspace(0, 4*np.pi, 100)y1 = np.sin(x)y2 = np.sin(x+1)y3 = np.sin(x+2) 绘图plt.plot(x, y1, color=’#009900’, lw=6, alpha=0.6)plt.plot(x, y2, color=’#990000’, lw=6, alpha=0.6)plt.plot(x, y3, color=’#000099’, lw=6, alpha=0.6) 展示plt.show()` 9.4ECharts//ECharts提供了常规的折线图、柱状图、散点图、饼图、K线图等等，功能强大。//ECharts图形绘制略 //文本向量化//文本向量化函数 文本TfIdf向量化from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidVectorizer() vectors = vectorizer.fit_transform(datas) //文本向量化的数据进行降维//LDA降维from sklearn.decomposition import LatentDirichletAllocation lda = LatenDirichletAllocation(n_components=n_topic, max_iter=5, learning_method = 'online', learning_offset = 50., radom_state = 0) 用LDA方法降维数据dr_vectors = lad.fit_transform(vectors) 9.6三维可视化//ECharts地图柱状图myChart.setOption({ visualMap: { show: flase, calculable: true, realtime: false, inRange: { color: ['#313695', '#4575b4', '#74add1', '#abd9e9', '#e0f3f8', '#ffffbf', '#fee090', '#fdae61', '#f46d43', '#d73027', '#d73027', 'a50026'] }, outOfRange: { colorAlpha: 0 }, max: linedata[1] }, ... series: [{ type: 'bar3D', shading: 'realistic', coordinateSystem: 'mapbox', barSize: 0.2, silent: true, data: linedata[0] }] }); //利用Matplotlib完成对三维数据的可视化任务from mpl_toolkits.mplot3d import axes3d import matplotlib.pyplot as plt from matplotlib import cm import matplotlib.style as style style.use('seaborn-darkgrid') 定义三维画布fig = plt.figure() ax = fig.gca(projection='3d') 获取数据X, Y, Z = axes3d.get_test_data(0.05) 绘制surfaceax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.3) 绘制等值线cst = ax.contourf(X, Y, Z, zdir='z', offset=-100, cmap=cm.coolwarm) cst = ax.contourf(X, Y, Z, zdir='x', offset=-40, cmap=cm.coolwarm) cst = ax.contourf(X, Y, Z, zdir='y', offset=40, cmap=cm.coolwarm) plt.show() 9.7动态可视化//Matplotlib中用于数据动态演示的方法为animation,其可以通过函数进行简单的调用，以进行动态图形的演示工作//动画展示import matplotlib.animation as animation animation.FuncAniamtion( ... ) //动态可视化的展示方式是在普通的图表之上通过不断地修改数据并进行展示，这种修改可以通过setOption而得到的，在实现上可以通过函数递归的方式实现动态数据的可视化工作function update(){ myChart.setOption(...); setTimeout(update, UPDATE_DURATION); } update(); //优化实践10.1通用深度神经网络训练优化建议 1）通用的较为优化的训练过程1.将问题转换为相似的经典问题场景，参照paper中的配置和调优技巧进行最初的实验与优化2.优化算法：选用随机梯度下降（SGD)算法，虽然批量梯度下降（BGD)相比SGD有一些优势，但是在处理大规模数据时，SGD及其优化变种更加简单和快速。3.随机Shuffle样本：应尽量避免连续处理的样本属于同一类别的情况。尽量选择当前样本能让模型产生较大的误差，而不是较小的误差4.规范化数据：输入的每个变量均值最好趋近于0.变换输入变量，使其协方差相近，变量间尽量不要相关5.激活函数的选取：相比Sigmoid函数，tanh和Relu有更好的收敛速度。6.权重初始化：可以随机通过一种分布，均值为0.7.选择学习率：每个权重都可以选取属于自己的学习率。处于低层的权重学习率最好大于高层的权重学习率。学习率最好正比于每个单元的输入数量。 2）CNN训练过程中通常关注的优化点和参数一般比较关注：Learning Rate,Weight Decay,Momentum,Batchsize,Init Weights,数据增强eg:在Resnet中，使用SGD优化算法优化方法训练，mini-batch的大小设置为256，学习率初始化为0.1.随着训练进行，当Loss不再下降，会每次自适应以10倍进行缩减学习率。模型训练用了60x10^4轮迭代。Weight Decay设置为0.0001，同时设置momentum为0.9 3)RNN训练过程中通常关注的优化点和参数一般比较关注：SGD,正则化，规范化梯度，Pad Sentence,Init Weight, Batch Size, Embedding输入，输出控制，Vacabulary Size, Sampled Softmaxeg:Google发布的TTS模型TACOTRON为例 10.1.1 过拟合和欠拟合欠拟合：若训练集和测试集的误差有收敛但很高时，则为高偏差过拟合：若训练集和测试集的误差较大时，则为高方差 解决过拟合的方法：正则化，数据增强，Early Stop, Dropout, Batch Normalization 解决欠拟合的方法：1.使用更加复杂的深度学习网络架构2.添加其他特征项，有时候模型出现欠拟合的情况是因为特征项不够导致的，可以添加其他特征项来很好的解决这个问题3.减少正则化参数和组件，正则化的目的是用来防止过拟合。 10.1.2数据增强//数据增强的根本原因在于机器在学习的过程中会在模型中遇到大量的参数，同时为了防止过拟合1）对于图像数据，可采取：1.图像平移：使得网络学习到平移不变的特性2.图像旋转：使得网络学习到旋转不变的特性3.图像亮度变化4.裁剪5.缩放6.图像模糊:用不同的卷积模板产生模糊图像2）语音识别中对输入数据添加随机噪声等方式3）NLP中最常用的方式就是进行近义词替换等方式4）噪声注入，可以对输入添加噪声，也可以对隐藏层或者输出层添加噪声 10.1.3梯度消失//实验数据显示了深度神经网络在训练过程中，随着epoch的增加各隐藏层的学习率变化。前面隐藏层的学习速度要低于后面的隐藏层//梯度消失的原因：根据链式法则，如果每一层神经元对上一层输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层的传播后，误差对输入层的偏导也会趋近于0解决梯度消失的策略：1.BN2.RNN中使用LSTM:适用于RNN,门控制和长时记忆可缓解和解决梯度消失问题3.激活函数Relu:新的激活函数解析性质更好，其在一定程度上克服了sigmoid函数和tanh函数的梯度消失问题。4.在RNN反向传播过程中减少时间步长度。 10.1.4初始化权重//在参数解空间内，好的权重初始化方式，意味着离全局最小值更近。1.高斯初始化，为权重初始化较小的值，权重按照高斯分布随机进行初始化，固定均值和方差2.Xaiver更新方法，使用tanh为激活函数，效果较好。进行梯度更新时，收敛速度较快，然而没有考虑Relu3.MSRA方法，适用于从头训练深层深度神经网络的网络结构。权重以高斯分布随机进行初始化，方差需要考虑空间过滤器的大小和过滤器数量的影响。 10.1.5优化算法近些年最常用的是采用Adam优化算法，也可以采用自适应学习率的方法实现快速收敛。 10.1.6超参数选择一些实践经验：1.在验证集上进行调参2.优先调Learning Rate3.通过初期设计卷积层尽量深、卷积核尽量多的模型，强行让模型拟合训练集，这时会出现过拟合，之后通过Dropout、正则化和Data Augument等等方式去改善模型结果4.调整模型的层数和卷积核数量 //通过Scikit-learn的网格搜索库进行参数调优实例1.常见搜索参数学习率、Dropout、Epochs和神经元数量2.数据集下载数据集为Pima Indians Onset of Diabetes分类数据集下载地址：https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/3.搜索最优batchsize和epochs//以20的步长，从10到100逐步评估不同的微型批尺寸，epochs分别设置为10、50、100import numpy from sklearn.grida_search import GridSearchCV from keras.models import Sequential from keras.layers import Dense from keras.wrappers.scikit_learn import KerasClassifier Function to create model, required for KerasClassifier`def create_model(): #create model model = Sequential() model.add(Dense(12, input_dim=8, activation='relu')) model.add(Dense(1, activation='sigmoid'))` #compile model model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) return model `#fix random seed for reproducibilityseed = 7numpy.random.seed(seed) load datasetdataset = numpy.loadtxt(“pima-indians-diabetes.csv”, delimiter=’,’) split into input (x) and output (Y) variablesX = [:, 0:8]Y = [:, 8] create modelmodel = KerasClassifier(build_fn=create_model, verbose=0) define the grid search parametersbatch_size = [10, 20, 40, 60, 80, 100]epochs = [10, 50, 100]param_grid = dict(batch_size=batch_size, nb_epoch=epochs)grid = GridSearchCV(estimator=model, param_grid=parm_grid, n_jobs=-1)grid_result = grid_fit(X,Y) summarize resultsprint(“Best: %f using %s” % (gridresult.best_score, gridresult.best_params))for params, mean_score, scores in grid_result.grid_scores: print(“%f (%f) with: %r” % (scores.mean(), scores.std(), params))` 10.2深度学习系统性能优化建议10.2.1输入及预处理流水线优化输入流水线：从磁盘读取图像，将JPEG预处理为张量，进行数据预处理裁剪、翻转等，然后进行批处理操作1.在CPU端进行预处理//在CPU端上放置输入预处理操作可以显著提高性能，GPU专注训练//控制代码在CPU端执行`with tf.device(“/cpu:0”): # function to get and process data. ​ distored_inputs = load_and_preprocess_images()` 2.使用大文件读取大量的小文件会显著影响I/O性能1）转换为TFRecord格式2）小数据集加载到内存 10.2.2数据格式NHWC的方存局部性更好（每三个输入像素即可得到一个输出像素），NCHW则必须等所有通道输入都准备好后才能得到最终的输出结果，需要占用较大的临时空间。tf默认NHWC格式，Nvidia cuDNN默认NCHW格式注：设计网络时充分考虑这两种格式，最好能够灵活切换，在GPU上训练时使用NCHW格式，在CPU上做预测时使用NHWC格式 10.2.3编译优化//通过bazel命令对特定平台对tf进行编译bazel build -c opt --copt=-march=\"brodewell\" --config=cuda //tensorflow/tools/pip_package:build_pip_package 10.2.4GPU性能瓶颈诊断//参考如下分析步骤对作业进行优化1）对代码进行性能分析2）找到运行慢的阶段3）分析慢的原因4）修改成更快的实现5）再次对代码进行性能分析 //处理器有两个关键的性能瓶颈：浮点计算量和内存吞吐量。//可通过以下工具进行深度学习作业的性能分析1.Tensorflow性能分析工具Timeline(获取执行图中每个节点的执行时间）1）创建metadata运行时对象2）获取运行时信息创建Timeline对象3）将Timeline对象写入json文件4）Chrome加载trace的json文件 //tensorflow使用Timeline进行性能分析import tensorflow as tf from tensorflow.python.client import timeline x = tf.random_normal([1000, 1000]) y = tf.random_normal([1000, 1000]) res = tf.matmul(x, y) #run the graph with full trace option with tf.Session() as sess: run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) run_metadata = tf.RunMetadata() sess.run(res, options=run_options, run_metadata=run_metadata) #create Timeline variable，then write it into json file t1 = timeline.Timeline(run_metadata.step_stats) ctf = t1.generate_chrome_trace_format() with open('timeline.json', 'w') as f: f.write(ctf) 可以打开谷歌chrome浏览器，转到chrome://tracing页并加载timeline.json文件，接下来，可以进行程序的profiling 2.常用的GPU分析工具1）nvprof是英伟达性能分析工具2）nvvp则是带GUI的英伟达可视化性能分析工具 10.2.5CPU瓶颈优化1）多线程方式优化以下两个针对tensorflow的配置可以通过适配线程池进行CPU的性能优化intra_op_parallelism_threads：对tf操作符内部的任务进行并行化inter_op_parallelism_threads: 控制多个运算符之间的并行化运算//多线程优化config = tf.ConfigProto() config.intra_op_parallelism_threads = 22 config.inter_op_parallelism_threads = 22 tf.session(config=config) 2)使用SIMD高级指令集参考tf官方文档的”Performance Guide”章节 10.2.6模型压缩模型小型化：从模型权重的角度进行压缩和从网络架构的角度进行压缩网络架构角度：提出新的网络结构或卷积方法进行压缩优化，如SqueezeNet, MobileNets等模型权重角度：一般是在已经训练好的模型上进行裁剪，然后fine-tuning到原有模型的准确率，一般的优化方式包括剪枝、权值共享、神经网络二值化等。 10.3工程实践建议10.3.1Model格式转换框架间的模型转换参考链接：1.https://github.com/ysh329/deep-learning-model-convertor2.https://github.com/Microsoft/MMdnn 10.3.2迁移学习（Transfer Learning)其思想是将训练好的模型参数迁移到新的模型来帮助新模型的训练和预测。 //通过MNIST数据集0~4的数字训练一个模型，然后将模型迁移到5~9数据集上进行迁移学习1）在MNIST数据集上训练一个简单的卷积神经网络，只预测0~4的数字2）将训练好的预测0~4数据集的模型，应用到5~9数据集上。对模型冻结卷积层参数，Fine-Tuning全连接层。//keras迁移学习实例from __future__ import print_function import datetime import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Conv2D, MaxPooling2D from keras import backend as K now = datetime.datetime.now batch_size = 128 num_classes = 5 epochs = 5 `#input images dimensionsimg_rows, img_cols = 28, 28 number of convolutional filters to usefilters = 32 size of pooling area for max poolingpool_size = 2 convolution kernel sizekernel_size = 3` if K.image_data_format()=='channels_first': input_shape = (1, img_rows, img_cols) else: input_shape = (img_rows, img_cols, 1) def train_model(model, train, test, num_classes): x_train = train[0].reshape((train[0].shape[0],)+input_shape) x_test = test[0].reshape((test[0].shape[0],)+input_shape) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') #convert class vectors to binary class matrics y_train = keras.utils.to_categorical(train[1], num_classes) y_test = keras.utils.to_categorical(test[1], num_classes) model.compile( loss = 'categorical_crossentropy', optimizer = 'adadelta', metrics = ['accuracy'] ) t = now() model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, verbose = 1, validation_data = (x_test, y_test)) print('Training time: %s' %(now() -t)) score = model.evaluate(x_test, y_test, verbose=0) print('Test score:', score[0]) `#the data,shuffled and spilt between train and test sets(x_train, y_train), (x_test, y_test) = mnist.load-data() create two datasets one with digits below 5 and one with 5 and abovex_train_lt5 = x_train[y_train&lt;5]y_train_lt5 = x_train[y_train&lt;5]x_test_lt5 = x_test[y_test&lt;5]y_test_lt5 = y_test[y_test&lt;5]` x_train_get5 = x_train[y_train&gt;=5] y_train_get5 = y_train[y_train&gt;=5]-5 x_test_get5 = x_test[y_test&gt;=5] y_test_get5 = y_test[y_test&gt;=5]-5 #define two groups of layers:feature(convolutions) and classification(dense) feature_layers = [ Conv2D(filters, kernel_size, padding='valid', input_shape=input_shape), Activation('relu'), Conv2D(filters, kernel_size), Activation('relu'), MaxPooling2D(pool_size=pool_size), Dropout(0.5), Flatten()] classification_layers=[ Dense(128), Activation('relu'), Dropout(0.5), Dense(num_classes), Activation('softmax')] #create complete model model = Sequential(feature_layers+classification_layers) #train model for 5-digit classification(0~4) train_model(model, (x_train_lt5, y_train_lt5), (x_test_lt5, y_test_lt5), num_classes) #freeze feature layers and rebuild model for l in feature_layers: l.trainable = False #transfer: train dense layers for new classification task(5~9) train_model(model, (x_train_gte5, y_train_get5), (x_test_get5, y_test_get5), num_classes) ​​​","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://pistachio0812.github.io/categories/tensorflow/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"author":"pistachio"},{"title":"CV必备文献","slug":"文献阅读合集","date":"2022-04-16T07:26:49.087Z","updated":"2022-10-04T13:48:43.099Z","comments":true,"path":"zh-CN/文献阅读合集/","permalink":"http://pistachio0812.github.io/zh-CN/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E5%90%88%E9%9B%86/","excerpt":"","text":"这里是我平时会看的一些文献，有些没得时间看，但这些确实很值得一看，放在这里，做个提醒，如果后续我记起来了，后续还会有补充，大家可以留言推荐，仅限计算机视觉方向 1.RFBNet 论文标题：Receptive Field Block Net for Accurate and Fast Object Detection 论文地址：Receptive Field Block Net 源码地址：https://github.com/ruinmessi/RFBNet 2.Fast R-CNN 3.Faster R-CNN 4.ResNet 5.Inception 6.Mask R-CNN 7.YOLOv1 8.SSD 9.DSSD 10.ASDD 11.FSSD 12.FASSD 论文标题：FASSD: A Feature Fusion and Spatial Attention-Based Single Shot Detector for Small Object Detection 论文地址：https://www.mdpi.com/2079-9292/9/9/1536 源码地址： 13.AlexNet 14.SIFT 15.HOG 16.FPN 论文标题：Feature Pyramid Networks for Object Detection 论文地址：Feature Pyramid Networks for Object Detection (thecvf.com) 源码地址：FPN: Feature Pyramid Networks for Object Detection 17.RefineDet 18.M2Det 19.RSSD 论文标题：Enhancement of SSD by concatenating feature maps for object detection 论文地址： 源码地址： 20.MDSSD 论文标题：Mdssd: Multi-scale deconvolutional single shot detector for small objects 论文地址： 论文源码：","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"文献推荐","slug":"文献推荐","permalink":"http://pistachio0812.github.io/tags/%E6%96%87%E7%8C%AE%E6%8E%A8%E8%8D%90/"}],"author":"pistachio"},{"title":"数据处理","slug":"数据处理","date":"2022-04-16T07:26:49.041Z","updated":"2022-10-04T13:44:25.122Z","comments":true,"path":"zh-CN/数据处理/","permalink":"http://pistachio0812.github.io/zh-CN/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/","excerpt":"","text":"单进程​ 在单进程模式下，DataLoader 初始化的进程和取数据的进程是一样的 。因此，数据加载可能会阻止计算。但是，当用于在进程之间共享数据的资源（例如共享内存，文件描述符）有限时，或者当整个数据集很小并且可以完全加载到内存中时，此模式可能是我们首选。此外，单进程加载通常可以显示更多可读的错误跟踪，这对于我们调试代码很有用。 多进程·多进程处理 ​ 为了避免在加载数据时阻塞计算，PyTorch 提供了一个简单的开关，只需将参数设置 num_workers 为正整数即可执行多进程数据加载，而设置为 0 时执行单线程数据加载。 ​ 在设置多进程模式时，每次 DataLoader 创建 iterator 时（例如，当调用 enumerate(dataloader) 时），都会创建 num_workers 个工作进程。此时dataset, collate_fn, worker_init_fn 都会被传到每个worker中，而每个 worker 都用独立的进程。 ​ 对于 map-style 数据，主线程会用 Sampler 产生 indices，并将它们送到 worker 里。因此，shuffle 是在主线程做的。 ​ 而对于 iterable-style 数据，因为每个 worker 都有相同的 data 复制样本，并在各个进程里进行不同的操作，以防止每个进程输出的数据是重复的，所以一般会使用 torch.utils.data.get_worker_info() 来进行辅助处理。这里，torch.utils.data.get_worker_info() 会返回 worker 进程的一些信息(如id, dataset, num_workers, seed)，如果在主线程的话返回 None。 ​ 注意，通常不建议在多进程加载中返回 CUDA 张量，因为在使用 CUDA 和在多处理中共享 CUDA 张量时存在许多微妙之处（文档中提出：只要接收过程保留张量的副本，就需要发送过程来保留原始张量）。建议采用 pin_memory=True ，以将数据快速传输到支持 CUDA 的 GPU。简而言之，不建议在使用多线程的情况下返回 CUDA 的 Tensor。 锁页内存​ 首先我们先了解一下锁页内存的概念。 ​ 主机中的内存，有两种存在方式，一是锁页，二是不锁页。锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。主机到 GPU 副本源自固定（页面锁定）内存时，速度要快得多。CPU 张量和存储暴露了一种 pin_memory() 方法，该方法返回对象的副本，并将数据放在固定的区域中。 ​ 而显卡中的显存全部是锁页内存！当计算机的内存充足的时候，可以设置 pin_memory=True。设置 pin_memory=True，则意味着生成的 Tensor 数据最开始是属于内存中的锁页内存，这样将内存的 Tensor 转义到 GPU 的显存就会更快一些。同时，由于 pin_memory 的作用是将张量返回之前将其复制到 CUDA 固定的内存中，所以只有在 CUDA 环境支持下才有用。 ​ PyTorch 原生的 pin_memory 方法如下，其支持大部分 python 数据类型的处理： 123456789101112131415def pin_memory(data): if isinstance(data, torch.Tensor): return data.pin_memory() elif isinstance(data, string_classes): return data elif isinstance(data, container_abcs.Mapping): return {k: pin_memory(sample) for k, sample in data.items()} elif isinstance(data, tuple) and hasattr(data, '_fields'): # namedtuple return type(data)(*(pin_memory(sample) for sample in data)) elif isinstance(data, container_abcs.Sequence): return [pin_memory(sample) for sample in data] elif hasattr(data, \"pin_memory\"): return data.pin_memory() else: return data ​ 默认情况下，如果固定逻辑对于一个属于自定义类型（custom type）的 batch（如果有一个 collate_fn 返回自定义批处理类型的批处理，则会发生），或者如果该批处理的每个元素都是 custom type，则该固定逻辑将无法识别它们，它会返回该批处理（或那些元素）而无需固定内存。而要为自定义批处理或数据类型启用内存固定，我们需使用 pin_memory() 在自定义类型上自定义一个方法。如下： 123456789101112131415161718192021222324252627class SimpleCustomBatch: # 自定义一个类，该类不能被PyTorch原生的pin_memory方法所支持 def __init__(self, data): transposed_data = list(zip(*data)) self.inp = torch.stack(transposed_data[0], 0) self.tgt = torch.stack(transposed_data[1], 0) # custom memory pinning method on custom type def pin_memory(self): self.inp = self.inp.pin_memory() self.tgt = self.tgt.pin_memory() return selfdef collate_wrapper(batch): return SimpleCustomBatch(batch)inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)dataset = TensorDataset(inps, tgts)loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper, pin_memory=True)for batch_ndx, sample in enumerate(loader): print(sample.inp.is_pinned()) # True print(sample.tgt.is_pinned()) # True 预取DataLoader 通过指定 prefetch_factor （默认为 2）来进行数据的预取。 12345678910class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter): def __init__(self, loader): ... self._reset(loader, first_iter=True) def _reset(self, loader, first_iter=False): ... # prime the prefetch loop for _ in range(self._prefetch_factor * self._num_workers): self._try_put_index() 通过源码可以看到，prefetch 功能仅适用于多进程加载中（下面也会有多进程 dataloader 的部分代码分析）。 代码详解那么现在让我们来看看具体的代码调用流程： 12for data, label in train_loader: ...... for 循环会调用 dataloader 的 iter(self) 方法，以此获得迭代器来遍历 dataset。 123456789101112class DataLoader(Generic[T_co]): ... def __iter__(self) -&gt; '_BaseDataLoaderIter': if self.persistent_workers and self.num_workers &gt; 0: if self._iterator is None: self._iterator = self._get_iterator() else: self._iterator._reset(self) return self._iterator else: return self._get_iterator() 在 iter(self) 方法中，dataloader 调用了 self._get_iterator() 方法，根据 num_workers 获得迭代器，并指示是进行单进程还是多进程处理。 123456789class DataLoader(Generic[T_co]): ... def _get_iterator(self) -&gt; '_BaseDataLoaderIter': if self.num_workers == 0: return _SingleProcessDataLoaderIter(self) else: self.check_worker_number_rationality() return _MultiProcessingDataLoaderIter(self) 为了描述更加清晰，我们只考虑单进程的代码。下面是 class _SingleProcessDataLoaderIter(_BaseDataLoaderIter) ，以及其父类 class _BaseDataLoaderIter(object): 的重点代码片段： 12345678910111213141516171819202122232425262728293031323334353637383940class _BaseDataLoaderIter(object): def __init__(self, loader: DataLoader) -&gt; None: # 初始化赋值一些 DataLoader 参数， # 以及用户输入合法性进行校验 self._dataset = loader.dataset self._dataset_kind = loader._dataset_kind self._index_sampler = loader._index_sampler ... def __iter__(self) -&gt; '_BaseDataLoaderIter': return self def _reset(self, loader, first_iter=False): self._sampler_iter = iter(self._index_sampler) self._num_yielded = 0 self._IterableDataset_len_called = loader._IterableDataset_len_called def _next_index(self): return next(self._sampler_iter) # may raise StopIteration def _next_data(self): raise NotImplementedError def __next__(self) -&gt; Any: with torch.autograd.profiler.record_function(self._profile_name): if self._sampler_iter is None: self._reset() data = self._next_data() # 重点代码行，通过此获取数据 self._num_yielded += 1 ... return data next = __next__ # Python 2 compatibility def __len__(self) -&gt; int: return len(self._index_sampler) # len(_BaseDataLoaderIter) == len(self._index_sampler) def __getstate__(self): raise NotImplementedError(\"{} cannot be pickled\", self.__class__.__name__) BaseDataLoaderIter 是所有 DataLoaderIter 的父类。dataloader获得了迭代器之后，for 循环需要调用 next() 来获得下一个对象，从而实现遍历。通过 **_next()** 方法调用 _next_data() 获取数据。 123456789101112131415class _SingleProcessDataLoaderIter(_BaseDataLoaderIter): def __init__(self, loader): super(_SingleProcessDataLoaderIter, self).__init__(loader) assert self._timeout == 0 assert self._num_workers == 0 self._dataset_fetcher = _DatasetKind.create_fetcher( self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last) def _next_data(self): index = self._next_index() # may raise StopIteration data = self._dataset_fetcher.fetch(index) # may raise StopIteration if self._pin_memory: data = _utils.pin_memory.pin_memory(data) return data 从 _SingleProcessDataLoaderIter 的初始化参数可以看到，其在父类 _BaseDataLoaderIter 的基础上定义了 _dataset_fetcher，并传入 _dataset，_auto_collation，_collate_fn 等参数，用于定义获取数据的方式。其具体实现会在稍后解释。 在 _next_data() 被调用后，其需要 _next_index() 获取 index，并通过获得的 index 传入 _dataset_fetcher 中获取对应样本。 12345678910111213141516171819202122class DataLoader(Generic[T_co]): ... @property def _auto_collation(self): return self.batch_sampler is not None @property def _index_sampler(self): if self._auto_collation: return self.batch_sampler else: return self.samplerclass _BaseDataLoaderIter(object): ... def _reset(self, loader, first_iter=False): self._sampler_iter = iter(self._index_sampler) ... def _next_index(self): # sampler_iter 来自于 index_sampler return next(self._sampler_iter) # may raise StopIteration 从这里看出，dataloader 提供了 sampler（可以是batch_sampler 或者是其他 sampler 子类），然后 _SingleProcessDataLoaderIter 迭代 sampler 获得索引。 下面我们来看看 fetcher，fetcher 需要 index 来获取元素，并同时支持 Map-style dataset（对应 _MapDatasetFetcher）和 Iterable-style dataset（对应 _IterableDatasetFetcher），使其在 Dataloader 内能使用相同的接口 fetch，代码更加简洁。 · 对于 Map-style：直接输入索引 index，作为 map 的 key，获得对应的样本（即 value）。 123456789101112class _MapDatasetFetcher(_BaseDatasetFetcher): def __init__(self, dataset, auto_collation, collate_fn, drop_last): super(_MapDatasetFetcher, self).__init__(dataset, auto_collation, collate_fn, drop_last) def fetch(self, possibly_batched_index): if self.auto_collation: # 有batch_sampler，_auto_collation就为True， # 就优先使用batch_sampler，对应在fetcher中传入的就是一个batch的索引 data = [self.dataset[idx] for idx in possibly_batched_index] else: data = self.dataset[possibly_batched_index] return self.collate_fn(data) · 对于 Iterable-style: init 方法内设置了 dataset 初始的迭代器，fetch 方法内获取元素，此时 index 其实已经没有多大作用了。 12345678910111213141516171819202122class _IterableDatasetFetcher(_BaseDatasetFetcher): def __init__(self, dataset, auto_collation, collate_fn, drop_last): super(_IterableDatasetFetcher, self).__init__(dataset, auto_collation, collate_fn, drop_last) self.dataset_iter = iter(dataset) def fetch(self, possibly_batched_index): if self.auto_collation: # 对于batch_sampler（即auto_collation==True） # 直接使用往后遍历并提取len(possibly_batched_index)个样本（即1个batch的样本） data = [] for _ in possibly_batched_index: try: data.append(next(self.dataset_iter)) except StopIteration: break if len(data) == 0 or (self.drop_last and len(data) &lt; len(possibly_batched_index)): raise StopIteration else: # 对于sampler，直接往后遍历并提取1个样本 data = next(self.dataset_iter) return self.collate_fn(data) 最后，我们通过索引传入 fetcher，fetch 得到想要的样本。因此，整个过程调用关系总结如下： 1loader.iter --&gt; self._get_iterator() --&gt; class _SingleProcessDataLoaderIter --&gt; class _BaseDataLoaderIter --&gt; __next__() --&gt; self._next_data() --&gt; self._next_index() --&gt;next(self._sampler_iter) 即 next(iter(self._index_sampler)) --&gt; 获得 index --&gt; self._dataset_fetcher.fetch(index) --&gt; 获得 data 而对于多进程而言，借用 PyTorch 内源码的注释，其运行流程解释如下： 12345678910111213141516171819# Our data model looks like this (queues are indicated with curly brackets):## main process ||# | ||# {index_queue} ||# | ||# worker processes || DATA# | ||# {worker_result_queue} || FLOW# | ||# pin_memory_thread of main process || DIRECTION# | ||# {data_queue} ||# | ||# data output \\/## P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if# `pin_memory=False`. 首先 dataloader 基于 multiprocessing 产生多进程，每个子进程的输入输出通过两个主要的队列（multiprocessing.Queue() 类）产生，分别为： · index_queue：每个子进程的队列中需要处理的任务的下标 · _worker_result_queue：返回时处理完任务的下标 · data_queue：表明经过 pin_memory 处理后的数据队列 并且有以下这些比较重要的 flag 参数来协调各个 worker 之间的工作： · _send_idx: 发送索引，用来记录这次要放 index_queue 中 batch 的 idx · _rcvd_idx: 接受索引，记录要从 data_queue 中取出的 batch 的 idx · _task_info: 存储将要产生的 data 信息的 dict，key为 task idx（由 0 开始的整形索引），value 为 (worker_id,) 或 (worker_id, data)，分别对应数据未取和已取的情况 · _tasks_outstanding: 整形，代表已经准备好的 task/batch 的数量（可能有些正在准备中） 每个 worker 一次产生一个 batch 的数据，返回 batch 数据前放入下一个批次要处理的数据下标，对应构造函数子进程初始化如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter): def __init__(self, loader): super(_MultiProcessingDataLoaderIter, self).__init__(loader) ... self._worker_result_queue = multiprocessing_context.Queue() # 把该worker取出的数放入该队列，用于进程间通信 ... self._workers_done_event = multiprocessing_context.Event() self._index_queues = [] self._workers = [] for i in range(self._num_workers): index_queue = multiprocessing_context.Queue() # 索引队列，每个子进程一个队列放要处理的下标 index_queue.cancel_join_thread() # _worker_loop 的作用是：从index_queue中取索引，然后通过collate_fn处理数据， # 然后再将处理好的 batch 数据放到 data_queue 中。（发送到队列中的idx是self.send_idx） w = multiprocessing_context.Process( target=_utils.worker._worker_loop, # 每个worker子进程循环执行的函数，主要将数据以(idx, data)的方式传入_worker_result_queue中 args=(self._dataset_kind, self._dataset, index_queue, self._worker_result_queue, self._workers_done_event, self._auto_collation, self._collate_fn, self._drop_last, self._base_seed + i, self._worker_init_fn, i, self._num_workers, self._persistent_workers)) w.daemon = True w.start() self._index_queues.append(index_queue) self._workers.append(w) if self._pin_memory: self._pin_memory_thread_done_event = threading.Event() self._data_queue = queue.Queue() # 用于存取出的数据进行 pin_memory 操作后的结果 pin_memory_thread = threading.Thread( target=_utils.pin_memory._pin_memory_loop, args=(self._worker_result_queue, self._data_queue, torch.cuda.current_device(), self._pin_memory_thread_done_event)) pin_memory_thread.daemon = True pin_memory_thread.start() # Similar to workers (see comment above), we only register # pin_memory_thread once it is started. self._pin_memory_thread = pin_memory_thread else: self._data_queue = self._worker_result_queue ... self._reset(loader, first_iter=True) def _reset(self, loader, first_iter=False): super()._reset(loader, first_iter) self._send_idx = 0 # idx of the next task to be sent to workers，发送索引，用来记录这次要放 index_queue 中 batch 的 idx self._rcvd_idx = 0 # idx of the next task to be returned in __next__，接受索引，记录要从 data_queue 中取出的 batch 的 idx # information about data not yet yielded, i.e., tasks w/ indices in range [rcvd_idx, send_idx). # map: task idx =&gt; - (worker_id,) if data isn't fetched (outstanding) # \\ (worker_id, data) if data is already fetched (out-of-order) self._task_info = {} # _tasks_outstanding 指示当前已经准备好的 task/batch 的数量（可能有些正在准备中） # 初始值为 0, 在 self._try_put_index() 中 +1,在 self._next_data 中-1 self._tasks_outstanding = 0 # always equal to count(v for v in task_info.values() if len(v) == 1) # this indicates status that a worker still has work to do *for this epoch*. self._workers_status = [True for i in range(self._num_workers)] # We resume the prefetching in case it was enabled if not first_iter: for idx in range(self._num_workers): self._index_queues[idx].put(_utils.worker._ResumeIteration()) resume_iteration_cnt = self._num_workers while resume_iteration_cnt &gt; 0: data = self._get_data() if isinstance(data, _utils.worker._ResumeIteration): resume_iteration_cnt -= 1 ... # 初始化的时候，就将 2*num_workers 个 (batch_idx, sampler_indices) 放到 index_queue 中 for _ in range(self._prefetch_factor * self._num_workers): self._try_put_index() # 进行预取 dataloader 初始化的时候，每个 worker 的 index_queue 默认会放入两个 batch 的 index，从 index_queue 中取出要处理的下标。 1234567891011121314151617181920def _try_put_index(self): # self._prefetch_factor 默认为 2 assert self._tasks_outstanding &lt; self._prefetch_factor * self._num_workers try: index = self._next_index() except StopIteration: return for _ in range(self._num_workers): # find the next active worker, if any worker_queue_idx = next(self._worker_queue_idx_cycle) if self._workers_status[worker_queue_idx]: break else: # not found (i.e., didn't break) return self._index_queues[worker_queue_idx].put((self._send_idx, index)) # 放入 任务下标 和 数据下标 self._task_info[self._send_idx] = (worker_queue_idx,) # _tasks_outstanding + 1，表明预备好的batch个数+1 self._tasks_outstanding += 1 # send_idx 发送索引, 记录从sample_iter中发送索引到index_queue的次数 self._send_idx += 1 调用 _next_data(self) 方法进行数据读取，其中 _process_data(self, data) 用于返回数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def _next_data(self): while True: while self._rcvd_idx &lt; self._send_idx: # 确保待处理的任务(待取的batch)下标 &gt; 处理完毕要返回的任务(已经取完的batch)下标 info = self._task_info[self._rcvd_idx] worker_id = info[0] if len(info) == 2 or self._workers_status[worker_id]: # has data or is still active break del self._task_info[self._rcvd_idx] self._rcvd_idx += 1 else: # no valid `self._rcvd_idx` is found (i.e., didn't break) if not self._persistent_workers: self._shutdown_workers() raise StopIteration # Now `self._rcvd_idx` is the batch index we want to fetch # Check if the next sample has already been generated if len(self._task_info[self._rcvd_idx]) == 2: data = self._task_info.pop(self._rcvd_idx)[1] return self._process_data(data) assert not self._shutdown and self._tasks_outstanding &gt; 0 idx, data = self._get_data() # 调用 self._try_get_data() 从 self._data_queue 中取数 self._tasks_outstanding -= 1 # 表明预备好的batch个数需要减1 if self._dataset_kind == _DatasetKind.Iterable: # Check for _IterableDatasetStopIteration if isinstance(data, _utils.worker._IterableDatasetStopIteration): if self._persistent_workers: self._workers_status[data.worker_id] = False else: self._mark_worker_as_unavailable(data.worker_id) self._try_put_index() continue if idx != self._rcvd_idx: # store out-of-order samples self._task_info[idx] += (data,) else: del self._task_info[idx] return self._process_data(data) # 返回数据 def _process_data(self, data): self._rcvd_idx += 1 self._try_put_index() # 同上，主要放入队列索引 以及 更新flag if isinstance(data, ExceptionWrapper): data.reraise() return data 这样，多进程模式的 dataloader 就能通过多个 worker 的协作来共同完成数据的加载。","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://pistachio0812.github.io/categories/pytorch/"}],"tags":[{"name":"数据处理","slug":"数据处理","permalink":"http://pistachio0812.github.io/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}],"author":"Daniel"},{"title":"色彩搭配","slug":"教你学会色彩搭配","date":"2022-04-16T07:26:49.025Z","updated":"2022-10-04T13:41:59.983Z","comments":true,"path":"zh-CN/教你学会色彩搭配/","permalink":"http://pistachio0812.github.io/zh-CN/%E6%95%99%E4%BD%A0%E5%AD%A6%E4%BC%9A%E8%89%B2%E5%BD%A9%E6%90%AD%E9%85%8D/","excerpt":"","text":"中国色中国色 COULEURcouleur color spacecolor space hyper colorHypercolor colorableColorable brandcolorsBrandColors 九月ppt九月PPT huemintHuemint - AI color palette generator Adobe Color色輪、調色盤產生器 | Adobe Color","categories":[{"name":"色彩搭配","slug":"色彩搭配","permalink":"http://pistachio0812.github.io/categories/%E8%89%B2%E5%BD%A9%E6%90%AD%E9%85%8D/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"author":"coolboy"},{"title":"pytorch官方文档中文版","slug":"pytorch官方文档","date":"2022-04-16T07:26:49.017Z","updated":"2022-10-04T13:26:40.180Z","comments":true,"path":"zh-CN/pytorch官方文档/","permalink":"http://pistachio0812.github.io/zh-CN/pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/","excerpt":"","text":"torch.nnContainerModuleCLASS torch.nn.Module SOURCE 它是所有神经网络模型的基类，你的模块应该继承于该类。 模块还能包含其他模块，允许把它们嵌套在一个树结构中。你可以分配子模块作为常规属性： 123456789101112import torch.nn as nnimport torch.nn.functional as Fclass Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) 用这种方式分配的模块将会显示，并且当你调用to( )等等方法，它们的参数也将被转换。 注： 正如上面的例子一样，一个__init__()调用父类必须在子类赋值之前完成。 变量 training(bool)-布尔值代表这个模块是训练模式还是评估模式 add_module(name, module) SOURCE ​ 添加一个子模块到当前模块 ​ 这个模块可以用给定的名称作为属性访问模块 ​ 参数： ​ ·name(string)-子模块的名字，这个子模块可以用给定的名称作为属性访问。 ​ ·module(Module)-子模块添加到模块上","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://pistachio0812.github.io/categories/pytorch/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"author":"coolboy"},{"title":"pip换源","slug":"pip换源","date":"2022-04-16T07:26:49.002Z","updated":"2022-11-03T12:36:52.958Z","comments":true,"path":"zh-CN/pip换源/","permalink":"http://pistachio0812.github.io/zh-CN/pip%E6%8D%A2%E6%BA%90/","excerpt":"","text":"由于使用pip或pip3安装python第三方包时，经常出现read timed out问题，所以需要将pip的官方软件源服务器换成国内的镜像服务器，从而提升python软件包安装效率和成功率，pip 国内的一些镜像： 阿里云 http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 豆瓣(douban) http://pypi.douban.com/simple/ 清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/ 中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/ 更换源临时使用可以在使用 pip 的时候在后面加上-i 参数，指定 pip 源 1eg: pip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple 永久修改linux修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件，文件夹要加“.”，表示是隐藏文件夹)， 内容如下： 1234[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host = https://pypi.tuna.tsinghua.edu.cn windows1.pip永久换源 1pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/ 在cmd命令行中输入上述命令即可。 最后，升级 pip 到最新的版本 1pip install pip -U 1python -m pip install --user --upgrade pip 2.直接在 user 目录中创建一个 pip 目录，如：C:\\Users\\xx\\pip，在 pip 目录下新建文件 pip.ini，即 %HOMEPATH%\\pip\\pip.ini，内容如下： 1234[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host = pypi.tuna.tsinghua.edu.cn 可以在开始运行里面输入三个点 ...，敲回车即可打开用户目录。","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://pistachio0812.github.io/categories/pytorch/"}],"tags":[{"name":"pip换源","slug":"pip换源","permalink":"http://pistachio0812.github.io/tags/pip%E6%8D%A2%E6%BA%90/"}],"author":"pistachio"},{"title":"How to use Linux","slug":"Linux系统学习笔记","date":"2022-04-16T07:26:48.985Z","updated":"2022-10-04T13:19:31.225Z","comments":true,"path":"zh-CN/Linux系统学习笔记/","permalink":"http://pistachio0812.github.io/zh-CN/Linux%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"Linux简介UNIX 是一个交互式系统，用于同时处理多进程和多用户同时在线。为什么要说 UNIX，那是因为 Linux 是由 UNIX 发展而来的，UNIX 是由程序员设计，它的主要服务对象也是程序员。Linux 继承了 UNIX 的设计目标。从智能手机到汽车，超级计算机和家用电器，从家用台式机到企业服务器，Linux 操作系统无处不在。 大多数程序员都喜欢让系统尽量简单，优雅并具有一致性。举个例子，从最底层的角度来讲，一个文件应该只是一个字节集合。为了实现顺序存取、随机存取、按键存取、远程存取只能是妨碍你的工作。相同的，如果命令 ls A*意味着只列出以 A 为开头的所有文件，那么命令 rm A应该会移除所有以 A 为开头的文件而不是只删除文件名是 A 的文件。这个特性也是最小吃惊原则(principle of least surprise) 最小吃惊原则一般常用于用户界面和软件设计。它的原型是：该功能或者特征应该符合用户的预期，不应该使用户感到惊讶和震惊。 一些有经验的程序员通常希望系统具有较强的功能性和灵活性。设计 Linux 的一个基本目标是每个应用程序只做一件事情并把他做好。所以编译器只负责编译的工作，编译器不会产生列表，因为有其他应用比编译器做的更好。 很多人都不喜欢冗余，为什么在 cp 就能描述清楚你想干什么时候还使用 copy？这完全是在浪费宝贵的 hacking time。为了从文件中提取所有包含字符串 ard 的行，Linux 程序员应该输入 grep ard f","categories":[{"name":"Linux","slug":"Linux","permalink":"http://pistachio0812.github.io/categories/Linux/"}],"tags":[{"name":"Liunx使用教程","slug":"Liunx使用教程","permalink":"http://pistachio0812.github.io/tags/Liunx%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"}],"author":"coolboy"},{"title":"github使用指南","slug":"github使用指南","date":"2022-04-16T07:26:48.681Z","updated":"2022-03-25T12:50:05.222Z","comments":true,"path":"zh-CN/github使用指南/","permalink":"http://pistachio0812.github.io/zh-CN/github%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/","excerpt":"","text":"git安装1.安装git OSX版 下载地址：http://git-scm.com/download/mac 2.安装git Windows版 下载地址：http://book.git-scm.com/download/win 3.安装git Linux版 下载地址：http://book.git-scm.com/download/linux 创建新仓库创建新文件夹，打开，然后执行git init以创建新的git仓库 检出仓库执行如下命令以创建一个本地仓库的克隆版本： git clone /path/to/repository 如果是远端服务器上的仓库，则使用如下命令： git clone username@host:/path/to/repository 工作流你的本地仓库由git维护的三棵“树”组成。第一个是你的工作目录，它持有实际文件；第二个是暂存区（index),它像个缓存区域，临时保存你的改动；最后是HEAD,它指向你最后一次提交的结果。 添加和提交你可以提出更改（把它们添加到暂存区），使用如下命令： git add &lt;filename&gt; git add * 这是git基本工作流程的第一步；使用如下命令以实际提交改动： git commit -m \"代码提交信息\" 现在，你的改动已经提交到了HEAD,但是还没到你的远端仓库。 推送改动你的改动现在已经在本地仓库的HEAD中了。执行如下命令以将这些改动提交到远端仓库： git push origin master 可以把master换成你想要推送的任何分支。 如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加： git remote add origin &lt;server&gt; 如此你就能够将你的改动推送到所添加的服务器上去了。 分支分支是用来将特性开发绝缘开来的。在你创建仓库的时候，master是默认的分支。在其他分支上进行开发，完成后再将它们合并到主分支上。 创建一个叫做“feature_x”的分支，并切换过去： git checkout -b feature_x 切换回主分支： git checkout master 再把新建的分支删掉： git branch -d feature_x 除非你将分支推送到远端仓库，不然该分支就是不为他人所见的： git push origin &lt;branch&gt; 更新与合并要更新你的本地仓库至最新改动，执行：git pull以在你的工作目录中 获取（fetch） 并 合并（merge） 远端的改动。要合并其他分支到你的当前分支（例如 master），执行：git merge &lt;branch&gt;在这两种情况下，git 都会尝试去自动合并改动。遗憾的是，这可能并非每次都成功，并可能出现冲突（conflicts）。 这时候就需要你修改这些文件来手动合并这些冲突（conflicts）。改完之后，你需要执行如下命令以将它们标记为合并成功： git add &lt;filename&gt;在合并改动之前，你可以使用如下命令预览差异：git diff &lt;source_branch&gt; &lt;target_branch&gt; 标签为软件发布创建标签是推荐的。这个概念早已存在，在 SVN 中也有。你可以执行如下命令创建一个叫做 1.0.0 的标签：git tag 1.0.0 1b2e1d63ff1b2e1d63ff 是你想要标记的提交 ID 的前 10 位字符。可以使用下列命令获取提交 ID：git log你也可以使用少一点的提交 ID 前几位，只要它的指向具有唯一性。 log如果你想了解本地仓库的历史记录，最简单的命令就是使用:git log你可以添加一些参数来修改他的输出，从而得到自己想要的结果。 只看某一个人的提交记录:git log --author=bob一个压缩后的每一条提交记录只占一行的输出:git log --pretty=oneline或者你想通过 ASCII 艺术的树形结构来展示所有的分支, 每个分支都标示了他的名字和标签:git log --graph --oneline --decorate --all看看哪些文件改变了:git log --name-status这些只是你可以使用的参数中很小的一部分。更多的信息，参考：git log --help 替换本地改动假如你操作失误（当然，这最好永远不要发生），你可以使用如下命令替换掉本地改动：git checkout -- &lt;filename&gt;此命令会使用 HEAD 中的最新内容替换掉你的工作目录中的文件。已添加到暂存区的改动以及新文件都不会受到影响。 假如你想丢弃你在本地的所有改动与提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它：git fetch origingit reset --hard origin/master 实用小贴士内建的图形化 git：gitk彩色的 git 输出：git config color.ui true显示历史记录时，每个提交的信息只显示一行：git config format.pretty oneline交互式添加文件到暂存区：git add -i 更多内容请参考：git - 简明指南","categories":[{"name":"github","slug":"github","permalink":"http://pistachio0812.github.io/categories/github/"}],"tags":[{"name":"使用指南","slug":"使用指南","permalink":"http://pistachio0812.github.io/tags/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"},{"name":"github","slug":"github","permalink":"http://pistachio0812.github.io/tags/github/"}]},{"title":"Matplotlib学习笔记","slug":"Matplotlib学习笔记","date":"2022-04-03T12:25:29.304Z","updated":"2022-10-04T13:20:44.189Z","comments":true,"path":"zh-CN/Matplotlib学习笔记/","permalink":"http://pistachio0812.github.io/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"介绍Matplotlib 是 Python 的绘图库，它能让使用者很轻松地将数据图形化，并且提供多样化的输出格式。可以用来绘制各种静态，动态，交互式的图表。是一个非常强大的 Python 画图工具，我们可以使用该工具将很多数据通过图表的形式更直观的呈现出来。可以绘制线图、散点图、等高线图、条形图、柱状图、3D 图形、甚至是图形动画等等。 应用Matplotlib 通常与 NumPy 和 SciPy（Scientific Python）一起使用， 这种组合广泛用于替代 MatLab，是一个强大的科学计算环境，有助于我们通过 Python 学习数据科学或者机器学习。 SciPy 是一个开源的 Python 算法库和数学工具包。 SciPy 包含的模块有最优化、线性代数、积分、插值、特殊函数、快速傅里叶变换、信号处理和图像处理、常微分方程求解和其他科学与工程中常用的计算。 安装本章节，我们使用 pip 工具来安装 Matplotlib 库，如果还未安装该工具，可以参考 Python pip 安装与使用。 升级 pip： 1python3 -m pip install -U pip 安装 matplotlib 库： 1python3 -m pip install -U matplotlib 安装完成后，我们就可以通过 import 来导入matplotlib 库： import matplotlib 以下实例，我们通过导入 matplotlib 库，然后查看 matplotlib库的版本号： 实例1: import matplotlib print(matplotlib.__version__) 执行以上代码，输出结果如下： 13.4.2 Matplotlib PyplotPyplot 是 Matplotlib 的子库，提供了和 MATLAB 类似的绘图 API。 Pyplot 是常用的绘图模块，能很方便让用户绘制 2D 图表。 Pyplot 包含一系列绘图函数的相关函数，每个函数会对当前的图像进行一些修改，例如：给图像加上标记，生新的图像，在图像中产生新的绘图区域等等。 使用的时候，我们可以使用 import 导入 pyplot 库，并设置一个别名plt： import matplotlib.pyplot as plt 这样我们就可以使用 plt 来引用 Pyplot 包的方法。 以下实例，我们通过两个坐标 (0,0) 到 (6,100) 来绘制一条线: 实例1: 12345678import matplotlib.pyplot as pltimport numpy as npxpoints = np.array([0, 6])ypoints = np.array([0, 100])plt.plot(xpoints, ypoints)plt.show() 输出结果如下： .ivtpuxivxdnv{} 以上实例中我们使用了 Pyplot 的plot() 函数， plot() 函数是绘制二维图形的最基本函数。 plot() 用于画图它可以绘制点和线，语法格式如下： 1234# 画单条线plot([x], y, [fmt], *, data=None, **kwargs)# 画多条线plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs) 参数说明： x, y：点或线的节点，x 为 x 轴数据，y 为 y 轴数据，数据可以列表或数组。 fmt：可选，定义基本格式（如颜色、标记和线条样式）。 \\kwargs：可选，用在二维平面图上，设置指定属性，如标签，线的宽度等。 1234&gt;&gt;&gt; plot(x, y) # 创建 y 中数据与 x 中对应值的二维线图，使用默认样式&gt;&gt;&gt; plot(x, y, 'bo') # 创建 y 中数据与 x 中对应值的二维线图，使用蓝色实心圈绘制&gt;&gt;&gt; plot(y) # x 的值为 0..N-1&gt;&gt;&gt; plot(y, 'r+') # 使用红色 + 号 颜色字符：‘b’ 蓝色，’m’ 洋红色，’g’ 绿色，’y’ 黄色，’r’ 红色，’k’ 黑色，’w’ 白色，’c’ 青绿色，’#008000’ RGB 颜色符串。多条曲线不指定颜色时，会自动选择不同颜色。 线型参数：‘‐’ 实线，’‐‐’ 破折线，’‐.’ 点划线，’:’ 虚线。 标记字符：‘.’ 点标记，’,’ 像素标记(极小点)，’o’ 实心圈标记，’v’ 倒三角标记，’^’ 上三角标记，’&gt;’ 右三角标记，’&lt;’ 左三角标记…等等。 如果我们只想绘制两个坐标点，而不是一条线，可以使用 o 参数，表示一个实心圈的标记。 实例2：绘制坐标 (1, 3) 和 (8, 10) 的两个点 12345678import matplotlib.pyplot as pltimport numpy as npxpoints = np.array([1, 8])ypoints = np.array([3, 10])plt.plot(xpoints, ypoints, 'o')plt.show() 我们也可以绘制任意数量的点，只需确保两个轴上的点数相同即可。 实例3：绘制一条不规则线，坐标为 (1, 3) 、 (2, 8) 、(6, 1) 、(8, 10)，对应的两个数组为：[1, 2, 6, 8] 与 [3, 8, 1, 10]。 12345678import matplotlib.pyplot as pltimport numpy as npxpoints = np.array([1, 2, 6, 8])ypoints = np.array([3, 8, 1, 10])plt.plot(xpoints, ypoints)plt.show() .udhqfhbgnanf{} 实例4：如果我们不指定 x 轴上的点，y只限定范围。 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([3, 10])plt.plot(ypoints)plt.show() .oqdpmqriiukj{} 从上图可以看出 x 的值默认设置为 [0, 1]。 实例5：如果我们不指定 x 轴上的点，y表明具体的点，则 x 会根据 y 的值来设置为 0, 1, 2, 3..N-1 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([3, 8, 1, 10, 5, 7])plt.plot(ypoints)plt.show() .towtkkqbqtmb{} 实例6：以下实例我们绘制一个正弦和余弦图，在 plt.plot() 参数中包含两对 x,y 值，第一对是 x,y，这对应于正弦函数，第二对是 x,z，这对应于余弦函数。 12345678import matplotlib.pyplot as pltimport numpy as npx = np.arange(0,4*np.pi,0.1) # start,stop,stepy = np.sin(x)z = np.cos(x)plt.plot(x,y,x,z)plt.show() .kdrdtknddwuo{} Matplotlib 绘图标记绘图过程如果我们想要给坐标自定义一些不一样的标记，就可以使用 plot() 方法的 marker参数来定义。 实例1:定义实心圆标记 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([1,3,4,5,8,9,6,1,3,4,5,2,4])plt.plot(ypoints, marker = 'o')plt.show() .evfieocutult{} marker可以定义的符号如下： 标记 符号 描述 “.” 点 “,” 像素点 “o” 实心圆 “v” 下三角 “^” 上三角 “&lt;” 左三角 “&gt;” 右三角 “1” 下三叉 “2” 上三叉 “3” 左三叉 “4” 右三叉 “8” 八角形 “s” 正方形 “p” 五边形 “P” 加号（填充） “*” 星号 “h” 六边形 1 “H” 六边形 2 “+” 加号 “x” 乘号 x “X” 乘号 x (填充) “D” 菱形 “d” 瘦菱形 “\\ “ 竖线 “_” 横线 0 (TICKLEFT) 左横线 1 (TICKRIGHT) 右横线 2 (TICKUP) 上竖线 3 (TICKDOWN) 下竖线 4 (CARETLEFT) 左箭头 5 (CARETRIGHT) 右箭头 6 (CARETUP) 上箭头 7 (CARETDOWN) 下箭头 8 (CARETLEFTBASE) 左箭头 (中间点为基准) 9 (CARETRIGHTBASE) 右箭头 (中间点为基准) 10 (CARETUPBASE) 上箭头 (中间点为基准) 11 (CARETDOWNBASE) 下箭头 (中间点为基准) “None”, “ “ or “” 没有任何标记 ‘$…$’ 渲染指定的字符。例如 “$f$” 以字母 f 为标记。 实例2:定义了 * 标记 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([1,3,4,5,8,9,6,1,3,4,5,2,4])plt.plot(ypoints, marker = '*')plt.show() .thjbltfrcvcb{} 实例3:定义下箭头 12345import matplotlib.pyplot as pltimport matplotlib.markersplt.plot([1, 2, 3], marker=matplotlib.markers.CARETDOWNBASE)plt.show() .qgmwgmclzlgf{} fmt 参数fmt 参数定义了基本格式，如标记、线条样式和颜色。 1fmt = '[marker][line][color]' 例如 o:r，o 表示实心圆标记，: 表示虚线，r 表示颜色为红色。 实例4： 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([6, 2, 13, 10])plt.plot(ypoints, 'o:r')plt.show() .qdlbmehhdtyj{} 线类型： 线类型标记 描述 ‘-‘ 实线 ‘:’ 虚线 ‘—‘ 破折线 ‘-.’ 点划线 颜色类型： 颜色标记 描述 ‘r’ 红色 ‘g’ 绿色 ‘b’ 蓝色 ‘c’ 青色 ‘m’ 品红 ‘y’ 黄色 ‘k’ 黑色 ‘w’ 白色 标记大小和颜色我们可以自定义标记的大小与颜色，使用的参数分别是： markersize，简写为 ms：定义标记的大小。 markerfacecolor，简写为 mfc：定义标记内部的颜色。 markeredgecolor，简写为 mec：定义标记边框的颜色。 实例5：设置标记大小 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([6, 2, 13, 10])plt.plot(ypoints, marker = 'o', ms = 20)plt.show() .vizemmqejfgd{} 实例6：设置标记外边框颜色 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([6, 2, 13, 10])plt.plot(ypoints, marker = 'o', ms = 20, mec = 'r')plt.show() .mdptvohookof{} 实例7：设置标记内部颜色 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([6, 2, 13, 10])plt.plot(ypoints, marker = 'o', ms = 20, mfc = 'r')plt.show() .wwjfdgpeocfr{} 实例8：自定义标记内部与边框的颜色 123456import matplotlib.pyplot as pltimport numpy as npypoints = np.array([6, 2, 13, 10])plt.plot(ypoints, marker = 'o', ms = 20, mec = '#4CAF50', mfc = '#4CAF50')plt.show() .nhhjkreumkse{} Matplotlib 绘图线绘图过程如果我们自定义线的样式，包括线的类型、颜色和大小等。 线的类型线的类型可以使用 linestyle 参数来定义，简写为 ls 类型 简写 说明 ‘solid’ (默认) ‘-‘ 实线 ‘dotted’ ‘:’ 点虚线 ‘dashed’ ‘—‘ 破折线 ‘dashdot’ ‘-.’ 点划线 ‘None’ ‘’ 或 ‘ ‘ 不画线 实例1： 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([6, 2, 13, 10])plt.plot(ypoints, linestyle = 'dotted')plt.show() .ggkzimhbohmc{} 实例2：使用简写 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([6, 2, 13, 10])plt.plot(ypoints, ls = '-.')plt.show() .vsqkpqclghdl{} 线的颜色线的颜色可以使用 color 参数来定义，简写为 c。 颜色类型： 颜色标记 描述 ‘r’ 红色 ‘g’ 绿色 ‘b’ 蓝色 ‘c’ 青色 ‘m’ 品红 ‘y’ 黄色 ‘k’ 黑色 ‘w’ 白色 当然也可以自定义颜色类型，例如：SeaGreen、#8FBC8F 等，完整样式可以参考 HTML 颜色值。 实例3： 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([6, 2, 13, 10])plt.plot(ypoints, color = 'r')plt.show() .jjrbbayiwuxq{} 实例4： 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([6, 2, 13, 10])plt.plot(ypoints, c = '#8FBC8F')plt.show() .cokssspvygfg{} 实例5： 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([6, 2, 13, 10])plt.plot(ypoints, c = 'SeaGreen')plt.show() .wjzokppyzohx{} 线的宽度线的宽度可以使用 linewidth 参数来定义，简写为 lw，值可以是浮点数，如：1、2.0、5.67 等 实例6： 1234567import matplotlib.pyplot as pltimport numpy as npypoints = np.array([6, 2, 13, 10])plt.plot(ypoints, linewidth = '12.5')plt.show() .tkrrfbhhmvot{} 多条线plot() 方法中可以包含多对 x,y 值来绘制多条线。 实例7： 12345678910import matplotlib.pyplot as pltimport numpy as npy1 = np.array([3, 7, 5, 9])y2 = np.array([6, 2, 13, 10])plt.plot(y1)plt.plot(y2)plt.show() 从上图可以看出 x 的值默认设置为 [0, 1, 2, 3]。 .ijjvtyeuibzk{} 实例8： 12345678910import matplotlib.pyplot as pltimport numpy as npx1 = np.array([0, 1, 2, 3])y1 = np.array([3, 7, 5, 9])x2 = np.array([0, 1, 2, 3])y2 = np.array([6, 2, 13, 10])plt.plot(x1, y1, x2, y2)plt.show() .smjztbytddfd{} Matplotlib 轴标签和标题我们可以使用 xlabel() 和 ylabel() 方法来设置 x 轴和 y 轴的标签。 实例1： 1234567891011import numpy as npimport matplotlib.pyplot as pltx = np.array([1, 2, 3, 4])y = np.array([1, 4, 9, 16])plt.plot(x, y)plt.xlabel(\"x - label\")plt.ylabel(\"y - label\")plt.show() .guznogyocuto{} 标题实例2：使用 title() 方法来设置标题: 123456789101112import numpy as npimport matplotlib.pyplot as pltx = np.array([1, 2, 3, 4])y = np.array([1, 4, 9, 16])plt.plot(x, y)plt.title(\"line chart\")plt.xlabel(\"x - label\")plt.ylabel(\"y - label\")plt.show() .pmhvppeccgjn{} 图像中文显示Matplotlib 默认情况不支持中文，我们可以使用以下简单的方法来解决。 这里我们使用思源黑体，思源黑体是 Adobe 与 Google 推出的一款开源字体。 官网：https://source.typekit.com/source-han-serif/cn/ GitHub 地址：https://github.com/adobe-fonts/source-han-sans/tree/release/OTF/SimplifiedChinese 打开链接后，在里面选一个就好了： .vwpmxwselpio{} 你也可以在网盘下载: https://pan.baidu.com/s/10-w1JbXZSnx3Tm6uGpPGOw，提取码：**yxqu**。 可以下载个 OTF 字体，比如 SourceHanSansSC-Bold.otf，将该文件文件放在当前执行的代码文件中： SourceHanSansSC-Bold.otf 文件放在当前执行的代码文件中。 实例3： 12345678910111213141516import numpy as np from matplotlib import pyplot as plt import matplotlib # fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径zhfont1 = matplotlib.font_manager.FontProperties(fname=\"SourceHanSansSC-Bold.otf\") x = np.arange(1,11) y = 2 * x + 5 plt.title(\"测试\", fontproperties=zhfont1) # fontproperties 设置中文显示，fontsize 设置字体大小plt.xlabel(\"x标签\", fontproperties=zhfont1)plt.ylabel(\"y标签\", fontproperties=zhfont1)plt.plot(x,y) plt.show() .ktiyewzuclxw{} 此外，我们还可以使用系统的字体： 123456from matplotlib import pyplot as pltimport matplotliba=sorted([f.name for f in matplotlib.font_manager.fontManager.ttflist])for i in a: print(i) 打印出你的 font_manager 的 ttflist 中所有注册的名字，找一个看中文字体例如：STFangsong(仿宋）,然后添加以下代码即可： 1plt.rcParams['font.family']=['STFangsong'] 实例4：:自定义字体的样式 12345678910111213141516171819import numpy as npfrom matplotlib import pyplot as pltimport matplotlib # fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径，size 参数设置字体大小zhfont1 = matplotlib.font_manager.FontProperties(fname=\"SourceHanSansSC-Bold.otf\", size=18)font1 = {'color':'blue','size':20}font2 = {'color':'darkred','size':15}x = np.arange(1,11)y = 2 * x + 5# fontdict 可以使用 css 来设置字体样式plt.title(\"菜鸟教程 - 测试\", fontproperties=zhfont1, fontdict = font1) # fontproperties 设置中文显示，fontsize 设置字体大小plt.xlabel(\"x 轴\", fontproperties=zhfont1)plt.ylabel(\"y 轴\", fontproperties=zhfont1)plt.plot(x,y)plt.show() .ydolyqslrgpp{} 标题与标签的定位title() 方法提供了 loc 参数来设置标题显示的位置，可以设置为: ‘left’, ‘right’, 和 ‘center’， 默认值为 ‘center’。 xlabel() 方法提供了 loc 参数来设置 x 轴显示的位置，可以设置为: ‘left’, ‘right’, 和 ‘center’， 默认值为 ‘center’。 ylabel() 方法提供了 loc 参数来设置 y 轴显示的位置，可以设置为: ‘bottom’, ‘top’, 和 ‘center’， 默认值为 ‘center’。 实例5： 12345678910111213141516171819import numpy as npfrom matplotlib import pyplot as pltimport matplotlib # fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径，size 参数设置字体大小zhfont1 = matplotlib.font_manager.FontProperties(fname=\"SourceHanSansSC-Bold.otf\", size=18)font1 = {'color':'blue','size':20}font2 = {'color':'darkred','size':15}x = np.arange(1,11)y = 2 * x + 5# fontdict 可以使用 css 来设置字体样式plt.title(\"菜鸟教程 - 测试\", fontproperties=zhfont1, fontdict = font1, loc=\"left\") # fontproperties 设置中文显示，fontsize 设置字体大小plt.xlabel(\"x 轴\", fontproperties=zhfont1, loc=\"left\")plt.ylabel(\"y 轴\", fontproperties=zhfont1, loc=\"top\")plt.plot(x,y)plt.show() .nmiltmcdfnro{} Matplotlib 网格线我们可以使用 pyplot 中的 grid() 方法来设置图表中的网格线。 grid() 方法语法格式如下： 1matplotlib.pyplot.grid(b=None, which='major', axis='both', ) 参数说明： b：可选，默认为 None，可以设置布尔值，true 为显示网格线，false 为不显示，如果设置 **kwargs 参数，则值为 true。 which：可选，可选值有 ‘major’、’minor’ 和 ‘both’，默认为 ‘major’，表示应用更改的网格线。 axis：可选，设置显示哪个方向的网格线，可以是取 ‘both’（默认），’x’ 或 ‘y’，分别表示两个方向，x 轴方向或 y 轴方向。 \\kwargs：可选，设置网格样式，可以是 color=’r’, linestyle=’-‘ 和 linewidth=2，分别表示网格线的颜色，样式和宽度。 实例1：添加一个简单的网格线，参数使用默认值12345678910111213141516import numpy as npimport matplotlib.pyplot as pltx = np.array([1, 2, 3, 4])y = np.array([1, 4, 9, 16])plt.title(\"RUNOOB grid() Test\")plt.xlabel(\"x - label\")plt.ylabel(\"y - label\")plt.plot(x, y)plt.grid()plt.show() .lrnrszisejay{} 实例2:添加一个简单的网格线，axis 参数使用 x，设置 x 轴方向显示网格线 12345678910111213141516import numpy as npimport matplotlib.pyplot as pltx = np.array([1, 2, 3, 4])y = np.array([1, 4, 9, 16])plt.title(\"RUNOOB grid() Test\")plt.xlabel(\"x - label\")plt.ylabel(\"y - label\")plt.plot(x, y)plt.grid(axis='x') # 设置 y 就在轴方向显示网格线plt.show() .ksjgkdsorkib{} 以下实例添加一个简单的网格线，并设置网格线的样式，格式如下： 1grid(color = 'color', linestyle = 'linestyle', linewidth = number) 参数说明： color：‘b’ 蓝色，’m’ 洋红色，’g’ 绿色，’y’ 黄色，’r’ 红色，’k’ 黑色，’w’ 白色，’c’ 青绿色，’#008000’ RGB 颜色符串。 linestyle：‘‐’ 实线，’‐‐’ 破折线，’‐.’ 点划线，’:’ 虚线。 linewidth：设置线的宽度，可以设置一个数字。 实例3： 12345678910111213141516import numpy as npimport matplotlib.pyplot as pltx = np.array([1, 2, 3, 4])y = np.array([1, 4, 9, 16])plt.title(\"RUNOOB grid() Test\")plt.xlabel(\"x - label\")plt.ylabel(\"y - label\")plt.plot(x, y)plt.grid(color = 'r', linestyle = '--', linewidth = 0.5)plt.show() .yqcbwyiwvijd{} Matplotlib绘制多图我们可以使用 pyplot 中的 subplot() 和 subplots() 方法来绘制多个子图。 subplot() 方法在绘图时需要指定位置，subplots() 方法可以一次生成多个，在调用时只需要调用生成对象的 ax 即可。 subplot1234subplot(nrows, ncols, index, **kwargs)subplot(pos, **kwargs)subplot(**kwargs)subplot(ax) 以上函数将整个绘图区域分成 nrows 行和 ncols 列，然后从左到右，从上到下的顺序对每个子区域进行编号 1…N ，左上的子区域的编号为 1、右下的区域编号为 N，编号可以通过参数 index 来设置。 设置 numRows ＝ 1，numCols ＝ 2，就是将图表绘制成 1x2 的图片区域, 对应的坐标为： 1(1, 1), (1, 2) plotNum ＝ 1, 表示的坐标为(1, 1), 即第一行第一列的子图。 plotNum ＝ 2, 表示的坐标为(1, 2), 即第一行第二列的子图。 实例1： 123456789101112131415161718192021import matplotlib.pyplot as pltimport numpy as np#plot 1:xpoints = np.array([0, 6])ypoints = np.array([0, 100])plt.subplot(1, 2, 1)plt.plot(xpoints,ypoints)plt.title(\"plot 1\")#plot 2:x = np.array([1, 2, 3, 4])y = np.array([1, 4, 9, 16])plt.subplot(1, 2, 2)plt.plot(x,y)plt.title(\"plot 2\")plt.suptitle(\"RUNOOB subplot Test\")plt.show() .jltnbipyhkjg{} 设置 numRows ＝ 2，numCols ＝ 2，就是将图表绘制成 2x2 的图片区域, 对应的坐标为： 12(1, 1), (1, 2)(2, 1), (2, 2) plotNum ＝ 1, 表示的坐标为(1, 1), 即第一行第一列的子图。 plotNum ＝ 2, 表示的坐标为(1, 2), 即第一行第二列的子图。 plotNum ＝ 3, 表示的坐标为(2, 1), 即第二行第一列的子图。 plotNum ＝ 4, 表示的坐标为(2, 2), 即第二行第二列的子图。 实例2： 12345678910111213141516171819202122232425262728293031323334353637import matplotlib.pyplot as pltimport numpy as np#plot 1:x = np.array([0, 6])y = np.array([0, 100])plt.subplot(2, 2, 1)plt.plot(x,y)plt.title(\"plot 1\")#plot 2:x = np.array([1, 2, 3, 4])y = np.array([1, 4, 9, 16])plt.subplot(2, 2, 2)plt.plot(x,y)plt.title(\"plot 2\")#plot 3:x = np.array([1, 2, 3, 4])y = np.array([3, 5, 7, 9])plt.subplot(2, 2, 3)plt.plot(x,y)plt.title(\"plot 3\")#plot 4:x = np.array([1, 2, 3, 4])y = np.array([4, 5, 6, 7])plt.subplot(2, 2, 4)plt.plot(x,y)plt.title(\"plot 4\")plt.suptitle(\"RUNOOB subplot Test\")plt.show() .fpgomribpuou{} subplots()subplots() 方法语法格式如下： 1matplotlib.pyplot.subplots(nrows=1, ncols=1, *, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None, **fig_kw) 参数说明： nrows：默认为 1，设置图表的行数。 ncols：默认为 1，设置图表的列数。 sharex、sharey：设置 x、y 轴是否共享属性，默认为 false，可设置为 ‘none’、’all’、’row’ 或 ‘col’。 False 或 none 每个子图的 x 轴或 y 轴都是独立的，True 或 ‘all’：所有子图共享 x 轴或 y 轴，’row’ 设置每个子图行共享一个 x 轴或 y 轴，’col’：设置每个子图列共享一个 x 轴或 y 轴。 squeeze：布尔值，默认为 True，表示额外的维度从返回的 Axes(轴)对象中挤出，对于 N1 或 1N 个子图，返回一个 1 维数组，对于 N*M，N&gt;1 和 M&gt;1 返回一个 2 维数组。如果设置为 False，则不进行挤压操作，返回一个元素为 Axes 实例的2维数组，即使它最终是1x1。 subplot_kw：可选，字典类型。把字典的关键字传递给 add_subplot() 来创建每个子图。 gridspec_kw：可选，字典类型。把字典的关键字传递给 GridSpec 构造函数创建子图放在网格里(grid)。 \\fig_kw：把详细的关键字参数传给 figure() 函数。 实例3： 123456789101112131415161718192021222324252627282930313233343536373839import matplotlib.pyplot as pltimport numpy as np# 创建一些测试数据 -- 图1x = np.linspace(0, 2*np.pi, 400)y = np.sin(x**2)# 创建一个画像和子图 -- 图2fig, ax = plt.subplots()ax.plot(x, y)ax.set_title('Simple plot')# 创建两个子图 -- 图3f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)ax1.plot(x, y)ax1.set_title('Sharing Y axis')ax2.scatter(x, y)# 创建四个子图 -- 图4fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))axs[0, 0].plot(x, y)axs[1, 1].scatter(x, y)# 共享 x 轴plt.subplots(2, 2, sharex='col')# 共享 y 轴plt.subplots(2, 2, sharey='row')# 共享 x 轴和 y 轴plt.subplots(2, 2, sharex='all', sharey='all')# 这个也是共享 x 轴和 y 轴plt.subplots(2, 2, sharex=True, sharey=True)# 创建10 张图，已经存在的则删除fig, ax = plt.subplots(num=10, clear=True)plt.show() .okwuoqiusnco{} .igfnkclavvmp{} .jqmpliolkvit{} .vwwlcevdifdm{} Matplotlib散点图我们可以使用 pyplot 中的 scatter() 方法来绘制散点图。 scatter() 方法语法格式如下： 1matplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs) 参数说明： x，y：长度相同的数组，也就是我们即将绘制散点图的数据点，输入数据。 s：点的大小，默认 20，也可以是个数组，数组每个参数为对应点的大小。 c：点的颜色，默认蓝色 ‘b’，也可以是个 RGB 或 RGBA 二维行数组。 marker：点的样式，默认小圆圈 ‘o’。 cmap：Colormap，默认 None，标量或者是一个 colormap 的名字，只有 c 是一个浮点数数组的时才使用。如果没有申明就是 image.cmap。 norm：Normalize，默认 None，数据亮度在 0-1 之间，只有 c 是一个浮点数的数组的时才使用。 vmin，vmax：：亮度设置，在 norm 参数存在时会忽略。 alpha：：透明度设置，0-1 之间，默认 None，即不透明。 linewidths：：标记点的长度。 edgecolors：：颜色或颜色序列，默认为 ‘face’，可选值有 ‘face’, ‘none’, None。 plotnonfinite：：布尔值，设置是否使用非限定的 c ( inf, -inf 或 nan) 绘制点。 \\kwargs：：其他参数。 以下实例 scatter() 函数接收长度相同的数组参数，一个用于 x 轴的值，另一个用于 y 轴上的值： 实例1： 12345678import matplotlib.pyplot as pltimport numpy as npx = np.array([1, 2, 3, 4, 5, 6, 7, 8])y = np.array([1, 4, 9, 16, 7, 11, 23, 18])plt.scatter(x, y)plt.show() .rpjigfsevifd{} 实例2：设置图标大小 12345678import matplotlib.pyplot as pltimport numpy as npx = np.array([1, 2, 3, 4, 5, 6, 7, 8])y = np.array([1, 4, 9, 16, 7, 11, 23, 18])sizes = np.array([20,50,100,200,500,1000,60,90])plt.scatter(x, y, s=sizes)plt.show() .krbdjhentibt{} 实例3：自定义点的颜色 123456789import matplotlib.pyplot as pltimport numpy as npx = np.array([1, 2, 3, 4, 5, 6, 7, 8])y = np.array([1, 4, 9, 16, 7, 11, 23, 18])colors = np.array([\"red\",\"green\",\"black\",\"orange\",\"purple\",\"beige\",\"cyan\",\"magenta\"])plt.scatter(x, y, c=colors)plt.show() .cyredgalxudn{} 实例4：设置两组散点图 123456789101112import matplotlib.pyplot as pltimport numpy as npx = np.array([5,7,8,7,2,17,2,9,4,11,12,9,6])y = np.array([99,86,87,88,111,86,103,87,94,78,77,85,86])plt.scatter(x, y, color = 'hotpink')x = np.array([2,2,8,1,15,8,12,9,7,3,11,4,7,14,12])y = np.array([100,105,84,105,90,99,90,95,94,100,79,112,91,80,85])plt.scatter(x, y, color = '#88c999')plt.show() .dnssygodwtgh{} 实例5：使用随机数来设置散点图 123456789101112131415161718import numpy as npimport matplotlib.pyplot as plt# 随机数生成器的种子np.random.seed(19680801)N = 50x = np.random.rand(N)y = np.random.rand(N)colors = np.random.rand(N)area = (30 * np.random.rand(N))**2 # 0 to 15 point radiiplt.scatter(x, y, s=area, c=colors, alpha=0.5) # 设置颜色及透明度plt.title(\"RUNOOB Scatter Test\") # 设置标题plt.show() .bngwtxrcdggw{} 颜色条ColormapMatplotlib 模块提供了很多可用的颜色条。 颜色条就像一个颜色列表，其中每种颜色都有一个范围从 0 到 100 的值。 下面是一个颜色条的例子： .mtphvnqeqven{} 实例6：设置颜色条需要使用 cmap 参数，默认值为 ‘viridis’，之后颜色值设置为 0 到 100 的数组 12345678910import matplotlib.pyplot as pltimport numpy as npx = np.array([5,7,8,7,2,17,2,9,4,11,12,9,6])y = np.array([99,86,87,88,111,86,103,87,94,78,77,85,86])colors = np.array([0, 10, 20, 30, 40, 45, 50, 55, 60, 70, 80, 90, 100])plt.scatter(x, y, c=colors, cmap='viridis')plt.show() .fhueiryxjshr{} 实例7：如果要显示颜色条，需要使用 plt.colorbar() 方法 123456789101112import matplotlib.pyplot as pltimport numpy as npx = np.array([5,7,8,7,2,17,2,9,4,11,12,9,6])y = np.array([99,86,87,88,111,86,103,87,94,78,77,85,86])colors = np.array([0, 10, 20, 30, 40, 45, 50, 55, 60, 70, 80, 90, 100])plt.scatter(x, y, c=colors, cmap='viridis')plt.colorbar()plt.show() .rgtfojffpitc{} 实例8：换个颜色条参数， cmap 设置为 afmhot_r 12345678910import matplotlib.pyplot as pltimport numpy as npx = np.array([5,7,8,7,2,17,2,9,4,11,12,9,6])y = np.array([99,86,87,88,111,86,103,87,94,78,77,85,86])colors = np.array([0, 10, 20, 30, 40, 45, 50, 55, 60, 70, 80, 90, 100])plt.scatter(x, y, c=colors, cmap='afmhot_r')plt.colorbar()plt.show() .vpvbmytgogje{} 颜色条参数值可以是以下值： 颜色名称 保留关键字 Accent Accent_r Blues Blues_r BrBG BrBG_r BuGn BuGn_r BuPu BuPu_r CMRmap CMRmap_r Dark2 Dark2_r GnBu GnBu_r Greens Greens_r Greys Greys_r OrRd OrRd_r Oranges Oranges_r PRGn PRGn_r Paired Paired_r Pastel1 Pastel1_r Pastel2 Pastel2_r PiYG PiYG_r PuBu PuBu_r PuBuGn PuBuGn_r PuOr PuOr_r PuRd PuRd_r Purples Purples_r RdBu RdBu_r RdGy RdGy_r RdPu RdPu_r RdYlBu RdYlBu_r RdYlGn RdYlGn_r Reds Reds_r Set1 Set1_r Set2 Set2_r Set3 Set3_r Spectral Spectral_r Wistia Wistia_r YlGn YlGn_r YlGnBu YlGnBu_r YlOrBr YlOrBr_r YlOrRd YlOrRd_r afmhot afmhot_r autumn autumn_r binary binary_r bone bone_r brg brg_r bwr bwr_r cividis cividis_r cool cool_r coolwarm coolwarm_r copper copper_r cubehelix cubehelix_r flag flag_r gist_earth gist_earth_r gist_gray gist_gray_r gist_heat gist_heat_r gist_ncar gist_ncar_r gist_rainbow gist_rainbow_r gist_stern gist_stern_r gist_yarg gist_yarg_r gnuplot gnuplot_r gnuplot2 gnuplot2_r gray gray_r hot hot_r hsv hsv_r inferno inferno_r jet jet_r magma magma_r nipy_spectral nipy_spectral_r ocean ocean_r pink pink_r plasma plasma_r prism prism_r rainbow rainbow_r seismic seismic_r spring spring_r summer summer_r tab10 tab10_r tab20 tab20_r tab20b tab20b_r tab20c tab20c_r terrain terrain_r twilight twilight_r twilight_shifted twilight_shifted_r viridis viridis_r winter winter_r .kwrzxvfegzfv{} .zzbqvpdwatap{} .tedmrkrrtkpm{} Matplotlib柱形图我们可以使用 pyplot 中的 bar() 方法来绘制柱形图。 bar() 方法语法格式如下： 1matplotlib.pyplot.bar(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs) 参数说明： x：浮点型数组，柱形图的 x 轴数据。 height：浮点型数组，柱形图的高度。 width：浮点型数组，柱形图的宽度。 bottom：浮点型数组，底座的 y 坐标，默认 0。 align：柱形图与 x 坐标的对齐方式，’center’ 以 x 位置为中心，这是默认值。 ‘edge’：将柱形图的左边缘与 x 位置对齐。要对齐右边缘的条形，可以传递负数的宽度值及 align=’edge’。 \\kwargs：：其他参数。 实例1：简单实用 bar() 来创建一个柱形图 12345678import matplotlib.pyplot as pltimport numpy as npx = np.array([\"Runoob-1\", \"Runoob-2\", \"Runoob-3\", \"C-RUNOOB\"])y = np.array([12, 22, 6, 18])plt.bar(x,y)plt.show() .jefvpqiobbqc{} 实例2：垂直方向的柱形图可以使用 barh() 方法来设置 12345678import matplotlib.pyplot as pltimport numpy as npx = np.array([\"Runoob-1\", \"Runoob-2\", \"Runoob-3\", \"C-RUNOOB\"])y = np.array([12, 22, 6, 18])plt.barh(x,y)plt.show() .xuhbocomqjyu{} 实例3：设置柱形图颜色 12345678import matplotlib.pyplot as pltimport numpy as npx = np.array([\"Runoob-1\", \"Runoob-2\", \"Runoob-3\", \"C-RUNOOB\"])y = np.array([12, 22, 6, 18])plt.bar(x, y, color = \"#4CAF50\")plt.show() .escuxxgnmqps{} 实例4：自定义各个柱形的颜色 12345678import matplotlib.pyplot as pltimport numpy as npx = np.array([\"Runoob-1\", \"Runoob-2\", \"Runoob-3\", \"C-RUNOOB\"])y = np.array([12, 22, 6, 18])plt.bar(x, y, color = [\"#4CAF50\",\"red\",\"hotpink\",\"#556B2F\"])plt.show() .bnevkxqimuhn{} 实例5：设置柱形图宽度，bar() 方法使用 width 设置 12345678import matplotlib.pyplot as pltimport numpy as npx = np.array([\"Runoob-1\", \"Runoob-2\", \"Runoob-3\", \"C-RUNOOB\"])y = np.array([12, 22, 6, 18])plt.bar(x, y, width = 0.1)plt.show() .nndmgmxkytso{} 实例6：barh() 方法使用 height 设置 height 12345678import matplotlib.pyplot as pltimport numpy as npx = np.array([\"Runoob-1\", \"Runoob-2\", \"Runoob-3\", \"C-RUNOOB\"])y = np.array([12, 22, 6, 18])plt.barh(x, y, height = 0.1)plt.show() .copynwemkeog{} Matplotlib饼图我们可以使用 pyplot 中的 pie() 方法来绘制饼图。 pie() 方法语法格式如下： 1matplotlib.pyplot.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=0, radius=1, counterclock=True, wedgeprops=None, textprops=None, center=0, 0, frame=False, rotatelabels=False, *, normalize=None, data=None)[source] 参数说明： x：浮点型数组，表示每个扇形的面积。 explode：数组，表示各个扇形之间的间隔，默认值为0。 labels：列表，各个扇形的标签，默认值为 None。 colors：数组，表示各个扇形的颜色，默认值为 None。 autopct：设置饼图内各个扇形百分比显示格式，%d%% 整数百分比，%0.1f 一位小数， %0.1f%% 一位小数百分比， %0.2f%% 两位小数百分比。 labeldistance：标签标记的绘制位置，相对于半径的比例，默认值为 1.1，如 &lt;1则绘制在饼图内侧。 pctdistance：：类似于 labeldistance，指定 autopct 的位置刻度，默认值为 0.6。 shadow：：布尔值 True 或 False，设置饼图的阴影，默认为 False，不设置阴影。 radius：：设置饼图的半径，默认为 1。 startangle：：起始绘制饼图的角度，默认为从 x 轴正方向逆时针画起，如设定 =90 则从 y 轴正方向画起。 counterclock：布尔值，设置指针方向，默认为 True，即逆时针，False 为顺时针。 wedgeprops ：字典类型，默认值 None。参数字典传递给 wedge 对象用来画一个饼图。例如：wedgeprops={‘linewidth’:5} 设置 wedge 线宽为5。 textprops ：字典类型，默认值为：None。传递给 text 对象的字典参数，用于设置标签（labels）和比例文字的格式。 center ：浮点类型的列表，默认值：(0,0)。用于设置图标中心位置。 frame ：布尔类型，默认值：False。如果是 True，绘制带有表的轴框架。 rotatelabels ：布尔类型，默认为 False。如果为 True，旋转每个 label 到指定的角度。 实例1：简单实用 pie() 来创建一个柱形图 1234567import matplotlib.pyplot as pltimport numpy as npy = np.array([35, 25, 25, 15])plt.pie(y)plt.show() .cvyfewgstdkh{} 实例2：设置饼图各个扇形的标签与颜色 1234567891011import matplotlib.pyplot as pltimport numpy as npy = np.array([35, 25, 25, 15])plt.pie(y, labels=['A','B','C','D'], # 设置饼图标签 colors=[\"#d5695d\", \"#5d8ca8\", \"#65a479\", \"#a564c9\"], # 设置饼图颜色 )plt.title(\"RUNOOB Pie Test\") # 设置标题plt.show() .snqecnwroqcb{} 实例3：突出显示第二个扇形，并格式化输出百分比 12345678910111213import matplotlib.pyplot as pltimport numpy as npy = np.array([35, 25, 25, 15])plt.pie(y, labels=['A','B','C','D'], # 设置饼图标签 colors=[\"#d5695d\", \"#5d8ca8\", \"#65a479\", \"#a564c9\"], # 设置饼图颜色 explode=(0, 0.2, 0, 0), # 第二部分突出显示，值越大，距离中心越远 autopct='%.2f%%', # 格式化输出百分比 )plt.title(\"RUNOOB Pie Test\")plt.show() .mygcvfxzscri{} 注意：默认情况下，第一个扇形的绘制是从 x 轴开始并逆时针移动： .btnylejchpia{zoom:50%;}","categories":[{"name":"python","slug":"python","permalink":"http://pistachio0812.github.io/categories/python/"}],"tags":[{"name":"matplotlib","slug":"matplotlib","permalink":"http://pistachio0812.github.io/tags/matplotlib/"}],"author":"pistachio"},{"title":"详解注意力机制","slug":"注意力机制","date":"2022-03-31T07:14:05.585Z","updated":"2022-10-04T13:55:27.428Z","comments":true,"path":"zh-CN/注意力机制/","permalink":"http://pistachio0812.github.io/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","excerpt":"","text":"注意力机制注意力机制就是让网络关注到它更需要关注的地方，是一种网络自适应注意的方式。注意力机制可以分为通道注意力，空间注意力以及二者的结合。 相关论文SENet2017年提出的SENet是最后一届ImageNet竞赛的冠军，其实现示意图如下所示，对于输入进来的特征层，我们关注其每一个通道的权重，对于SENet而言，其重点是获得输入进来的特征层，每一个通道的权值。利用SENet，我们可以让网络关注它最需要关注的通道。 其具体实现方式就是：1、对输入进来的特征层进行全局平均池化。2、然后进行两次全连接，第一次全连接神经元个数较少，第二次全连接神经元个数和输入特征层相同。3、在完成两次全连接后，我们再取一次Sigmoid将值固定到0-1之间，此时我们获得了输入特征层每一个通道的权值（0-1之间）。4、在获得这个权值后，我们将这个权值乘上原输入特征层即可。 .yccsjexbpdqn{} 123456789101112131415161718192021import torchimport torch.nn as nnimport mathclass se_block(nn.Module): def __init__(self, channel, ratio=16): super(se_block, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.fc = nn.Sequential( nn.Linear(channel, channel // ratio, bias=False), nn.ReLU(inplace=True), nn.Linear(channel // ratio, channel, bias=False), nn.Sigmoid() ) def forward(self, x): b, c, _, _ = x.size() y = self.avg_pool(x).view(b, c) y = self.fc(y).view(b, c, 1, 1) return x * y CBAMCBAM将通道注意力机制和空间注意力机制进行一个结合，相比于SENet只关注通道的注意力机制可以取得更好的效果。其实现示意图如下所示，CBAM会对输入进来的特征层，分别进行通道注意力机制的处理和空间注意力机制的处理。 .zoovsphljddn{} 下图是通道注意力机制和空间注意力机制的具体实现方式：图像的上半部分为通道注意力机制，通道注意力机制的实现可以分为两个部分，我们会对输入进来的单个特征层，分别进行全局平均池化和全局最大池化。之后对平均池化和最大池化的结果，利用共享的全连接层进行处理，我们会对处理后的两个结果进行相加，然后取一个sigmoid，此时我们获得了输入特征层每一个通道的权值（0-1之间）。在获得这个权值后，我们将这个权值乘上原输入特征层即可。 图像的下半部分为空间注意力机制，我们会对输入进来的特征层，在每一个特征点的通道上取最大值和平均值。之后将这两个结果进行一个堆叠，利用一次通道数为1的卷积调整通道数，然后取一个sigmoid，此时我们获得了输入特征层每一个特征点的权值（0-1之间）。在获得这个权值后，我们将这个权值乘上原输入特征层即可。 .bcntktfjkpkn{} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class ChannelAttention(nn.Module): def __init__(self, in_planes, ratio=8): super(ChannelAttention, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) # 利用1x1卷积代替全连接 self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False) self.relu1 = nn.ReLU() self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x)))) max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x)))) out = avg_out + max_out return self.sigmoid(out)class SpatialAttention(nn.Module): def __init__(self, kernel_size=7): super(SpatialAttention, self).__init__() assert kernel_size in (3, 7), 'kernel size must be 3 or 7' padding = 3 if kernel_size == 7 else 1 self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = torch.mean(x, dim=1, keepdim=True) max_out, _ = torch.max(x, dim=1, keepdim=True) x = torch.cat([avg_out, max_out], dim=1) x = self.conv1(x) return self.sigmoid(x)class cbam_block(nn.Module): def __init__(self, channel, ratio=8, kernel_size=7): super(cbam_block, self).__init__() self.channelattention = ChannelAttention(channel, ratio=ratio) self.spatialattention = SpatialAttention(kernel_size=kernel_size) def forward(self, x): x = x * self.channelattention(x) x = x * self.spatialattention(x) return x ECANetECANet是也是通道注意力机制的一种实现形式。ECANet可以看作是SENet的改进版。ECANet的作者认为SENet对通道注意力机制的预测带来了副作用，捕获所有通道的依赖关系是低效并且是不必要的。在ECANet的论文中，作者认为卷积具有良好的跨通道信息获取能力。 ECA模块的思想是非常简单的，它去除了原来SE模块中的全连接层，直接在全局平均池化之后的特征上通过一个1D卷积进行学习。 既然使用到了1D卷积，那么1D卷积的卷积核大小的选择就变得非常重要了，了解过卷积原理的同学很快就可以明白，1D卷积的卷积核大小会影响注意力机制每个权重的计算要考虑的通道数量。用更专业的名词就是跨通道交互的覆盖率。 如下图所示，左图是常规的SE模块，右图是ECA模块。ECA模块用1D卷积替换两次全连接。 .ocveofytclmi{} 12345678910111213141516class eca_block(nn.Module): def __init__(self, channel, b=1, gamma=2): super(eca_block, self).__init__() kernel_size = int(abs((math.log(channel, 2) + b) / gamma)) kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1 self.avg_pool = nn.AdaptiveAvgPool2d(1) self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): y = self.avg_pool(x) y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1) y = self.sigmoid(y) return x * y.expand_as(x) CAMobile Network设计的最新研究成果表明，通道注意力（例如，SE注意力）对于提升模型性能具有显著效果，但它们通常会忽略位置信息，而位置信息对于生成空间选择性attention maps是非常重要。 coordinate注意力将通道注意力分解为两个1维特征编码过程，分别沿2个空间方向聚合特征。这样，可以沿一个空间方向捕获远程依赖关系，同时可以沿另一空间方向保留精确的位置信息。然后将生成的特征图分别编码为一对方向感知和位置敏感的attention map，可以将其互补地应用于输入特征图，以增强关注对象的表示。 如下图所示，Coordinate Attention通过精确的位置信息对通道关系和长期依赖性进行编码，具体操作分为Coordinate信息嵌入和Coordinate Attention生成2个步骤。 .mcdmhoydnzpq{} 实现代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchfrom torch import nnclass CA_Block(nn.Module): def __init__(self, channel, h, w, reduction=16): super(CA_Block, self).__init__() self.h = h self.w = w self.avg_pool_x = nn.AdaptiveAvgPool2d((h, 1)) self.avg_pool_y = nn.AdaptiveAvgPool2d((1, w)) self.conv_1x1 = nn.Conv2d(in_channels=channel, out_channels=channel//reduction, kernel_size=1, stride=1, bias=False) self.relu = nn.ReLU() self.bn = nn.BatchNorm2d(channel//reduction) self.F_h = nn.Conv2d(in_channels=channel//reduction, out_channels=channel, kernel_size=1, stride=1, bias=False) self.F_w = nn.Conv2d(in_channels=channel//reduction, out_channels=channel, kernel_size=1, stride=1, bias=False) self.sigmoid_h = nn.Sigmoid() self.sigmoid_w = nn.Sigmoid() def forward(self, x): x_h = self.avg_pool_x(x).permute(0, 1, 3, 2) x_w = self.avg_pool_y(x) x_cat_conv_relu = self.relu(self.conv_1x1(torch.cat((x_h, x_w), 3))) x_cat_conv_split_h, x_cat_conv_split_w = x_cat_conv_relu.split([self.h, self.w], 3) s_h = self.sigmoid_h(self.F_h(x_cat_conv_split_h.permute(0, 1, 3, 2))) s_w = self.sigmoid_w(self.F_w(x_cat_conv_split_w)) out = x * s_h.expand_as(x) * s_w.expand_as(x) return outif __name__ == '__main__': x = torch.randn(1, 16, 128, 64) # b, c, h, w ca_model = CA_Block(channel=16, h=128, w=64) y = ca_model(x) print(y.shape) 注意力机制的应用注意力机制是一个即插即用的模块，理论上可以放在任何一个特征层后面，可以放在主干网络，也可以放在加强特征提取网络。 由于放置在主干会导致网络的预训练权重无法使用，本文以YoloV4-tiny为例，将注意力机制应用加强特征提取网络上。 如下图所示，我们在主干网络提取出来的两个有效特征层上增加了注意力机制，同时对上采样后的结果增加了注意力机制。 .znxdahweibos{} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253attention_block = [se_block, cbam_block, eca_block]#---------------------------------------------------## 特征层-&gt;最后的输出#---------------------------------------------------#class YoloBody(nn.Module): def __init__(self, anchors_mask, num_classes, phi=0): super(YoloBody, self).__init__() self.phi = phi self.backbone = darknet53_tiny(None) self.conv_for_P5 = BasicConv(512,256,1) self.yolo_headP5 = yolo_head([512, len(anchors_mask[0]) * (5 + num_classes)],256) self.upsample = Upsample(256,128) self.yolo_headP4 = yolo_head([256, len(anchors_mask[1]) * (5 + num_classes)],384) if 1 &lt;= self.phi and self.phi &lt;= 3: self.feat1_att = attention_block[self.phi - 1](256) self.feat2_att = attention_block[self.phi - 1](512) self.upsample_att = attention_block[self.phi - 1](128) def forward(self, x): #---------------------------------------------------# # 生成CSPdarknet53_tiny的主干模型 # feat1的shape为26,26,256 # feat2的shape为13,13,512 #---------------------------------------------------# feat1, feat2 = self.backbone(x) if 1 &lt;= self.phi and self.phi &lt;= 3: feat1 = self.feat1_att(feat1) feat2 = self.feat2_att(feat2) # 13,13,512 -&gt; 13,13,256 P5 = self.conv_for_P5(feat2) # 13,13,256 -&gt; 13,13,512 -&gt; 13,13,255 out0 = self.yolo_headP5(P5) # 13,13,256 -&gt; 13,13,128 -&gt; 26,26,128 P5_Upsample = self.upsample(P5) # 26,26,256 + 26,26,128 -&gt; 26,26,384 if 1 &lt;= self.phi and self.phi &lt;= 3: P5_Upsample = self.upsample_att(P5_Upsample) P4 = torch.cat([P5_Upsample,feat1],axis=1) # 26,26,384 -&gt; 26,26,256 -&gt; 26,26,255 out1 = self.yolo_headP4(P4) return out0, out1# 研究方向为CV的可以关注Bubbliiiing,也可以顺道关注一下博主心系五道口，谢谢！！！————————————————版权声明：本文为CSDN博主「Bubbliiiing」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/weixin_44791964/article/details/121371986","categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"Attention","slug":"Attention","permalink":"http://pistachio0812.github.io/tags/Attention/"}],"author":"coolboy"},{"title":"MySQL学习笔记","slug":"MySQL学习笔记","date":"2022-03-26T11:07:11.619Z","updated":"2022-10-04T13:21:47.306Z","comments":true,"path":"zh-CN/MySQL学习笔记/","permalink":"http://pistachio0812.github.io/zh-CN/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"MySQL入门教程什么是数据库数据库是按照数据结构来组织、存储和管理数据的仓库 关系型数据库：建立在关系模型基础上的数据库，借助于集合代数等数学概念和方法来处理数据库中的数据 RDBMS即关系数据库管理系统的特点： 1.数据以表格的形式出现 2.每行为各种记录名称 3.每列为记录名称所对应的数据域 4.许多行和列组成一张表单 5.若干的表单组成数据库 RDBMS相关概念· MySQL创建数据表语法CREATE TABLE table_name (column_name column_type) 举例：在W3CSCHOOL数据库中创建数据表w3cschool_tbl: 1234567CREATE TABLE IF NOT EXISTS tutorials_tbl( tutorial_id INT NOT NULL AUTO_INCREMENT, tutorial_title VARCHAR(100) NOT NULL, tutorial_author VARCHAR(40) NOT NULL, submission_date DATE, PRIMARY KEY (tutorial_id) ); 注： ·如果你不想字段为NULL可以设置字段的属性为NOT NULL,在操作数据库如果输入该字段的数据为NULL,则会报错。 ·AUTO_INCREMENT定义列为自增的属性，一般用于主键，数值会自动加1。 ·PRIMARY KEY关键字用于定义列为主键。你可以使用多列来定义主键，列间以逗号分隔。 MySQL删除数据表语法DROP TABLE table_name MySQL插入数据语法123INSERT INTO table_name(field1, field2, ...fieldN) VALUES (value1, value2, ...valueN); 举例：使用SQL INSERT INTO语句向MySQL数据表w3cschool_tbl插入数据： 1234INSERT INTO w3cschool_tbl(w3cschool_title, w3cschool_author, submission_date)VALUES(\"Learn PHP\", \"John Poul\", NOW()); MySQL查询数据语法1234SELECT column_name, column_nameFROM table_name[WHERE Clause][OFFSET M][LIMIT N] 注： ·查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分隔，并使用WHERE语句来设定查询条件。 ·SELECT命令可以读取一条或者多条记录。 ·你可以使用星号(*)来代替其他字段，SELECT语句会返回表的所有字段数据。 ·你可以使用WHERE语句来包含任何条件。 ·你可以通过OFFSET指定SELECT语句开始查询的数据偏移量，默认偏移量为0。 ·你可以使用LIMIT属性来设定返回的记录数。 举例：通过SQL SELECT命令来获取MySQL数据表w3cschool_tbl的数据： SELECT * from w3cschool_tbl; MySQL where子句语法123SELECT field1, field2, ...fieldNFROM table_name1, table_name2...[WHERE condition1 [[AND][OR]] condition2...] 注： ·查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分隔，并使用WHERE语句来设定查询条件。 ·你可以在WHERE子句中指定任何条件。 ·你可以使用AND或者OR指定一个或多个条件。 ·WHERE子句也可以运用于SQL的DELETE或UPDATE命令。 ·WHERE子句类似于程序中的if条件，根据MySQL表中的字段值来读取指定的数据。 操作符列表，实例假定A=10,B=20: 操作符 描述 实例 = 等号，检测两个值是否相等，如果相等返回True (A=B)返回false &lt;&gt;或！= 不等于，检测两个值是否相等，如果不相等返回True （A!=B)返归true &gt; 大于号，检测左边的值是否大于右边的值，如果左边的值大于右边的值返回True (A&gt;B)返回false &lt; 小于号，检测左边的值是否小于右边的值，如果左边的值小于右边的值返回True (A&lt;B)返回true &gt;= 大于等于号，检测左边的值是否大于等于右边的值，如果左边的值大于或等于右边的值返回True (A&gt;=B)返回false &lt;= 小于等于号，检测左边的值是否小于或等于右边的值，如果左边的值小于或等于右边的值返回True (A&lt;=B)返回true 举例：读取w3cschool_tbl表中w3cschool_author字段值为Sanjay的所有记录： SELECT * from w3cschool_tbl WHERE w3cschool_author='Sanjay'; 除非你使用LIKE来比较字符串，否则MySQL的WHERE子句的字符串比较是不区分大小写的。你可以使用BINARY关键字来设定WHERE子句的紫福春是区分大小写的。 SELECT * from w3cschool_tbl WHERE BINARY w3cschool_author='sanjay'; MySQL UPDATE查询语法123UPDATE table_name SET field1=new-value1, field2=new-value2[WHERE Clause] 注： ·你可以同时更新一个或多个字段 ·你可以在WHERE字句中指定任何条件 ·你可以在一个单独表中同时更新数据 当你需要更新表中指定行的数据时，WHERE子句是非常有用的。 举例：更新数据表中w3cschool_id为3的w3cschool_title字段值： 123UPDATE w3cschool_tblSET w3cschool_title='Learning JAVA'WHERE w3cschool_id=3; MySQL DELETE语句语法DELETE FROM table_name [WHERE Clause] 注： ·如果没有指定WHERE子句，MySQL表中的所有记录将被删除 ·你可以在WHERE字句中指定任何条件 ·你可以在单个表中一次性删除记录 当你想删除数据表中的指定记录时，WHERE子句是非常有用的 举例：删除w3cschool_tbl表中w3cschool_id为3的记录： 1DELETE FROM w3cschool_tbl WHERE w3cschool_id=3; MySQL LIKE子句SQL LIKE子句中使用百分号（%）字符来表示任意字符，类似于UNIX或者正则表达式中的星号（*）。 语法123SELECT field1, field2, ...fieldNFROM table_name1, table_name2...WHERE field1 LIKE condition [[AND][OR]] field2='somevalue' 注： ·你可以在WHERE子句中指定任何条件 ·你可以在WHERE字句中使用LIKE子句 ·你可以使用LIKE子句代替等号 ·LIKE通常与%一同使用，类似于一个元字符的搜索 ·你可以使用AND或OR指定一个或多个条件 ·你可以在DELETE或UPDATE命令中使用WHERE…LIKE子句来指定条件 举例：查询w3cschool_tbl表中的w3cschool_author字段中以’jay’为结尾的所有记录： 12SELECT * from w3cschool_tblWHERE w3cschool_author LIKE '%jay'; MySQL排序语法12SELECT field1, field2,...fieldN FROM table_name1, table_name2...ORDER BY field1, [field2...] [ASC[DESC]] 注： ·你可以使用任何字段来作为排序的条件，从而返回排序后的查询结果 ·你可以设定多个字段来排序 ·你可以使用ASC或DESC关键字来设置查询结果是按升序或者降序排列。默认情况下，它是按升序排列 ·你可以添加WHERE…LIKE子句来设置条件 举例：使用ORDER BY子句来读取MySQL数据表w3cschool_tbl中的数据： SELECT * from w3cschool_tbl ORDER BY w3cschool_author ASC; SELECT * from w3cschool_tbl ORDER BY w3cschool_author DESC; MySQL 分组GROUP BY语句根据一个或者多个列对结果集进行分组，在分组的列上我们可以使用COUNT,SUM,AVG等函数。 语法1234SELECT column_name, function(column_name)FROM table_nameWHERE column_name operator valueGROUP BY column_name; employee_tbl表格信息如下： id name date singin 1 小明 2016-04-22 15:25:33 1 2 小王 2016-04-20 15:25:47 3 3 小丽 2016-04-19 15:26:02 2 4 小王 2016-04-07 15:26:14 4 5 小明 2016-04-11 15:26:40 4 6 小明 2016-04-04 15:26:54 2 举例：将数据表按名字进行分组，并统计每个人有多少条记录： SELECT name, COUNT(*) FROM employ_tbl GROUP BY name; 使用WITH ROLLUPWITH ROLLUP可以实现在分组统计数据基础上再进行相同的统计（SUM.AVG,COUNT). 举例：将以上的数据表按名字进行分组，再统计每个人登录的次数： 123SELECT name, SUM(singin) as singin_countFROM employee_tblGROUP BY name WITH ROLLUP; 其中结果如下： name singin_out 小丽 2 小明 7 小王 7 NULL 16 其中记录NULL表示所有人的登录次数，我们可以使用coalesce来设置一个可以取代NULL的名称。 语法select coalesce(a, b, c); 参数说明：如果a==null,则选择b;如果b==null,则选择c;如果a!=null,则选择a;如果abc都为null,则返回null(没意义)。 举例：如果名字为空，使用总数代替： 123SELECT coalesce(name, '总数'), SUM(singin) as singin_outFROM employee_tblGROUP BY name WITH ROLLUP; MySQL连接的使用JOIN按照功能大致分为如下三类： ·INNER JOIN(内连接，或等值连接)：获取两个表中字段匹配关系的记录 ·LEFT JOIN（左连接）：获取左表所有记录，即使右表没有对应匹配的记录 ·RIGHT JOIN(右连接)：用于获取右表所有记录，即使左表没有对应匹配的记录 SQL JOINS 假设W3CSCHOOL数据库中有两张表tcount_tbl和w3cschool_tbl,两张数据表数据如下： 1.tcount_tbl w3cschool_author w3cschool_count mahran 20 mahnaz NULL Jen NULL Gill 20 John Poul 1 Sanjay 1 2.w3cschool_tbl w3cschool_id w3cschool_title w3cschool_author submission_date 1 Learn PHP John Poul 2007-05-24 2 Learn MyQL Abdul S 2007-05-24 3 JAVA Tutorial Sanjay 2007-05-06 举例：使用INNER JOIN来连接以上两张表来读取w3cschool_tbl表中所有w3cschool_author字段在tcount_tbl表中对应的w3cschool_count字段值。 123SELECT a.w3cschool_id, a.w3cschool_author, b.w3cschool_countFROM w3cschool_tbl a INNER JOIN tcount_tbl bON a.w3cschool_author = b.w3cschool_author 等价于： 123SELECT a.w3cschool_id, a_w3cschool_author, b.w3cschool_countFROM w3cschool_tbl a, tcount_tbl bWHERE a.w3cschool_author = b.w3cschool_author; 举例：以w3cschool_tbl为左表， t_count_tbl为右表，理解MySQL LEFT JOIN的应用： 123SELECT a.w3cschool_id, a.w3cschool_author, b.w3cschool_countFROM w3cschool_tbl a LEFT JOIN tcount_tbl bON a.w3cschool_author = b.w3cschool_author; 举例：以tcount_tbl为左表， w3cschool_tbl为右表，理解MySQL RIGHT JOIN的应用： 123SELECT b.w3cschool_id, b.w3cschool_author, a.w3cschool_countFROM tcount_tbl a RIGHT JOIN w3cschool_tbl bON a.w3cschool_author = b.w3cschool_author; MySQL NULL值处理查询条件字段为NULL时，该命令可能无法正常工作，为了处理这种情况，MySQL提供了三大运算符： ·IS NULL:当列的值为NULL,此运算符返回True ·IS NOT NULL:当列的值不为NULL,运算符返回True ·&lt;=&gt;:比较运算符，当比较的两个值为NULL时返回True 关于NULL的条件比较运算是比较特殊的，你不能够使用=NULL或！=NULL在列中查找NULL值，在MySQL中，NULL值与任何其他值的比较永远返回false,即NULL=NULL返回false。 在命令提示符中使用NULL值假设数据库W3CSCHOOL中的表tcount_tbl含有两列w3cschool_author和w3cschool_count, w3cschool_count中设置插入NULL值。 假设表如下所示： w3cschool_author w3cschool_count mahran 20 mahnaz NULL Jen NULL Gill 20 查询数据表中w3cschool_count列是否为NULL,必须使用IS NULL和IS NOT NULL,如下实例： 12SELECT * FROM tcount_tblWHERE w3cschool_count IS NULL; 12SELECT * FROM tcount_tblWHERE w3cschool_count IS NOT NULL; MySQL正则表达式下表中的正则模式可应用于REGEXP操作符中。 ^ 匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配’\\n’或’\\r’之后的位置。 $ 匹配输入字符串的结束位置。如果设置了RegExp对象的Multiline属性，^也匹配’\\n’或’\\r’之前的位置。 . 匹配除’\\n’之外的任何单个字符。要匹配包括’\\n’在内的任何字符，请使用’[.\\n]’的模式。 […] 字符集合。匹配所包含的任何一个字符。例如’[abc]’可以匹配’plain’中的’a’。 ... 负值字符集合。匹配未包含的任意字符。例如，’abc‘可以匹配’plain’中的’p’。 p1\\ p2\\ p3 匹配p1或p2或p3.例如，’z\\ food’能匹配’z’或’food’。，’(z\\ f)food’能匹配’zood’或’food’ * 匹配前面的子表达式零次或多次。例如，zo能匹配’z’以及’zoo’.*等价于{0，}。 + 匹配前面的子表达式一次或多次。例如，’zo+’能匹配’zo’以及’zoo’,但不能匹配’z’。+等价于{1，} {n} n是一个非负整数。匹配确定的n次。例如，’o{2}’不能匹配’Bob’中的’o’,但是能匹配’food’中的两个o。 {n, m} m和n均为非负整数，其中n&lt;=m。最少匹配n次且最多匹配m次。 举例： 1.查找name字段中以’st’为开头的所有数据： 1SELECT name FROM person_tbl WHERE name REGEXP '^st'; 2.查找name字段中以’ok’为结尾的所有数据： 1SELECT name FROM person_tbl WHERE name REGEXP 'ok$'; 3.查找name字段中包含’mar’字符串的所有数据： 1SELECT name FROM person_tbl WHERE name REGEXP 'mar' 4.查找name字段中以元音字符开头或以’ok’字符串结尾的所有数据： 1SELECT name FROM person_tbl WHERE name REGEXP '^[aeiou]|ok$' MySQL事务MySQL 事务主要用于处理操作量大，复杂度高的数据。比如说，在人员管理系统中，你删除一个人员，你即需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等，这样，这些数据库操作语句就构成一个事务！ 在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 事务处理可以用来维护数据库的完整性，保证成批的SQL语句要么全部执行，要么全部不执行。 事务用来管理 insert , update , delete 语句。 一般来说，事务是必须满足4个条件（ACID）： Atomicity（原子性或不可分割性）、Consistency（一致性）、Isolation（隔离性或独立性）、Durability（持久性） 1、原子性：一组事务，要么成功；要么撤回，即事务在执行过程中出错会回滚到事务开始前的状态。 2、一致性 ： 一个事务不论是开始前还是结束后，数据库的完整性都没有被破坏。因此写入的数据必须完全符合所有预设规则（资料精确度、串联性以及后续数据库能够自发完成预定工作）。 3、隔离性：数据库允许多个事务并发的同时对其数据进行读写修改等操作，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离可分为：Read uncommitted（读未提交）、Read committed（读提交）、Repeatable read（可重复读）、Serializable（串行化）。 4、持久性：事务在处理结束后对数据做出的修改是永久的，无法丢失 事务控制语句1.显式的开始一个事务： start transaction或者begin 2.做保存点，一个事务中可以有多个保存点： savepoint [savepoint_name] 3.提交事务，并使数据库中进行的修改成为永久性的： commit或commit work 4.回滚结束用户的事务，并撤销正在进行的所有未提交的修改： rollback或rollback work 5.删除一个事务的保存点，若没有指定保存点，执行该语句操作则会抛错： release savepoint [savepoint_name] 6.将事务滚回标记点： rollback to 标记点 7.设置事务的隔离级别。InnoDB 存储引擎提供事务的隔离级别有READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ 和 SERIALIZABLE。 1set transaction 事务处理方法1.用 begin ， rollback ， commit 来实现事务处理。 2.用 set 来改变 MySQL 的自动提交模式。 set autocommit = 0 （禁止自动提交）。 set autocommit = 1 （开启自动提交）。 MySQL ALTER命令当我们需要修改数据表名或者修改数据表字段时，就需要使用到MySQL ALTER命令。 开始本章教程前让我们先创建一张表，表名为：testalter_tbl。 12345create table testalter_tbl( i INT, c CHAR(1) ); SHOW COLUMNS FROM testalter_tbl; Field Type Null Key Default Extra i int(11) YES NULL c char(1) YES NULL 删除、添加或修改表字段如下命令使用了 ALTER 命令及 DROP 子句来删除以上创建表的 i 字段： mysql&gt; ALTER TABLE testalter_tbl DROP i; 如果数据表中只剩余一个字段则无法使用DROP来删除字段。 MySQL 中使用 ADD 子句来想数据表中添加列，如下实例在表 testalter_tbl 中添加 i 字段，并定义数据类型: mysql&gt; ALTER TABLE testalter_tbl ADD i INT; 执行以上命令后，i 字段会自动添加到数据表字段的末尾。 `mysql&gt; SHOW COLUMNS FROM testalter_tbl; +-------+---------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+---------+------+-----+---------+-------+ | c | char(1) | YES | | NULL | | | i | int(11) | YES | | NULL | | +-------+---------+------+-----+---------+-------+ 2 rows in set (0.00 sec) 如果你需要指定新增字段的位置，可以使用MySQL提供的关键字 FIRST (设定位第一列)， AFTER 字段名（设定位于某个字段之后）。 尝试以下 ALTER TABLE 语句, 在执行成功后，使用 SHOW COLUMNS 查看表结构的变化： ALTER TABLE testalter_tbl DROP i; ALTER TABLE testalter_tbl ADD i INT FIRST; ALTER TABLE testalter_tbl DROP i; ALTER TABLE testalter_tbl ADD i INT AFTER c; FIRST 和 AFTER 关键字只占用于 ADD 子句，所以如果你想重置数据表字段的位置就需要先使用 DROP 删除字段然后使用 ADD 来添加字段并设置位置。 修改字段类型及名称如果需要修改字段类型及名称, 你可以在ALTER命令中使用 MODIFY 或 CHANGE 子句 。 例如，把字段 c 的类型从 CHAR(1) 改为 CHAR(10)，可以执行以下命令: mysql&gt; ALTER TABLE testalter_tbl MODIFY c CHAR(10); 使用 CHANGE 子句, 语法有很大的不同。 在 CHANGE 关键字之后，紧跟着的是你要修改的字段名，然后指定新字段的类型及名称。尝试如下实例： mysql&gt; ALTER TABLE testalter_tbl CHANGE i j BIGINT; 如果你现在想把字段 j 从 BIGINT 修改为 INT，SQL语句如下： mysql&gt; ALTER TABLE testalter_tbl CHANGE j j INT; ALTER TABLE 对 Null 值和默认值的影响当你修改字段时，你可以指定是否包含只或者是否设置默认值。以下实例，指定字段 j 为 NOT NULL 且默认值为100 。 mysql&gt; ALTER TABLE testalter_tbl ​ -&gt; MODIFY j BIGINT NOT NULL DEFAULT 100; 如果你不设置默认值，MySQL会自动设置该字段默认为 NULL。 修改字段默认值你可以使用 ALTER 来修改字段的默认值，尝试以下实例： mysql&gt; ALTER TABLE testalter_tbl ALTER i SET DEFAULT 1000; mysql&gt; SHOW COLUMNS FROM testalter_tbl; +-------+---------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+---------+------+-----+---------+-------+ | c | char(1) | YES | | NULL | | | i | int(11) | YES | | 1000 | | +-------+---------+------+-----+---------+-------+ 2 rows in set (0.00 sec) 你也可以使用 ALTER 命令及 DROP子句来删除字段的默认值，如下实例： mysql&gt; ALTER TABLE testalter_tbl ALTER i DROP DEFAULT; mysql&gt; SHOW COLUMNS FROM testalter_tbl; +-------+---------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+---------+------+-----+---------+-------+ | c | char(1) | YES | | NULL | | | i | int(11) | YES | | NULL | | +-------+---------+------+-----+---------+-------+ 2 rows in set (0.00 sec) Changing a Table Type: 修改数据表类型，可以使用 ALTER 命令及 TYPE 子句来完成。尝试以下实例，我们将表 testalter_tbl 的类型修改为 MYISAM ：注意：查看数据表类型可以使用 SHOW TABLE STATUS 语句。* 12345678910111213141516171819mysql&gt; ALTER TABLE testalter_tbl TYPE = MYISAM;mysql&gt; SHOW TABLE STATUS LIKE 'testalter_tbl'\\G 1. row ** Name: testalter_tbl Type: MyISAM Row_format: Fixed Rows: 0 Avg_row_length: 0 Data_length: 0Max_data_length: 25769803775 Index_length: 1024 Data_free: 0 Auto_increment: NULL Create_time: 2007-06-03 08:04:36 Update_time: 2007-06-03 08:04:36 Check_time: NULL Create_options: Comment:1 row in set (0.00 sec) 修改表名如果需要修改数据表的名称，可以在 ALTER TABLE 语句中使用 RENAME 子句来实现。 尝试以下实例将数据表 testalter_tbl 重命名为 alter_tbl： 1mysql&gt; ALTER TABLE testalter_tbl RENAME TO alter_tbl; MySQL索引MySQL索引的建立对于MySQL的高效运行是很重要的，索引可以大大提高MySQL的检索速度。 打个比方，如果合理的设计且使用索引的MySQL是一辆兰博基尼的话，那么没有设计和使用索引的MySQL就是一个人力三轮车。 拿汉语字典的目录页（索引）打比方，我们可以按拼音、笔画、偏旁部首等排序的目录（索引）快速查找到需要的字。 索引分单列索引和组合索引。单列索引，即一个索引只包含单个列，一个表可以有多个单列索引，但这不是组合索引。组合索引，即一个索引包含多个列。 创建索引时，你需要确保该索引是应用在 SQL 查询语句的条件(一般作为 WHERE 子句的条件)。 实际上，索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录。 上面都在说使用索引的好处，但过多的使用索引将会造成滥用。因此索引也会有它的缺点：虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。 建立索引会占用磁盘空间的索引文件。 普通索引创建索引 这是最基本的索引，它没有任何限制。它有以下几种创建方式： 1CREATE INDEX indexName ON mytable(username(length)); 如果是CHAR，VARCHAR类型，length可以小于字段实际长度；如果是BLOB和TEXT类型，必须指定 length。 修改表结构(添加索引) 1ALTER table tableName ADD INDEX indexName(columnName) 创建表的时候直接指定 123456789CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, INDEX [indexName] (username(length)) ); 删除索引的语法 1DROP INDEX [indexName] ON mytable; 唯一索引它与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。它有以下几种创建方式： 创建索引 1CREATE UNIQUE INDEX indexName ON mytable(username(length)) 修改表结构 1ALTER table mytable ADD UNIQUE [indexName] (username(length)) 创建表的时候直接指定 123456789CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, UNIQUE [indexName] (username(length)) ); 使用ALTER 命令添加和删除索引 有四种方式来添加数据表的索引： ALTER TABLE tbl_name ADD PRIMARY KEY (column_list): 该语句添加一个主键，这意味着索引值必须是唯一的，且不能为NULL。 ALTER TABLE tbl_name ADD UNIQUE index_name (column_list): 这条语句创建索引的值必须是唯一的（除了NULL外，NULL可能会出现多次）。 ALTER TABLE tbl_name ADD INDEX index_name (column_list): 添加普通索引，索引值可出现多次。 ALTER TABLE tbl_name ADD FULLTEXT index_name (column_list):该语句指定了索引为 FULLTEXT ，用于全文索引。 以下实例为在表中添加索引。 1mysql&gt; ALTER TABLE testalter_tbl ADD INDEX (c); 你还可以在 ALTER 命令中使用 DROP 子句来删除索引。尝试以下实例删除索引: 1mysql&gt; ALTER TABLE testalter_tbl DROP INDEX c; 使用 ALTER 命令添加和删除主键 主键只能作用于一个列上，添加主键索引时，你需要确保该主键默认不为空（NOT NULL）。实例如下： 12mysql&gt; ALTER TABLE testalter_tbl MODIFY i INT NOT NULL;mysql&gt; ALTER TABLE testalter_tbl ADD PRIMARY KEY (i); 你也可以使用 ALTER 命令删除主键： 1mysql&gt; ALTER TABLE testalter_tbl DROP PRIMARY KEY; 删除主键时只需指定PRIMARY KEY，但在删除索引时，你必须知道索引名。 显示索引信息 你可以使用 SHOW INDEX 命令来列出表中的相关的索引信息。可以通过添加 \\G 来格式化输出信息。 尝试以下实例: 12mysql&gt; SHOW INDEX FROM table_name; \\G........ MySQL临时表MySQL 临时表在我们需要保存一些临时数据时是非常有用的。临时表只在当前连接可见，当关闭连接时，MySQL会自动删除表并释放所有空间。 临时表在MySQL 3.23版本中添加，如果你的MySQL版本低于 3.23版本就无法使用MySQL的临时表。不过现在一般很少有再使用这么低版本的MySQL数据库服务了。 MySQL临时表只在当前连接可见，如果你使用PHP脚本来创建MySQL临时表，那每当PHP脚本执行完成后，该临时表也会自动销毁。 如果你使用了其他MySQL客户端程序连接MySQL数据库服务器来创建临时表，那么只有在关闭客户端程序时才会销毁临时表，当然你也可以手动销毁。 实例 以下展示了使用MySQL 临时表的简单实例，以下的SQL代码可以适用于PHP脚本的mysql_query()函数。 1234567891011121314151617181920mysql&gt; CREATE TEMPORARY TABLE SalesSummary ( -&gt; product_name VARCHAR(50) NOT NULL -&gt; , total_sales DECIMAL(12,2) NOT NULL DEFAULT 0.00 -&gt; , avg_unit_price DECIMAL(7,2) NOT NULL DEFAULT 0.00 -&gt; , total_units_sold INT UNSIGNED NOT NULL DEFAULT 0);Query OK, 0 rows affected (0.00 sec)mysql&gt; INSERT INTO SalesSummary -&gt; (product_name, total_sales, avg_unit_price, total_units_sold) -&gt; VALUES -&gt; ('cucumber', 100.25, 90, 2);mysql&gt; SELECT * FROM SalesSummary;+--------------+-------------+----------------+------------------+| product_name | total_sales | avg_unit_price | total_units_sold |+--------------+-------------+----------------+------------------+| cucumber | 100.25 | 90.00 | 2 |+--------------+-------------+----------------+------------------+1 row in set (0.00 sec) 当你使用 SHOW TABLES命令显示数据表列表时，你将无法看到 SalesSummary表。 如果你退出当前MySQL会话，再使用 SELECT命令来读取原先创建的临时表数据，那你会发现数据库中没有该表的存在，因为在你退出时该临时表已经被销毁了。 删除MySQL 临时表 默认情况下，当你断开与数据库的连接后，临时表就会自动被销毁。当然你也可以在当前MySQL会话使用 DROP TABLE 命令来手动删除临时表。 以下是手动删除临时表的实例： 1234567891011121314151617181920212223mysql&gt; CREATE TEMPORARY TABLE SalesSummary ( -&gt; product_name VARCHAR(50) NOT NULL -&gt; , total_sales DECIMAL(12,2) NOT NULL DEFAULT 0.00 -&gt; , avg_unit_price DECIMAL(7,2) NOT NULL DEFAULT 0.00 -&gt; , total_units_sold INT UNSIGNED NOT NULL DEFAULT 0);Query OK, 0 rows affected (0.00 sec)mysql&gt; INSERT INTO SalesSummary -&gt; (product_name, total_sales, avg_unit_price, total_units_sold) -&gt; VALUES -&gt; ('cucumber', 100.25, 90, 2);mysql&gt; SELECT * FROM SalesSummary;+--------------+-------------+----------------+------------------+| product_name | total_sales | avg_unit_price | total_units_sold |+--------------+-------------+----------------+------------------+| cucumber | 100.25 | 90.00 | 2 |+--------------+-------------+----------------+------------------+1 row in set (0.00 sec)mysql&gt; DROP TABLE SalesSummary;mysql&gt; SELECT * FROM SalesSummary;ERROR 1146: Table 'W3CSCHOOL.SalesSummary' doesn't exist","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://pistachio0812.github.io/categories/MySQL/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"Q & A","slug":"遇到的问题","date":"2022-03-17T11:30:56.879Z","updated":"2022-10-04T13:54:47.529Z","comments":true,"path":"zh-CN/遇到的问题/","permalink":"http://pistachio0812.github.io/zh-CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"","text":"就地操作问题来源：看代码发现self.relu=nn.ReLU(inplace=True)不明白inplace=True什么意思。 解答：查看pytorch官网关于ReLU定义，ReLU其中inplace参数表示可以选择就地执行操作，默认为False,就地执行操作是指图像处理函数的输入图像和输出图像是同一对象，即同一张图像，常规的图像处理函数是不支持输入图像和输出图像是同一图像的。 eg:中值滤波函数 medianBlur(src, dst, 7); //常规操作 medianBlur(src, src, 7); //就地操作 就地操作直接更改张量的内容，而无需复制它。由于它不创建输入的副本，因此在处理高维数据时减少了内存使用，就地操作有助于使用更少的GPU内存，详情请看该博客如何在Pytorch中执行就地操作 torch.max中keepdim的作用torch.max的用法： (max, max_indices) = torch.max(input, dim, keepdim=False) 输入： input 是输入的tensor。 dim 是索引的维度，dim=0寻找每一列的最大值，dim=1寻找每一行的最大值。 keepdim 表示是否需要保持输出的维度与输入一样，keepdim=True表示输出和输入的维度一样，keepdim=False表示输出的维度被压缩了，也就是输出会比输入低一个维度。 输出： max 表示取最大值后的结果。 2max_indices 表示最大值的索引 import torch import numpy as np x = torch.randint(0,9,(2,4)) print(x) tensor([[7, 8, 7, 2], [6, 0, 3, 0]]) 取每一行的最大值，torch.max的输出结果y = torch.max(x, 1) print(y) torch.return_types.max(values=tensor([8, 6]),indices=tensor([1, 0])) #索引值 y = torch.max(x, 1, keepdim=True)[0]print(y)print(np.shape(y)) # keepdim=True，输出仍然是二维的tensor([[8], [6]])torch.Size([2, 1])y = torch.max(x, 1, keepdim=False)[0]print(y)print(np.shape(y)) keepdim=False # 输出变成了一维tensor([8, 6])torch.Size([2]) ConstantPad2d的用法torch.nn.ConstantPad2d(padding, value) 参数：padding(int, tuple)-padding的尺寸，如果是整型，那么所有的边界都使用相同的填充，如果是四元组，使用（padding_left, padding_right, padding_top, padding_bottom) 形状： 输入： (N, C, H_{in}, W_{in}) or (C, H_{in}, W_{in}) 输出： (N, C, H_{out}, W_{out}) or (C, H_{out}, W_{out})其中， H_{out} = H_{in}+padding_{top}+padding_{bottom} W_{out}=W_{in}+padding_{left}+padding_{right} 测试用例： 1234567891011121314import torchimport torch.nn as nnn1 = nn.ConstantPad2d(2, 0)n2 = nn.ConstantPad2d((0, 1, 0, 1), 0)n3 = nn.ConstantPad2d((-1, 0, -1, 0), 0)input = torch.randn(1, 2, 2)print(input)t = n1(input)print(t)x = n2(input)y = n3(x)print(x)print(y) 结果： tensor([[[ 1.0826, 0.1191], [-0.3506, 0.1677]]])tensor([[[ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 1.0826, 0.1191, 0.0000, 0.0000], [ 0.0000, 0.0000, -0.3506, 0.1677, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])tensor([[[ 1.0826, 0.1191, 0.0000], [-0.3506, 0.1677, 0.0000], [ 0.0000, 0.0000, 0.0000]]])tensor([[[0.1677, 0.0000], [0.0000, 0.0000]]]) 更多详情参考ConstantPad2d enumerate()函数描述enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。Python 2.3. 以上版本可用，2.6 添加 start 参数。 语法enumerate(sequence, [start=0]) 参数·sequence—一个序列、迭代器或其他支持迭代对象 ·start—下标起始位置的值 返回值返回enumerate(枚举)对象 实例以下展示了使用enumerate()方法的实例： seasons = [‘Spring’, ‘Summer’, ‘Fall’, ‘Winter’] list(enumerate(seasons)) [(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')] list(enumerate(seasons, start=1)) # 下标从1开始 [(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')] 普通的for循环 12345678910i = 0seq = ['one', 'two', 'three']for i in enumerate(seq): print(i, seq[i]) i += 1result:0 one1 two2 three for循环使用enumerate 12345678seq = ['one', 'two', 'three']for i, element in enumerate(seq): print(i, element)result:0 one1 two2 three torch.clamptorch.clamp(input, min=None, max=None, *, out=None)-&gt;Tensor Clamps中所有输入的元素都在[min, max]范围内，让最小值和最大值分别是min和max，将会返回： y_i=min(max(x_i,min\\_value_i),max\\_value_i)如果min为空，就没有下界。或者，如果max为空，没有上界。 注： 如果min大于max,torch.clamp(...,min,max)设置输入的所有元素为max的值。 参数： ·input(Tensor)-输入张量 ·min(Number或Tensor,可选)-被限制范围的下界 ·max(Number或Tensor,可选)-被限制范围的上界 关键字参数： out(Tensor, 可选)-输出的张量 举例： 123456789&gt;&gt;&gt;a = torch.randn(4)&gt;&gt;&gt;atensor([-1.7120, 0.1734, -0.0478, -0.0922])&gt;&gt;&gt;torch.clamp(a, min=-0.5, max=0.5)tensor([-0.5000, 0.1734, -0.0478, -0.0922])&gt;&gt;&gt;min = torch.linspace(-1, 1, steps=4)&gt;&gt;&gt;torch.clamp(a, min=min)tensor([-1.000, 0.1734, 0.3333, 1.0000]) FPPI(68条消息) Recall/Precision/FPPI评价方式详解_Bruce_0712的博客-CSDN博客 torchvision.ops.box_iou语法torchvision.ops.box_iou(boxes1:torch.Tensor, boxes2:torch.Tensor)-&gt;torch.TensorSOURCE 返回两个框的交并比，这两个框的形式都是(x_1,y_1,x_2,y_2)并且0","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://pistachio0812.github.io/categories/pytorch/"}],"tags":[{"name":"函数库","slug":"函数库","permalink":"http://pistachio0812.github.io/tags/%E5%87%BD%E6%95%B0%E5%BA%93/"}],"author":"pistachio"},{"title":"学术搜索网站集合","slug":"学术搜索网站导航","date":"2022-03-15T01:58:59.113Z","updated":"2022-10-04T13:50:32.301Z","comments":true,"path":"zh-CN/学术搜索网站导航/","permalink":"http://pistachio0812.github.io/zh-CN/%E5%AD%A6%E6%9C%AF%E6%90%9C%E7%B4%A2%E7%BD%91%E7%AB%99%E5%AF%BC%E8%88%AA/","excerpt":"","text":"搞学术研究的必不可少的就是论文，而且是大量的论文。因此，在这里我特意把我平时用到的积累到的论文搜索网站集中到了这里，后续遇到了其他的还会继续补充。 谷歌学术谷歌学术是一个可以免费搜索学术文章的Google网络应用。2004年11月，Google第一次发布了Google学术搜索的试用版。该项索引包括了世界上绝大部分出版的学术期刊， 可广泛搜索学术文献的简便方法。您可以从一个位置搜索众多学科和资料来源：来自学术著作出版商、专业性社团、预印本、各大学及其他学术组织的经同行评论的文章、论文、图书、摘要和文章。Google 学术搜索可帮助您在整个学术领域中确定相关性最强的研究。 相关页面如下： githubgithub于2008年4月10日正式上线，除了代码仓库托管及基本的Web管理界面以外，还提供了订阅、讨论组、文本渲染、在线文件编辑器、协作图谱（报表）、代码片段分享（Gist）等功能。目前，其注册用户已经超过350万，托管版本数量也是非常之多，其中不乏知名开源项目Ruby on Rails、jQuery、python等。 相关页面如下： CVPR国际计算机视觉与模式识别会议（CVPR）是IEEE一年一度的学术性会议，会议的主要内容是计算机视觉与模式识别技术。CVPR是世界顶级的计算机视觉会议（三大顶会之一，另外两个是ICCV和ECCV，近年来每年有约1500名参加者，收录的论文数量一般300篇左右。本会议每年都会有固定的研讨主题，而每一年都会有公司赞助该会议并获得在会场展示的机会。 相关页面如下： CVFCVF研究论文是由计算机视觉基金会提供的开放获取版本。除水印外，它们与接受的版本相同;最后发表的论文集可以在IEEE Xplore上找到。本材料的提出，以确保及时传播学术和技术工作。版权和其中的所有权利由作者或其他版权持有人保留。所有复制此信息的人都应遵守每个作者的版权所援引的条款和约束。 arXivarXiv是一个免费分发服务和开放获取的档案，涵盖物理、数学、计算机科学、定量生物学、定量金融学、统计学、电气工程和系统科学以及经济学领域的2,040,232篇学术文章。 paperswithcodePapers With Code代码论文的任务是创建一个免费和开放的资源与机器学习论文，代码，数据集，方法和评估表。我们相信，在NLP和ML的支持下，与社区合作是最好的。这个网站上的所有内容都是公开许可的CC-BY-SA(与维基百科一样)，每个人都可以贡献——寻找“编辑”按钮!我们还运营专门的门户网站，提供天文学、物理学、计算机科学、数学和统计学的论文代码。","categories":[{"name":"科研推荐","slug":"科研推荐","permalink":"http://pistachio0812.github.io/categories/%E7%A7%91%E7%A0%94%E6%8E%A8%E8%8D%90/"}],"tags":[{"name":"文献工具","slug":"文献工具","permalink":"http://pistachio0812.github.io/tags/%E6%96%87%E7%8C%AE%E5%B7%A5%E5%85%B7/"}],"author":"pistachio"},{"title":"计算机视觉单词表","slug":"计算机视觉单词表","date":"2022-03-12T08:48:16.114Z","updated":"2022-10-04T13:40:42.144Z","comments":true,"path":"zh-CN/计算机视觉单词表/","permalink":"http://pistachio0812.github.io/zh-CN/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%8D%95%E8%AF%8D%E8%A1%A8/","excerpt":"","text":"artificial neural network,ANN 人工神经网络 perceptron 感知机，人工神经元 activation function 激活函数 rectified linear unit,RELU 修正线性单元 bias 偏置 loss function 损失函数 universal approximation theorem 万能逼近定理 one-hot encoding 独热编码 cross-entropy 交叉熵 dropout 丢弃 bagging 装袋 model averaging 模型平均 batch normalization 批归一化 backpropagation 反向传播 stochastic gradient descent,SGD 随机梯度下降 acquisition 学习，获得 integrate 整合，集成，合并 diverse 多样化的，不同的 tune 调优 curation 内容管理 projection 投影，预测 coherent 有条理的，连贯的 redundant 冗余的 entity 实体 synthetic 合成的，虚假的，不诚恳的 spammy 垃圾邮件式的，无聊的 crowdsourcing 众包 continuity 连续性，连贯性 manifold 多种多样的 inherent 固有的，内在的 pseudo 假的，仿冒的 ensemble 套 heuristic 启发式的；启发式教育法 erroneous 错误的，不正确的 resilient 有弹性的，可迅速恢复的 degraded 堕落的，退化的 converge 收敛，集中 outlier 离群值，异常值 violate 违反，违背 syntactic 语法的 cartesian 笛卡尔的 categorical 分类，绝对的 prune 修剪 param 停止 translation invariance 平移不变性 suppress 抑制，镇压，阻止 bidirectional 双向 tabular 扁平的，列成表格的 revenue 收入，税收 latency 延迟 harmonic 和声的，谐和的，音乐般的 harmonic mean 调和平均数 harmonic series 调和级数 rote 死记硬背，生搬硬套 bid 出价，投标 leaderboard 排行榜，通栏广告 minor 较小的，次要的，轻微的 contaminated 受污染的，弄脏的 tradeoff 权衡，折中 ensemble learning 集成学习 decompose 分解，使腐烂 intrinsic 内在的，固有的 notable 显要的，值得注意的；非常成功的，令人尊敬的 camouflaged 伪装的 facilitate 促进，使便利 overlap 与……重叠，部分地相同；重叠的部分，互搭量 threshold 入口，门槛，开始，极限，临界值 conjecture 猜测，推测 within 在……之内 oversample:过采样 trade off:权衡，卖掉，折中方案 ultimately:最后，根本，基本上 robotics：机器人学 areial:空中的，航空的，空气的 underperform:表现不佳，工作不如预期 crucial:重要的，决定性的 high-resolution:高分辨率的 deploy:配置，展开，部署 barely:仅仅，勉强，几乎不 tumor:肿瘤，肿块 diagnosis:诊断 inspection:检查，视察 defect:缺陷，缺点，不足之处 annotate:注释，作注解 address:地址，编址 potentially:可能地，潜在地 imply:意味，暗示，隐含 diversity:多样性，差异 generalize:概括，推广，使……一般化 portion:部分 crop:裁剪 merge:合并 align:匹配，排列，对齐，对准 mask:掩码，掩膜 cascade:小瀑布，串联，级联 fuse:融合，熔接，熔化 computational:计算的 overhead:经常性费用，运营费用 fraction:分数，部分，小部分，稍微 schematic illustration:示意图 respect to:关于，考虑 validate:验证，确认，使生效 stochastic:随机的，猜测的 decay 衰退，衰减 coefficient 系数，率 explicitly 明确地，明白地 outline 大纲，概要 distillation:蒸馏 curvature:曲率 stochastic 随机 variance 差异，方差 spectrum 光谱，频谱；范围 neat 灵巧的，整洁的；优雅的，平滑的 heterogenerous 由很多种类组成的 intricate 复杂的，错综的 arbitrary 任意的，武断的 vanilla 香草，比较原始的 sketch 示意图 incarnation 化身，典型 waive 放弃，搁置 shrinkage 收缩，皱缩，缩水; 跌价; 抽缩 alleviate 缓解，减轻 de-facto 事实上 corpus 文集，语料库 unprecedented 前所未有的 inductive 归纳的 empirical 经验主义的 allergic 过敏的，反感的 pollen 花粉 badminton 羽毛球运动 pharmacy 药房 jasmine 茉莉 latent 潜在的，潜伏的，潜意识的 prepend 预先考虑 embedding 编码 alternating 交互的 interpolation 插入，篡改，添写 de-duplicate 删除重复数据 suite 一套，套件 geometric 几何图形的，几何的 intermediate 中间的 fine-tuning 微调 appendix 附录 warmup 预热 least-squares regression 最小二乘回归 on-the-fly 匆匆忙忙地；在空中；（计）运行中 literature 文献 outperform 胜过，做的比……好 substantially 实质上；大体上；充分地 standard deviation 标准差 co-training 协同训练 boost 促进，增加 overtake 赶上，压倒，突然来袭 plateau 趋于平稳，进入停滞期 vanish 消失 versus 与 saturate 饱和的 principal 最主要的 plausible 貌似可信的，花言巧语的；貌似真实的，貌似有理的 sinusoidal 正弦曲线的 degree 程度 analogous 类似的 preliminary 初步的 manual 手动的，手工的 insight 洞察力，领悟 exponentially 以指数方式的 holistically 整体论地 unidirectional 单向性的 incorporate 包含，吸收，体现；把……合并 alleviate 减轻 shallow 浅的，肤浅的 discriminate 区分，辨别 coarser 粗糙的 granularity 间隔尺寸，粒度 derived 导出的，衍生的，派生的 predecessor 前任，前辈;(被取代的)原有事物，前身 cloze adj. 完形的；填充测验法的 cloze task 完形填空 recipe 秘诀，处方 distinctive 有特色的，与众不同的 unambiguously 不含糊地，明白地 intuitively 直观地；直觉地 trivially 琐细地，平凡地，无能地 mitigate 使缓和，使减轻 monolingual 单语的；仅用一种语言的；仅懂一种语言的 procedure 程序，手续，步骤 degenerate 使退化，恶化 de-facto (法)实际上的 explicitly 显式地 reformulate 重新构造 ensemble 全体，总效果 nontrivial 重要的，显著的 obstacle 阻碍，障碍 notorious 臭名昭著的，声名狼藉的 vanishing/exploding gradients 梯度消失/梯度爆炸 hamper 妨碍，束缚 degradation 退化，降级，堕落 thoroughly 完全地，彻底地 counterpart 副本，配对物 feasible 可行的，可能的 akin to 类似于 generic 类的，属性的; 一般的; 不受商标保护的; [生]属的，类的 retrieval 检索 quantization 量化 partial differential equations 偏微分方程 auxiliary 辅助的，备用的 Concurrent 并发的，同时发生的 asymptotically 渐近地 counterintuitive 违反直觉的 perturbations [流]扰动，不安 trial 测试 curse 咒骂，诅咒 estimation 评估，评价，判断 surrogate 代理的 prominent 突出的，显著的，卓越的，杰出的 thes 命题，论文 recalibrate 重新校准 pruning 剪枝 proxy 代理人，代表权 compound 加重; 使复杂化; 混合;混合的 criteria 标准，条件 panoptic 全景的 controversial 有争议的 problematic 有疑问的，有问题的 contrastive 对比的 intuitive 直觉的; 凭直觉获知的; 直观的 preserve 保存；保护；维持；腌；禁猎 intractable 棘手的；难治的；倔强的；不听话的 pretext 借口，托辞; 假象，掩饰 permutation 排列，置换 discrimination 区别对待; 鉴别力; 区别 shuffle 洗牌; 曳脚而行; 搬移; 搁置，随手放 neatness 整洁，干净 blur 模糊 permutation 排列，置换 infrared 红外线的 attenuation 衰减，衰变 tricky 棘手的，难对付的 plethora 过多，过剩 deluge 泛滥，淹没 elaborate 精心制作的，详尽的 repurpose 改换意图，重新 assistive 辅助性的 eliminate 消除 duplicate 重复的 coordinate 坐标 refreshingly 清爽地，有精神地，令人耳目一新地 millisecond 毫秒 implicitly 隐式地 delimiter 分隔符 diverge 分歧，相异 remedy 解决方法，纠正方法 deviation 偏差 coarse 粗糙的 begnign 无有害的，认为无关紧要的 malicious 恶意的，怀恨的 rigorous 严格的 outlier 离群值 deliberate 故意的；深思熟虑的；从容的 susceptible 易受影响的；易感动的；容许…的 leverage 利用 kinda 有点，有几分 centroid 形心，重心 exclusively 专门地，唯一地 collision 碰撞，警告 hint 暗示，示意 stand-alone （计算机）独立运行的；（公司、组织）独立的 photometric distortion 光度失真 geometric 几何失真 hue 色调 saturation 饱和度 superimpose 叠加 adjacent 相邻的 incorporate 包含，吸收；体现；把……合并 mimic 模仿 tentative 初步的 tackle 处理 cortex 皮层 factorization 因子分解，因式分解 compatible 兼容的 substantially 实质上；大体上；充分地 explicitly 明确地；明白地 reformulate v. 再制订；换种方式说（或表达） nontrival 重要的 notorious 声名狼藉的，臭名昭著的 aggregated 聚合的，合计的 cardinality 基数 acquisition 获得物 obscured 遮挡 out-of-view 看不见的 precedent 先前的 intuitive 直观的 overhead 开销 efficacy 功效，效力 hierarchical 分层的 foeval 视网膜中心的 iteratively 迭代地 salient 重点的 modality 形式，形态 in a conditional fashion 有条件的方式 squeeze 挤压 excitation 激励 self-contained 独立的，设备齐全的，沉默寡言的 aggregate 集合，聚集 superscript 上标 an apples to apples comparison 比较两个相近的事物 thoroughly 彻底地，完全地 conjecture 推测，猜测 auxiliary 辅助的 parentheses 圆括号，插入成分 nest 嵌套 the best of both worlds 两全其美 incur 带来（成本、花费）等；招致，遭受 decimal 十进位的，小数的 timestamp 时间戳 clause 从句，分句；（法律文件的）条款 overlap 重叠 coalesce 合并，联合 rollup 归纳，卷曲，袅袅上升 coarse-to-fine 由粗到细，由繁到简 tic-tac-toe 井字棋，圈叉游戏 overlay 覆在……上面，覆盖 cached 贮藏起来，高速缓存 eigen 特征，固有的 tremendous 巨大的，极好的 versus （比赛或诉讼中）以……为对手，与……竞争；与……相对，与……相比 delineating 描述，描绘 primitive 原始的 recalibrating 重新调整 magenta 洋红色 cyan 青绿色 pentagon 五边形 hexagon 六边形 diamond 菱形 line chart 折线图 flip 翻转 alias 别名 incurring 招致，遭受 ellipse 椭圆 tile 平铺，瓷砖 diversity 多样性 discard 丢弃 adequately 充分地，足够地 flaw 缺陷，缺点 susceptible 易得病的，易受影响的；（人）易受感动的，易动感情的 caret 脱字符号，插入符号 teardown 拆卸 untangle 理清，整顿，解开……纠结 duration 持续时间 optical 光学的；（装置）光电的 decent 像样的，尚好的，得体的 inevitable 必然发生的，不可避免的 leverage n.影响力，杠杆作用adj.充分利用 overwhelm 压倒，压垮 degenerate 恶化，堕落，退化 elaborate 详细说明，复杂的 mitigate 减轻 severe 严重的，艰巨的 hamper 阻碍 induce 诱导，引诱 deteriorate 恶化，变坏 atomic 原子的，核能的","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"专业名词","slug":"专业名词","permalink":"http://pistachio0812.github.io/tags/%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D/"}],"author":"pistachio"},{"title":"Hello World","slug":"hello-world","date":"2022-03-11T08:11:46.667Z","updated":"2022-10-04T13:15:52.175Z","comments":true,"path":"zh-CN/hello-world/","permalink":"http://pistachio0812.github.io/zh-CN/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"Hexo博客搭建","slug":"Hexo博客搭建","permalink":"http://pistachio0812.github.io/categories/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"博客搭建","slug":"博客搭建","permalink":"http://pistachio0812.github.io/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"},{"name":"Hexo","slug":"Hexo","permalink":"http://pistachio0812.github.io/tags/Hexo/"}]}],"categories":[{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"vue","slug":"vue","permalink":"http://pistachio0812.github.io/categories/vue/"},{"name":"React","slug":"React","permalink":"http://pistachio0812.github.io/categories/React/"},{"name":"echarts","slug":"echarts","permalink":"http://pistachio0812.github.io/categories/echarts/"},{"name":"开发日记","slug":"开发日记","permalink":"http://pistachio0812.github.io/categories/%E5%BC%80%E5%8F%91%E6%97%A5%E8%AE%B0/"},{"name":"rss","slug":"rss","permalink":"http://pistachio0812.github.io/categories/rss/"},{"name":"github","slug":"github","permalink":"http://pistachio0812.github.io/categories/github/"},{"name":"typora","slug":"typora","permalink":"http://pistachio0812.github.io/categories/typora/"},{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"Hexo博客搭建","slug":"Hexo博客搭建","permalink":"http://pistachio0812.github.io/categories/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"},{"name":"ubuntu","slug":"ubuntu","permalink":"http://pistachio0812.github.io/categories/ubuntu/"},{"name":"pytorch","slug":"pytorch","permalink":"http://pistachio0812.github.io/categories/pytorch/"},{"name":"python","slug":"python","permalink":"http://pistachio0812.github.io/categories/python/"},{"name":"卷积神经网络","slug":"计算机视觉/卷积神经网络","permalink":"http://pistachio0812.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"Leetcode","slug":"Leetcode","permalink":"http://pistachio0812.github.io/categories/Leetcode/"},{"name":"leetcode","slug":"leetcode","permalink":"http://pistachio0812.github.io/categories/leetcode/"},{"name":"文学黑洞","slug":"文学黑洞","permalink":"http://pistachio0812.github.io/categories/%E6%96%87%E5%AD%A6%E9%BB%91%E6%B4%9E/"},{"name":"novel","slug":"novel","permalink":"http://pistachio0812.github.io/categories/novel/"},{"name":"C语言","slug":"C语言","permalink":"http://pistachio0812.github.io/categories/C%E8%AF%AD%E8%A8%80/"},{"name":"机器学习","slug":"机器学习","permalink":"http://pistachio0812.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"科研推荐","slug":"科研推荐","permalink":"http://pistachio0812.github.io/categories/%E7%A7%91%E7%A0%94%E6%8E%A8%E8%8D%90/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://pistachio0812.github.io/categories/tensorflow/"},{"name":"色彩搭配","slug":"色彩搭配","permalink":"http://pistachio0812.github.io/categories/%E8%89%B2%E5%BD%A9%E6%90%AD%E9%85%8D/"},{"name":"Linux","slug":"Linux","permalink":"http://pistachio0812.github.io/categories/Linux/"},{"name":"MySQL","slug":"MySQL","permalink":"http://pistachio0812.github.io/categories/MySQL/"}],"tags":[{"name":"tensorboard","slug":"tensorboard","permalink":"http://pistachio0812.github.io/tags/tensorboard/"},{"name":"目标检测","slug":"目标检测","permalink":"http://pistachio0812.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"SSD","slug":"SSD","permalink":"http://pistachio0812.github.io/tags/SSD/"},{"name":"文件转换","slug":"文件转换","permalink":"http://pistachio0812.github.io/tags/%E6%96%87%E4%BB%B6%E8%BD%AC%E6%8D%A2/"},{"name":"学习笔记","slug":"学习笔记","permalink":"http://pistachio0812.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"vue","slug":"vue","permalink":"http://pistachio0812.github.io/tags/vue/"},{"name":"React","slug":"React","permalink":"http://pistachio0812.github.io/tags/React/"},{"name":"echarts","slug":"echarts","permalink":"http://pistachio0812.github.io/tags/echarts/"},{"name":"微信小程序","slug":"微信小程序","permalink":"http://pistachio0812.github.io/tags/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"name":"logo","slug":"logo","permalink":"http://pistachio0812.github.io/tags/logo/"},{"name":"rss","slug":"rss","permalink":"http://pistachio0812.github.io/tags/rss/"},{"name":"CDN","slug":"CDN","permalink":"http://pistachio0812.github.io/tags/CDN/"},{"name":"基础概念","slug":"基础概念","permalink":"http://pistachio0812.github.io/tags/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"},{"name":"epoch&iteration","slug":"epoch-iteration","permalink":"http://pistachio0812.github.io/tags/epoch-iteration/"},{"name":"Hexo","slug":"Hexo","permalink":"http://pistachio0812.github.io/tags/Hexo/"},{"name":"open_graph","slug":"open-graph","permalink":"http://pistachio0812.github.io/tags/open-graph/"},{"name":"软件安装","slug":"软件安装","permalink":"http://pistachio0812.github.io/tags/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"},{"name":"论文笔记","slug":"论文笔记","permalink":"http://pistachio0812.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"warnings","slug":"warnings","permalink":"http://pistachio0812.github.io/tags/warnings/"},{"name":"grad-cam","slug":"grad-cam","permalink":"http://pistachio0812.github.io/tags/grad-cam/"},{"name":"可视化","slug":"可视化","permalink":"http://pistachio0812.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"zetane","slug":"zetane","permalink":"http://pistachio0812.github.io/tags/zetane/"},{"name":"matplotlib","slug":"matplotlib","permalink":"http://pistachio0812.github.io/tags/matplotlib/"},{"name":"函数库","slug":"函数库","permalink":"http://pistachio0812.github.io/tags/%E5%87%BD%E6%95%B0%E5%BA%93/"},{"name":"newaxis","slug":"newaxis","permalink":"http://pistachio0812.github.io/tags/newaxis/"},{"name":"输入与输出","slug":"输入与输出","permalink":"http://pistachio0812.github.io/tags/%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BA/"},{"name":"力扣题解","slug":"力扣题解","permalink":"http://pistachio0812.github.io/tags/%E5%8A%9B%E6%89%A3%E9%A2%98%E8%A7%A3/"},{"name":"relu","slug":"relu","permalink":"http://pistachio0812.github.io/tags/relu/"},{"name":"cumsum","slug":"cumsum","permalink":"http://pistachio0812.github.io/tags/cumsum/"},{"name":"contiguous","slug":"contiguous","permalink":"http://pistachio0812.github.io/tags/contiguous/"},{"name":"RFBNet","slug":"RFBNet","permalink":"http://pistachio0812.github.io/tags/RFBNet/"},{"name":"yolov3","slug":"yolov3","permalink":"http://pistachio0812.github.io/tags/yolov3/"},{"name":"darknet","slug":"darknet","permalink":"http://pistachio0812.github.io/tags/darknet/"},{"name":"FPS","slug":"FPS","permalink":"http://pistachio0812.github.io/tags/FPS/"},{"name":"Concat&add","slug":"Concat-add","permalink":"http://pistachio0812.github.io/tags/Concat-add/"},{"name":"今日故事","slug":"今日故事","permalink":"http://pistachio0812.github.io/tags/%E4%BB%8A%E6%97%A5%E6%95%85%E4%BA%8B/"},{"name":"volantis","slug":"volantis","permalink":"http://pistachio0812.github.io/tags/volantis/"},{"name":"损失函数","slug":"损失函数","permalink":"http://pistachio0812.github.io/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"name":"yolov5","slug":"yolov5","permalink":"http://pistachio0812.github.io/tags/yolov5/"},{"name":"resnet","slug":"resnet","permalink":"http://pistachio0812.github.io/tags/resnet/"},{"name":"博客提升","slug":"博客提升","permalink":"http://pistachio0812.github.io/tags/%E5%8D%9A%E5%AE%A2%E6%8F%90%E5%8D%87/"},{"name":"fork","slug":"fork","permalink":"http://pistachio0812.github.io/tags/fork/"},{"name":"感受野","slug":"感受野","permalink":"http://pistachio0812.github.io/tags/%E6%84%9F%E5%8F%97%E9%87%8E/"},{"name":"Inception","slug":"Inception","permalink":"http://pistachio0812.github.io/tags/Inception/"},{"name":"激活函数","slug":"激活函数","permalink":"http://pistachio0812.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"},{"name":"参数计算","slug":"参数计算","permalink":"http://pistachio0812.github.io/tags/%E5%8F%82%E6%95%B0%E8%AE%A1%E7%AE%97/"},{"name":"GAN","slug":"GAN","permalink":"http://pistachio0812.github.io/tags/GAN/"},{"name":"数据集","slug":"数据集","permalink":"http://pistachio0812.github.io/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"},{"name":"retinanet","slug":"retinanet","permalink":"http://pistachio0812.github.io/tags/retinanet/"},{"name":"transformer","slug":"transformer","permalink":"http://pistachio0812.github.io/tags/transformer/"},{"name":"faster-rcnn","slug":"faster-rcnn","permalink":"http://pistachio0812.github.io/tags/faster-rcnn/"},{"name":"文献推荐","slug":"文献推荐","permalink":"http://pistachio0812.github.io/tags/%E6%96%87%E7%8C%AE%E6%8E%A8%E8%8D%90/"},{"name":"数据处理","slug":"数据处理","permalink":"http://pistachio0812.github.io/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"},{"name":"pip换源","slug":"pip换源","permalink":"http://pistachio0812.github.io/tags/pip%E6%8D%A2%E6%BA%90/"},{"name":"Liunx使用教程","slug":"Liunx使用教程","permalink":"http://pistachio0812.github.io/tags/Liunx%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"},{"name":"使用指南","slug":"使用指南","permalink":"http://pistachio0812.github.io/tags/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"},{"name":"github","slug":"github","permalink":"http://pistachio0812.github.io/tags/github/"},{"name":"Attention","slug":"Attention","permalink":"http://pistachio0812.github.io/tags/Attention/"},{"name":"文献工具","slug":"文献工具","permalink":"http://pistachio0812.github.io/tags/%E6%96%87%E7%8C%AE%E5%B7%A5%E5%85%B7/"},{"name":"专业名词","slug":"专业名词","permalink":"http://pistachio0812.github.io/tags/%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D/"},{"name":"博客搭建","slug":"博客搭建","permalink":"http://pistachio0812.github.io/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"}]}