<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CNN参数计算</title>
    <url>/zh-CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在CNN的<emp>前向传播</emp>过程中，我们通常需要参数量，那么参数量的计算又当如何计算呢，不急，听我慢慢道来。</p>
<p>实例：</p>
<p>1.定义一个简单网络</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 该文件为test.py</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_out_size_same</span>(<span class="params">size, stride</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(math.ceil(<span class="built_in">float</span>(size) / <span class="built_in">float</span>(stride)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d=<span class="number">128</span>, input_shape=[<span class="number">64</span>, <span class="number">64</span>]</span>):</span><br><span class="line">        <span class="built_in">super</span>(discriminator, self).__init__()</span><br><span class="line">        s_h, s_w = input_shape[<span class="number">0</span>], input_shape[<span class="number">1</span>]</span><br><span class="line">        s_h2, s_w2 = conv_out_size_same(s_h, <span class="number">2</span>), conv_out_size_same(s_w, <span class="number">2</span>)</span><br><span class="line">        s_h4, s_w4 = conv_out_size_same(s_h2, <span class="number">2</span>), conv_out_size_same(s_w2, <span class="number">2</span>)</span><br><span class="line">        s_h8, s_w8 = conv_out_size_same(s_h4, <span class="number">2</span>), conv_out_size_same(s_w4, <span class="number">2</span>)</span><br><span class="line">        self.s_h16, self.s_w16 = conv_out_size_same(</span><br><span class="line">            s_h8, <span class="number">2</span>), conv_out_size_same(s_w8, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 64,64,3 -&gt; 32,32,128</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, d, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 32,32,128 -&gt; 16,16,256</span></span><br><span class="line">        self.conv2 = nn.Conv2d(d, d * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2_bn = nn.BatchNorm2d(d * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 16,16,256 -&gt; 8,8,512</span></span><br><span class="line">        self.conv3 = nn.Conv2d(d * <span class="number">2</span>, d * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv3_bn = nn.BatchNorm2d(d * <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 8,8,512 -&gt; 4,4,1024</span></span><br><span class="line">        self.conv4 = nn.Conv2d(d * <span class="number">4</span>, d * <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv4_bn = nn.BatchNorm2d(d * <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4,4,1024 -&gt; 1</span></span><br><span class="line">        self.linear = nn.Linear(self.s_h16 * self.s_w16 * d * <span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.leaky_relu = nn.LeakyReLU(negative_slope=<span class="number">0.2</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">weight_init</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                m.weight.data.normal_(<span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.normal_(<span class="number">0.1</span>, <span class="number">0.02</span>)</span><br><span class="line">                m.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                m.weight.data.normal_(<span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">                m.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        bs, _, _, _ = x.size()</span><br><span class="line">        <span class="comment"># (3, 64, 64)-&gt;(128, 32, 32)</span></span><br><span class="line">        x = self.leaky_relu(self.conv1(x))</span><br><span class="line">        <span class="comment"># (128, 32, 32)-&gt;(256, 16, 16)</span></span><br><span class="line">        x = self.leaky_relu(self.conv2_bn(self.conv2(x)))</span><br><span class="line">        <span class="comment"># (256, 16, 16)-&gt;(512, 8, 8)</span></span><br><span class="line">        x = self.leaky_relu(self.conv3_bn(self.conv3(x)))</span><br><span class="line">        <span class="comment"># (512, 8, 8)-&gt;(1024, 4, 4)</span></span><br><span class="line">        x = self.leaky_relu(self.conv4_bn(self.conv4(x)))</span><br><span class="line">        <span class="comment"># (1024, 4, 4)-&gt;(bs, 16*1024)</span></span><br><span class="line">        x = x.view([bs, -<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># (bs, 16*1024)-&gt;(bs, 1)</span></span><br><span class="line">        x = self.sigmoid(self.linear(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x.squeeze()</span><br></pre></td></tr></tbody></table></figure>
<p>2.调用summary()函数计算参数以及输出</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> test <span class="keyword">import</span> discriminator</span><br><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">    m = discriminator(d=<span class="number">128</span>, input_shape=[<span class="number">64</span>, <span class="number">64</span>]).to(device)</span><br><span class="line">    summary(m, input_size=(<span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>))</span><br></pre></td></tr></tbody></table></figure>
<p>结果如下：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line"><span class="section">        Layer (type)               Output Shape         Param #</span></span><br><span class="line"><span class="section">================================================================</span></span><br><span class="line"><span class="code">            Conv2d-1          [-1, 128, 32, 32]           6,272</span></span><br><span class="line"><span class="code">         LeakyReLU-2          [-1, 128, 32, 32]               0</span></span><br><span class="line"><span class="code">            Conv2d-3          [-1, 256, 16, 16]         524,544</span></span><br><span class="line"><span class="code">       BatchNorm2d-4          [-1, 256, 16, 16]             512</span></span><br><span class="line"><span class="code">         LeakyReLU-5          [-1, 256, 16, 16]               0</span></span><br><span class="line"><span class="code">            Conv2d-6            [-1, 512, 8, 8]       2,097,664</span></span><br><span class="line"><span class="code">       BatchNorm2d-7            [-1, 512, 8, 8]           1,024</span></span><br><span class="line"><span class="code">         LeakyReLU-8            [-1, 512, 8, 8]               0</span></span><br><span class="line"><span class="code">            Conv2d-9           [-1, 1024, 4, 4]       8,389,632</span></span><br><span class="line"><span class="code">      BatchNorm2d-10           [-1, 1024, 4, 4]           2,048</span></span><br><span class="line"><span class="code">        LeakyReLU-11           [-1, 1024, 4, 4]               0</span></span><br><span class="line"><span class="code">           Linear-12                    [-1, 1]          16,385</span></span><br><span class="line"><span class="code">          Sigmoid-13                    [-1, 1]               0</span></span><br><span class="line"><span class="code">================================================================</span></span><br><span class="line"><span class="code">Total params: 11,038,081</span></span><br><span class="line"><span class="code">Trainable params: 11,038,081</span></span><br><span class="line"><span class="code">Non-trainable params: 0</span></span><br><span class="line"><span class="code">----------------------------------------------------------------</span></span><br><span class="line"><span class="code">Input size (MB): 0.05</span></span><br><span class="line"><span class="code">Forward/backward pass size (MB): 4.63</span></span><br><span class="line"><span class="code">Params size (MB): 42.11</span></span><br><span class="line"><span class="code">Estimated Total Size (MB): 46.78</span></span><br><span class="line"><span class="code">----------------------------------------------------------------</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">Process finished with exit code 0</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>1.卷积（Conv)</p>
<p>以Conv2d-1为例：</p>
<p>变化：（3, 64, 64)-&gt;(128, 32, 32)</p>
<p>计算方法：</p>
<p><style>.frqigwnvjiqs{}</style><img src="/zh-CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/image-20220421212236498.png" class="lazyload" data-srcset="/zh-CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/image-20220421212236498.png" srcset="data:image/png;base64,666" class="frqigwnvjiqs lazyload"></p>
<p>其中bias等于输出通道数</p>
<p>因此：params=128×(4×4×3)+128=6272</p>
<p>2.激活函数（Activation)</p>
<p>不产生参数</p>
<p>3.正则化（BN)</p>
<p>以BatchNorm2d-4为例：</p>
<p>（256，16， 16)-&gt;(256， 16， 16 )</p>
<p>计算方法：</p>
<p><style>.hboxcoexrprc{}</style><img src="/zh-CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/image-20220421212929933.png" class="lazyload" data-srcset="/zh-CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/image-20220421212929933.png" srcset="data:image/png;base64,666" class="hboxcoexrprc lazyload"></p>
<p>因此：params=256×2=512</p>
<p>4.全连接（FC)</p>
<p>以Linear-12为例：</p>
<p>（1024， 4， 4）-&gt;(1)</p>
<p>计算方法：</p>
<p><style>.fxklikgoeixn{}</style><img src="/zh-CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/image-20220421213603964.png" class="lazyload" data-srcset="/zh-CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/image-20220421213603964.png" srcset="data:image/png;base64,666" class="fxklikgoeixn lazyload"></p>
<p>其中：bias等于输出通道数</p>
<p>因此：params=1024×4×4×1+1=16385</p>
<p>5.池化层（pooling)</p>
<p>不产生参数</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>参数计算</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN输入与输出的关系</title>
    <url>/zh-CN/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>原文链接：<a href="https://www.zhihu.com/question/56688854">卷积神经网络里输入图像大小何时是固定，何时是任意</a></p>
<p>相关论文链接：</p>
<p>1.<a href="https://arxiv.org/abs/1411.4038">Fully Convolutional Networks for Semantic Segmentation</a></p>
<p>2.<a href="https://arxiv.org/abs/1312.4400">Network In Network </a></p>
<p>3.<a href="https://arxiv.53yu.com/abs/1712.03452">SPP-Net: Deep absolute pose regression with synthetic views</a></p>
<h2 id="卷积神经网络里输入图像大小何时是固定，何时是任意"><a href="#卷积神经网络里输入图像大小何时是固定，何时是任意" class="headerlink" title="卷积神经网络里输入图像大小何时是固定，何时是任意"></a>卷积神经网络里输入图像大小何时是固定，何时是任意</h2><p>典型的CNN架构如下：</p>
<p><style>.bxdhbmtpvvrf{}</style><img src="/zh-CN/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/v2-14d0b66fd755750e48b08cfd1b6bc2e9_r.jpg" class="lazyload" data-srcset="/zh-CN/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/v2-14d0b66fd755750e48b08cfd1b6bc2e9_r.jpg" srcset="data:image/png;base64,666" class="bxdhbmtpvvrf lazyload"></p>
<p>也就是<a href="https://www.zhihu.com/search?q=卷积层&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A463698542}">卷积层</a>（卷积+非线性激活）+<a href="https://www.zhihu.com/search?q=池化层&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A463698542}">池化层</a>+全连接层+分类层。其中，卷积层、池化层、<a href="https://www.zhihu.com/search?q=分类层&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A463698542}">分类层</a>其实都不在意图像大小，但是全连接层有问题。</p>
<p>一般而言，全连接层的一个神经元对应一个输入。换句话说，全连接层要求固定的输入维度。而不同大小的图像，卷积模块（卷积+非线性激活+池化）输出的特征映射维度是不一样的。因此，从这个意义上说，因为有全连接层存在，决定了输入的图像的大小必须是固定的。也就是你提到的：</p>
<blockquote>
<p>如果神经网络里不仅仅只有卷积层，还有全连接层，那么输入的图像的大小必须是固定的。</p>
</blockquote>
<p>既然是全连接层的限制，那么，如果我们去掉全连接层，岂不就可以支持任意大小的输入图像了？也就是你提到的：</p>
<blockquote>
<p>如果一个神经网络里面只有卷积层，那么我输入的图像大小是可以任意的</p>
</blockquote>
<p>这正是<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1411.4038">Jonathan Long</a>提出的<strong>FCN（Fully Convolutional Networks，全卷积网络）</strong>背后的思路，<strong>用卷积层替换全连接层</strong>。</p>
<p><style>.lkloixjfhlec{}</style><img src="/zh-CN/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/v2-0bd8d5c23e469f40cc9e7847d5d6491b_r.jpg" class="lazyload" data-srcset="/zh-CN/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/v2-0bd8d5c23e469f40cc9e7847d5d6491b_r.jpg" srcset="data:image/png;base64,666" class="lkloixjfhlec lazyload"></p>
<p>当然，除了卷积层外，还可以用别的层替换全连接层。比如用<strong>全局平均池化（Global Average Pooling）层</strong>替换全连接层。</p>
<p><style>.ogwlotxcujfm{}</style><img src="/zh-CN/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/v2-4d6368c19a126926541a157da37f4742_r.jpg" class="lazyload" data-srcset="/zh-CN/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/v2-4d6368c19a126926541a157da37f4742_r.jpg" srcset="data:image/png;base64,666" class="ogwlotxcujfm lazyload"></p>
<p>全局平均池化最早是由<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1312.4400">Min Lin</a>等在<strong>网中网（Network In Network）</strong>里提出的<strong>，</strong> 其动机是为了缓解全连接层可能导致的过拟合问题。但因为全局平均池化将任意<code>h * w * d</code>的张量（特征映射）转换为<code>1 * 1 * d</code>的张量，因此碰巧可以自动适应不同尺寸的输入图像。之后的Inception-V3等架构借鉴了这一想法，用全局平均池化层代替全连接层，以适应不同尺寸的输入图像。</p>
<p>不过，也不是说只要有全连接层，输入的图像大小就一定要固定。之前提到过，固定输入图像大小的原因是：全连接层要求固定的输入维度。那么，除了像上面这样干脆用卷积层替换全连接层，还有一种思路，就是在卷积模块和全连接层之间加一个中间层，整理一下卷积模块的输出，保证不管输入图像大小怎么变，传给全连接层的始终是固定维数的输入。</p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1406.4729">Kaiming He</a>等提出的<strong>SPP（Spatial Pyramid Pooling，空间金字塔池化）</strong>就是做这个的。</p>
<p><style>.skgrgiuizxup{}</style><img src="/zh-CN/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/v2-a3f23245d7a2f41554b27e38755927c8_r.jpg" class="lazyload" data-srcset="/zh-CN/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/v2-a3f23245d7a2f41554b27e38755927c8_r.jpg" srcset="data:image/png;base64,666" class="skgrgiuizxup lazyload"></p>
<p>上图示意了引入SPP前后的不同流程。上为传统的CNN架构，图像（image）经过裁剪或拉伸（crop/warp）统一尺寸，再传给卷积层（conv layers）；下为应用了SPP层后的架构，图像直接传给卷积层，然后经过SPP处理，统一维度，再传给全连接层（fc layers）。</p>
<p><style>.wfulvgnsgyml{zoom:50%;}</style><img src="/zh-CN/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/v2-53cfe32119cfeba1e2bc73a18015f450_r.jpg" class="lazyload" data-srcset="/zh-CN/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/CNN%E8%BE%93%E5%85%A5%E8%AF%A6%E8%A7%A3/v2-53cfe32119cfeba1e2bc73a18015f450_r.jpg" srcset="data:image/png;base64,666" class="wfulvgnsgyml lazyload"></p>
<p>上为SPP层示意图，当中的大方框表示SPP层。SPP层的输入是卷积模块输出的任意尺寸的特征映射，SPP层的输出是固定长度的表示，也就是说，SPP层将固定维度的向量传给之后的全连接层。其中，SPP层使用很多<strong>spatial bins（空间箱）</strong>对特征映射应用池化操作（比如最大池化）。空间箱的个数是固定的，大小与特征映射的尺寸（对应输入图像的尺寸）成比例，这就保证了SPP层的输出向量的维数是固定的。</p>
<p>个人总结如下：</p>
<p>1.当卷积神经网络中出现了全连接时，由于网络模型固定，因此倒推过去输入尺寸必然固定</p>
<p>2.全局平均池化可以将任意h×w×c的张量变为1×1×c的张量，因此可以输入任意尺寸</p>
<p>3.SPPNet在卷积层和全连接层之间设置中间层，将全连接层前的输入设置为满足要求，因此输入可以任何尺寸</p>
<p>4.论文中较为常见的是第一种</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>输入与输出</tag>
      </tags>
  </entry>
  <entry>
    <title>CABNet论文笔记</title>
    <url>/zh-CN/CABNet/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://sci-hub.et-fine.com/10.1109/tcyb.2020.3004636">Sci-Hub | Context-Aware Block Net for Small Object Detection | 10.1109/TCYB.2020.3004636 (et-fine.com)</a></p>
<p>今天看的这篇论文是吕培教授发表在IEEE TRANS上的<code>Context-Aware Block Net for Small Object Detection</code>,ok,开始进入正文</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>CABNet能够捕捉基本的视觉模式，也可以捕捉小物体的语义信息。看到这个，其实我也有点懵，基本的视觉模式是啥，搞不懂，这个得好好查查，还有就是如何捕捉的呢，那得继续看论文喽，毕竟这只是摘要</p>
<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>文中引入文献，讲了几个近些年提升小目标检测性能的方法，第一种：用高分辨率特征图做预测，这样做是因为这些特征图能保留小目标的细节信息，如下图a所示，作者认为不可取，因为高分辨率特征图包含的上下文信息较少，会影响检测精度。第二种：如下图b所示，带有跳跃连接的自上而下的结构，能够在所有尺度上构建高级语义信息。这些引入了额外的上下文信息给高分辨率特征图，因此，提高了检测精度，然而，作者认为还是不可取，因为在训练和测试的时候计算开销很大，如果网络中使用了下采样，就会丢失小目标的信息，这是不可恢复的。</p>
<p><style>.mrqnnvaubewn{}</style><img src="/zh-CN/CABNet/CABNet/image-20220927153422183.png" class="lazyload" data-srcset="/zh-CN/CABNet/CABNet/image-20220927153422183.png" srcset="data:image/png;base64,666" class="mrqnnvaubewn lazyload" alt="image-20220927153422183"></p>
<p>有文献指出，对于小目标检测，高分辨率特征图的表征更适合准确定位目标，那为什么不保持更高分辨率的特征图（64×64）来检测小目标呢，如下图所示，主要的原因高分辨率特征图底层的神经元产生的感受野是有限的，也就是说，在特征图上包含的上下文信息也是受到限制的。</p>
<p><style>.zxqkxfehnmto{}</style><img src="/zh-CN/CABNet/CABNet/image-20220927154959945.png" class="lazyload" data-srcset="/zh-CN/CABNet/CABNet/image-20220927154959945.png" srcset="data:image/png;base64,666" class="zxqkxfehnmto lazyload" alt="image-20220927154959945"></p>
<p>因此，如下图c所示，在CABNet上，只会在骨干网络（文中指VGG16)上下采样几次，目的就是保持小目标的空间信息（未完。。。）</p>
<p><style>.bvtjpmsbyzzz{zoom:80%;}</style><img src="/zh-CN/CABNet/CABNet/image-20220927155113203.png" class="lazyload" data-srcset="/zh-CN/CABNet/CABNet/image-20220927155113203.png" srcset="data:image/png;base64,666" class="bvtjpmsbyzzz lazyload" alt="image-20220927155113203"></p>
<h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>A目标检测</p>
<p>介绍了很多文献，总结下来就是小目标的检测性能之所以差劲，关键原因就是在一个深的网络里过度下采样了。</p>
<p>B小目标检测</p>
<p>由于内容太杂，简单说一下吧，里面介绍了前人的一些工作，比如，有人通过缩小小物体和大物体的表征差异来提升小目标的检测性能；有人通过简单的粘贴复制小目标多次提升性能；有人通过增加输入图片的大小来提升小目标的检测性能。</p>
<p>C空洞卷积</p>
<p>使用空洞卷积是能够丰富特征图的语义信息的，文章中提到了RFBNet使用空洞卷积保证了特征的可区分性和鲁棒性，受此启发，CAB通过不同扩张率的金字塔扩张卷积来包含多级上下文信息，如下图所示。</p>
<p><style>.ockjdimgmxxg{zoom:80%;}</style><img src="/zh-CN/CABNet/CABNet/image-20220927160946040.png" class="lazyload" data-srcset="/zh-CN/CABNet/CABNet/image-20220927160946040.png" srcset="data:image/png;base64,666" class="ockjdimgmxxg lazyload" alt="image-20220927160946040"></p>
<h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><h4 id="上下文模块"><a href="#上下文模块" class="headerlink" title="上下文模块"></a>上下文模块</h4>]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>C语言基础学习</title>
    <url>/zh-CN/C%E8%AF%AD%E8%A8%80/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="参考博文"><a href="#参考博文" class="headerlink" title="参考博文"></a>参考博文</h2><p>1.<a href="https://www.runoob.com/cprogramming">菜鸟教程C语言教程</a></p>
<h2 id="编译-执行C程序"><a href="#编译-执行C程序" class="headerlink" title="编译/执行C程序"></a>编译/执行C程序</h2><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">{</span><br><span class="line">    <span class="comment">/* 我的第一个 C 程序 */</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Hello, World! \n"</span>);</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>实例解析：</p>
<ul>
<li>所有的 C 语言程序都需要包含 <strong>main()</strong> 函数。 代码从 <strong>main()</strong> 函数开始执行。</li>
<li><strong>/* … */</strong> 用于注释说明。</li>
<li><strong>printf()</strong> 用于格式化输出到屏幕。<strong>printf()</strong> 函数在 <strong>“stdio.h”</strong> 头文件中声明。</li>
<li><strong>stdio.h</strong> 是一个头文件 (标准输入输出头文件) , <strong>#include</strong> 是一个预处理命令，用来引入头文件。 当编译器遇到 <strong>printf()</strong> 函数时，如果没有找到 <strong>stdio.h</strong> 头文件，会发生编译错误。</li>
<li><strong>return 0;</strong> 语句用于表示退出程序。</li>
</ul>
<h2 id="C简介"><a href="#C简介" class="headerlink" title="C简介"></a>C简介</h2><p>C 语言是一种通用的高级语言，最初是由丹尼斯·里奇在贝尔实验室为开发 UNIX 操作系统而设计的。C 语言最开始是于 1972 年在 DEC PDP-11 计算机上被首次实现。</p>
<p>在 1978 年，布莱恩·柯林汉（Brian Kernighan）和丹尼斯·里奇（Dennis Ritchie）制作了 C 的第一个公开可用的描述，现在被称为 K&amp;R 标准。</p>
<p>UNIX 操作系统，C编译器，和几乎所有的 UNIX 应用程序都是用 C 语言编写的。由于各种原因，C 语言现在已经成为一种广泛使用的专业语言。</p>
<ul>
<li>易于学习。</li>
<li>结构化语言。</li>
<li>它产生高效率的程序。</li>
<li>它可以处理底层的活动。</li>
</ul>
<h3 id="关于C"><a href="#关于C" class="headerlink" title="关于C"></a>关于C</h3><ul>
<li><p>C 语言是为了编写 UNIX 操作系统而被发明的。</p>
</li>
<li><p>C 语言是以 B 语言为基础的，B 语言大概是在 1970 年被引进的。</p>
</li>
<li><p>C 语言标准是于 1988 年由美国国家标准协会（ANSI，全称 American National Standard Institute）制定的。</p>
</li>
<li><p>截至 1973 年，UNIX 操作系统完全使用 C 语言编写。</p>
</li>
<li><p>目前，C 语言是最广泛使用的系统程序设计语言。</p>
</li>
<li><p>大多数先进的软件都是使用 C 语言实现的。</p>
</li>
<li><p>当今最流行的 Linux 操作系统和 RDBMS（Relational Database Management System：关系数据库管理系统） MySQL 都是使用 C 语言编写的。</p>
</li>
</ul>
<h3 id="为什么要使用C"><a href="#为什么要使用C" class="headerlink" title="为什么要使用C"></a>为什么要使用C</h3><p>  C 语言最初是用于系统开发工作，特别是组成操作系统的程序。由于 C 语言所产生的代码运行速度与汇编语言编写的代码运行速度几乎一样，所以采用 C 语言作为系统开发语言。下面列举几个使用 C 的实例：操作系统、语言编译器、汇编器、文本编辑器、打印机、网络驱动器、现代程序、数据库、语言解释器、实体工具。</p>
<h3 id="C程序"><a href="#C程序" class="headerlink" title="C程序"></a>C程序</h3><p>一个 C 语言程序，可以是 3 行，也可以是数百万行，它可以写在一个或多个扩展名为 <strong>“.c”</strong> 的文本文件中，例如，<em>hello.c</em>。您可以使用 <strong>“vi”</strong>、<strong>“vim”</strong> 或任何其他文本编辑器来编写您的 C 语言程序。</p>
<p>本教程假定您已经知道如何编辑一个文本文件，以及如何在程序文件中编写源代码。</p>
<h3 id="C11"><a href="#C11" class="headerlink" title="C11"></a>C11</h3><p>C11（也被称为C1X）指ISO标准ISO/IEC 9899:2011，是当前最新的C语言标准。在它之前的C语言标准为C99。</p>
<h3 id="新特性"><a href="#新特性" class="headerlink" title="新特性"></a>新特性</h3><ul>
<li>对齐处理（Alignment）的标准化（包括_Alignas标志符，alignof运算符，aligned_alloc函数以及<stdalign.h>头文件）。</stdalign.h></li>
<li><em>Noreturn 函数标记，类似于 gcc 的 <em>_attribute</em></em>((noreturn))。</li>
<li>_Generic 关键字。</li>
<li>多线程（Multithreading）支持，包括：<br>_Thread_local存储类型标识符，<threads.h>头文件，里面包含了线程的创建和管理函数。<br>_Atomic类型修饰符和<stdatomic.h>头文件。</stdatomic.h></threads.h></li>
<li>增强的Unicode的支持。基于C Unicode技术报告ISO/IEC TR 19769:2004，增强了对Unicode的支持。包括为UTF-16/UTF-32编码增加了char16_t和char32_t数据类型，提供了包含unicode字符串转换函数的头文件<uchar.h>。</uchar.h></li>
<li>删除了 gets() 函数，使用一个新的更安全的函数gets_s()替代。</li>
<li>增加了边界检查函数接口，定义了新的安全的函数，例如 fopen_s()，strcat_s() 等等。</li>
<li>增加了更多浮点处理宏(宏)。</li>
<li>匿名结构体/联合体支持。这个在gcc早已存在，C11将其引入标准。</li>
<li>静态断言（Static assertions），_Static_assert()，在解释 #if 和 #error 之后被处理。</li>
<li>新的 fopen() 模式，(“…x”)。类似 POSIX 中的 O_CREAT|O_EXCL，在文件锁中比较常用。</li>
<li>新增 quick_exit() 函数作为第三种终止程序的方式。当 exit()失败时可以做最少的清理工作。</li>
</ul>
<h2 id="C环境设置"><a href="#C环境设置" class="headerlink" title="C环境设置"></a>C环境设置</h2><h2 id=""><a href="#" class="headerlink" title="#"></a>#</h2>]]></content>
      <categories>
        <category>C语言</category>
      </categories>
      <tags>
        <tag>函数库</tag>
      </tags>
  </entry>
  <entry>
    <title>DCGAN论文笔记</title>
    <url>/zh-CN/DCGAN/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/weixin_44791964/article/details/110475425">pytorch搭建DCGAN</a></p>
<p>论文地址：<a href="https://arxiv.53yu.com/pdf/1511.06434.pdf">https://arxiv.53yu.com/pdf/1511.06434.pdf</a></p>
<p>论文源码：略</p>
<p>文章引用源码：<a href="https://github.com/bubbliiiing/dcgan-pytorch">https://github.com/bubbliiiing/dcgan-pytorch</a></p>
<h2 id="网络构建"><a href="#网络构建" class="headerlink" title="网络构建"></a>网络构建</h2><h3 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h3><p>DCGAN的全称是Deep Convolutional Generative Adversarial Networks，即深度卷积对抗生成网络。</p>
<p>它是由Alec Radford在论文Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks中提出的。</p>
<p>实际上它就是在GAN的基础上增加深度卷积网络结构。</p>
<p>论文中给出的DCGAN结构如图所示。其使用反卷积将特征层的高宽不断扩大，整体结构看起来像普通神经网络的逆过程。</p>
<p><style>.woxrujdfxbxv{}</style><img src="/zh-CN/DCGAN/DCGAN/image-20220421201415525.png" class="lazyload" data-srcset="/zh-CN/DCGAN/DCGAN/image-20220421201415525.png" srcset="data:image/png;base64,666" class="woxrujdfxbxv lazyload"></p>
<h3 id="生成网络的构建"><a href="#生成网络的构建" class="headerlink" title="生成网络的构建"></a>生成网络的构建</h3><p>对于生成网络来讲，它的目的是生成假图片，它的输入是正态分布随机数。输出是假图片。</p>
<p>在GAN当中，我们将这个正态分布随机数长度定义为100，在经过处理后，我们会得到一个(64,64,3)的假图片。</p>
<p>在处理过程中，我们会使用到反卷积，反卷积的概念是相对于正常卷积的，在正常卷积下，我们的特征层的高宽会不断被压缩；在反卷积下，我们的特征层的高宽会不断变大。</p>
<p><style>.iudbqrarblhn{}</style><img src="/zh-CN/DCGAN/DCGAN/20201202124131662.png" class="lazyload" data-srcset="/zh-CN/DCGAN/DCGAN/20201202124131662.png" srcset="data:image/png;base64,666" class="iudbqrarblhn lazyload"></p>
<p>在DCGAN的生成网络中，我们首先利用一个全连接，将输入长条全连接到16,384（4x4x1024）这样一个长度上，这样我们才可以对这个全连接的结果进行reshape，使它变成(4,4,1024)的特征层。</p>
<p>在获得这个特征层之后，我们就可以利用反卷积进行上采样了。</p>
<p>在每次反卷积后，特征层的高和宽会变为原来的两倍，在四次反卷积后，我们特征层的shape变化是这样的：( 4 , 4 , 1024 ) − &gt; ( 8 , 8 , 512 ) − &gt; ( 16 , 16 , 256 ) − &gt; ( 32 , 32 , 128 ) − &gt; ( 64 , 64 , 3 )</p>
<p>此时我们再进行一次tanh激活函数，我们就可以获得一张假图片了。<br>实现代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果stride=2,就是宽高减半，下采样操作</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_out_size_same</span>(<span class="params">size, stride</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(math.ceil(<span class="built_in">float</span>(size) / <span class="built_in">float</span>(stride)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 反卷积公式：H_out=(H_in −1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d=<span class="number">128</span>, input_shape=[<span class="number">64</span>, <span class="number">64</span>]</span>):</span><br><span class="line">        <span class="built_in">super</span>(generator, self).__init__()</span><br><span class="line">        <span class="comment"># 64, 64</span></span><br><span class="line">        s_h, s_w = input_shape[<span class="number">0</span>], input_shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 32, 32</span></span><br><span class="line">        s_h2, s_w2 = conv_out_size_same(s_h, <span class="number">2</span>), conv_out_size_same(s_w, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 16, 16</span></span><br><span class="line">        s_h4, s_w4 = conv_out_size_same(s_h2, <span class="number">2</span>), conv_out_size_same(s_w2, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 8, 8</span></span><br><span class="line">        s_h8, s_w8 = conv_out_size_same(s_h4, <span class="number">2</span>), conv_out_size_same(s_w4, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 4, 4</span></span><br><span class="line">        self.s_h16, self.s_w16 = conv_out_size_same(s_h8, <span class="number">2</span>), conv_out_size_same(s_w8, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># (bs, 100)-&gt; (bs, 4*4*128*8)</span></span><br><span class="line">        self.linear = nn.Linear(<span class="number">100</span>, self.s_h16 * self.s_w16 * d * <span class="number">8</span>)</span><br><span class="line">        self.linear_bn = nn.BatchNorm2d(d * <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (bs, 1024, 4, 4)-&gt;(bs, 512, 8, 8)</span></span><br><span class="line">        self.deconv1 = nn.ConvTranspose2d(d * <span class="number">8</span>, d * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.deconv1_bn = nn.BatchNorm2d(d * <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (bs, 512, 8, 8)-&gt;(bs, 256, 16, 16)</span></span><br><span class="line">        self.deconv2 = nn.ConvTranspose2d(d * <span class="number">4</span>, d * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.deconv2_bn = nn.BatchNorm2d(d * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (bs, 256, 16, 16)-&gt;(bs, 128, 32, 32)</span></span><br><span class="line">        self.deconv3 = nn.ConvTranspose2d(d * <span class="number">2</span>, d, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.deconv3_bn = nn.BatchNorm2d(d)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (bs, 128, 8, 8)-&gt;(bs, 3, 64, 64)</span></span><br><span class="line">        self.deconv4 = nn.ConvTranspose2d(d, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">weight_init</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.ConvTranspose2d):</span><br><span class="line">                m.weight.data.normal_(<span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">                m.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.normal_(<span class="number">0.1</span>, <span class="number">0.02</span>)</span><br><span class="line">                m.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                m.weight.data.normal_(<span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">                m.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># (bs, 100)</span></span><br><span class="line">        bs, _ = x.size()</span><br><span class="line">        <span class="comment"># (bs, 16*1024)</span></span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="comment"># (bs, 1024, 4, 4)</span></span><br><span class="line">        x = x.view([bs, -<span class="number">1</span>, self.s_h16, self.s_w16])</span><br><span class="line">        x = self.relu(self.linear_bn(x))</span><br><span class="line">        <span class="comment"># (bs, 1024, 4, 4)-&gt;(bs, 512, 8, 8)</span></span><br><span class="line">        x = self.relu(self.deconv1_bn(self.deconv1(x)))</span><br><span class="line">        <span class="comment"># (bs, 512, 8, 8)-&gt;(bs, 256, 16, 16)</span></span><br><span class="line">        x = self.relu(self.deconv2_bn(self.deconv2(x)))</span><br><span class="line">        <span class="comment"># (bs, 256, 16, 16)-&gt;(bs, 128, 32, 32)</span></span><br><span class="line">        x = self.relu(self.deconv3_bn(self.deconv3(x)))</span><br><span class="line">        <span class="comment"># (bs, 128, 32, 32)-&gt;(bs, 3, 64, 64)</span></span><br><span class="line">        x = torch.tanh(self.deconv4(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>
<h3 id="判别网络的构建"><a href="#判别网络的构建" class="headerlink" title="判别网络的构建"></a>判别网络的构建</h3><p>对于生成网络来讲，它的目的是生成假图片，它的输入是正态分布随机数。输出是假图片。</p>
<p>对于判别网络来讲，它的目的是判断输入图片的真假，它的输入是图片，输出是判断结果。</p>
<p>判断结果处于0-1之间，利用接近1代表判断为真图片，接近0代表判断为假图片。</p>
<p>判别网络的构建和普通卷积网络差距不大，都是不断的卷积对图片进行下采用，在多次卷积后，最终接一次全连接判断结果。</p>
<p>实现代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_out_size_same</span>(<span class="params">size, stride</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(math.ceil(<span class="built_in">float</span>(size) / <span class="built_in">float</span>(stride)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d=<span class="number">128</span>, input_shape=[<span class="number">64</span>, <span class="number">64</span>]</span>):</span><br><span class="line">        <span class="built_in">super</span>(discriminator, self).__init__()</span><br><span class="line">        s_h, s_w = input_shape[<span class="number">0</span>], input_shape[<span class="number">1</span>]</span><br><span class="line">        s_h2, s_w2 = conv_out_size_same(s_h, <span class="number">2</span>), conv_out_size_same(s_w, <span class="number">2</span>)</span><br><span class="line">        s_h4, s_w4 = conv_out_size_same(s_h2, <span class="number">2</span>), conv_out_size_same(s_w2, <span class="number">2</span>)</span><br><span class="line">        s_h8, s_w8 = conv_out_size_same(s_h4, <span class="number">2</span>), conv_out_size_same(s_w4, <span class="number">2</span>)</span><br><span class="line">        self.s_h16, self.s_w16 = conv_out_size_same(</span><br><span class="line">            s_h8, <span class="number">2</span>), conv_out_size_same(s_w8, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 64,64,3 -&gt; 32,32,128</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, d, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 32,32,128 -&gt; 16,16,256</span></span><br><span class="line">        self.conv2 = nn.Conv2d(d, d * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2_bn = nn.BatchNorm2d(d * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 16,16,256 -&gt; 8,8,512</span></span><br><span class="line">        self.conv3 = nn.Conv2d(d * <span class="number">2</span>, d * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv3_bn = nn.BatchNorm2d(d * <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 8,8,512 -&gt; 4,4,1024</span></span><br><span class="line">        self.conv4 = nn.Conv2d(d * <span class="number">4</span>, d * <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv4_bn = nn.BatchNorm2d(d * <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4,4,1024 -&gt; 1</span></span><br><span class="line">        self.linear = nn.Linear(self.s_h16 * self.s_w16 * d * <span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.leaky_relu = nn.LeakyReLU(negative_slope=<span class="number">0.2</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">weight_init</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                m.weight.data.normal_(<span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.normal_(<span class="number">0.1</span>, <span class="number">0.02</span>)</span><br><span class="line">                m.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                m.weight.data.normal_(<span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">                m.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        bs, _, _, _ = x.size()</span><br><span class="line">        <span class="comment"># (3, 64, 64)-&gt;(128, 32, 32)</span></span><br><span class="line">        x = self.leaky_relu(self.conv1(x))</span><br><span class="line">        <span class="comment"># (128, 32, 32)-&gt;(256, 16, 16)</span></span><br><span class="line">        x = self.leaky_relu(self.conv2_bn(self.conv2(x)))</span><br><span class="line">        <span class="comment"># (256, 16, 16)-&gt;(512, 8, 8)</span></span><br><span class="line">        x = self.leaky_relu(self.conv3_bn(self.conv3(x)))</span><br><span class="line">        <span class="comment"># (512, 8, 8)-&gt;(1024, 4, 4)</span></span><br><span class="line">        x = self.leaky_relu(self.conv4_bn(self.conv4(x)))</span><br><span class="line">        <span class="comment"># (1024, 4, 4)-&gt;(bs, 16*1024)</span></span><br><span class="line">        x = x.view([bs, -<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># (bs, 16*1024)-&gt;(bs, 1)</span></span><br><span class="line">        x = self.sigmoid(self.linear(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x.squeeze()</span><br></pre></td></tr></tbody></table></figure>
<h2 id="训练思路"><a href="#训练思路" class="headerlink" title="训练思路"></a>训练思路</h2><p>DCGAN的训练可以分为生成器训练和判别器训练，每一个step中一般先训练判别器，然后训练生成器</p>
<h3 id="判别器的训练"><a href="#判别器的训练" class="headerlink" title="判别器的训练"></a>判别器的训练</h3><p>在训练判别器的时候我们希望判别器可以判断输入图片的真伪，因此我们的输入就是真图片、假图片和它们对应的标签。</p>
<p>因此判别器的训练步骤如下：</p>
<p>1、随机选取batch_size个真实的图片。<br>2、随机生成batch_size个N维向量，传入到Generator中生成batch_size个虚假图片。<br>3、真实图片的label为1，虚假图片的label为0，将真实图片和虚假图片当作训练集传入到Discriminator中进行训练。</p>
<p><style>.tduuigdmhkgl{zoom: 25%;}</style><img src="/zh-CN/DCGAN/DCGAN/20201203114355820.png" class="lazyload" data-srcset="/zh-CN/DCGAN/DCGAN/20201203114355820.png" srcset="data:image/png;base64,666" class="tduuigdmhkgl lazyload"></p>
<h3 id="生成器训练"><a href="#生成器训练" class="headerlink" title="生成器训练"></a>生成器训练</h3><p>在训练生成器的时候我们希望生成器可以生成极为真实的假图片。因此我们在训练生成器需要知道判别器认为什么图片是真图片。</p>
<p>因此生成器的训练步骤如下：</p>
<p>1、随机生成batch_size个N维向量，传入到Generator中生成batch_size个虚假图片。<br>2、将虚假图片的Discriminator预测结果与1的对比作为loss对Generator进行训练（与1对比的意思是，让生成器根据判别器判别的结果进行训练）</p>
<p><style>.cpckcbwewixb{zoom: 50%;}</style><img src="/zh-CN/DCGAN/DCGAN/20201203114415814.png" class="lazyload" data-srcset="/zh-CN/DCGAN/DCGAN/20201203114415814.png" srcset="data:image/png;base64,666" class="cpckcbwewixb lazyload"></p>
<h2 id="利用DCGAN生成图片"><a href="#利用DCGAN生成图片" class="headerlink" title="利用DCGAN生成图片"></a>利用DCGAN生成图片</h2><p>详情见源码和参考博文</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>FPS计算</title>
    <url>/zh-CN/FPS/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://baijiahao.baidu.com/s?id=1714660168795599325&amp;wfr=spider&amp;for=pc">帧率(FPS)计算的几种方法总结 (baidu.com)</a></p>
<p>帧率(FPS， frame per second)计算是游戏编程中常见的一个话题，因为表现在画面刷新与视觉感官上，所以相对而言，帧率非常影响用户体验。这也是很多大型3D游戏所要提升的重要点，意味着你要不断优化渲染速度与性能，不断提升画面质量。以下是几种计算帧率fps的方法。</p>
<h3 id="固定时间帧数法"><a href="#固定时间帧数法" class="headerlink" title="固定时间帧数法"></a>固定时间帧数法</h3><p><strong>其实这个方法的核心是1s内刷新了多少帧，完全不考虑其他设备，相对参照来计算帧率，最准的方法也可以叫做dps，即data per second.</strong></p>
<p>帧率计算公式：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">fps = frame / elapsedTime</span><br></pre></td></tr></tbody></table></figure>
<p>如果记录固定时间内的帧数，就可以计算出同步率。此种方法用得较多。</p>
<p>实现代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">fps</span><span class="params">()</span></span><br><span class="line">{</span><br><span class="line">	<span class="type">static</span> <span class="type">int</span> fps = <span class="number">0</span>;</span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> startTime = getTime(); <span class="comment">//ms</span></span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> frameCount = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    ++frameCount;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> curTime = getTime();</span><br><span class="line">    <span class="keyword">if</span>(curTime - startTime &gt; <span class="number">1000</span>)<span class="comment">//取固定时间为1s</span></span><br><span class="line">    {</span><br><span class="line">        fps = frameCount;</span><br><span class="line">        frameCount = <span class="number">0</span>;</span><br><span class="line">        startTime = curTime;</span><br><span class="line">    }</span><br><span class="line">	<span class="keyword">return</span> fps;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>这个固定时间为1s，其实本文的获取方法是精度比较低，也就是没有采用高精度获取时间戳的方法，在一些要求数据比较高的方法中，最好采用高精度获取时间的方法。</p>
<p>另一种实现方式如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">fps</span><span class="params">(<span class="type">int</span> deltatime)</span></span><br><span class="line">{</span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> fps = <span class="number">0</span>;</span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> timeLeft = <span class="number">1000</span>; <span class="comment">//取固定时间间隔为1s</span></span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> frameCount = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    ++frameCount;</span><br><span class="line">    timeLeft -= deltaTime;</span><br><span class="line">    <span class="keyword">if</span>(timeLeft &lt; <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        fps = frameCount;</span><br><span class="line">        frameCount = <span class="number">0</span>; <span class="comment">//重新计算</span></span><br><span class="line">        timeLeft = <span class="number">1000</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> fps;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="固定帧数时间法"><a href="#固定帧数时间法" class="headerlink" title="固定帧数时间法"></a>固定帧数时间法</h3><p>帧率计算公式为：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">fps = frameNum / elapsedTime</span><br></pre></td></tr></tbody></table></figure>
<p>如果每隔固定的帧数，计算帧数使用的时间，也可求出帧率。此种方法使用得较少。这个方法其实已经不能成为实时刷新的，因为帧率最好能够再1s内，如果说固定的帧数计算帧数使用的时间，那么我1s的帧率可能得等到10s后才能采集数据完毕计算出来结果。但是不可置否，这个帧率会比较稳定，变化跳动可能不会那么大。</p>
<p>实现代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">fps</span><span class="params">()</span></span><br><span class="line">{</span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> fps = <span class="number">0</span>;</span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> frameCount = <span class="number">0</span>;</span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> startTime = getTime();<span class="comment">//ms</span></span><br><span class="line">    </span><br><span class="line">    ++frameCount;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(frameCount &gt;= <span class="number">100</span>)<span class="comment">//取固定帧数为100帧</span></span><br><span class="line">    {</span><br><span class="line">        <span class="type">int</span> curTime = getTime();</span><br><span class="line">        fps = frameCount / (curTime - startTime) * <span class="number">1000</span>;</span><br><span class="line">        startTime = curTime;</span><br><span class="line">        frameCount = <span class="number">0</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> fps;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="实时计算法"><a href="#实时计算法" class="headerlink" title="实时计算法"></a>实时计算法</h3><p>实时计算法直接使用上一帧的时间间隔进行计算，结果具有实时性，但平滑性不好。这是刷新最快的，但是很明显这个帧率极其不稳定的，因为帧与帧之间的间隔总是不会那么稳定。</p>
<p>实现代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">fps</span><span class="params">(<span class="type">int</span> deltaTime)</span><span class="comment">//ms</span></span><br><span class="line">{</span><br><span class="line">    <span class="type">int</span> fps = static_cast&lt;<span class="type">int</span>&gt;(<span class="number">1.f</span> / deltaTime * <span class="number">1000</span>);</span><br><span class="line">    <span class="comment">//别忘了先转换为浮点数，否则会有精度损失</span></span><br><span class="line">    <span class="keyword">return</span> fps;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="总平均法"><a href="#总平均法" class="headerlink" title="总平均法"></a>总平均法</h3><p>总平均法使用全局帧数除以全局时间，以求出帧率。这个刷新就更慢了，这个方法的使用可能是放在一些对帧率这个参数要求没那么高的场景中。</p>
<p>实现代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">int</span> startTime = getTime();</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">fps</span><span class="params">()</span></span><br><span class="line">{</span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> frameCount = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    ++frameCount;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> deltaTime = getTime() - startTime();</span><br><span class="line">    <span class="type">int</span> fps = static_cast&lt;<span class="type">int</span>&gt;(frameCount * <span class="number">1.f</span> / deltaTime * <span class="number">1000</span>);</span><br><span class="line">    <span class="comment">//别忘了先转换为浮点数，否则会有精度损失</span></span><br><span class="line">    <span class="keyword">return</span> fps;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="精确采样法"><a href="#精确采样法" class="headerlink" title="精确采样法"></a>精确采样法</h3><p>精确采样法采样前N个帧，然后计算平均值。此种方法需要额外的内存空间，所以不常用。一般而言，大都数实验场景中不会用如此复杂的方法。</p>
<p>实现代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">fps</span><span class="params">(<span class="type">int</span> deltaTime)</span><span class="comment">//ms</span></span><br><span class="line">{</span><br><span class="line">    <span class="type">static</span> <span class="built_in">std</span>::<span class="built_in">queue</span>&lt;<span class="type">int</span>&gt; q;</span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> sumDuration = <span class="number">0</span>; <span class="comment">//ms</span></span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> fps = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(q.size() &lt; <span class="number">100</span>)<span class="comment">//设置样本数为100</span></span><br><span class="line">    {</span><br><span class="line">        sumDuration += deltaTime;</span><br><span class="line">        q.push(deltaTime);</span><br><span class="line">        fps = statci_cast&lt;<span class="type">int</span>&gt;(q.size() * <span class="number">1.f</span> / sumDuration * <span class="number">1000.f</span>);</span><br><span class="line">        <span class="comment">//别忘了先转换为浮点数，否则会有精度损失</span></span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    {</span><br><span class="line">        sumDuration -= q.front();</span><br><span class="line">        sumDuration += deltaTime;</span><br><span class="line">        sumDuration.pop();</span><br><span class="line">        sumDuration.push(deltaTime);</span><br><span class="line">        fps = static_cast&lt;<span class="type">int</span>&gt;(<span class="number">100.f</span> / sumDuration * <span class="number">1000.f</span>);</span><br><span class="line">        <span class="comment">//别忘了先转换为浮点数，否则会有精度损失</span></span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> fps;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="平均采样法"><a href="#平均采样法" class="headerlink" title="平均采样法"></a>平均采样法</h3><p>平均采样法利用上次的统计结果，克服了精确采样法需要使用额外空间的缺点。此种方法较常用。</p>
<p>实现代码如下：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">fps</span><span class="params">(<span class="type">int</span> deltaTime)</span><span class="comment">//ms</span></span><br><span class="line">{</span><br><span class="line">    <span class="type">static</span> <span class="type">float</span> avgDuration = <span class="number">0.f</span>;</span><br><span class="line">    <span class="type">static</span> alpha = <span class="number">1.f</span> / <span class="number">100.f</span>;<span class="comment">//采样数设置为100</span></span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> frameCount = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    ++frameCount;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> fps = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="number">1</span> == frameCount)</span><br><span class="line">    {</span><br><span class="line">        avgDuration = static_cast&lt;<span class="type">float</span>&gt;(deltaTime);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    {</span><br><span class="line">        avgDuration = avgDuration * (<span class="number">1</span> - alpha) + deltaTime * alpha;</span><br><span class="line">    }</span><br><span class="line">    fps = static_cast&lt;<span class="type">int</span>&gt;(<span class="number">1.f</span> / avgDuration * <span class="number">1000</span>);</span><br><span class="line">    <span class="keyword">return</span> fps;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>帧率的计算应该来说不能影响线程，进程中的性能，不要把这部分代码放在你的业务逻辑中，这样子其实又包含了你业务逻辑执行完毕后的运行时间，显得有点臃肿，并且代码不是那么优雅。最好是创建一个线程去计算帧率，这样会显得比较优雅。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>FPS</tag>
      </tags>
  </entry>
  <entry>
    <title>Darknet53</title>
    <url>/zh-CN/Darknet53/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/weixin_44791964/article/details/105310627?ops_request_misc=%7B%22request%5Fid%22%3A%22165296353216781685319292%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fblog.%22%7D&amp;request_id=165296353216781685319292&amp;biz_id=0&amp;spm=1018.2226.3001.4450">睿智的目标检测26——Pytorch搭建yolo3目标检测平台<em>Bubbliiiing的博客-CSDN博客</em>睿智的目标检测26</a></p>
<p>2.<a href="https://www.jianshu.com/p/ab8392ef3394">Darknet53网络各层参数详解 - 简书 (jianshu.com)</a></p>
<p>3.<a href="https://blog.csdn.net/weixin_48167570/article/details/120688156">Darknet53网络结构及代码实现_Tc.小浩的博客-CSDN博客_darknet53</a></p>
<p>4.<a href="https://www.cnblogs.com/chenhuabin/p/13908615.html">Yolov3算法详解 - 奥辰 - 博客园 (cnblogs.com)</a></p>
<p>Darkenet53是Yolov3网络中的一部分（backbone），为了更加详细了解darknet53网络的结构，现将Darknet53各层输入与输出的形状列举下来，便于分析理解。</p>
<p>Darknet53的网络结构如图1所示，其中蓝色方块×1，x2，x8分别表示该模块重复1次、2次和8次，黄色方块是该模块的名字，<code>Conv Block</code>表示该模块是一个普通的卷积模块，<code>Residual Bolck</code>代表该模块是一个残差网络。  读者可以将图1和图2结合对比着看更容易理解Daeknet53网络三种不同的输出位置。</p>
<p><style>.dqfttxppgjxv{zoom:50%;}</style><img src="/zh-CN/Darknet53/Darknet53/16006821-59dd6b77da823e7f.png" class="lazyload" data-srcset="/zh-CN/Darknet53/Darknet53/16006821-59dd6b77da823e7f.png" srcset="data:image/png;base64,666" class="dqfttxppgjxv lazyload" alt="img"></p>
<p>实现代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------------------------------------------------#</span></span><br><span class="line"><span class="comment">#   残差结构</span></span><br><span class="line"><span class="comment">#   利用一个1x1卷积下降通道数，然后利用一个3x3卷积提取特征并且上升通道数</span></span><br><span class="line"><span class="comment">#   最后接上一个残差边</span></span><br><span class="line"><span class="comment"># ---------------------------------------------------------------------#</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        <span class="comment"># (kernel_size, stride, padding)=(1, 1, 0)保持了宽高不变</span></span><br><span class="line">        self.conv1 = nn.Conv2d(</span><br><span class="line">            inplanes, planes[<span class="number">0</span>], kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes[<span class="number">0</span>])</span><br><span class="line">        self.relu1 = nn.LeakyReLU(<span class="number">0.1</span>)</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># (kernel_size, stride, padding)=(3, 1, 1)同样保持了宽高不变</span></span><br><span class="line">        self.conv2 = nn.Conv2d(</span><br><span class="line">            planes[<span class="number">0</span>], planes[<span class="number">1</span>], kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes[<span class="number">1</span>])</span><br><span class="line">        self.relu2 = nn.LeakyReLU(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu1(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu2(out)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DarkNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layers</span>):</span><br><span class="line">        <span class="built_in">super</span>(DarkNet, self).__init__()</span><br><span class="line">        self.inplanes = <span class="number">32</span></span><br><span class="line">        <span class="comment"># 416,416,3 -&gt; 416,416,32</span></span><br><span class="line">        self.conv1 = nn.Conv2d(</span><br><span class="line">            <span class="number">3</span>, self.inplanes, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(self.inplanes)</span><br><span class="line">        self.relu1 = nn.LeakyReLU(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 416,416,32 -&gt; 208,208,64</span></span><br><span class="line">        self.layer1 = self._make_layer([<span class="number">32</span>, <span class="number">64</span>], layers[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 208,208,64 -&gt; 104,104,128</span></span><br><span class="line">        self.layer2 = self._make_layer([<span class="number">64</span>, <span class="number">128</span>], layers[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 104,104,128 -&gt; 52,52,256</span></span><br><span class="line">        self.layer3 = self._make_layer([<span class="number">128</span>, <span class="number">256</span>], layers[<span class="number">2</span>])</span><br><span class="line">        <span class="comment"># 52,52,256 -&gt; 26,26,512</span></span><br><span class="line">        self.layer4 = self._make_layer([<span class="number">256</span>, <span class="number">512</span>], layers[<span class="number">3</span>])</span><br><span class="line">        <span class="comment"># 26,26,512 -&gt; 13,13,1024</span></span><br><span class="line">        self.layer5 = self._make_layer([<span class="number">512</span>, <span class="number">1024</span>], layers[<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">        self.layers_out_filters = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行权值初始化</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                n = m.kernel_size[<span class="number">0</span>] * m.kernel_size[<span class="number">1</span>] * m.out_channels</span><br><span class="line">                m.weight.data.normal_(<span class="number">0</span>, math.sqrt(<span class="number">2.</span> / n))</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ---------------------------------------------------------------------#</span></span><br><span class="line">    <span class="comment">#   在每一个layer里面，首先利用一个步长为2的3x3卷积进行下采样</span></span><br><span class="line">    <span class="comment">#   然后进行残差结构的堆叠,使用了上面的BasicBlock模块</span></span><br><span class="line">    <span class="comment">#   planes = [last_layer_out_channels, this_layer_in_channels]</span></span><br><span class="line">    <span class="comment"># ---------------------------------------------------------------------#</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, planes, blocks</span>):</span><br><span class="line">        layers = []</span><br><span class="line">        <span class="comment"># 下采样，步长为2，卷积核大小为3</span></span><br><span class="line">        layers.append((<span class="string">"ds_conv"</span>, nn.Conv2d(</span><br><span class="line">            self.inplanes, planes[<span class="number">1</span>], kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)))</span><br><span class="line">        layers.append((<span class="string">"ds_bn"</span>, nn.BatchNorm2d(planes[<span class="number">1</span>])))</span><br><span class="line">        layers.append((<span class="string">"ds_relu"</span>, nn.LeakyReLU(<span class="number">0.1</span>)))</span><br><span class="line">        <span class="comment"># 加入残差结构</span></span><br><span class="line">        self.inplanes = planes[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 将Residual Block重复layer[i]次</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, blocks):</span><br><span class="line">            layers.append((<span class="string">"residual_{}"</span>.<span class="built_in">format</span>(</span><br><span class="line">                i), BasicBlock(self.inplanes, planes)))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(OrderedDict(layers))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu1(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        out3 = self.layer3(x)</span><br><span class="line">        out4 = self.layer4(out3)</span><br><span class="line">        out5 = self.layer5(out4)</span><br><span class="line">		<span class="comment"># 取出来了三层特征继续改进</span></span><br><span class="line">        <span class="keyword">return</span> out3, out4, out5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">darknet53</span>():</span><br><span class="line">    model = DarkNet([<span class="number">1</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">4</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>说明：</p>
<p>Darknet53中的53说的是卷积和全连接层数之和，53 =  1 + （1+2+8+8+4）*2 +5+1</p>
<p>看结构图就能明白，最后那个1表示全连接，图中没有画出来，得去看原论文，因为这里的代码是利用其做主干得到特征再进行改进，因此没有全连接。我更愿意说它是Darknet52,但根据习惯，还是叫它Darknet53。</p>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>yolov3</tag>
        <tag>darknet</tag>
      </tags>
  </entry>
  <entry>
    <title>Faster-RCNN论文笔记</title>
    <url>/zh-CN/Faster-rcnn/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文地址：<a href="https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf">Faster R-CNN</a></p>
<p>源码地址：<a href="https://github.com/ShaoqingRen/faster_rcnn">ShaoqingRen/faster_rcnn: Faster R-CNN (github.com)</a></p>
<p>文章引用源码：<a href="https://github.com/bubbliiiing/faster-rcnn-pytorch">https://github.com/bubbliiiing/faster-rcnn-pytorch</a></p>
<p>文章出处：<a href="https://blog.csdn.net/weixin_44791964/article/details/105739918">https://blog.csdn.net/weixin_44791964/article/details/105739918</a></p>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><h3 id="预测部分"><a href="#预测部分" class="headerlink" title="预测部分"></a>预测部分</h3><h4 id="主干网络"><a href="#主干网络" class="headerlink" title="主干网络"></a>主干网络</h4><p><style>.wngxnxytryrd{}</style><img src="/zh-CN/Faster-rcnn/Faster-rcnn/image-20220417152429746.png" class="lazyload" data-srcset="/zh-CN/Faster-rcnn/Faster-rcnn/image-20220417152429746.png" srcset="data:image/png;base64,666" class="wngxnxytryrd lazyload"></p>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>faster-rcnn</tag>
      </tags>
  </entry>
  <entry>
    <title>fork me on github</title>
    <url>/zh-CN/Fork_github/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/weiwosuoai/article/details/88312326">如何在博客园添加 Fork me on GitHub 彩带效果</a></p>
<p>2.<a href="https://github.blog/2008-12-19-github-ribbons/">GitHub Ribbons | The GitHub Blog</a></p>
<p>3.<a href="https://blog.csdn.net/weixin_44543463/article/details/119750964">Hexo添加Follow me on CSDN效果</a></p>
<h2 id="博客园添加fork"><a href="#博客园添加fork" class="headerlink" title="博客园添加fork"></a>博客园添加fork</h2><h3 id="进入博客园后台"><a href="#进入博客园后台" class="headerlink" title="进入博客园后台"></a>进入博客园后台</h3><p>进入博客园的管理界面，依次点击 <strong>管理 </strong> -&gt; <strong>设置</strong>，进入到设置页面后，将页面拖动到最下面，您会看到：<strong>页首Html代码</strong>一栏</p>
<p><style>.xbvmlhnxrmvv{zoom:50%;}</style><img src="/zh-CN/Fork_github/Fork_github/image-20220502215437444.png" class="lazyload" data-srcset="/zh-CN/Fork_github/Fork_github/image-20220502215437444.png" srcset="data:image/png;base64,666" class="xbvmlhnxrmvv lazyload" alt="image-20220502215437444"></p>
<h3 id="添加代码"><a href="#添加代码" class="headerlink" title="添加代码"></a>添加代码</h3><p>在页面Html代码框中输入如下代码：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">&lt;a href="https://github.com/pistachio"&gt;  </span><br><span class="line">&lt;img style="position: fixed; top: 0; right: 0; border: 0; z-index:9999;" </span><br><span class="line">  	 src="https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png" </span><br><span class="line">  	 alt="Fork me on GitHub"&gt;</span><br><span class="line">&lt;/a&gt;</span><br></pre></td></tr></tbody></table></figure>
<p><strong>注意，您需要将 <code>&lt;a href=""&gt;</code> 中的链接换成您自己的 GitHub 主页地址。</strong></p>
<p>保存后，随意打开一篇您自己的博客，就可以看见和教程开头展示的效果一样了。大功告成！</p>
<h3 id="更换彩带颜色"><a href="#更换彩带颜色" class="headerlink" title="更换彩带颜色"></a>更换彩带颜色</h3><p>我上面使用的是红色的彩带，如果您需要更换成其他颜色，只需将 <code>&lt;img&gt;</code> 标签中的 <code>src</code> 地址更换成您想要颜色的地址即可。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"># 绿色</span><br><span class="line">https://github.blog/wp-content/uploads/2008/12/forkme_right_green_007200.png</span><br><span class="line"># 黑色</span><br><span class="line">https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png</span><br><span class="line"># 橘黄色</span><br><span class="line">https://github.blog/wp-content/uploads/2008/12/forkme_right_orange_ff7600.png</span><br><span class="line"># 灰色</span><br><span class="line">https://github.blog/wp-content/uploads/2008/12/forkme_right_gray_6d6d6d.png</span><br><span class="line"># 白色  </span><br><span class="line">https://github.blog/wp-content/uploads/2008/12/forkme_right_white_ffffff.png</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>从上面挑选一款您喜欢的样式颜色吧！！</p>
<h2 id="hexo添加fork"><a href="#hexo添加fork" class="headerlink" title="hexo添加fork"></a>hexo添加fork</h2><h3 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h3><p>粘贴复制如下的代码到<code>themes\hexo-theme-next\layout\layout.ejs</code>文件中(放在<code>&lt;div id="l_body"&gt;&lt;/div&gt;</code>的下面 如图)，并把href改为你的csdn主页,换成github同上，记得换上自己的链接。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"># 黑色版本</span><br><span class="line"># 个人认为position属性值改为fixed好一点~</span><br><span class="line">  &lt;!--Follow me on CSDN--&gt;</span><br><span class="line">  &lt;a href="https://blog.csdn.net/qq_38452951"&gt; &lt;img loading="lazy" width="149" height="149" style="position: absolute; top: 0; right: 0; border: 0;" src="https://img-blog.csdnimg.cn/abe3797b7d77419b81ecc02dd1bf8c34.png" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"&gt;&lt;/a&gt;</span><br><span class="line"></span><br><span class="line"># 白色版本</span><br><span class="line">  &lt;!--Follow me on CSDN--&gt;</span><br><span class="line">  &lt;a href="https://blog.csdn.net/qq_38452951"&gt;&lt;img loading="lazy" width="149" height="149" style="position: absolute; top: 0; right: 0; border: 0;" src="https://img-blog.csdnimg.cn/1f8e1ef9be9f4f7db01fe3a2d57829de.png" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"&gt;&lt;/a&gt;</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p><style>.hpvwetbxpofp{zoom:50%;}</style><img src="/zh-CN/Fork_github/Fork_github/image-20220502220838865.png" class="lazyload" data-srcset="/zh-CN/Fork_github/Fork_github/image-20220502220838865.png" srcset="data:image/png;base64,666" class="hpvwetbxpofp lazyload" alt="image-20220502220838865"></p>
<h3 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h3><p>详情请见：</p>
<p>1.<a href="https://www.cnblogs.com/pistachio0812/">追风赶月的少年 - 博客园 (cnblogs.com)</a></p>
<p>2.<a href="https://pistachio0812.github.io/">相思似海深旧事如天远 (pistachio0812.github.io)</a></p>
]]></content>
      <categories>
        <category>Hexo博客搭建</category>
      </categories>
      <tags>
        <tag>博客提升</tag>
        <tag>fork</tag>
      </tags>
  </entry>
  <entry>
    <title>InceptionV3论文笔记</title>
    <url>/zh-CN/InceptionV3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>论文地址：<a href="https://arxiv.org/pdf/1512.00567.pdf">Inceptionv3 </a></p>
<p>源码地址：<a href="https://github.com/MasazI/InceptionV3_TensorFlow">InceptionV3_TensorFlow: Inception v3 </a></p>
<p>文章引用出处1：<a href="https://blog.csdn.net/weixin_44791964/article/details/102802866">https://blog.csdn.net/weixin_44791964/article/details/102802866</a></p>
<p>文章引用出处2：<a href="https://cloud.tencent.com/developer/article/1006032">InceptionV3 网络模型</a></p>
<h2 id="InceptionV3模型"><a href="#InceptionV3模型" class="headerlink" title="InceptionV3模型"></a>InceptionV3模型</h2><p>InceptionV3模型是谷歌Inception系列里面的第三代模型，其模型结构与InceptionV2模型放在了同一篇论文里，其实二者模型结构差距不大，相比于其它神经网络模型，Inception网络最大的特点在于将神经网络层与层之间的卷积运算进行了拓展。<br>如VGG，AlexNet网络，它就是一直卷积下来的，一层接着一层；<br>ResNet则是创新性的引入了残差网络的概念，使得靠前若干层的某一层数据输出直接跳过多层引入到后面数据层的输入部分，后面的特征层的内容会有一部分由其前面的某一层线性贡献。<br>而Inception网络则是采用不同大小的卷积核，使得存在不同大小的感受野，最后实现拼接达到不同尺度特征的融合。<br>对于InceptionV3而言，其网络中存在着如下的结构。<br>这个结构使用不同大小的卷积核对输入进行卷积（这个结构主要在代码中的block1使用）。</p>
<p><style>.issxuxgsehlq{}</style><img src="/zh-CN/InceptionV3/InceptionV3/2019111309515588.png" class="lazyload" data-srcset="/zh-CN/InceptionV3/InceptionV3/2019111309515588.png" srcset="data:image/png;base64,666" class="issxuxgsehlq lazyload" alt="Inceptionv3_block1"></p>
<p>还存在着这样的结构，利用1x7的卷积和7x1的卷积代替7x7的卷积，这样可以只使用约（1x7 + 7x1) / (7x7) = 28.6%的计算开销；利用1x3的卷积和3x1的卷积代替3x3的卷积，这样可以只使用约（1x3 + 3x1) / (3x3) = 67%的计算开销。<br>下图利用1x7的卷积和7x1的卷积代替7x7的卷积（这个结构主要在代码中的block2使用）。</p>
<p><style>.xkoyuhaicxcm{}</style><img src="/zh-CN/InceptionV3/InceptionV3/20191113095610269.png" class="lazyload" data-srcset="/zh-CN/InceptionV3/InceptionV3/20191113095610269.png" srcset="data:image/png;base64,666" class="xkoyuhaicxcm lazyload" alt="Inceptionv3_block2"></p>
<p>下图利用1x3的卷积和3x1的卷积代替3x3的卷积（这个结构主要在代码中的block3使用）。</p>
<p><style>.imbhroycgmqp{}</style><img src="/zh-CN/InceptionV3/InceptionV3/2019111309572648.png" class="lazyload" data-srcset="/zh-CN/InceptionV3/InceptionV3/2019111309572648.png" srcset="data:image/png;base64,666" class="imbhroycgmqp lazyload" alt="inceptionv3_block3"></p>
<h2 id="网络部分实现代码"><a href="#网络部分实现代码" class="headerlink" title="网络部分实现代码"></a>网络部分实现代码</h2><p>一共将InceptionV3划分为3个block，对应着35x35、17x17，8x8维度大小的图像。每个block中间有许多的part，对应着不同的特征层深度，用于特征提取。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#-------------------------------------------------------------#</span></span><br><span class="line"><span class="comment">#   InceptionV3的网络部分</span></span><br><span class="line"><span class="comment">#-------------------------------------------------------------#</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Activation,Dense,Input,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> GlobalAveragePooling2D,GlobalMaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.engine.topology <span class="keyword">import</span> get_source_inputs</span><br><span class="line"><span class="keyword">from</span> keras.utils.layer_utils <span class="keyword">import</span> convert_all_kernels_in_model</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> decode_predictions</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d_bn</span>(<span class="params">x,</span></span><br><span class="line"><span class="params">              filters,</span></span><br><span class="line"><span class="params">              num_row,</span></span><br><span class="line"><span class="params">              num_col,</span></span><br><span class="line"><span class="params">              padding=<span class="string">'same'</span>,</span></span><br><span class="line"><span class="params">              strides=(<span class="params"><span class="number">1</span>, <span class="number">1</span></span>),</span></span><br><span class="line"><span class="params">              name=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> name <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        bn_name = name + <span class="string">'_bn'</span></span><br><span class="line">        conv_name = name + <span class="string">'_conv'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        bn_name = <span class="literal">None</span></span><br><span class="line">        conv_name = <span class="literal">None</span></span><br><span class="line">    x = Conv2D(</span><br><span class="line">        filters, (num_row, num_col),</span><br><span class="line">        strides=strides,</span><br><span class="line">        padding=padding,</span><br><span class="line">        use_bias=<span class="literal">False</span>,</span><br><span class="line">        name=conv_name)(x)</span><br><span class="line">    x = BatchNormalization(scale=<span class="literal">False</span>, name=bn_name)(x)</span><br><span class="line">    x = Activation(<span class="string">'relu'</span>, name=name)(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">InceptionV3</span>(<span class="params">input_shape=[<span class="number">299</span>,<span class="number">299</span>,<span class="number">3</span>],</span></span><br><span class="line"><span class="params">                classes=<span class="number">1000</span></span>):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    img_input = Input(shape=input_shape)</span><br><span class="line"></span><br><span class="line">    x = conv2d_bn(img_input, <span class="number">32</span>, <span class="number">3</span>, <span class="number">3</span>, strides=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">'valid'</span>)</span><br><span class="line">    x = conv2d_bn(x, <span class="number">32</span>, <span class="number">3</span>, <span class="number">3</span>, padding=<span class="string">'valid'</span>)</span><br><span class="line">    x = conv2d_bn(x, <span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">    x = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(x)</span><br><span class="line"></span><br><span class="line">    x = conv2d_bn(x, <span class="number">80</span>, <span class="number">1</span>, <span class="number">1</span>, padding=<span class="string">'valid'</span>)</span><br><span class="line">    x = conv2d_bn(x, <span class="number">192</span>, <span class="number">3</span>, <span class="number">3</span>, padding=<span class="string">'valid'</span>)</span><br><span class="line">    x = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#--------------------------------#</span></span><br><span class="line">    <span class="comment">#   Block1 35x35</span></span><br><span class="line">    <span class="comment">#--------------------------------#</span></span><br><span class="line">    <span class="comment"># Block1 part1</span></span><br><span class="line">    <span class="comment"># 35 x 35 x 192 -&gt; 35 x 35 x 256</span></span><br><span class="line">    branch1x1 = conv2d_bn(x, <span class="number">64</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    branch5x5 = conv2d_bn(x, <span class="number">48</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    branch5x5 = conv2d_bn(branch5x5, <span class="number">64</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    branch3x3dbl = conv2d_bn(x, <span class="number">64</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    branch3x3dbl = conv2d_bn(branch3x3dbl, <span class="number">96</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">    branch3x3dbl = conv2d_bn(branch3x3dbl, <span class="number">96</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    branch_pool = AveragePooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>)(x)</span><br><span class="line">    branch_pool = conv2d_bn(branch_pool, <span class="number">32</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    x = layers.concatenate(</span><br><span class="line">        [branch1x1, branch5x5, branch3x3dbl, branch_pool],</span><br><span class="line">        axis=<span class="number">3</span>,</span><br><span class="line">        name=<span class="string">'mixed0'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Block1 part2</span></span><br><span class="line">    <span class="comment"># 35 x 35 x 256 -&gt; 35 x 35 x 288</span></span><br><span class="line">    branch1x1 = conv2d_bn(x, <span class="number">64</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    branch5x5 = conv2d_bn(x, <span class="number">48</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    branch5x5 = conv2d_bn(branch5x5, <span class="number">64</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    branch3x3dbl = conv2d_bn(x, <span class="number">64</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    branch3x3dbl = conv2d_bn(branch3x3dbl, <span class="number">96</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">    branch3x3dbl = conv2d_bn(branch3x3dbl, <span class="number">96</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    branch_pool = AveragePooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>)(x)</span><br><span class="line">    branch_pool = conv2d_bn(branch_pool, <span class="number">64</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    x = layers.concatenate(</span><br><span class="line">        [branch1x1, branch5x5, branch3x3dbl, branch_pool],</span><br><span class="line">        axis=<span class="number">3</span>,</span><br><span class="line">        name=<span class="string">'mixed1'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Block1 part3</span></span><br><span class="line">    <span class="comment"># 35 x 35 x 288 -&gt; 35 x 35 x 288</span></span><br><span class="line">    branch1x1 = conv2d_bn(x, <span class="number">64</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    branch5x5 = conv2d_bn(x, <span class="number">48</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    branch5x5 = conv2d_bn(branch5x5, <span class="number">64</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    branch3x3dbl = conv2d_bn(x, <span class="number">64</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    branch3x3dbl = conv2d_bn(branch3x3dbl, <span class="number">96</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">    branch3x3dbl = conv2d_bn(branch3x3dbl, <span class="number">96</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    branch_pool = AveragePooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>)(x)</span><br><span class="line">    branch_pool = conv2d_bn(branch_pool, <span class="number">64</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    x = layers.concatenate(</span><br><span class="line">        [branch1x1, branch5x5, branch3x3dbl, branch_pool],</span><br><span class="line">        axis=<span class="number">3</span>,</span><br><span class="line">        name=<span class="string">'mixed2'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#--------------------------------#</span></span><br><span class="line">    <span class="comment">#   Block2 17x17</span></span><br><span class="line">    <span class="comment">#--------------------------------#</span></span><br><span class="line">    <span class="comment"># Block2 part1</span></span><br><span class="line">    <span class="comment"># 35 x 35 x 288 -&gt; 17 x 17 x 768</span></span><br><span class="line">    branch3x3 = conv2d_bn(x, <span class="number">384</span>, <span class="number">3</span>, <span class="number">3</span>, strides=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">'valid'</span>)</span><br><span class="line"></span><br><span class="line">    branch3x3dbl = conv2d_bn(x, <span class="number">64</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    branch3x3dbl = conv2d_bn(branch3x3dbl, <span class="number">96</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">    branch3x3dbl = conv2d_bn(</span><br><span class="line">        branch3x3dbl, <span class="number">96</span>, <span class="number">3</span>, <span class="number">3</span>, strides=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">'valid'</span>)</span><br><span class="line"></span><br><span class="line">    branch_pool = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(x)</span><br><span class="line">    x = layers.concatenate(</span><br><span class="line">        [branch3x3, branch3x3dbl, branch_pool], axis=<span class="number">3</span>, name=<span class="string">'mixed3'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Block2 part2</span></span><br><span class="line">    <span class="comment"># 17 x 17 x 768 -&gt; 17 x 17 x 768</span></span><br><span class="line">    branch1x1 = conv2d_bn(x, <span class="number">192</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    branch7x7 = conv2d_bn(x, <span class="number">128</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    branch7x7 = conv2d_bn(branch7x7, <span class="number">128</span>, <span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line">    branch7x7 = conv2d_bn(branch7x7, <span class="number">192</span>, <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    branch7x7dbl = conv2d_bn(x, <span class="number">128</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    branch7x7dbl = conv2d_bn(branch7x7dbl, <span class="number">128</span>, <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line">    branch7x7dbl = conv2d_bn(branch7x7dbl, <span class="number">128</span>, <span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line">    branch7x7dbl = conv2d_bn(branch7x7dbl, <span class="number">128</span>, <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line">    branch7x7dbl = conv2d_bn(branch7x7dbl, <span class="number">192</span>, <span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">    branch_pool = AveragePooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>)(x)</span><br><span class="line">    branch_pool = conv2d_bn(branch_pool, <span class="number">192</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    x = layers.concatenate(</span><br><span class="line">        [branch1x1, branch7x7, branch7x7dbl, branch_pool],</span><br><span class="line">        axis=<span class="number">3</span>,</span><br><span class="line">        name=<span class="string">'mixed4'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Block2 part3 and part4</span></span><br><span class="line">    <span class="comment"># 17 x 17 x 768 -&gt; 17 x 17 x 768 -&gt; 17 x 17 x 768</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        branch1x1 = conv2d_bn(x, <span class="number">192</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        branch7x7 = conv2d_bn(x, <span class="number">160</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        branch7x7 = conv2d_bn(branch7x7, <span class="number">160</span>, <span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line">        branch7x7 = conv2d_bn(branch7x7, <span class="number">192</span>, <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        branch7x7dbl = conv2d_bn(x, <span class="number">160</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        branch7x7dbl = conv2d_bn(branch7x7dbl, <span class="number">160</span>, <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line">        branch7x7dbl = conv2d_bn(branch7x7dbl, <span class="number">160</span>, <span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line">        branch7x7dbl = conv2d_bn(branch7x7dbl, <span class="number">160</span>, <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line">        branch7x7dbl = conv2d_bn(branch7x7dbl, <span class="number">192</span>, <span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">        branch_pool = AveragePooling2D(</span><br><span class="line">            (<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>)(x)</span><br><span class="line">        branch_pool = conv2d_bn(branch_pool, <span class="number">192</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        x = layers.concatenate(</span><br><span class="line">            [branch1x1, branch7x7, branch7x7dbl, branch_pool],</span><br><span class="line">            axis=<span class="number">3</span>,</span><br><span class="line">            name=<span class="string">'mixed'</span> + <span class="built_in">str</span>(<span class="number">5</span> + i))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Block2 part5</span></span><br><span class="line">    <span class="comment"># 17 x 17 x 768 -&gt; 17 x 17 x 768</span></span><br><span class="line">    branch1x1 = conv2d_bn(x, <span class="number">192</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    branch7x7 = conv2d_bn(x, <span class="number">192</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    branch7x7 = conv2d_bn(branch7x7, <span class="number">192</span>, <span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line">    branch7x7 = conv2d_bn(branch7x7, <span class="number">192</span>, <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    branch7x7dbl = conv2d_bn(x, <span class="number">192</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    branch7x7dbl = conv2d_bn(branch7x7dbl, <span class="number">192</span>, <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line">    branch7x7dbl = conv2d_bn(branch7x7dbl, <span class="number">192</span>, <span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line">    branch7x7dbl = conv2d_bn(branch7x7dbl, <span class="number">192</span>, <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line">    branch7x7dbl = conv2d_bn(branch7x7dbl, <span class="number">192</span>, <span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">    branch_pool = AveragePooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>)(x)</span><br><span class="line">    branch_pool = conv2d_bn(branch_pool, <span class="number">192</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    x = layers.concatenate(</span><br><span class="line">        [branch1x1, branch7x7, branch7x7dbl, branch_pool],</span><br><span class="line">        axis=<span class="number">3</span>,</span><br><span class="line">        name=<span class="string">'mixed7'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#--------------------------------#</span></span><br><span class="line">    <span class="comment">#   Block3 8x8</span></span><br><span class="line">    <span class="comment">#--------------------------------#</span></span><br><span class="line">    <span class="comment"># Block3 part1</span></span><br><span class="line">    <span class="comment"># 17 x 17 x 768 -&gt; 8 x 8 x 1280</span></span><br><span class="line">    branch3x3 = conv2d_bn(x, <span class="number">192</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    branch3x3 = conv2d_bn(branch3x3, <span class="number">320</span>, <span class="number">3</span>, <span class="number">3</span>,</span><br><span class="line">                          strides=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">'valid'</span>)</span><br><span class="line"></span><br><span class="line">    branch7x7x3 = conv2d_bn(x, <span class="number">192</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    branch7x7x3 = conv2d_bn(branch7x7x3, <span class="number">192</span>, <span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line">    branch7x7x3 = conv2d_bn(branch7x7x3, <span class="number">192</span>, <span class="number">7</span>, <span class="number">1</span>)</span><br><span class="line">    branch7x7x3 = conv2d_bn(</span><br><span class="line">        branch7x7x3, <span class="number">192</span>, <span class="number">3</span>, <span class="number">3</span>, strides=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">'valid'</span>)</span><br><span class="line"></span><br><span class="line">    branch_pool = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(x)</span><br><span class="line">    x = layers.concatenate(</span><br><span class="line">        [branch3x3, branch7x7x3, branch_pool], axis=<span class="number">3</span>, name=<span class="string">'mixed8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Block3 part2 part3</span></span><br><span class="line">    <span class="comment"># 8 x 8 x 1280 -&gt; 8 x 8 x 2048 -&gt; 8 x 8 x 2048</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        branch1x1 = conv2d_bn(x, <span class="number">320</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        branch3x3 = conv2d_bn(x, <span class="number">384</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        branch3x3_1 = conv2d_bn(branch3x3, <span class="number">384</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        branch3x3_2 = conv2d_bn(branch3x3, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        branch3x3 = layers.concatenate(</span><br><span class="line">            [branch3x3_1, branch3x3_2], axis=<span class="number">3</span>, name=<span class="string">'mixed9_'</span> + <span class="built_in">str</span>(i))</span><br><span class="line"></span><br><span class="line">        branch3x3dbl = conv2d_bn(x, <span class="number">448</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        branch3x3dbl = conv2d_bn(branch3x3dbl, <span class="number">384</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, <span class="number">384</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        branch3x3dbl = layers.concatenate(</span><br><span class="line">            [branch3x3dbl_1, branch3x3dbl_2], axis=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        branch_pool = AveragePooling2D(</span><br><span class="line">            (<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>)(x)</span><br><span class="line">        branch_pool = conv2d_bn(branch_pool, <span class="number">192</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        x = layers.concatenate(</span><br><span class="line">            [branch1x1, branch3x3, branch3x3dbl, branch_pool],</span><br><span class="line">            axis=<span class="number">3</span>,</span><br><span class="line">            name=<span class="string">'mixed'</span> + <span class="built_in">str</span>(<span class="number">9</span> + i))</span><br><span class="line">    <span class="comment"># 平均池化后全连接。</span></span><br><span class="line">    x = GlobalAveragePooling2D(name=<span class="string">'avg_pool'</span>)(x)</span><br><span class="line">    x = Dense(classes, activation=<span class="string">'softmax'</span>, name=<span class="string">'predictions'</span>)(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    inputs = img_input</span><br><span class="line"></span><br><span class="line">    model = Model(inputs, x, name=<span class="string">'inception_v3'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="预测部分"><a href="#预测部分" class="headerlink" title="预测部分"></a>预测部分</h2><p>建立网络后就可以进行预测了，实现代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_input</span>(<span class="params">x</span>):</span><br><span class="line">    x /= <span class="number">255.</span></span><br><span class="line">    x -= <span class="number">0.5</span></span><br><span class="line">    x *= <span class="number">2.</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    model = InceptionV3()</span><br><span class="line"></span><br><span class="line">    model.load_weights(<span class="string">"inception_v3_weights_tf_dim_ordering_tf_kernels.h5"</span>)</span><br><span class="line">    </span><br><span class="line">    img_path = <span class="string">'elephant.jpg'</span></span><br><span class="line">    img = image.load_img(img_path, target_size=(<span class="number">299</span>, <span class="number">299</span>))</span><br><span class="line">    x = image.img_to_array(img)</span><br><span class="line">    x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    x = preprocess_input(x)</span><br><span class="line"></span><br><span class="line">    preds = model.predict(x)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'Predicted:'</span>, decode_predictions(preds))</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>预测所需的已经训练好的InceptionV3模型可以在<a href="https://github.com/fchollet/deep-learning-models/releases下载。非常方便。">https://github.com/fchollet/deep-learning-models/releases下载。非常方便。</a><br>预测结果为：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">Predicted: [[('n02504458', 'African<span class="emphasis">_elephant', 0.50874853), ('n01871265', 'tusker', 0.19524273), ('n02504013', 'Indian_</span>elephant', 0.1566972), ('n01917289', 'brain<span class="emphasis">_coral', 0.0008956835), ('n01695060', 'Komodo_</span>dragon', 0.0008260256)]]</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>Inception</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use Linux</title>
    <url>/zh-CN/Linux%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="Linux简介"><a href="#Linux简介" class="headerlink" title="Linux简介"></a>Linux简介</h3><p>UNIX 是一个交互式系统，用于同时处理多进程和多用户同时在线。为什么要说 UNIX，那是因为 Linux 是由 UNIX 发展而来的，UNIX 是由程序员设计，它的主要服务对象也是程序员。Linux 继承了 UNIX 的设计目标。从智能手机到汽车，超级计算机和家用电器，从家用台式机到企业服务器，Linux 操作系统无处不在。</p>
<p>大多数程序员都喜欢让系统尽量简单，优雅并具有一致性。举个例子，从最底层的角度来讲，一个文件应该只是一个字节集合。为了实现顺序存取、随机存取、按键存取、远程存取只能是妨碍你的工作。相同的，如果命令</p>
<p>ls A*<br>意味着只列出以 A 为开头的所有文件，那么命令</p>
<p>rm A<em><br>应该会移除所有以 A 为开头的文件而不是只删除文件名是 A</em> 的文件。这个特性也是最小吃惊原则(principle of least surprise)</p>
<p>最小吃惊原则一般常用于用户界面和软件设计。它的原型是：该功能或者特征应该符合用户的预期，不应该使用户感到惊讶和震惊。</p>
<p>一些有经验的程序员通常希望系统具有较强的功能性和灵活性。设计 Linux 的一个基本目标是每个应用程序只做一件事情并把他做好。所以编译器只负责编译的工作，编译器不会产生列表，因为有其他应用比编译器做的更好。</p>
<p>很多人都不喜欢冗余，为什么在 cp 就能描述清楚你想干什么时候还使用 copy？这完全是在浪费宝贵的 hacking time。为了从文件中提取所有包含字符串 ard 的行，Linux 程序员应该输入</p>
<p>grep ard f</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Liunx使用教程</tag>
      </tags>
  </entry>
  <entry>
    <title>Logo语言的学习</title>
    <url>/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://baike.baidu.com/item/logo/4689862">logo（程序设计语言）</a></p>
<p>2.<a href="https://www.w3cschool.cn/logo_cn/logo_cn-4fnq3l4z.html">Logo 简介_w3cschool</a></p>
<p>Logo 是一种易于学习的编程语言。它用于教学生和儿童如何编程计算机。它被开发用于处理单词列表。</p>
<h3 id="安装logo"><a href="#安装logo" class="headerlink" title="安装logo"></a>安装logo</h3><p>如果想要安装logo编程软件，点击<a href="https://www.softronix.com/logo_downloads/">MSWLogo Downloads – Softronics Inc. (softronix.com)</a></p>
<p>下载setup kit安装即可</p>
<h3 id="turtle"><a href="#turtle" class="headerlink" title="turtle"></a>turtle</h3><p>简单的 Logo 绘制命令可以前后移动 Turtle，也可以向右或向左转动。命令及其缩写如下:</p>
<ul>
<li><code>fd</code> – 前进</li>
<li><code>bk</code> – 向后</li>
<li><code>rt</code> – 右</li>
<li><code>lt</code> – 左</li>
<li><code>cs</code> – 清屏</li>
</ul>
<p>可以使用这些命令的任一版本。除了<code>cs</code>命令，这些命令中的每一个都必须跟一个称为参数的值。<code>fd</code>和<code>bk</code>的参数是单位；<code>rt</code>和<code>lt</code>的角度可以是任何整数。<strong>旋转 360 度</strong>是完整的旋转，因此<strong>旋转 375 度</strong>与 <strong>1/15 度</strong>相同。</p>
<ul>
<li><code>forward 60</code>或<code>fd 60</code>表示前进 60 步</li>
<li><code>right 90</code>或<code>rt 90</code>表示右转 90 度</li>
<li><code>left 90</code>或<code>lt 90</code>表示左转 90 度</li>
<li><code>back 60</code>或<code>bk 60</code>表示返回 60 步</li>
<li><code>clearscreen</code>或<code>cs</code>表示擦除所有绘图。这将 Turtle 设置在中心</li>
</ul>
<p>图形窗口有一个坐标系。中心的两个坐标（通常称为 x 和 y）的值为0、0。在东北角，它们是250、250；在东南角，它们是 250，-250。在西南角，它们是-250、-250；等等。如果 Turtle 试图走到屏幕的一侧，它会绕过去。右侧绕到左侧，顶部绕到底部。</p>
<h3 id="turtle命令"><a href="#turtle命令" class="headerlink" title="turtle命令"></a>turtle命令</h3><p>现在让我们尝试一些命令。命令将每行发出一个，然后是回车。可以在命令窗口中连续键入其中几个命令，然后按回车符。对 Turtle 的效果是一样的。但是，如果您键入一个命令，该命令需要一个或多个输入并在下一行提供缺少的输入，Logo 将显示错误。</p>
<p>示例1：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 尝试画一个领奖台</span></span><br><span class="line">fd 40 rt 90 fd 40 lt 90 fd 40 rt 90 fd 40 rt 90 fd 40 lt 90 fd 40 rt 90 fd 40 rt 90 fd 120</span><br></pre></td></tr></tbody></table></figure>
<p><style>.sitnefudxggh{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011194845234.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011194845234.png" srcset="data:image/png;base64,666" class="sitnefudxggh lazyload" alt="image-20221011194845234"></p>
<p>示例2：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 尝试画一个正五边形</span></span><br><span class="line">fd 60 rt 72 fd 60 rt 72 fd 60 rt 72 fd 60 rt 72 fd 60</span><br></pre></td></tr></tbody></table></figure>
<p><style>.ykgurhplqcgn{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011195053589.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011195053589.png" srcset="data:image/png;base64,666" class="ykgurhplqcgn lazyload" alt="image-20221011195053589"></p>
<p>现在你可以尝试画出以下图形或者任何你想画的了</p>
<p><style>.tpfrchmrurop{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011195259241.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011195259241.png" srcset="data:image/png;base64,666" class="tpfrchmrurop lazyload" alt="image-20221011195259241"></p>
<h3 id="控制turtle和笔"><a href="#控制turtle和笔" class="headerlink" title="控制turtle和笔"></a>控制turtle和笔</h3><p>Logo 还有许多其他绘图命令，其中一些在下面给出。</p>
<ul>
<li><strong>pu</strong> - penup</li>
<li><strong>pd</strong> - pendown</li>
<li><strong>ht</strong> - hideturtle</li>
<li><strong>dt</strong> - showturtle</li>
<li><strong>setpensize</strong></li>
</ul>
<p>pendown 和 penup 命令分别告诉 turtle 在移动时在屏幕上留下墨迹或不留下墨迹。该hideturtle和showturtle命令隐藏或显示 turtle，但不影响其离开墨因为它移动的能力。home 命令使 turtle 返回到屏幕的中心。当 turtle 回到屏幕中心时，它可能会留下墨水。setpensize 命令决定绘图笔的大小。</p>
<ul>
<li><strong>penup</strong>或<strong>pu</strong>表示拿起笔，因此您可以移动 turtle 而不会留下痕迹。</li>
<li><strong>pendown</strong>或<strong>pd</strong>表示放下笔，因此您可以移动 turtle 并留下轨迹。</li>
<li><strong>hideturtle</strong>或<strong>ht</strong>表示隐藏 turtle，这样您就可以欣赏您的画作。</li>
<li><strong>showturtle</strong>或<strong>st</strong>表示显示 turtle，因此您可以继续绘图。</li>
<li><strong>setpensize</strong>意味着它可以使笔更大，更容易看到。默认笔大小为<strong>–[1  1]</strong>。</li>
</ul>
<p>示例1：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 设置笔的大小，是[1 1],中间有空格</span></span><br><span class="line">setpensize [1 1] fd 40 setpensize [3 3] fd 40</span><br></pre></td></tr></tbody></table></figure>
<p><style>.sgnpcphwvmzv{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011201718341.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011201718341.png" srcset="data:image/png;base64,666" class="sgnpcphwvmzv lazyload" alt="image-20221011201718341"></p>
<p>示例2：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># repeat表示重复， 4为重复次数</span></span><br><span class="line"><span class="comment"># 展示pu, pd功能</span></span><br><span class="line">repeat 4[fd 30 pu fd 30 pd fd 30 rt 90]</span><br></pre></td></tr></tbody></table></figure>
<p><style>.lnumkvgprfjx{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011201922042.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011201922042.png" srcset="data:image/png;base64,666" class="lnumkvgprfjx lazyload" alt="image-20221011201922042"></p>
<p>示例3：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 展示ht, 默认st,如果你想重新显示turtle, 后面加上st就行了</span></span><br><span class="line">repeat 4[fd 30 pu fd 30 pd fd 30 rt 90] ht</span><br></pre></td></tr></tbody></table></figure>
<p><style>.efbzlyughdui{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011202159256.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011202159256.png" srcset="data:image/png;base64,666" class="efbzlyughdui lazyload" alt="image-20221011202159256"></p>
<h3 id="turtle世界"><a href="#turtle世界" class="headerlink" title="turtle世界"></a>turtle世界</h3><p>Logo 还有许多其他附加绘图命令，其中一些在下面给出。</p>
<ul>
<li><code>Home</code></li>
<li><code>cleartext</code> 或 <code>ct</code></li>
<li><code>label</code></li>
<li><code>setxy</code></li>
</ul>
<p><code>label</code>命令将单个单词作为带引号的字符串（例如“a_string”）或 [ ] 括号中不带引号的单词列表（例如 [Programming]），并将它们打印在 turtle 所在位置的图形窗口上. 让我们考虑以下代码。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">label <span class="string">"Programming repeat 3 [rt 90 label "</span>Programming ] ht</span><br></pre></td></tr></tbody></table></figure>
<p><style>.cwrzmfvoqcgn{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011202750309.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011202750309.png" srcset="data:image/png;base64,666" class="cwrzmfvoqcgn lazyload" alt="image-20221011202750309"></p>
<p><code>setxy</code>命令采用两个参数，第一个参数作为横坐标（水平轴）的值和第二个参数为纵坐标（垂直轴）的值。它将 turtle 放置在这些坐标处，可能在到达这些坐标时留下墨迹。</p>
<p>示例1：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">setxy 60 60</span><br></pre></td></tr></tbody></table></figure>
<p><style>.uxkqfpdtvtbs{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011203557341.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011203557341.png" srcset="data:image/png;base64,666" class="uxkqfpdtvtbs lazyload" alt="image-20221011203557341"></p>
<p>示例2：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">setxy 60 60 pu setxy -60 60</span><br></pre></td></tr></tbody></table></figure>
<p><style>.ybnbwosfynsc{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011203925009.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011203925009.png" srcset="data:image/png;base64,666" class="ybnbwosfynsc lazyload" alt="image-20221011203925009"></p>
<p><code>cleartext</code>命令，缩写为<code>ct</code>，用于清除命令窗口的文本区域。</p>
<p>尝试一下下面的命令</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">cs pu setxy -60 60 pd home rt 45 fd 85 lt 135 fd 120</span><br></pre></td></tr></tbody></table></figure>
<p><style>.tcvongchehhx{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011204450563.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011204450563.png" srcset="data:image/png;base64,666" class="tcvongchehhx lazyload" alt="image-20221011204450563"></p>
<p>当您从左到右阅读这些命令时，请对其进行解释。尝试找出结果。 以下是命令摘要表。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">命令名称</th>
<th style="text-align:left">目的</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">setx 100</td>
<td style="text-align:left">将turtle的 x 坐标设置为 +100;将其移动到中心右侧 100 点;无垂直变化</td>
</tr>
<tr>
<td style="text-align:left">setx -200</td>
<td style="text-align:left">将turtle移动到中心左侧 200 点;无垂直变化</td>
</tr>
<tr>
<td style="text-align:left">150</td>
<td style="text-align:left">将turtle的 y 坐标设置为 150;将其移动到中心上方 150 点;无横向变化</td>
</tr>
<tr>
<td style="text-align:left">sety - 50</td>
<td style="text-align:left">将turtle移动到中心下方 50 点; 无横向变化</td>
</tr>
<tr>
<td style="text-align:left">setxy 100 100</td>
<td style="text-align:left">将turtle移动到 xy 坐标 (100,100)</td>
</tr>
<tr>
<td style="text-align:left">show xcor;show ycor</td>
<td style="text-align:left">报告turtle的 x 坐标;报告turtle的 y 坐标</td>
</tr>
<tr>
<td style="text-align:left">setheading 0;seth 0</td>
<td style="text-align:left">将turtle直指，“正午”</td>
</tr>
<tr>
<td style="text-align:left">seth 120</td>
<td style="text-align:left">将turtle移动 120 度以指向四点钟位置</td>
</tr>
</tbody>
</table>
</div>
<h3 id="logo变量"><a href="#logo变量" class="headerlink" title="logo变量"></a>logo变量</h3><p>变量是可以包含值的内存位置的名称。在计算机中，每个内存位置都有一个整数地址。由于很难记住包含程序使用的值的每个位置的地址，计算机科学家已经找到了给这些位置、符号名称的方法。一旦变量有了名称，我们就可以使用和操作它。</p>
<p>变量的名称是字母串。变量名可以包含字母（不区分大小写）、数字和下划线。变量名可以在计算中使用<code>:</code>在它之前访问。让我们在屏幕截图中考虑以下示例。</p>
<p>示例：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">make <span class="string">"val1 100</span></span><br><span class="line"><span class="string">make "</span>val2 200</span><br><span class="line"><span class="built_in">print</span> :val1</span><br><span class="line">100</span><br><span class="line"><span class="built_in">print</span> : val2</span><br><span class="line"> has no value</span><br><span class="line"><span class="built_in">print</span> :val1 + :val2</span><br><span class="line">300</span><br><span class="line"><span class="built_in">print</span> :val1 - :val2</span><br><span class="line">-100</span><br></pre></td></tr></tbody></table></figure>
<h3 id="logo运算符"><a href="#logo运算符" class="headerlink" title="logo运算符"></a>logo运算符</h3><p>Logo 提供了常用的加、减、乘、除算术运算，用符号<code>+、-、*、/</code>表示。这些操作中的每一个都会产生一个结果。如果对结果不做任何处理，例如打印它，Logo 将显示错误。</p>
<p>使用打印命令，可以在命令窗口中使用和打印算术运算的结果。以下屏幕截图中给出的示例演示了相同的内容。</p>
<p>其他有用的命令是：</p>
<ul>
<li><strong>sqrt</strong> - 它接受一个非负参数并返回其平方根。</li>
<li><strong>power</strong> - 它需要两个参数，称为“a”和“b”，并生成 a 的 b 次幂。</li>
<li><strong>ln</strong> - 它接受一个参数并返回其自然对数。</li>
<li><strong>exp</strong> - 它需要一个参数并计算 e 的那个幂，e 是自然数 2.718281828。</li>
<li><strong>log10</strong> - 取其一个参数的以 10 为底的对数。</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">print</span> sqrt 16</span><br><span class="line">4</span><br><span class="line"><span class="built_in">print</span> power 10 3</span><br><span class="line">1000</span><br><span class="line"><span class="built_in">print</span> <span class="built_in">ln</span> 2</span><br><span class="line">0.693147180559945</span><br><span class="line"><span class="built_in">print</span> exp 2</span><br><span class="line">7.38905609893065</span><br><span class="line"><span class="built_in">print</span> log10 1000</span><br><span class="line">3</span><br></pre></td></tr></tbody></table></figure>
<p>算术运算符的优先级决定了它们的计算顺序。</p>
<p><strong>注意:</strong> 打印<code>60 * sqrt 2</code>和打印<code>sqrt 2 * 60</code>会产生不同的答案。这里<code>*</code>运算符优先于 sqrt 运算符。因此，如果有选择，<code>*</code>将在 sqrt 之前完成，就像在第二种情况下一样。</p>
<h3 id="logo重复"><a href="#logo重复" class="headerlink" title="logo重复"></a>logo重复</h3><p>让我们假设我们想画一个边长为 100 的正方形，我们可以使用以下程序来做到这一点：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">fd 100</span><br><span class="line">rt 90</span><br><span class="line">fd 100</span><br><span class="line">rt 90</span><br><span class="line">fd 100</span><br><span class="line">rt 90</span><br><span class="line">fd 100</span><br><span class="line">rt 90</span><br><span class="line"><span class="comment"># 你可以写成下面的形式</span></span><br><span class="line">repeat 4[fd 100 rt 90]</span><br></pre></td></tr></tbody></table></figure>
<h3 id="logo随机化"><a href="#logo随机化" class="headerlink" title="logo随机化"></a>logo随机化</h3><p>有时，计算出不可预测的结果很有趣。Logo 提供了一个随机程序来生成一个随机数。它有一个参数并产生一个随机统一选择的整数值，该值大于或等于 0 且小于其参数的值。因此，如果您想要一个 0 到 359 度之间的随机角度，您可以使用命令<code>random 360</code>来生成它。请记住，除非您对结果执行某些操作（例如打印），否则 Logo 将显示错误。</p>
<p>示例1：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">print</span> random 360</span><br><span class="line">231</span><br></pre></td></tr></tbody></table></figure>
<p>示例2：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">repeat 100[fd random 80 rt 90]</span><br></pre></td></tr></tbody></table></figure>
<p><style>.podvsjqiigyd{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011211802488.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011211802488.png" srcset="data:image/png;base64,666" class="podvsjqiigyd lazyload" alt="image-20221011211802488"></p>
<p>示例3：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">repeat 1000[fd 10 rt random 360]</span><br></pre></td></tr></tbody></table></figure>
<p><style>.jvxodnubmfbw{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011212013978.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011212013978.png" srcset="data:image/png;base64,666" class="jvxodnubmfbw lazyload" alt="image-20221011212013978"></p>
<h3 id="logo程序"><a href="#logo程序" class="headerlink" title="logo程序"></a>logo程序</h3><p>过程提供了一种封装命令集合的方法。一旦创建了过程，就可以像使用内置命令一样使用它。一个过程的“意义”就是它的各个命令的意义。</p>
<p>没有参数的过程在第一行有单词<code>to</code>（保留字）和过程名称。（Logo 中的保留字不能作为变量使用，有明确的含义和用途。）它在最后一行有保留字<code>end</code>。</p>
<p>子程序是供另一个程序执行的命名步骤序列。子程序的其他名称是过程和函数。在 Logo 中，你告诉计算机如何做某事——例如:</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">to square</span><br><span class="line">repeat 4 [fd 100 rt 90]</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure>
<p>一旦我们向 Logo 描述了我们的过程，我们就可以在命令行上输入它的名称，就像我们对任何内置的东西所做的一样。在这种情况下，我们将在命令行上输入<code>square</code>，Logo 会查找命令以制作一个正方形。</p>
<p>单击显示Edall（用于编辑全部）的按钮以调出 Logo 的内置编辑器。（如果您的徽标没有 Edall 按钮，请在命令行中输入<code>edall</code>）。以下代码块具有子程序所需的结构。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">to procedurename</span><br><span class="line">steps of your procedure here</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure>
<p>过程或子程序必须以<code>to</code>这个词开头，后面跟着一个我们想到的名字。下一步是键入我们将在命令行上编写的所有相同步骤。该过程必须以<code>end</code>一词结尾。所有注释或备注行都应以分号 <code>;</code>开头。</p>
<p>我们不希望每个方格的大小都一样——我们想要多样性。在 Logo 中，我们创建了变量，我们可以更改其值。在以下示例中，我们将使用相同的平方程序，但稍作改动。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">to square :n</span><br><span class="line">repeat 4 [fd :n rt 90]</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure>
<p>我们在命令行上给 Logo 一个<code>:n</code>的替换值，如下所示。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">square 40</span><br><span class="line">square 50</span><br><span class="line">square 70</span><br></pre></td></tr></tbody></table></figure>
<p><style>.ruuxrgrpduld{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011213810341.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011213810341.png" srcset="data:image/png;base64,666" class="ruuxrgrpduld lazyload" alt="image-20221011213810341"></p>
<h3 id="logo递归过程"><a href="#logo递归过程" class="headerlink" title="logo递归过程"></a>logo递归过程</h3><p>在递归过程中，过程中会有一个过程的递归调用。让我们考虑以下代码 </p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"># 螺旋线， 图形中n=30</span><br><span class="line">to spiral_recur :n</span><br><span class="line">   if :n &lt; 1 [stop]</span><br><span class="line">   fd :n</span><br><span class="line">   rt 20</span><br><span class="line">   spiral_recur 0.95 * :n</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure>
<p><style>.lrslqqlvlonm{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011214339527.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011214339527.png" srcset="data:image/png;base64,666" class="lrslqqlvlonm lazyload" alt="image-20221011214339527"></p>
<h3 id="logo决策"><a href="#logo决策" class="headerlink" title="logo决策"></a>logo决策</h3><p>决策和变量相辅相成。程序需要能够根据情况改变课程。例如，下面是绘制螺旋的框架。它有一个循环，是前面显示的重复的变体，循环的主体供我们填写。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">to spiral</span><br><span class="line">   make <span class="string">"n 1</span></span><br><span class="line"><span class="string">   while [:n &lt; 100] [</span></span><br><span class="line"><span class="string">      make "</span>n :n + 5</span><br><span class="line">      fd :n rt 90</span><br><span class="line">   ]</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure>
<p>上面的代码展示了 MSW Logo 语法的几个新特性。我们通过键入<code>make</code>将一个变量设置为一个新值，然后变量的名称前面是双引号<code>"</code>而不是冒号<code>:</code> ，如下所示。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">make <span class="string">"n 1</span></span><br></pre></td></tr></tbody></table></figure>
<p>不过，我们使用了一个变量，在它的名称前有一个冒号<code>:</code></p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> [:n &lt; 100]</span><br></pre></td></tr></tbody></table></figure>
<p><code>while [condition]</code>后括号内的代码被执行，而条件为真。当它不再为真时，因为（在这种情况下）<code>:n</code>的值增长大于 100，执行括号后面的代码。</p>
<p><style>.vwqegucamdzs{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011220420163.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011220420163.png" srcset="data:image/png;base64,666" class="vwqegucamdzs lazyload" alt="image-20221011220420163"></p>
<p>现在，我们将讨论<strong>if 语句</strong>的使用，它具有仅在给定条件为真时才会执行的代码。</p>
<p>它还显示了一个生成随机数的内置徽标。语句random 3在随机序列中任意生成任意数字 0 或 1 或 2。然后程序决定“随机”走哪条路。生成的随机数将保存在 <code>r</code> 中，稍后将根据变量<code>r</code>的值执行 if 语句之一，这将满足条件。因此，如果:</p>
<ul>
<li><code>r</code> 的值为 0，则将执行[fd 20]。</li>
<li><code>r</code> 的值为 1，则将执行[rt 90 fd 20]。</li>
<li><code>r</code> 的值为 2，则将执行[lt 90 fd 20]。</li>
</ul>
<p>代码：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">to randomwalk</span><br><span class="line">repeat 100[</span><br><span class="line"> make "r random 3</span><br><span class="line"> if :r = 0 [fd 20]</span><br><span class="line"> if :r = 1 [rt 90 fd 20]</span><br><span class="line"> if :r = 2 [lt 90 fd 20]</span><br><span class="line">]</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure>
<p><style>.gueckcgudymo{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011221139685.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011221139685.png" srcset="data:image/png;base64,666" class="gueckcgudymo lazyload" alt="image-20221011221139685"></p>
<h3 id="logo字符串"><a href="#logo字符串" class="headerlink" title="logo字符串"></a>logo字符串</h3><p>任何字母数字字符序列，例如<code>america</code>、<code>emp1234</code>等，都是字符串的示例。计算字符数是所有字符串过程中最基本的。问题<code>stringlength "abc12ef</code>的答案由以下过程给出:</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">to stringlength :s</span><br><span class="line">   make <span class="string">"inputstring :s</span></span><br><span class="line"><span class="string">   make "</span>count 0</span><br><span class="line">   <span class="keyword">while</span> [not emptyp :s] [</span><br><span class="line">      make <span class="string">"count :count + 1</span></span><br><span class="line"><span class="string">      print first :s</span></span><br><span class="line"><span class="string">      make "</span>s butfirst :s</span><br><span class="line">   ]</span><br><span class="line">   <span class="built_in">print</span> (sentence :inputstring <span class="string">"has :count "</span>letters)</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure>
<p>在上面的过程中<code>s</code>是包含输入字符串的变量。变量<code>inputstring</code>包含输入字符串的副本。变量计数初始化为 0。在 while 循环中，条件检查字符串是否为空。在每个循环计数中，一个变量增加 1 以保持长度计数。语句<code>print first :s</code>仅打印存储在<code>s</code>中的字符串的第一个字符。</p>
<p>语句<code>make "s butfirst :s</code>，检索不包括第一个字符的子字符串。退出while循环后，我们打印了输入字符串的字符数或长度。</p>
<h3 id="logo颜色"><a href="#logo颜色" class="headerlink" title="logo颜色"></a>logo颜色</h3><p>计算机屏幕使用红色、绿色和蓝色的光成分，因此它们有时被称为RGB 屏幕。 在 Logo 的设置菜单上，我们可以设置三个屏幕元素的颜色:</p>
<ul>
<li>turtle 的笔</li>
<li>turtle 的填充物（就像围栏的油漆桶）</li>
<li>画面背景</li>
</ul>
<p><style>.xhvfvoqqwice{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011221830591.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011221830591.png" srcset="data:image/png;base64,666" class="xhvfvoqqwice lazyload" alt="image-20221011221830591"></p>
<p>我们通过左右移动这三个滑块来设置颜色。请记住，黑色是所有颜色的缺失，而白色是所有颜色的结合。混合光不像混合油漆。例如，如果您将红色和绿色颜料混合，则会得到浑浊的颜色。由于这是一台计算机，因此每种颜色都有一个内部数字表示。</p>
<p>滑动刻度的左端为零 (0)。右端是 255，这有点像计算机的 99（它是 2 8 - 1）。因此黑色是[0 0 0]，红色是[255 0 0]，绿色是[0 255 0]，蓝色是[0 0 255]。你可以在这些颜色之间制作任何东西，在所有这些颜色中，有<code>256 * 256 * 256</code>种可能的颜色。那是<code>2^8 * 2^8 * 2^8</code>，或 24 位颜色 — 机器内部的 24 位二进制数字。 以下命令会给你一个大红笔 -</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">setpensize [5 5]</span><br><span class="line">setpencolor [255 0 0]</span><br></pre></td></tr></tbody></table></figure>
<p>当您使用滑块找到您喜欢的颜色时，您可以询问 Logo 它是什么：选择笔的颜色，然后在命令窗口中输入以下命令。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">show pencolor</span><br></pre></td></tr></tbody></table></figure>
<p>您可以使用以下步骤制作彩色方块 -</p>
<p><strong>步骤 1</strong> - 使用以下命令绘制边长为 40 的正方形。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">repeat 4 [fd 40 rt 90]</span><br></pre></td></tr></tbody></table></figure>
<p><strong>步骤 2</strong> - 使用以下命令完成。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">pu</span><br></pre></td></tr></tbody></table></figure>
<p><strong>步骤 3</strong> - 转到正方形内的一个点。例如，使用以下命令将海龟放置在坐标 (20, 20) 处。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">setxy 20 20</span><br></pre></td></tr></tbody></table></figure>
<p><strong>步骤 4</strong> - 用设置的泛色填充正方形。例如，要将泛光颜色设置为蓝色，请使用以下命令。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">setfloodcolor [0 0 255]</span><br></pre></td></tr></tbody></table></figure>
<p>下表列出了更多与颜色和笔相关的命令。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">颜色和笔命令</th>
<th style="text-align:left">命令的目的</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">setpencolor [rgb]；setpc [rgb]</td>
<td style="text-align:left">设置turtle笔的颜色 rgb 是 [0, 255] 范围内的数字</td>
</tr>
<tr>
<td style="text-align:left">setfloodcolor [rgb]； setfc [rgb]</td>
<td style="text-align:left">设置批注区域的颜色</td>
</tr>
<tr>
<td style="text-align:left">设置屏幕颜色 [rgb]; 设置sc [rgb]</td>
<td style="text-align:left">设置背景颜色</td>
</tr>
<tr>
<td style="text-align:left">显示笔色; 显示泛色; 显示屏幕颜色</td>
<td style="text-align:left">指定命名项的 [rgb] 的当前值</td>
</tr>
<tr>
<td style="text-align:left">填充</td>
<td style="text-align:left">在光标位置倾倒一桶当前的泛色</td>
</tr>
</tbody>
</table>
</div>
<p>尝试执行以下命令集：</p>
<ul>
<li><strong>cs</strong> - 清除屏幕。</li>
<li><strong>home</strong> - 将turtle放置在初识位置。</li>
<li><strong>setpensize [5 5]</strong> - 设置笔的大小。</li>
<li><strong>setpencolor [255 0 0]</strong> - 将笔颜色设置为红色。</li>
<li><strong>setfloodcolor [0 0 255]</strong> - 将泛色设置为蓝色。</li>
<li><strong>setscreencolor [0 255 0]</strong> - 将屏幕颜色设置为绿色。</li>
<li><strong>repeat 4 [fd 40 rt 90]</strong> - 画一个边长为 40 的正方形。</li>
<li><strong>pu</strong> - 拿起钢笔。</li>
<li><strong>setxy 20 20</strong> - 将turtle放在坐标 (20, 20) 处。</li>
<li><strong>fill</strong>- 用设置的泛光蓝色填充正方形。</li>
<li><strong>ht</strong> - 隐藏turtle。</li>
</ul>
<p><style>.vhfzgdixfbwn{zoom:50%;}</style><img src="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011222542070.png" class="lazyload" data-srcset="/zh-CN/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Logo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20221011222542070.png" srcset="data:image/png;base64,666" class="vhfzgdixfbwn lazyload" alt="image-20221011222542070"></p>
<p>好了，到此结束，有点像python里的turtle库，或许就是，再见！！！</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>logo</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Matplotlib学习笔记</title>
    <url>/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Matplotlib 是 Python 的绘图库，它能让使用者很轻松地将数据图形化，并且提供多样化的输出格式。可以用来绘制各种静态，动态，交互式的图表。是一个非常强大的 Python 画图工具，我们可以使用该工具将很多数据通过图表的形式更直观的呈现出来。可以绘制线图、散点图、等高线图、条形图、柱状图、3D 图形、甚至是图形动画等等。</p>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>Matplotlib 通常与 NumPy 和 SciPy（Scientific Python）一起使用， 这种组合广泛用于替代 MatLab，是一个强大的科学计算环境，有助于我们通过 Python 学习数据科学或者机器学习。</p>
<p>SciPy 是一个开源的 Python 算法库和数学工具包。</p>
<p>SciPy 包含的模块有最优化、线性代数、积分、插值、特殊函数、快速傅里叶变换、信号处理和图像处理、常微分方程求解和其他科学与工程中常用的计算。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>本章节，我们使用 pip 工具来安装 <code>Matplotlib</code> 库，如果还未安装该工具，可以参考 <a href="https://www.runoob.com/w3cnote/python-pip-install-usage.html">Python pip 安装与使用</a>。</p>
<p>升级 pip：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">python3 -m pip install -U pip</span><br></pre></td></tr></tbody></table></figure>
<p>安装 <code>matplotlib</code> 库：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">python3 -m pip install -U matplotlib</span><br></pre></td></tr></tbody></table></figure>
<p>安装完成后，我们就可以通过 import 来导入<code>matplotlib</code> 库：</p>
<p><code>import matplotlib</code></p>
<p>以下实例，我们通过导入 <code>matplotlib</code> 库，然后查看 <code>matplotlib</code>库的版本号：</p>
<p>实例1:</p>
<p><code>import matplotlib</code></p>
<p><code>print(matplotlib.__version__)</code></p>
<p>执行以上代码，输出结果如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">3.4.2</span><br></pre></td></tr></tbody></table></figure>
<h2 id="Matplotlib-Pyplot"><a href="#Matplotlib-Pyplot" class="headerlink" title="Matplotlib Pyplot"></a>Matplotlib Pyplot</h2><p>Pyplot 是 Matplotlib 的子库，提供了和 MATLAB 类似的绘图 API。</p>
<p>Pyplot 是常用的绘图模块，能很方便让用户绘制 2D 图表。</p>
<p>Pyplot 包含一系列绘图函数的相关函数，每个函数会对当前的图像进行一些修改，例如：给图像加上标记，生新的图像，在图像中产生新的绘图区域等等。</p>
<p>使用的时候，我们可以使用 import 导入 pyplot 库，并设置一个别名<code>plt</code>：</p>
<p><code>import matplotlib.pyplot as plt</code></p>
<p>这样我们就可以使用 <strong>plt</strong> 来引用 Pyplot 包的方法。</p>
<p>以下实例，我们通过两个坐标 <strong>(0,0)</strong> 到 <strong>(6,100)</strong> 来绘制一条线:</p>
<p>实例1:</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">xpoints = np.array([<span class="number">0</span>, <span class="number">6</span>])</span><br><span class="line">ypoints = np.array([<span class="number">0</span>, <span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(xpoints, ypoints)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p>输出结果如下：</p>
<p><style>.sexxekknwfwg{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403204022943.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403204022943.png" srcset="data:image/png;base64,666" class="sexxekknwfwg lazyload"></p>
<p>以上实例中我们使用了 Pyplot 的<code>plot()</code> 函数， <strong>plot()</strong> 函数是绘制二维图形的最基本函数。</p>
<p><strong>plot()</strong> 用于画图它可以绘制点和线，语法格式如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 画单条线</span></span><br><span class="line">plot([x], y, [fmt], *, data=<span class="literal">None</span>, **kwargs)</span><br><span class="line"><span class="comment"># 画多条线</span></span><br><span class="line">plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)</span><br></pre></td></tr></tbody></table></figure>
<p>参数说明：</p>
<ul>
<li><strong>x, y：</strong>点或线的节点，x 为 x 轴数据，y 为 y 轴数据，数据可以列表或数组。</li>
<li><strong>fmt：</strong>可选，定义基本格式（如颜色、标记和线条样式）。</li>
<li><strong><em>\</em>kwargs：</strong>可选，用在二维平面图上，设置指定属性，如标签，线的宽度等。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>plot(x, y)        <span class="comment"># 创建 y 中数据与 x 中对应值的二维线图，使用默认样式</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plot(x, y, <span class="string">'bo'</span>)  <span class="comment"># 创建 y 中数据与 x 中对应值的二维线图，使用蓝色实心圈绘制</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plot(y)           <span class="comment"># x 的值为 0..N-1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plot(y, <span class="string">'r+'</span>)     <span class="comment"># 使用红色 + 号</span></span><br></pre></td></tr></tbody></table></figure>
<p><strong>颜色字符：</strong>‘b’ 蓝色，’m’ 洋红色，’g’ 绿色，’y’ 黄色，’r’ 红色，’k’ 黑色，’w’ 白色，’c’ 青绿色，’#008000’ RGB 颜色符串。多条曲线不指定颜色时，会自动选择不同颜色。</p>
<p><strong>线型参数：</strong>‘‐’ 实线，’‐‐’ 破折线，’‐.’ 点划线，’:’ 虚线。</p>
<p><strong>标记字符：</strong>‘.’ 点标记，’,’ 像素标记(极小点)，’o’ 实心圈标记，’v’ 倒三角标记，’^’ 上三角标记，’&gt;’ 右三角标记，’&lt;’ 左三角标记…等等。</p>
<p>如果我们只想绘制两个坐标点，而不是一条线，可以使用 <strong>o</strong> 参数，表示一个实心圈的标记。</p>
<p>实例2：绘制坐标 (1, 3) 和 (8, 10) 的两个点</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">xpoints = np.array([<span class="number">1</span>, <span class="number">8</span>])</span><br><span class="line">ypoints = np.array([<span class="number">3</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(xpoints, ypoints, <span class="string">'o'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403205635489.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403205635489.png" srcset="data:image/png;base64,666"></p>
<p>我们也可以绘制任意数量的点，只需确保两个轴上的点数相同即可。</p>
<p>实例3：绘制一条不规则线，坐标为 (1, 3) 、 (2, 8) 、(6, 1) 、(8, 10)，对应的两个数组为：[1, 2, 6, 8] 与 [3, 8, 1, 10]。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">xpoints = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">8</span>])</span><br><span class="line">ypoints = np.array([<span class="number">3</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(xpoints, ypoints)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.vbttoxlfufnd{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403205827906.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403205827906.png" srcset="data:image/png;base64,666" class="vbttoxlfufnd lazyload"></p>
<p>实例4：如果我们不指定 x 轴上的点，y只限定范围。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">3</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.vpfhmzlthvdy{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403210031623.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403210031623.png" srcset="data:image/png;base64,666" class="vpfhmzlthvdy lazyload"></p>
<p>从上图可以看出 x 的值默认设置为 <strong>[0, 1]</strong>。</p>
<p>实例5：如果我们不指定 x 轴上的点，y表明具体的点，则 x 会根据 y 的值来设置为 <strong>0, 1, 2, 3..N-1</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">3</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.sexddnpydusj{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403210331668.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403210331668.png" srcset="data:image/png;base64,666" class="sexddnpydusj lazyload"></p>
<p>实例6：以下实例我们绘制一个正弦和余弦图，在 plt.plot() 参数中包含两对 <strong>x,y</strong> 值，第一对是 <strong>x,y</strong>，这对应于正弦函数，第二对是 <strong>x,z</strong>，这对应于余弦函数。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">4</span>*np.pi,<span class="number">0.1</span>)   <span class="comment"># start,stop,step</span></span><br><span class="line">y = np.sin(x)</span><br><span class="line">z = np.cos(x)</span><br><span class="line">plt.plot(x,y,x,z)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.mytbetdbpxul{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403210514868.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403210514868.png" srcset="data:image/png;base64,666" class="mytbetdbpxul lazyload"></p>
<h2 id="Matplotlib-绘图标记"><a href="#Matplotlib-绘图标记" class="headerlink" title="Matplotlib 绘图标记"></a>Matplotlib 绘图标记</h2><p>绘图过程如果我们想要给坐标自定义一些不一样的标记，就可以使用 <strong>plot()</strong> 方法的 <code>marker</code>参数来定义。</p>
<p>实例1:定义实心圆标记</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">6</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints, marker = <span class="string">'o'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.hvqlefmnbsmw{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403212010830.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403212010830.png" srcset="data:image/png;base64,666" class="hvqlefmnbsmw lazyload"></p>
<p>marker可以定义的符号如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">标记</th>
<th style="text-align:left">符号</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">“.”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m00.png" class="lazyload" data-srcset="https://www.runoob.com/images/m00.png" srcset="data:image/png;base64,666" alt="m00"></td>
<td style="text-align:left">点</td>
</tr>
<tr>
<td style="text-align:left">“,”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m01.png" class="lazyload" data-srcset="https://www.runoob.com/images/m01.png" srcset="data:image/png;base64,666" alt="m01"></td>
<td style="text-align:left">像素点</td>
</tr>
<tr>
<td style="text-align:left">“o”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m02.png" class="lazyload" data-srcset="https://www.runoob.com/images/m02.png" srcset="data:image/png;base64,666" alt="m02"></td>
<td style="text-align:left">实心圆</td>
</tr>
<tr>
<td style="text-align:left">“v”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m03.png" class="lazyload" data-srcset="https://www.runoob.com/images/m03.png" srcset="data:image/png;base64,666" alt="m03"></td>
<td style="text-align:left">下三角</td>
</tr>
<tr>
<td style="text-align:left">“^”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m04.png" class="lazyload" data-srcset="https://www.runoob.com/images/m04.png" srcset="data:image/png;base64,666" alt="m04"></td>
<td style="text-align:left">上三角</td>
</tr>
<tr>
<td style="text-align:left">“&lt;”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m05.png" class="lazyload" data-srcset="https://www.runoob.com/images/m05.png" srcset="data:image/png;base64,666" alt="m05"></td>
<td style="text-align:left">左三角</td>
</tr>
<tr>
<td style="text-align:left">“&gt;”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m06.png" class="lazyload" data-srcset="https://www.runoob.com/images/m06.png" srcset="data:image/png;base64,666" alt="m06"></td>
<td style="text-align:left">右三角</td>
</tr>
<tr>
<td style="text-align:left">“1”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m07.png" class="lazyload" data-srcset="https://www.runoob.com/images/m07.png" srcset="data:image/png;base64,666" alt="m07"></td>
<td style="text-align:left">下三叉</td>
</tr>
<tr>
<td style="text-align:left">“2”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m08.png" class="lazyload" data-srcset="https://www.runoob.com/images/m08.png" srcset="data:image/png;base64,666" alt="m08"></td>
<td style="text-align:left">上三叉</td>
</tr>
<tr>
<td style="text-align:left">“3”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m09.png" class="lazyload" data-srcset="https://www.runoob.com/images/m09.png" srcset="data:image/png;base64,666" alt="m09"></td>
<td style="text-align:left">左三叉</td>
</tr>
<tr>
<td style="text-align:left">“4”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m10.png" class="lazyload" data-srcset="https://www.runoob.com/images/m10.png" srcset="data:image/png;base64,666" alt="m10"></td>
<td style="text-align:left">右三叉</td>
</tr>
<tr>
<td style="text-align:left">“8”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m11.png" class="lazyload" data-srcset="https://www.runoob.com/images/m11.png" srcset="data:image/png;base64,666" alt="m11"></td>
<td style="text-align:left">八角形</td>
</tr>
<tr>
<td style="text-align:left">“s”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m12.png" class="lazyload" data-srcset="https://www.runoob.com/images/m12.png" srcset="data:image/png;base64,666" alt="m12"></td>
<td style="text-align:left">正方形</td>
</tr>
<tr>
<td style="text-align:left">“p”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m13.png" class="lazyload" data-srcset="https://www.runoob.com/images/m13.png" srcset="data:image/png;base64,666" alt="m13"></td>
<td style="text-align:left">五边形</td>
</tr>
<tr>
<td style="text-align:left">“P”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m23.png" class="lazyload" data-srcset="https://www.runoob.com/images/m23.png" srcset="data:image/png;base64,666" alt="m23"></td>
<td style="text-align:left">加号（填充）</td>
</tr>
<tr>
<td style="text-align:left">“*”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m14.png" class="lazyload" data-srcset="https://www.runoob.com/images/m14.png" srcset="data:image/png;base64,666" alt="m14"></td>
<td style="text-align:left">星号</td>
</tr>
<tr>
<td style="text-align:left">“h”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m15.png" class="lazyload" data-srcset="https://www.runoob.com/images/m15.png" srcset="data:image/png;base64,666" alt="m15"></td>
<td style="text-align:left">六边形 1</td>
</tr>
<tr>
<td style="text-align:left">“H”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m16.png" class="lazyload" data-srcset="https://www.runoob.com/images/m16.png" srcset="data:image/png;base64,666" alt="m16"></td>
<td style="text-align:left">六边形 2</td>
</tr>
<tr>
<td style="text-align:left">“+”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m17.png" class="lazyload" data-srcset="https://www.runoob.com/images/m17.png" srcset="data:image/png;base64,666" alt="m17"></td>
<td style="text-align:left">加号</td>
</tr>
<tr>
<td style="text-align:left">“x”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m18.png" class="lazyload" data-srcset="https://www.runoob.com/images/m18.png" srcset="data:image/png;base64,666" alt="m18"></td>
<td style="text-align:left">乘号 x</td>
</tr>
<tr>
<td style="text-align:left">“X”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m24.png" class="lazyload" data-srcset="https://www.runoob.com/images/m24.png" srcset="data:image/png;base64,666" alt="m24"></td>
<td style="text-align:left">乘号 x (填充)</td>
</tr>
<tr>
<td style="text-align:left">“D”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m19.png" class="lazyload" data-srcset="https://www.runoob.com/images/m19.png" srcset="data:image/png;base64,666" alt="m19"></td>
<td style="text-align:left">菱形</td>
</tr>
<tr>
<td style="text-align:left">“d”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m20.png" class="lazyload" data-srcset="https://www.runoob.com/images/m20.png" srcset="data:image/png;base64,666" alt="m20"></td>
<td style="text-align:left">瘦菱形</td>
</tr>
<tr>
<td style="text-align:left">“\</td>
<td style="text-align:left">“</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m21.png" class="lazyload" data-srcset="https://www.runoob.com/images/m21.png" srcset="data:image/png;base64,666" alt="m21"></td>
<td>竖线</td>
</tr>
<tr>
<td style="text-align:left">“_”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m22.png" class="lazyload" data-srcset="https://www.runoob.com/images/m22.png" srcset="data:image/png;base64,666" alt="m22"></td>
<td style="text-align:left">横线</td>
</tr>
<tr>
<td style="text-align:left">0 (TICKLEFT)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m25.png" class="lazyload" data-srcset="https://www.runoob.com/images/m25.png" srcset="data:image/png;base64,666" alt="m25"></td>
<td style="text-align:left">左横线</td>
</tr>
<tr>
<td style="text-align:left">1 (TICKRIGHT)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m26.png" class="lazyload" data-srcset="https://www.runoob.com/images/m26.png" srcset="data:image/png;base64,666" alt="m26"></td>
<td style="text-align:left">右横线</td>
</tr>
<tr>
<td style="text-align:left">2 (TICKUP)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m27.png" class="lazyload" data-srcset="https://www.runoob.com/images/m27.png" srcset="data:image/png;base64,666" alt="m27"></td>
<td style="text-align:left">上竖线</td>
</tr>
<tr>
<td style="text-align:left">3 (TICKDOWN)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m28.png" class="lazyload" data-srcset="https://www.runoob.com/images/m28.png" srcset="data:image/png;base64,666" alt="m28"></td>
<td style="text-align:left">下竖线</td>
</tr>
<tr>
<td style="text-align:left">4 (CARETLEFT)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m29.png" class="lazyload" data-srcset="https://www.runoob.com/images/m29.png" srcset="data:image/png;base64,666" alt="m29"></td>
<td style="text-align:left">左箭头</td>
</tr>
<tr>
<td style="text-align:left">5 (CARETRIGHT)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m30.png" class="lazyload" data-srcset="https://www.runoob.com/images/m30.png" srcset="data:image/png;base64,666" alt="m30"></td>
<td style="text-align:left">右箭头</td>
</tr>
<tr>
<td style="text-align:left">6 (CARETUP)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m31.png" class="lazyload" data-srcset="https://www.runoob.com/images/m31.png" srcset="data:image/png;base64,666" alt="m31"></td>
<td style="text-align:left">上箭头</td>
</tr>
<tr>
<td style="text-align:left">7 (CARETDOWN)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m32.png" class="lazyload" data-srcset="https://www.runoob.com/images/m32.png" srcset="data:image/png;base64,666" alt="m32"></td>
<td style="text-align:left">下箭头</td>
</tr>
<tr>
<td style="text-align:left">8 (CARETLEFTBASE)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m33.png" class="lazyload" data-srcset="https://www.runoob.com/images/m33.png" srcset="data:image/png;base64,666" alt="m33"></td>
<td style="text-align:left">左箭头 (中间点为基准)</td>
</tr>
<tr>
<td style="text-align:left">9 (CARETRIGHTBASE)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m34.png" class="lazyload" data-srcset="https://www.runoob.com/images/m34.png" srcset="data:image/png;base64,666" alt="m34"></td>
<td style="text-align:left">右箭头 (中间点为基准)</td>
</tr>
<tr>
<td style="text-align:left">10 (CARETUPBASE)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m35.png" class="lazyload" data-srcset="https://www.runoob.com/images/m35.png" srcset="data:image/png;base64,666" alt="m35"></td>
<td style="text-align:left">上箭头 (中间点为基准)</td>
</tr>
<tr>
<td style="text-align:left">11 (CARETDOWNBASE)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m36.png" class="lazyload" data-srcset="https://www.runoob.com/images/m36.png" srcset="data:image/png;base64,666" alt="m36"></td>
<td style="text-align:left">下箭头 (中间点为基准)</td>
</tr>
<tr>
<td style="text-align:left">“None”, “ “ or “”</td>
<td style="text-align:left"></td>
<td style="text-align:left">没有任何标记</td>
</tr>
<tr>
<td style="text-align:left">‘$…$’</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m37.png" class="lazyload" data-srcset="https://www.runoob.com/images/m37.png" srcset="data:image/png;base64,666" alt="m37"></td>
<td style="text-align:left">渲染指定的字符。例如 “$f$” 以字母 f 为标记。</td>
</tr>
</tbody>
</table>
</div>
<p>实例2:定义了 <strong>*</strong> 标记</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">6</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints, marker = <span class="string">'*'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.jggupqxmtkhw{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403212950411.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403212950411.png" srcset="data:image/png;base64,666" class="jggupqxmtkhw lazyload"></p>
<p>实例3:定义下箭头</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.markers</span><br><span class="line"></span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], marker=matplotlib.markers.CARETDOWNBASE)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.wxjhfhlwohtk{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403213418027.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403213418027.png" srcset="data:image/png;base64,666" class="wxjhfhlwohtk lazyload"></p>
<h3 id="fmt-参数"><a href="#fmt-参数" class="headerlink" title="fmt 参数"></a>fmt 参数</h3><p>fmt 参数定义了基本格式，如标记、线条样式和颜色。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">fmt = <span class="string">'[marker][line][color]'</span></span><br></pre></td></tr></tbody></table></figure>
<p>例如 <strong>o:r</strong>，<strong>o</strong> 表示实心圆标记，<strong>:</strong> 表示虚线，<strong>r</strong> 表示颜色为红色。</p>
<p>实例4：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">6</span>, <span class="number">2</span>, <span class="number">13</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints, <span class="string">'o:r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.xdlcovfowwvy{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403213911622.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403213911622.png" srcset="data:image/png;base64,666" class="xdlcovfowwvy lazyload"></p>
<p>线类型：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">线类型标记</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">‘-‘</td>
<td style="text-align:left">实线</td>
</tr>
<tr>
<td style="text-align:left">‘:’</td>
<td style="text-align:left">虚线</td>
</tr>
<tr>
<td style="text-align:left">‘—‘</td>
<td style="text-align:left">破折线</td>
</tr>
<tr>
<td style="text-align:left">‘-.’</td>
<td style="text-align:left">点划线</td>
</tr>
</tbody>
</table>
</div>
<p>颜色类型：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">颜色标记</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">‘r’</td>
<td style="text-align:left">红色</td>
</tr>
<tr>
<td style="text-align:left">‘g’</td>
<td style="text-align:left">绿色</td>
</tr>
<tr>
<td style="text-align:left">‘b’</td>
<td style="text-align:left">蓝色</td>
</tr>
<tr>
<td style="text-align:left">‘c’</td>
<td style="text-align:left">青色</td>
</tr>
<tr>
<td style="text-align:left">‘m’</td>
<td style="text-align:left">品红</td>
</tr>
<tr>
<td style="text-align:left">‘y’</td>
<td style="text-align:left">黄色</td>
</tr>
<tr>
<td style="text-align:left">‘k’</td>
<td style="text-align:left">黑色</td>
</tr>
<tr>
<td style="text-align:left">‘w’</td>
<td style="text-align:left">白色</td>
</tr>
</tbody>
</table>
</div>
<h3 id="标记大小和颜色"><a href="#标记大小和颜色" class="headerlink" title="标记大小和颜色"></a>标记大小和颜色</h3><p>我们可以自定义标记的大小与颜色，使用的参数分别是：</p>
<ul>
<li>markersize，简写为 <strong>ms</strong>：定义标记的大小。</li>
<li>markerfacecolor，简写为 <strong>mfc</strong>：定义标记内部的颜色。</li>
<li>markeredgecolor，简写为 <strong>mec</strong>：定义标记边框的颜色。</li>
</ul>
<p>实例5：设置标记大小</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">6</span>, <span class="number">2</span>, <span class="number">13</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints, marker = <span class="string">'o'</span>, ms = <span class="number">20</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.obeidthmefdo{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403215102017.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403215102017.png" srcset="data:image/png;base64,666" class="obeidthmefdo lazyload"></p>
<p>实例6：设置标记外边框颜色</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">6</span>, <span class="number">2</span>, <span class="number">13</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints, marker = <span class="string">'o'</span>, ms = <span class="number">20</span>, mec = <span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.xrtmfpmycdke{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403215856505.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403215856505.png" srcset="data:image/png;base64,666" class="xrtmfpmycdke lazyload"></p>
<p>实例7：设置标记内部颜色</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">6</span>, <span class="number">2</span>, <span class="number">13</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints, marker = <span class="string">'o'</span>, ms = <span class="number">20</span>, mfc = <span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.wequjaziqhyv{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403220017186.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403220017186.png" srcset="data:image/png;base64,666" class="wequjaziqhyv lazyload"></p>
<p>实例8：自定义标记内部与边框的颜色</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">6</span>, <span class="number">2</span>, <span class="number">13</span>, <span class="number">10</span>])</span><br><span class="line">plt.plot(ypoints, marker = <span class="string">'o'</span>, ms = <span class="number">20</span>, mec = <span class="string">'#4CAF50'</span>, mfc = <span class="string">'#4CAF50'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.qoihlyssuyuf{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403220233003.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403220233003.png" srcset="data:image/png;base64,666" class="qoihlyssuyuf lazyload"></p>
<h2 id="Matplotlib-绘图线"><a href="#Matplotlib-绘图线" class="headerlink" title="Matplotlib 绘图线"></a>Matplotlib 绘图线</h2><p>绘图过程如果我们自定义线的样式，包括线的类型、颜色和大小等。</p>
<h3 id="线的类型"><a href="#线的类型" class="headerlink" title="线的类型"></a>线的类型</h3><p>线的类型可以使用 <strong>linestyle</strong> 参数来定义，简写为 <strong>ls</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">类型</th>
<th style="text-align:left">简写</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">‘solid’ (默认)</td>
<td style="text-align:left">‘-‘</td>
<td style="text-align:left">实线</td>
</tr>
<tr>
<td style="text-align:left">‘dotted’</td>
<td style="text-align:left">‘:’</td>
<td style="text-align:left">点虚线</td>
</tr>
<tr>
<td style="text-align:left">‘dashed’</td>
<td style="text-align:left">‘—‘</td>
<td style="text-align:left">破折线</td>
</tr>
<tr>
<td style="text-align:left">‘dashdot’</td>
<td style="text-align:left">‘-.’</td>
<td style="text-align:left">点划线</td>
</tr>
<tr>
<td style="text-align:left">‘None’</td>
<td style="text-align:left">‘’ 或 ‘ ‘</td>
<td style="text-align:left">不画线</td>
</tr>
</tbody>
</table>
</div>
<p>实例1：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">6</span>, <span class="number">2</span>, <span class="number">13</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints, linestyle = <span class="string">'dotted'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.jnamniptmokb{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403221432174.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403221432174.png" srcset="data:image/png;base64,666" class="jnamniptmokb lazyload"></p>
<p>实例2：使用简写</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">6</span>, <span class="number">2</span>, <span class="number">13</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints, ls = <span class="string">'-.'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.wanvrepeueqv{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403221604494.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403221604494.png" srcset="data:image/png;base64,666" class="wanvrepeueqv lazyload"></p>
<h3 id="线的颜色"><a href="#线的颜色" class="headerlink" title="线的颜色"></a>线的颜色</h3><p>线的颜色可以使用 <strong>color</strong> 参数来定义，简写为 <strong>c</strong>。</p>
<p>颜色类型：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">颜色标记</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">‘r’</td>
<td style="text-align:left">红色</td>
</tr>
<tr>
<td style="text-align:left">‘g’</td>
<td style="text-align:left">绿色</td>
</tr>
<tr>
<td style="text-align:left">‘b’</td>
<td style="text-align:left">蓝色</td>
</tr>
<tr>
<td style="text-align:left">‘c’</td>
<td style="text-align:left">青色</td>
</tr>
<tr>
<td style="text-align:left">‘m’</td>
<td style="text-align:left">品红</td>
</tr>
<tr>
<td style="text-align:left">‘y’</td>
<td style="text-align:left">黄色</td>
</tr>
<tr>
<td style="text-align:left">‘k’</td>
<td style="text-align:left">黑色</td>
</tr>
<tr>
<td style="text-align:left">‘w’</td>
<td style="text-align:left">白色</td>
</tr>
</tbody>
</table>
</div>
<p>当然也可以自定义颜色类型，例如：<strong>SeaGreen、#8FBC8F</strong> 等，完整样式可以参考 <a href="https://www.runoob.com/html/html-colorvalues.html">HTML 颜色值</a>。</p>
<p>实例3：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">6</span>, <span class="number">2</span>, <span class="number">13</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints, color = <span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.gcwxwbzfwosh{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403221822295.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403221822295.png" srcset="data:image/png;base64,666" class="gcwxwbzfwosh lazyload"></p>
<p>实例4：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">6</span>, <span class="number">2</span>, <span class="number">13</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints, c = <span class="string">'#8FBC8F'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.fhwlrviubuen{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222000966.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222000966.png" srcset="data:image/png;base64,666" class="fhwlrviubuen lazyload"></p>
<p>实例5：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">6</span>, <span class="number">2</span>, <span class="number">13</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints, c = <span class="string">'SeaGreen'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.vvsscbjymhhp{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222117925.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222117925.png" srcset="data:image/png;base64,666" class="vvsscbjymhhp lazyload"></p>
<h3 id="线的宽度"><a href="#线的宽度" class="headerlink" title="线的宽度"></a>线的宽度</h3><p>线的宽度可以使用 <strong>linewidth</strong> 参数来定义，简写为 <strong>lw</strong>，值可以是浮点数，如：<strong>1</strong>、<strong>2.0</strong>、<strong>5.67</strong> 等</p>
<p>实例6：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([<span class="number">6</span>, <span class="number">2</span>, <span class="number">13</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints, linewidth = <span class="string">'12.5'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.jeciemjqttsm{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222318825.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222318825.png" srcset="data:image/png;base64,666" class="jeciemjqttsm lazyload"></p>
<h3 id="多条线"><a href="#多条线" class="headerlink" title="多条线"></a>多条线</h3><p>plot() 方法中可以包含多对 x,y 值来绘制多条线。</p>
<p>实例7：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">y1 = np.array([<span class="number">3</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">9</span>])</span><br><span class="line">y2 = np.array([<span class="number">6</span>, <span class="number">2</span>, <span class="number">13</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(y1)</span><br><span class="line">plt.plot(y2)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p>从上图可以看出 <strong>x</strong> 的值默认设置为 <strong>[0, 1, 2, 3]</strong>。</p>
<p><style>.nnllfhxdbuic{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222526935.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222526935.png" srcset="data:image/png;base64,666" class="nnllfhxdbuic lazyload"></p>
<p>实例8：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x1 = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">y1 = np.array([<span class="number">3</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">9</span>])</span><br><span class="line">x2 = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">y2 = np.array([<span class="number">6</span>, <span class="number">2</span>, <span class="number">13</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(x1, y1, x2, y2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.easbytounyrp{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222628227.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222628227.png" srcset="data:image/png;base64,666" class="easbytounyrp lazyload"></p>
<h2 id="Matplotlib-轴标签和标题"><a href="#Matplotlib-轴标签和标题" class="headerlink" title="Matplotlib 轴标签和标题"></a>Matplotlib 轴标签和标题</h2><p>我们可以使用 <strong>xlabel()</strong> 和 <strong>ylabel()</strong> 方法来设置 x 轴和 y 轴的标签。</p>
<p>实例1：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>])</span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">"x - label"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y - label"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.xyxtxujcgciq{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404095612577.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404095612577.png" srcset="data:image/png;base64,666" class="xyxtxujcgciq lazyload"></p>
<h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><p>实例2：使用 <strong>title()</strong> 方法来设置标题:</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>])</span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"line chart"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x - label"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y - label"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.ewlriyxlbnfl{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404100123540.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404100123540.png" srcset="data:image/png;base64,666" class="ewlriyxlbnfl lazyload"></p>
<h3 id="图像中文显示"><a href="#图像中文显示" class="headerlink" title="图像中文显示"></a>图像中文显示</h3><p>Matplotlib 默认情况不支持中文，我们可以使用以下简单的方法来解决。</p>
<p>这里我们使用思源黑体，思源黑体是 Adobe 与 Google 推出的一款开源字体。</p>
<p>官网：<a href="https://source.typekit.com/source-han-serif/cn/">https://source.typekit.com/source-han-serif/cn/</a></p>
<p>GitHub 地址：<a href="https://github.com/adobe-fonts/source-han-sans/tree/release/OTF/SimplifiedChinese">https://github.com/adobe-fonts/source-han-sans/tree/release/OTF/SimplifiedChinese</a></p>
<p>打开链接后，在里面选一个就好了：</p>
<p><style>.bcueoqpgsutz{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104121709.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104121709.png" srcset="data:image/png;base64,666" class="bcueoqpgsutz lazyload"></p>
<p>你也可以在网盘下载: <a href="https://pan.baidu.com/s/10-w1JbXZSnx3Tm6uGpPGOw，提取码：**yxqu**。">https://pan.baidu.com/s/10-w1JbXZSnx3Tm6uGpPGOw，提取码：**yxqu**。</a></p>
<p>可以下载个 OTF 字体，比如 SourceHanSansSC-Bold.otf，将该文件文件放在当前执行的代码文件中：</p>
<p><code>SourceHanSansSC-Bold.otf</code> 文件放在当前执行的代码文件中。</p>
<p>实例3：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"> </span><br><span class="line"><span class="comment"># fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径</span></span><br><span class="line">zhfont1 = matplotlib.font_manager.FontProperties(fname=<span class="string">"SourceHanSansSC-Bold.otf"</span>) </span><br><span class="line"> </span><br><span class="line">x = np.arange(<span class="number">1</span>,<span class="number">11</span>) </span><br><span class="line">y =  <span class="number">2</span>  * x +  <span class="number">5</span> </span><br><span class="line">plt.title(<span class="string">"测试"</span>, fontproperties=zhfont1) </span><br><span class="line"> </span><br><span class="line"><span class="comment"># fontproperties 设置中文显示，fontsize 设置字体大小</span></span><br><span class="line">plt.xlabel(<span class="string">"x标签"</span>, fontproperties=zhfont1)</span><br><span class="line">plt.ylabel(<span class="string">"y标签"</span>, fontproperties=zhfont1)</span><br><span class="line">plt.plot(x,y) </span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.jtzpkfshxgye{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104616081.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104616081.png" srcset="data:image/png;base64,666" class="jtzpkfshxgye lazyload"></p>
<p><em>此外，我们还可以使用系统的字体：</em></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">a=<span class="built_in">sorted</span>([f.name <span class="keyword">for</span> f <span class="keyword">in</span> matplotlib.font_manager.fontManager.ttflist])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br></pre></td></tr></tbody></table></figure>
<p><em>打印出你的 font_manager 的 ttflist 中所有注册的名字，找一个看中文字体例如：STFangsong(仿宋）,然后添加以下代码即可：</em></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'font.family'</span>]=[<span class="string">'STFangsong'</span>]</span><br></pre></td></tr></tbody></table></figure>
<p>实例4：:自定义字体的样式</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"> </span><br><span class="line"><span class="comment"># fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径，size 参数设置字体大小</span></span><br><span class="line">zhfont1 = matplotlib.font_manager.FontProperties(fname=<span class="string">"SourceHanSansSC-Bold.otf"</span>, size=<span class="number">18</span>)</span><br><span class="line">font1 = {<span class="string">'color'</span>:<span class="string">'blue'</span>,<span class="string">'size'</span>:<span class="number">20</span>}</span><br><span class="line">font2 = {<span class="string">'color'</span>:<span class="string">'darkred'</span>,<span class="string">'size'</span>:<span class="number">15</span>}</span><br><span class="line">x = np.arange(<span class="number">1</span>,<span class="number">11</span>)</span><br><span class="line">y =  <span class="number">2</span>  * x +  <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fontdict 可以使用 css 来设置字体样式</span></span><br><span class="line">plt.title(<span class="string">"菜鸟教程 - 测试"</span>, fontproperties=zhfont1, fontdict = font1)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># fontproperties 设置中文显示，fontsize 设置字体大小</span></span><br><span class="line">plt.xlabel(<span class="string">"x 轴"</span>, fontproperties=zhfont1)</span><br><span class="line">plt.ylabel(<span class="string">"y 轴"</span>, fontproperties=zhfont1)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.bxvzrlusehec{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104841496.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104841496.png" srcset="data:image/png;base64,666" class="bxvzrlusehec lazyload"></p>
<h3 id="标题与标签的定位"><a href="#标题与标签的定位" class="headerlink" title="标题与标签的定位"></a>标题与标签的定位</h3><p><strong>title()</strong> 方法提供了 <strong>loc</strong> 参数来设置标题显示的位置，可以设置为: <strong>‘left’, ‘right’, 和 ‘center’， 默认值为 ‘center’</strong>。</p>
<p><strong>xlabel()</strong> 方法提供了 <strong>loc</strong> 参数来设置 x 轴显示的位置，可以设置为: <strong>‘left’, ‘right’, 和 ‘center’， 默认值为 ‘center’</strong>。</p>
<p><strong>ylabel()</strong> 方法提供了 <strong>loc</strong> 参数来设置 y 轴显示的位置，可以设置为: <strong>‘bottom’, ‘top’, 和 ‘center’， 默认值为 ‘center’</strong>。</p>
<p>实例5：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"> </span><br><span class="line"><span class="comment"># fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径，size 参数设置字体大小</span></span><br><span class="line">zhfont1 = matplotlib.font_manager.FontProperties(fname=<span class="string">"SourceHanSansSC-Bold.otf"</span>, size=<span class="number">18</span>)</span><br><span class="line">font1 = {<span class="string">'color'</span>:<span class="string">'blue'</span>,<span class="string">'size'</span>:<span class="number">20</span>}</span><br><span class="line">font2 = {<span class="string">'color'</span>:<span class="string">'darkred'</span>,<span class="string">'size'</span>:<span class="number">15</span>}</span><br><span class="line">x = np.arange(<span class="number">1</span>,<span class="number">11</span>)</span><br><span class="line">y =  <span class="number">2</span>  * x +  <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fontdict 可以使用 css 来设置字体样式</span></span><br><span class="line">plt.title(<span class="string">"菜鸟教程 - 测试"</span>, fontproperties=zhfont1, fontdict = font1, loc=<span class="string">"left"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># fontproperties 设置中文显示，fontsize 设置字体大小</span></span><br><span class="line">plt.xlabel(<span class="string">"x 轴"</span>, fontproperties=zhfont1, loc=<span class="string">"left"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y 轴"</span>, fontproperties=zhfont1, loc=<span class="string">"top"</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.fcqiykuqqqnr{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104916266.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104916266.png" srcset="data:image/png;base64,666" class="fcqiykuqqqnr lazyload"></p>
<h2 id="Matplotlib-网格线"><a href="#Matplotlib-网格线" class="headerlink" title="Matplotlib 网格线"></a>Matplotlib 网格线</h2><p>我们可以使用 pyplot 中的 grid() 方法来设置图表中的网格线。</p>
<p>grid() 方法语法格式如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">matplotlib.pyplot.grid(b=None, which='major', axis='both', )</span><br></pre></td></tr></tbody></table></figure>
<p><strong>参数说明：</strong></p>
<ul>
<li><p><strong>b</strong>：可选，默认为 None，可以设置布尔值，true 为显示网格线，false 为不显示，如果设置 **kwargs 参数，则值为 true。</p>
</li>
<li><p><strong>which</strong>：可选，可选值有 ‘major’、’minor’ 和 ‘both’，默认为 ‘major’，表示应用更改的网格线。</p>
</li>
<li><p><strong>axis</strong>：可选，设置显示哪个方向的网格线，可以是取 ‘both’（默认），’x’ 或 ‘y’，分别表示两个方向，x 轴方向或 y 轴方向。</p>
</li>
<li><p><strong><em>\</em>kwargs</strong>：可选，设置网格样式，可以是 color=’r’, linestyle=’-‘ 和 linewidth=2，分别表示网格线的颜色，样式和宽度。</p>
</li>
</ul>
<p>实例1：添加一个简单的网格线，参数使用默认值<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"RUNOOB grid() Test"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x - label"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y - label"</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line">plt.grid()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p><style>.ipgyhluxtqvk{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152016310.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152016310.png" srcset="data:image/png;base64,666" class="ipgyhluxtqvk lazyload"></p>
<p>实例2:添加一个简单的网格线，axis 参数使用 x，设置 x 轴方向显示网格线</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"RUNOOB grid() Test"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x - label"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y - label"</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line">plt.grid(axis=<span class="string">'x'</span>) <span class="comment"># 设置 y 就在轴方向显示网格线</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.mgwllfxzuuyt{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152129680.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152129680.png" srcset="data:image/png;base64,666" class="mgwllfxzuuyt lazyload"></p>
<p>以下实例添加一个简单的网格线，并设置网格线的样式，格式如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">grid(color = 'color', linestyle = 'linestyle', linewidth = number)</span><br></pre></td></tr></tbody></table></figure>
<p><strong>参数说明：</strong></p>
<p><strong>color：</strong>‘b’ 蓝色，’m’ 洋红色，’g’ 绿色，’y’ 黄色，’r’ 红色，’k’ 黑色，’w’ 白色，’c’ 青绿色，’#008000’ RGB 颜色符串。</p>
<p><strong>linestyle：</strong>‘‐’ 实线，’‐‐’ 破折线，’‐.’ 点划线，’:’ 虚线。</p>
<p><strong>linewidth</strong>：设置线的宽度，可以设置一个数字。</p>
<p>实例3：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"RUNOOB grid() Test"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x - label"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y - label"</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line">plt.grid(color = <span class="string">'r'</span>, linestyle = <span class="string">'--'</span>, linewidth = <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.tyxiiuuodojj{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152316633.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152316633.png" srcset="data:image/png;base64,666" class="tyxiiuuodojj lazyload"></p>
<h2 id="Matplotlib绘制多图"><a href="#Matplotlib绘制多图" class="headerlink" title="Matplotlib绘制多图"></a>Matplotlib绘制多图</h2><p>我们可以使用 pyplot 中的 <strong>subplot()</strong> 和 <strong>subplots()</strong> 方法来绘制多个子图。</p>
<p><strong>subplot()</strong> 方法在绘图时需要指定位置，<strong>subplots()</strong> 方法可以一次生成多个，在调用时只需要调用生成对象的 ax 即可。</p>
<h3 id="subplot"><a href="#subplot" class="headerlink" title="subplot"></a>subplot</h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">subplot(nrows, ncols, index, **kwargs)</span><br><span class="line">subplot(pos, **kwargs)</span><br><span class="line">subplot(**kwargs)</span><br><span class="line">subplot(ax)</span><br></pre></td></tr></tbody></table></figure>
<p>以上函数将整个绘图区域分成 nrows 行和 ncols 列，然后从左到右，从上到下的顺序对每个子区域进行编号 <strong>1…N</strong> ，左上的子区域的编号为 1、右下的区域编号为 N，编号可以通过参数 <strong>index</strong> 来设置。</p>
<p>设置 numRows ＝ 1，numCols ＝ 2，就是将图表绘制成 1x2 的图片区域, 对应的坐标为：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">(1, 1), (1, 2)</span><br></pre></td></tr></tbody></table></figure>
<p><strong>plotNum ＝ 1</strong>, 表示的坐标为(1, 1), 即第一行第一列的子图。</p>
<p><strong>plotNum ＝ 2</strong>, 表示的坐标为(1, 2), 即第一行第二列的子图。</p>
<p>实例1：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 1:</span></span><br><span class="line">xpoints = np.array([<span class="number">0</span>, <span class="number">6</span>])</span><br><span class="line">ypoints = np.array([<span class="number">0</span>, <span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(xpoints,ypoints)</span><br><span class="line">plt.title(<span class="string">"plot 1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 2:</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">"plot 2"</span>)</span><br><span class="line"></span><br><span class="line">plt.suptitle(<span class="string">"RUNOOB subplot Test"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.yxktqfuxyiru{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152756053.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152756053.png" srcset="data:image/png;base64,666" class="yxktqfuxyiru lazyload"></p>
<p>设置 numRows ＝ 2，numCols ＝ 2，就是将图表绘制成 2x2 的图片区域, 对应的坐标为：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">(1, 1), (1, 2)</span><br><span class="line">(2, 1), (2, 2)</span><br></pre></td></tr></tbody></table></figure>
<p><strong>plotNum ＝ 1</strong>, 表示的坐标为(1, 1), 即第一行第一列的子图。</p>
<p><strong>plotNum ＝ 2</strong>, 表示的坐标为(1, 2), 即第一行第二列的子图。</p>
<p><strong>plotNum ＝ 3</strong>, 表示的坐标为(2, 1), 即第二行第一列的子图。</p>
<p><strong>plotNum ＝ 4</strong>, 表示的坐标为(2, 2), 即第二行第二列的子图。</p>
<p>实例2：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 1:</span></span><br><span class="line">x = np.array([<span class="number">0</span>, <span class="number">6</span>])</span><br><span class="line">y = np.array([<span class="number">0</span>, <span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">"plot 1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 2:</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">"plot 2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 3:</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">"plot 3"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot 4:</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">"plot 4"</span>)</span><br><span class="line"></span><br><span class="line">plt.suptitle(<span class="string">"RUNOOB subplot Test"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.bwbrhvdxdhys{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152958366.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152958366.png" srcset="data:image/png;base64,666" class="bwbrhvdxdhys lazyload"></p>
<h3 id="subplots"><a href="#subplots" class="headerlink" title="subplots()"></a>subplots()</h3><p>subplots() 方法语法格式如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">matplotlib.pyplot.subplots(nrows=<span class="number">1</span>, ncols=<span class="number">1</span>, *, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, squeeze=<span class="literal">True</span>, subplot_kw=<span class="literal">None</span>, gridspec_kw=<span class="literal">None</span>, **fig_kw)</span><br></pre></td></tr></tbody></table></figure>
<p><strong>参数说明：</strong></p>
<ul>
<li><strong>nrows</strong>：默认为 1，设置图表的行数。</li>
<li><strong>ncols</strong>：默认为 1，设置图表的列数。</li>
<li><strong>sharex、sharey</strong>：设置 x、y 轴是否共享属性，默认为 false，可设置为 ‘none’、’all’、’row’ 或 ‘col’。 False 或 none 每个子图的 x 轴或 y 轴都是独立的，True 或 ‘all’：所有子图共享 x 轴或 y 轴，’row’ 设置每个子图行共享一个 x 轴或 y 轴，’col’：设置每个子图列共享一个 x 轴或 y 轴。</li>
<li><strong>squeeze</strong>：布尔值，默认为 True，表示额外的维度从返回的 Axes(轴)对象中挤出，对于 N<em>1 或 1</em>N 个子图，返回一个 1 维数组，对于 N*M，N&gt;1 和 M&gt;1 返回一个 2 维数组。如果设置为 False，则不进行挤压操作，返回一个元素为 Axes 实例的2维数组，即使它最终是1x1。</li>
<li><strong>subplot_kw</strong>：可选，字典类型。把字典的关键字传递给 add_subplot() 来创建每个子图。</li>
<li><strong>gridspec_kw</strong>：可选，字典类型。把字典的关键字传递给 GridSpec 构造函数创建子图放在网格里(grid)。</li>
<li><strong><em>\</em>fig_kw</strong>：把详细的关键字参数传给 figure() 函数。</li>
</ul>
<p>实例3：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一些测试数据 -- 图1</span></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">2</span>*np.pi, <span class="number">400</span>)</span><br><span class="line">y = np.sin(x**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个画像和子图 -- 图2</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(x, y)</span><br><span class="line">ax.set_title(<span class="string">'Simple plot'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建两个子图 -- 图3</span></span><br><span class="line">f, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, sharey=<span class="literal">True</span>)</span><br><span class="line">ax1.plot(x, y)</span><br><span class="line">ax1.set_title(<span class="string">'Sharing Y axis'</span>)</span><br><span class="line">ax2.scatter(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建四个子图 -- 图4</span></span><br><span class="line">fig, axs = plt.subplots(<span class="number">2</span>, <span class="number">2</span>, subplot_kw=<span class="built_in">dict</span>(projection=<span class="string">"polar"</span>))</span><br><span class="line">axs[<span class="number">0</span>, <span class="number">0</span>].plot(x, y)</span><br><span class="line">axs[<span class="number">1</span>, <span class="number">1</span>].scatter(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 共享 x 轴</span></span><br><span class="line">plt.subplots(<span class="number">2</span>, <span class="number">2</span>, sharex=<span class="string">'col'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 共享 y 轴</span></span><br><span class="line">plt.subplots(<span class="number">2</span>, <span class="number">2</span>, sharey=<span class="string">'row'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 共享 x 轴和 y 轴</span></span><br><span class="line">plt.subplots(<span class="number">2</span>, <span class="number">2</span>, sharex=<span class="string">'all'</span>, sharey=<span class="string">'all'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个也是共享 x 轴和 y 轴</span></span><br><span class="line">plt.subplots(<span class="number">2</span>, <span class="number">2</span>, sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建10 张图，已经存在的则删除</span></span><br><span class="line">fig, ax = plt.subplots(num=<span class="number">10</span>, clear=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.ivshrxkdnjty{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153408476.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153408476.png" srcset="data:image/png;base64,666" class="ivshrxkdnjty lazyload"></p>
<p><style>.cchjpocncvpo{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153431942.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153431942.png" srcset="data:image/png;base64,666" class="cchjpocncvpo lazyload"></p>
<p><style>.cqqzvhlfchoq{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153453565.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153453565.png" srcset="data:image/png;base64,666" class="cqqzvhlfchoq lazyload"></p>
<p><style>.vzvughxlsipn{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153511622.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153511622.png" srcset="data:image/png;base64,666" class="vzvughxlsipn lazyload"></p>
<h2 id="Matplotlib散点图"><a href="#Matplotlib散点图" class="headerlink" title="Matplotlib散点图"></a>Matplotlib散点图</h2><p>我们可以使用 pyplot 中的 scatter() 方法来绘制散点图。</p>
<p>scatter() 方法语法格式如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">matplotlib.pyplot.scatter(x, y, s=<span class="literal">None</span>, c=<span class="literal">None</span>, marker=<span class="literal">None</span>, cmap=<span class="literal">None</span>, norm=<span class="literal">None</span>, vmin=<span class="literal">None</span>, vmax=<span class="literal">None</span>, alpha=<span class="literal">None</span>, linewidths=<span class="literal">None</span>, *, edgecolors=<span class="literal">None</span>, plotnonfinite=<span class="literal">False</span>, data=<span class="literal">None</span>, **kwargs)</span><br></pre></td></tr></tbody></table></figure>
<p><strong>参数说明：</strong></p>
<p><strong>x，y</strong>：长度相同的数组，也就是我们即将绘制散点图的数据点，输入数据。</p>
<p><strong>s</strong>：点的大小，默认 20，也可以是个数组，数组每个参数为对应点的大小。</p>
<p><strong>c</strong>：点的颜色，默认蓝色 ‘b’，也可以是个 RGB 或 RGBA 二维行数组。</p>
<p><strong>marker</strong>：点的样式，默认小圆圈 ‘o’。</p>
<p><strong>cmap</strong>：Colormap，默认 None，标量或者是一个 colormap 的名字，只有 c 是一个浮点数数组的时才使用。如果没有申明就是 image.cmap。</p>
<p><strong>norm</strong>：Normalize，默认 None，数据亮度在 0-1 之间，只有 c 是一个浮点数的数组的时才使用。</p>
<p><strong>vmin，vmax：</strong>：亮度设置，在 norm 参数存在时会忽略。</p>
<p><strong>alpha：</strong>：透明度设置，0-1 之间，默认 None，即不透明。</p>
<p><strong>linewidths：</strong>：标记点的长度。</p>
<p><strong>edgecolors：</strong>：颜色或颜色序列，默认为 ‘face’，可选值有 ‘face’, ‘none’, None。</p>
<p><strong>plotnonfinite：</strong>：布尔值，设置是否使用非限定的 c ( inf, -inf 或 nan) 绘制点。</p>
<p><strong><em>\</em>kwargs：</strong>：其他参数。</p>
<p>以下实例 scatter() 函数接收长度相同的数组参数，一个用于 x 轴的值，另一个用于 y 轴上的值：</p>
<p>实例1：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">23</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.vlggnpmmzord{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155337371.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155337371.png" srcset="data:image/png;base64,666" class="vlggnpmmzord lazyload"></p>
<p>实例2：设置图标大小</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">23</span>, <span class="number">18</span>])</span><br><span class="line">sizes = np.array([<span class="number">20</span>,<span class="number">50</span>,<span class="number">100</span>,<span class="number">200</span>,<span class="number">500</span>,<span class="number">1000</span>,<span class="number">60</span>,<span class="number">90</span>])</span><br><span class="line">plt.scatter(x, y, s=sizes)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.hekxlszeokne{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155550197.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155550197.png" srcset="data:image/png;base64,666" class="hekxlszeokne lazyload"></p>
<p>实例3：自定义点的颜色</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">23</span>, <span class="number">18</span>])</span><br><span class="line">colors = np.array([<span class="string">"red"</span>,<span class="string">"green"</span>,<span class="string">"black"</span>,<span class="string">"orange"</span>,<span class="string">"purple"</span>,<span class="string">"beige"</span>,<span class="string">"cyan"</span>,<span class="string">"magenta"</span>])</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y, c=colors)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.owwwbikzoukt{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/owwwbikzoukt" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/owwwbikzoukt" srcset="data:image/png;base64,666" class=" lazyload" title="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155740263.png &quot;&quot;&quot;&quot;"></p>
<p>实例4：设置两组散点图</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">5</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">7</span>,<span class="number">2</span>,<span class="number">17</span>,<span class="number">2</span>,<span class="number">9</span>,<span class="number">4</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">9</span>,<span class="number">6</span>])</span><br><span class="line">y = np.array([<span class="number">99</span>,<span class="number">86</span>,<span class="number">87</span>,<span class="number">88</span>,<span class="number">111</span>,<span class="number">86</span>,<span class="number">103</span>,<span class="number">87</span>,<span class="number">94</span>,<span class="number">78</span>,<span class="number">77</span>,<span class="number">85</span>,<span class="number">86</span>])</span><br><span class="line">plt.scatter(x, y, color = <span class="string">'hotpink'</span>)</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">2</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">1</span>,<span class="number">15</span>,<span class="number">8</span>,<span class="number">12</span>,<span class="number">9</span>,<span class="number">7</span>,<span class="number">3</span>,<span class="number">11</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">14</span>,<span class="number">12</span>])</span><br><span class="line">y = np.array([<span class="number">100</span>,<span class="number">105</span>,<span class="number">84</span>,<span class="number">105</span>,<span class="number">90</span>,<span class="number">99</span>,<span class="number">90</span>,<span class="number">95</span>,<span class="number">94</span>,<span class="number">100</span>,<span class="number">79</span>,<span class="number">112</span>,<span class="number">91</span>,<span class="number">80</span>,<span class="number">85</span>])</span><br><span class="line">plt.scatter(x, y, color = <span class="string">'#88c999'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.vvwikyuoursd{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155923033.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155923033.png" srcset="data:image/png;base64,666" class="vvwikyuoursd lazyload"></p>
<p>实例5：使用随机数来设置散点图</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机数生成器的种子</span></span><br><span class="line">np.random.seed(<span class="number">19680801</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">N = <span class="number">50</span></span><br><span class="line">x = np.random.rand(N)</span><br><span class="line">y = np.random.rand(N)</span><br><span class="line">colors = np.random.rand(N)</span><br><span class="line">area = (<span class="number">30</span> * np.random.rand(N))**<span class="number">2</span>  <span class="comment"># 0 to 15 point radii</span></span><br><span class="line"></span><br><span class="line">plt.scatter(x, y, s=area, c=colors, alpha=<span class="number">0.5</span>) <span class="comment"># 设置颜色及透明度</span></span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"RUNOOB Scatter Test"</span>) <span class="comment"># 设置标题</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.ucspjttywxor{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160218777.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160218777.png" srcset="data:image/png;base64,666" class="ucspjttywxor lazyload"></p>
<h3 id="颜色条Colormap"><a href="#颜色条Colormap" class="headerlink" title="颜色条Colormap"></a>颜色条Colormap</h3><p>Matplotlib 模块提供了很多可用的颜色条。</p>
<p>颜色条就像一个颜色列表，其中每种颜色都有一个范围从 0 到 100 的值。</p>
<p>下面是一个颜色条的例子：</p>
<p><style>.jiuqgvjejiql{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160411035.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160411035.png" srcset="data:image/png;base64,666" class="jiuqgvjejiql lazyload"></p>
<p>实例6：设置颜色条需要使用 cmap 参数，默认值为 ‘viridis’，之后颜色值设置为 0 到 100 的数组</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">5</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">7</span>,<span class="number">2</span>,<span class="number">17</span>,<span class="number">2</span>,<span class="number">9</span>,<span class="number">4</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">9</span>,<span class="number">6</span>])</span><br><span class="line">y = np.array([<span class="number">99</span>,<span class="number">86</span>,<span class="number">87</span>,<span class="number">88</span>,<span class="number">111</span>,<span class="number">86</span>,<span class="number">103</span>,<span class="number">87</span>,<span class="number">94</span>,<span class="number">78</span>,<span class="number">77</span>,<span class="number">85</span>,<span class="number">86</span>])</span><br><span class="line">colors = np.array([<span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">60</span>, <span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>, <span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y, c=colors, cmap=<span class="string">'viridis'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.qbvwgtxbekot{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160555429.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160555429.png" srcset="data:image/png;base64,666" class="qbvwgtxbekot lazyload"></p>
<p>实例7：如果要显示颜色条，需要使用 <strong>plt.colorbar()</strong> 方法</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">5</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">7</span>,<span class="number">2</span>,<span class="number">17</span>,<span class="number">2</span>,<span class="number">9</span>,<span class="number">4</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">9</span>,<span class="number">6</span>])</span><br><span class="line">y = np.array([<span class="number">99</span>,<span class="number">86</span>,<span class="number">87</span>,<span class="number">88</span>,<span class="number">111</span>,<span class="number">86</span>,<span class="number">103</span>,<span class="number">87</span>,<span class="number">94</span>,<span class="number">78</span>,<span class="number">77</span>,<span class="number">85</span>,<span class="number">86</span>])</span><br><span class="line">colors = np.array([<span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">60</span>, <span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>, <span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y, c=colors, cmap=<span class="string">'viridis'</span>)</span><br><span class="line"></span><br><span class="line">plt.colorbar()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.whwsrbzwmaob{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160751739.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160751739.png" srcset="data:image/png;base64,666" class="whwsrbzwmaob lazyload"></p>
<p>实例8：换个颜色条参数， cmap 设置为 <strong>afmhot_r</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">5</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">7</span>,<span class="number">2</span>,<span class="number">17</span>,<span class="number">2</span>,<span class="number">9</span>,<span class="number">4</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">9</span>,<span class="number">6</span>])</span><br><span class="line">y = np.array([<span class="number">99</span>,<span class="number">86</span>,<span class="number">87</span>,<span class="number">88</span>,<span class="number">111</span>,<span class="number">86</span>,<span class="number">103</span>,<span class="number">87</span>,<span class="number">94</span>,<span class="number">78</span>,<span class="number">77</span>,<span class="number">85</span>,<span class="number">86</span>])</span><br><span class="line">colors = np.array([<span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">60</span>, <span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>, <span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y, c=colors, cmap=<span class="string">'afmhot_r'</span>)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.hdwiobgkbpnf{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160958004.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160958004.png" srcset="data:image/png;base64,666" class="hdwiobgkbpnf lazyload"></p>
<p>颜色条参数值可以是以下值：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">颜色名称</th>
<th style="text-align:left">保留关键字</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Accent</td>
<td style="text-align:left">Accent_r</td>
</tr>
<tr>
<td style="text-align:left">Blues</td>
<td style="text-align:left">Blues_r</td>
</tr>
<tr>
<td style="text-align:left">BrBG</td>
<td style="text-align:left">BrBG_r</td>
</tr>
<tr>
<td style="text-align:left">BuGn</td>
<td style="text-align:left">BuGn_r</td>
</tr>
<tr>
<td style="text-align:left">BuPu</td>
<td style="text-align:left">BuPu_r</td>
</tr>
<tr>
<td style="text-align:left">CMRmap</td>
<td style="text-align:left">CMRmap_r</td>
</tr>
<tr>
<td style="text-align:left">Dark2</td>
<td style="text-align:left">Dark2_r</td>
</tr>
<tr>
<td style="text-align:left">GnBu</td>
<td style="text-align:left">GnBu_r</td>
</tr>
<tr>
<td style="text-align:left">Greens</td>
<td style="text-align:left">Greens_r</td>
</tr>
<tr>
<td style="text-align:left">Greys</td>
<td style="text-align:left">Greys_r</td>
</tr>
<tr>
<td style="text-align:left">OrRd</td>
<td style="text-align:left">OrRd_r</td>
</tr>
<tr>
<td style="text-align:left">Oranges</td>
<td style="text-align:left">Oranges_r</td>
</tr>
<tr>
<td style="text-align:left">PRGn</td>
<td style="text-align:left">PRGn_r</td>
</tr>
<tr>
<td style="text-align:left">Paired</td>
<td style="text-align:left">Paired_r</td>
</tr>
<tr>
<td style="text-align:left">Pastel1</td>
<td style="text-align:left">Pastel1_r</td>
</tr>
<tr>
<td style="text-align:left">Pastel2</td>
<td style="text-align:left">Pastel2_r</td>
</tr>
<tr>
<td style="text-align:left">PiYG</td>
<td style="text-align:left">PiYG_r</td>
</tr>
<tr>
<td style="text-align:left">PuBu</td>
<td style="text-align:left">PuBu_r</td>
</tr>
<tr>
<td style="text-align:left">PuBuGn</td>
<td style="text-align:left">PuBuGn_r</td>
</tr>
<tr>
<td style="text-align:left">PuOr</td>
<td style="text-align:left">PuOr_r</td>
</tr>
<tr>
<td style="text-align:left">PuRd</td>
<td style="text-align:left">PuRd_r</td>
</tr>
<tr>
<td style="text-align:left">Purples</td>
<td style="text-align:left">Purples_r</td>
</tr>
<tr>
<td style="text-align:left">RdBu</td>
<td style="text-align:left">RdBu_r</td>
</tr>
<tr>
<td style="text-align:left">RdGy</td>
<td style="text-align:left">RdGy_r</td>
</tr>
<tr>
<td style="text-align:left">RdPu</td>
<td style="text-align:left">RdPu_r</td>
</tr>
<tr>
<td style="text-align:left">RdYlBu</td>
<td style="text-align:left">RdYlBu_r</td>
</tr>
<tr>
<td style="text-align:left">RdYlGn</td>
<td style="text-align:left">RdYlGn_r</td>
</tr>
<tr>
<td style="text-align:left">Reds</td>
<td style="text-align:left">Reds_r</td>
</tr>
<tr>
<td style="text-align:left">Set1</td>
<td style="text-align:left">Set1_r</td>
</tr>
<tr>
<td style="text-align:left">Set2</td>
<td style="text-align:left">Set2_r</td>
</tr>
<tr>
<td style="text-align:left">Set3</td>
<td style="text-align:left">Set3_r</td>
</tr>
<tr>
<td style="text-align:left">Spectral</td>
<td style="text-align:left">Spectral_r</td>
</tr>
<tr>
<td style="text-align:left">Wistia</td>
<td style="text-align:left">Wistia_r</td>
</tr>
<tr>
<td style="text-align:left">YlGn</td>
<td style="text-align:left">YlGn_r</td>
</tr>
<tr>
<td style="text-align:left">YlGnBu</td>
<td style="text-align:left">YlGnBu_r</td>
</tr>
<tr>
<td style="text-align:left">YlOrBr</td>
<td style="text-align:left">YlOrBr_r</td>
</tr>
<tr>
<td style="text-align:left">YlOrRd</td>
<td style="text-align:left">YlOrRd_r</td>
</tr>
<tr>
<td style="text-align:left">afmhot</td>
<td style="text-align:left">afmhot_r</td>
</tr>
<tr>
<td style="text-align:left">autumn</td>
<td style="text-align:left">autumn_r</td>
</tr>
<tr>
<td style="text-align:left">binary</td>
<td style="text-align:left">binary_r</td>
</tr>
<tr>
<td style="text-align:left">bone</td>
<td style="text-align:left">bone_r</td>
</tr>
<tr>
<td style="text-align:left">brg</td>
<td style="text-align:left">brg_r</td>
</tr>
<tr>
<td style="text-align:left">bwr</td>
<td style="text-align:left">bwr_r</td>
</tr>
<tr>
<td style="text-align:left">cividis</td>
<td style="text-align:left">cividis_r</td>
</tr>
<tr>
<td style="text-align:left">cool</td>
<td style="text-align:left">cool_r</td>
</tr>
<tr>
<td style="text-align:left">coolwarm</td>
<td style="text-align:left">coolwarm_r</td>
</tr>
<tr>
<td style="text-align:left">copper</td>
<td style="text-align:left">copper_r</td>
</tr>
<tr>
<td style="text-align:left">cubehelix</td>
<td style="text-align:left">cubehelix_r</td>
</tr>
<tr>
<td style="text-align:left">flag</td>
<td style="text-align:left">flag_r</td>
</tr>
<tr>
<td style="text-align:left">gist_earth</td>
<td style="text-align:left">gist_earth_r</td>
</tr>
<tr>
<td style="text-align:left">gist_gray</td>
<td style="text-align:left">gist_gray_r</td>
</tr>
<tr>
<td style="text-align:left">gist_heat</td>
<td style="text-align:left">gist_heat_r</td>
</tr>
<tr>
<td style="text-align:left">gist_ncar</td>
<td style="text-align:left">gist_ncar_r</td>
</tr>
<tr>
<td style="text-align:left">gist_rainbow</td>
<td style="text-align:left">gist_rainbow_r</td>
</tr>
<tr>
<td style="text-align:left">gist_stern</td>
<td style="text-align:left">gist_stern_r</td>
</tr>
<tr>
<td style="text-align:left">gist_yarg</td>
<td style="text-align:left">gist_yarg_r</td>
</tr>
<tr>
<td style="text-align:left">gnuplot</td>
<td style="text-align:left">gnuplot_r</td>
</tr>
<tr>
<td style="text-align:left">gnuplot2</td>
<td style="text-align:left">gnuplot2_r</td>
</tr>
<tr>
<td style="text-align:left">gray</td>
<td style="text-align:left">gray_r</td>
</tr>
<tr>
<td style="text-align:left">hot</td>
<td style="text-align:left">hot_r</td>
</tr>
<tr>
<td style="text-align:left">hsv</td>
<td style="text-align:left">hsv_r</td>
</tr>
<tr>
<td style="text-align:left">inferno</td>
<td style="text-align:left">inferno_r</td>
</tr>
<tr>
<td style="text-align:left">jet</td>
<td style="text-align:left">jet_r</td>
</tr>
<tr>
<td style="text-align:left">magma</td>
<td style="text-align:left">magma_r</td>
</tr>
<tr>
<td style="text-align:left">nipy_spectral</td>
<td style="text-align:left">nipy_spectral_r</td>
</tr>
<tr>
<td style="text-align:left">ocean</td>
<td style="text-align:left">ocean_r</td>
</tr>
<tr>
<td style="text-align:left">pink</td>
<td style="text-align:left">pink_r</td>
</tr>
<tr>
<td style="text-align:left">plasma</td>
<td style="text-align:left">plasma_r</td>
</tr>
<tr>
<td style="text-align:left">prism</td>
<td style="text-align:left">prism_r</td>
</tr>
<tr>
<td style="text-align:left">rainbow</td>
<td style="text-align:left">rainbow_r</td>
</tr>
<tr>
<td style="text-align:left">seismic</td>
<td style="text-align:left">seismic_r</td>
</tr>
<tr>
<td style="text-align:left">spring</td>
<td style="text-align:left">spring_r</td>
</tr>
<tr>
<td style="text-align:left">summer</td>
<td style="text-align:left">summer_r</td>
</tr>
<tr>
<td style="text-align:left">tab10</td>
<td style="text-align:left">tab10_r</td>
</tr>
<tr>
<td style="text-align:left">tab20</td>
<td style="text-align:left">tab20_r</td>
</tr>
<tr>
<td style="text-align:left">tab20b</td>
<td style="text-align:left">tab20b_r</td>
</tr>
<tr>
<td style="text-align:left">tab20c</td>
<td style="text-align:left">tab20c_r</td>
</tr>
<tr>
<td style="text-align:left">terrain</td>
<td style="text-align:left">terrain_r</td>
</tr>
<tr>
<td style="text-align:left">twilight</td>
<td style="text-align:left">twilight_r</td>
</tr>
<tr>
<td style="text-align:left">twilight_shifted</td>
<td style="text-align:left">twilight_shifted_r</td>
</tr>
<tr>
<td style="text-align:left">viridis</td>
<td style="text-align:left">viridis_r</td>
</tr>
<tr>
<td style="text-align:left">winter</td>
<td style="text-align:left">winter_r</td>
</tr>
</tbody>
</table>
</div>
<p><style>.dgjwdcxlbxbc{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404161117677.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404161117677.png" srcset="data:image/png;base64,666" class="dgjwdcxlbxbc lazyload"></p>
<p><style>.tteejxsupmfi{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404161139138.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404161139138.png" srcset="data:image/png;base64,666" class="tteejxsupmfi lazyload"></p>
<p><style>.jdrcunhqawhc{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404161157754.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404161157754.png" srcset="data:image/png;base64,666" class="jdrcunhqawhc lazyload"></p>
<h2 id="Matplotlib柱形图"><a href="#Matplotlib柱形图" class="headerlink" title="Matplotlib柱形图"></a>Matplotlib柱形图</h2><p>我们可以使用 pyplot 中的 bar() 方法来绘制柱形图。</p>
<p>bar() 方法语法格式如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">matplotlib.pyplot.bar(x, height, width=<span class="number">0.8</span>, bottom=<span class="literal">None</span>, *, align=<span class="string">'center'</span>, data=<span class="literal">None</span>, **kwargs)</span><br></pre></td></tr></tbody></table></figure>
<p><strong>参数说明：</strong></p>
<p><strong>x</strong>：浮点型数组，柱形图的 x 轴数据。</p>
<p><strong>height</strong>：浮点型数组，柱形图的高度。</p>
<p><strong>width</strong>：浮点型数组，柱形图的宽度。</p>
<p><strong>bottom</strong>：浮点型数组，底座的 y 坐标，默认 0。</p>
<p><strong>align</strong>：柱形图与 x 坐标的对齐方式，’center’ 以 x 位置为中心，这是默认值。 ‘edge’：将柱形图的左边缘与 x 位置对齐。要对齐右边缘的条形，可以传递负数的宽度值及 align=’edge’。</p>
<p><strong><em>\</em>kwargs：</strong>：其他参数。</p>
<p>实例1：简单实用 bar() 来创建一个柱形图</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="string">"Runoob-1"</span>, <span class="string">"Runoob-2"</span>, <span class="string">"Runoob-3"</span>, <span class="string">"C-RUNOOB"</span>])</span><br><span class="line">y = np.array([<span class="number">12</span>, <span class="number">22</span>, <span class="number">6</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">plt.bar(x,y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.pxanlmbpjnju{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162147295.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162147295.png" srcset="data:image/png;base64,666" class="pxanlmbpjnju lazyload"></p>
<p>实例2：垂直方向的柱形图可以使用 <strong>barh()</strong> 方法来设置</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="string">"Runoob-1"</span>, <span class="string">"Runoob-2"</span>, <span class="string">"Runoob-3"</span>, <span class="string">"C-RUNOOB"</span>])</span><br><span class="line">y = np.array([<span class="number">12</span>, <span class="number">22</span>, <span class="number">6</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">plt.barh(x,y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.tvoghzlhxces{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162341451.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162341451.png" srcset="data:image/png;base64,666" class="tvoghzlhxces lazyload"></p>
<p>实例3：设置柱形图颜色</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="string">"Runoob-1"</span>, <span class="string">"Runoob-2"</span>, <span class="string">"Runoob-3"</span>, <span class="string">"C-RUNOOB"</span>])</span><br><span class="line">y = np.array([<span class="number">12</span>, <span class="number">22</span>, <span class="number">6</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">plt.bar(x, y, color = <span class="string">"#4CAF50"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.djeihisjebkp{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162449487.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162449487.png" srcset="data:image/png;base64,666" class="djeihisjebkp lazyload"></p>
<p>实例4：自定义各个柱形的颜色</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="string">"Runoob-1"</span>, <span class="string">"Runoob-2"</span>, <span class="string">"Runoob-3"</span>, <span class="string">"C-RUNOOB"</span>])</span><br><span class="line">y = np.array([<span class="number">12</span>, <span class="number">22</span>, <span class="number">6</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">plt.bar(x, y,  color = [<span class="string">"#4CAF50"</span>,<span class="string">"red"</span>,<span class="string">"hotpink"</span>,<span class="string">"#556B2F"</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.ivaxhlwdbxeu{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162554129.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162554129.png" srcset="data:image/png;base64,666" class="ivaxhlwdbxeu lazyload"></p>
<p>实例5：设置柱形图宽度，<strong>bar()</strong> 方法使用 <strong>width</strong> 设置</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="string">"Runoob-1"</span>, <span class="string">"Runoob-2"</span>, <span class="string">"Runoob-3"</span>, <span class="string">"C-RUNOOB"</span>])</span><br><span class="line">y = np.array([<span class="number">12</span>, <span class="number">22</span>, <span class="number">6</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">plt.bar(x, y, width = <span class="number">0.1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.qbyhmiwazxed{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162717588.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162717588.png" srcset="data:image/png;base64,666" class="qbyhmiwazxed lazyload"></p>
<p>实例6：<strong>barh()</strong> 方法使用 <strong>height</strong> 设置 height</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="string">"Runoob-1"</span>, <span class="string">"Runoob-2"</span>, <span class="string">"Runoob-3"</span>, <span class="string">"C-RUNOOB"</span>])</span><br><span class="line">y = np.array([<span class="number">12</span>, <span class="number">22</span>, <span class="number">6</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">plt.barh(x, y, height = <span class="number">0.1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.esquxowtkbmd{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162753714.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162753714.png" srcset="data:image/png;base64,666" class="esquxowtkbmd lazyload"></p>
<h2 id="Matplotlib饼图"><a href="#Matplotlib饼图" class="headerlink" title="Matplotlib饼图"></a>Matplotlib饼图</h2><p>我们可以使用 pyplot 中的 pie() 方法来绘制饼图。</p>
<p>pie() 方法语法格式如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">matplotlib.pyplot.pie(x, explode=<span class="literal">None</span>, labels=<span class="literal">None</span>, colors=<span class="literal">None</span>, autopct=<span class="literal">None</span>, pctdistance=<span class="number">0.6</span>, shadow=<span class="literal">False</span>, labeldistance=<span class="number">1.1</span>, startangle=<span class="number">0</span>, radius=<span class="number">1</span>, counterclock=<span class="literal">True</span>, wedgeprops=<span class="literal">None</span>, textprops=<span class="literal">None</span>, center=<span class="number">0</span>, <span class="number">0</span>, frame=<span class="literal">False</span>, rotatelabels=<span class="literal">False</span>, *, normalize=<span class="literal">None</span>, data=<span class="literal">None</span>)[source]</span><br></pre></td></tr></tbody></table></figure>
<p><strong>参数说明：</strong></p>
<p><strong>x</strong>：浮点型数组，表示每个扇形的面积。</p>
<p><strong>explode</strong>：数组，表示各个扇形之间的间隔，默认值为0。</p>
<p><strong>labels</strong>：列表，各个扇形的标签，默认值为 None。</p>
<p><strong>colors</strong>：数组，表示各个扇形的颜色，默认值为 None。</p>
<p><strong>autopct</strong>：设置饼图内各个扇形百分比显示格式，<strong>%d%%</strong> 整数百分比，<strong>%0.1f</strong> 一位小数， <strong>%0.1f%%</strong> 一位小数百分比， <strong>%0.2f%%</strong> 两位小数百分比。</p>
<p><strong>labeldistance</strong>：标签标记的绘制位置，相对于半径的比例，默认值为 1.1，如 <strong>&lt;1</strong>则绘制在饼图内侧。</p>
<p><strong>pctdistance：</strong>：类似于 labeldistance，指定 autopct 的位置刻度，默认值为 0.6。</p>
<p><strong>shadow：</strong>：布尔值 True 或 False，设置饼图的阴影，默认为 False，不设置阴影。</p>
<p><strong>radius：</strong>：设置饼图的半径，默认为 1。</p>
<p><strong>startangle：</strong>：起始绘制饼图的角度，默认为从 x 轴正方向逆时针画起，如设定 =90 则从 y 轴正方向画起。</p>
<p><strong>counterclock</strong>：布尔值，设置指针方向，默认为 True，即逆时针，False 为顺时针。</p>
<p><strong>wedgeprops</strong> ：字典类型，默认值 None。参数字典传递给 wedge 对象用来画一个饼图。例如：wedgeprops={‘linewidth’:5} 设置 wedge 线宽为5。</p>
<p><strong>textprops</strong> ：字典类型，默认值为：None。传递给 text 对象的字典参数，用于设置标签（labels）和比例文字的格式。</p>
<p><strong>center</strong> ：浮点类型的列表，默认值：(0,0)。用于设置图标中心位置。</p>
<p><strong>frame</strong> ：布尔类型，默认值：False。如果是 True，绘制带有表的轴框架。</p>
<p><strong>rotatelabels</strong> ：布尔类型，默认为 False。如果为 True，旋转每个 label 到指定的角度。</p>
<p>实例1：简单实用 pie() 来创建一个柱形图</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">y = np.array([<span class="number">35</span>, <span class="number">25</span>, <span class="number">25</span>, <span class="number">15</span>])</span><br><span class="line"></span><br><span class="line">plt.pie(y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.sewefuzyedda{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404165657025.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404165657025.png" srcset="data:image/png;base64,666" class="sewefuzyedda lazyload"></p>
<p>实例2：设置饼图各个扇形的标签与颜色</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">y = np.array([<span class="number">35</span>, <span class="number">25</span>, <span class="number">25</span>, <span class="number">15</span>])</span><br><span class="line"></span><br><span class="line">plt.pie(y,</span><br><span class="line">        labels=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>], <span class="comment"># 设置饼图标签</span></span><br><span class="line">        colors=[<span class="string">"#d5695d"</span>, <span class="string">"#5d8ca8"</span>, <span class="string">"#65a479"</span>, <span class="string">"#a564c9"</span>], <span class="comment"># 设置饼图颜色</span></span><br><span class="line">       )</span><br><span class="line">plt.title(<span class="string">"RUNOOB Pie Test"</span>) <span class="comment"># 设置标题</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.hlrglguebqiz{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404165759838.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404165759838.png" srcset="data:image/png;base64,666" class="hlrglguebqiz lazyload"></p>
<p>实例3：突出显示第二个扇形，并格式化输出百分比</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">y = np.array([<span class="number">35</span>, <span class="number">25</span>, <span class="number">25</span>, <span class="number">15</span>])</span><br><span class="line"></span><br><span class="line">plt.pie(y,</span><br><span class="line">        labels=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>], <span class="comment"># 设置饼图标签</span></span><br><span class="line">        colors=[<span class="string">"#d5695d"</span>, <span class="string">"#5d8ca8"</span>, <span class="string">"#65a479"</span>, <span class="string">"#a564c9"</span>], <span class="comment"># 设置饼图颜色</span></span><br><span class="line">        explode=(<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0</span>, <span class="number">0</span>), <span class="comment"># 第二部分突出显示，值越大，距离中心越远</span></span><br><span class="line">        autopct=<span class="string">'%.2f%%'</span>, <span class="comment"># 格式化输出百分比</span></span><br><span class="line">       )</span><br><span class="line">plt.title(<span class="string">"RUNOOB Pie Test"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><style>.kmewsjjklujl{}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404170000561.png" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404170000561.png" srcset="data:image/png;base64,666" class="kmewsjjklujl lazyload"></p>
<p><strong>注意：</strong>默认情况下，第一个扇形的绘制是从 x 轴开始并逆时针移动：</p>
<p><style>.anjfccpomkgv{zoom:50%;}</style><img src="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/03D59143-B345-4B36-A7CD-53F698AB5284.jpg" class="lazyload" data-srcset="/zh-CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/03D59143-B345-4B36-A7CD-53F698AB5284.jpg" srcset="data:image/png;base64,666" class="anjfccpomkgv lazyload"></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title>RFBNet论文笔记</title>
    <url>/zh-CN/RFBNet/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文地址：<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper.pdf">Receptive Field Block Net for Accurate and Fast Object Detection (thecvf.com)</a></p>
<p>代码地址：<a href="https://github.com/ruinmessi/RFBNet">https://github.com/ruinmessi/RFBNet</a></p>
<h4 id="Q1"><a href="#Q1" class="headerlink" title="Q1"></a>Q1</h4><p>RFB模块是怎么考虑感受野尺寸和离心率之间的关系的？</p>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>RFBNet</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL学习笔记</title>
    <url>/zh-CN/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="MySQL入门教程"><a href="#MySQL入门教程" class="headerlink" title="MySQL入门教程"></a>MySQL入门教程</h2><h3 id="什么是数据库"><a href="#什么是数据库" class="headerlink" title="什么是数据库"></a>什么是数据库</h3><p>数据库是按照数据结构来组织、存储和管理数据的仓库</p>
<p>关系型数据库：建立在关系模型基础上的数据库，借助于集合代数等数学概念和方法来处理数据库中的数据</p>
<p>RDBMS即关系数据库管理系统的特点：</p>
<p>1.数据以表格的形式出现</p>
<p>2.每行为各种记录名称</p>
<p>3.每列为记录名称所对应的数据域</p>
<p>4.许多行和列组成一张表单</p>
<p>5.若干的表单组成数据库</p>
<h3 id="RDBMS相关概念"><a href="#RDBMS相关概念" class="headerlink" title="RDBMS相关概念"></a>RDBMS相关概念</h3><p>·</p>
<h3 id="MySQL创建数据表"><a href="#MySQL创建数据表" class="headerlink" title="MySQL创建数据表"></a>MySQL创建数据表</h3><h4 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h4><p><code>CREATE TABLE table_name (column_name column_type)</code></p>
<p>举例：在W3CSCHOOL数据库中创建数据表w3cschool_tbl:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS tutorials_tbl(</span><br><span class="line">	tutorial_id INT NOT NULL AUTO_INCREMENT,</span><br><span class="line">	tutorial_title VARCHAR(100) NOT NULL,</span><br><span class="line">	tutorial_author VARCHAR(40) NOT NULL,</span><br><span class="line">	submission_date DATE,</span><br><span class="line">	PRIMARY KEY (tutorial_id)</span><br><span class="line">	);</span><br></pre></td></tr></tbody></table></figure>
<p>注：</p>
<p>·如果你不想字段为NULL可以设置字段的属性为NOT NULL,在操作数据库如果输入该字段的数据为NULL,则会报错。</p>
<p>·AUTO_INCREMENT定义列为自增的属性，一般用于主键，数值会自动加1。</p>
<p>·PRIMARY KEY关键字用于定义列为主键。你可以使用多列来定义主键，列间以逗号分隔。</p>
<h3 id="MySQL删除数据表"><a href="#MySQL删除数据表" class="headerlink" title="MySQL删除数据表"></a>MySQL删除数据表</h3><h4 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h4><p><code>DROP TABLE table_name</code></p>
<h3 id="MySQL插入数据"><a href="#MySQL插入数据" class="headerlink" title="MySQL插入数据"></a>MySQL插入数据</h3><h4 id="语法-2"><a href="#语法-2" class="headerlink" title="语法"></a>语法</h4><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">INSERT INTO table_name(field1, field2, ...fieldN)</span><br><span class="line">					  VALUES</span><br><span class="line">					  (value1, value2, ...valueN);</span><br></pre></td></tr></tbody></table></figure>
<p>举例：使用SQL INSERT INTO语句向MySQL数据表w3cschool_tbl插入数据：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">INSERT INTO w3cschool_tbl</span><br><span class="line">(w3cschool_title, w3cschool_author, submission_date)</span><br><span class="line">VALUES</span><br><span class="line">("Learn PHP", "John Poul", NOW());</span><br></pre></td></tr></tbody></table></figure>
<h3 id="MySQL查询数据"><a href="#MySQL查询数据" class="headerlink" title="MySQL查询数据"></a>MySQL查询数据</h3><h4 id="语法-3"><a href="#语法-3" class="headerlink" title="语法"></a>语法</h4><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT column_name, column_name</span><br><span class="line">FROM table_name</span><br><span class="line">[WHERE Clause]</span><br><span class="line">[OFFSET M][LIMIT N]</span><br></pre></td></tr></tbody></table></figure>
<p>注：</p>
<p>·查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分隔，并使用WHERE语句来设定查询条件。</p>
<p>·SELECT命令可以读取一条或者多条记录。</p>
<p>·你可以使用星号(*)来代替其他字段，SELECT语句会返回表的所有字段数据。</p>
<p>·你可以使用WHERE语句来包含任何条件。</p>
<p>·你可以通过OFFSET指定SELECT语句开始查询的数据偏移量，默认偏移量为0。</p>
<p>·你可以使用LIMIT属性来设定返回的记录数。</p>
<p>举例：通过SQL SELECT命令来获取MySQL数据表w3cschool_tbl的数据：</p>
<p><code>SELECT * from w3cschool_tbl;</code></p>
<h3 id="MySQL-where子句"><a href="#MySQL-where子句" class="headerlink" title="MySQL where子句"></a>MySQL where子句</h3><h4 id="语法-4"><a href="#语法-4" class="headerlink" title="语法"></a>语法</h4><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT field1, field2, ...fieldN</span><br><span class="line">FROM table_name1, table_name2...</span><br><span class="line">[WHERE condition1 [[AND][OR]] condition2...]</span><br></pre></td></tr></tbody></table></figure>
<p>注：</p>
<p>·查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分隔，并使用WHERE语句来设定查询条件。</p>
<p>·你可以在WHERE子句中指定任何条件。</p>
<p>·你可以使用AND或者OR指定一个或多个条件。</p>
<p>·WHERE子句也可以运用于SQL的DELETE或UPDATE命令。</p>
<p>·WHERE子句类似于程序中的if条件，根据MySQL表中的字段值来读取指定的数据。</p>
<p>操作符列表，实例假定A=10,B=20:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">操作符</th>
<th>描述</th>
<th>实例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">=</td>
<td>等号，检测两个值是否相等，如果相等返回True</td>
<td>(A=B)返回false</td>
</tr>
<tr>
<td style="text-align:center">&lt;&gt;或！=</td>
<td>不等于，检测两个值是否相等，如果不相等返回True</td>
<td>（A!=B)返归true</td>
</tr>
<tr>
<td style="text-align:center">&gt;</td>
<td>大于号，检测左边的值是否大于右边的值，如果左边的值大于右边的值返回True</td>
<td>(A&gt;B)返回false</td>
</tr>
<tr>
<td style="text-align:center">&lt;</td>
<td>小于号，检测左边的值是否小于右边的值，如果左边的值小于右边的值返回True</td>
<td>(A&lt;B)返回true</td>
</tr>
<tr>
<td style="text-align:center">&gt;=</td>
<td>大于等于号，检测左边的值是否大于等于右边的值，如果左边的值大于或等于右边的值返回True</td>
<td>(A&gt;=B)返回false</td>
</tr>
<tr>
<td style="text-align:center">&lt;=</td>
<td>小于等于号，检测左边的值是否小于或等于右边的值，如果左边的值小于或等于右边的值返回True</td>
<td>(A&lt;=B)返回true</td>
</tr>
</tbody>
</table>
</div>
<p>举例：读取w3cschool_tbl表中w3cschool_author字段值为Sanjay的所有记录：</p>
<p><code>SELECT * from w3cschool_tbl WHERE w3cschool_author='Sanjay';</code></p>
<p>除非你使用LIKE来比较字符串，否则MySQL的WHERE子句的字符串比较是不区分大小写的。你可以使用BINARY关键字来设定WHERE子句的紫福春是区分大小写的。</p>
<p><code>SELECT * from w3cschool_tbl WHERE BINARY w3cschool_author='sanjay';</code></p>
<h3 id="MySQL-UPDATE查询"><a href="#MySQL-UPDATE查询" class="headerlink" title="MySQL UPDATE查询"></a>MySQL UPDATE查询</h3><h4 id="语法-5"><a href="#语法-5" class="headerlink" title="语法"></a>语法</h4><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">UPDATE table_name SET field1=new-value1, field2=new-value2</span><br><span class="line"></span><br><span class="line">[WHERE Clause]</span><br></pre></td></tr></tbody></table></figure>
<p>注：</p>
<p>·你可以同时更新一个或多个字段</p>
<p>·你可以在WHERE字句中指定任何条件</p>
<p>·你可以在一个单独表中同时更新数据</p>
<p>当你需要更新表中指定行的数据时，WHERE子句是非常有用的。</p>
<p>举例：更新数据表中w3cschool_id为3的w3cschool_title字段值：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">UPDATE w3cschool_tbl</span><br><span class="line">SET w3cschool_title='Learning JAVA'</span><br><span class="line">WHERE w3cschool_id=3;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="MySQL-DELETE语句"><a href="#MySQL-DELETE语句" class="headerlink" title="MySQL DELETE语句"></a>MySQL DELETE语句</h3><h4 id="语法-6"><a href="#语法-6" class="headerlink" title="语法"></a>语法</h4><p><code>DELETE FROM table_name [WHERE Clause]</code></p>
<p>注：</p>
<p>·如果没有指定WHERE子句，MySQL表中的所有记录将被删除</p>
<p>·你可以在WHERE字句中指定任何条件</p>
<p>·你可以在单个表中一次性删除记录</p>
<p>当你想删除数据表中的指定记录时，WHERE子句是非常有用的</p>
<p>举例：删除w3cschool_tbl表中w3cschool_id为3的记录：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">DELETE FROM w3cschool_tbl WHERE w3cschool_id=3;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="MySQL-LIKE子句"><a href="#MySQL-LIKE子句" class="headerlink" title="MySQL LIKE子句"></a>MySQL LIKE子句</h3><p>SQL LIKE子句中使用百分号（%）字符来表示任意字符，类似于UNIX或者正则表达式中的星号（*）。</p>
<h4 id="语法-7"><a href="#语法-7" class="headerlink" title="语法"></a>语法</h4><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT field1, field2, ...fieldN</span><br><span class="line">FROM table_name1, table_name2...</span><br><span class="line">WHERE field1 LIKE condition [[AND][OR]] field2='somevalue'</span><br></pre></td></tr></tbody></table></figure>
<p>注：</p>
<p>·你可以在WHERE子句中指定任何条件</p>
<p>·你可以在WHERE字句中使用LIKE子句</p>
<p>·你可以使用LIKE子句代替等号</p>
<p>·LIKE通常与%一同使用，类似于一个元字符的搜索</p>
<p>·你可以使用AND或OR指定一个或多个条件</p>
<p>·你可以在DELETE或UPDATE命令中使用WHERE…LIKE子句来指定条件</p>
<p>举例：查询w3cschool_tbl表中的w3cschool_author字段中以’jay’为结尾的所有记录：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT * from w3cschool_tbl</span><br><span class="line">WHERE w3cschool_author LIKE '%jay';</span><br></pre></td></tr></tbody></table></figure>
<h3 id="MySQL排序"><a href="#MySQL排序" class="headerlink" title="MySQL排序"></a>MySQL排序</h3><h4 id="语法-8"><a href="#语法-8" class="headerlink" title="语法"></a>语法</h4><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT field1, field2,...fieldN FROM table_name1, table_name2...</span><br><span class="line">ORDER BY field1, [field2...] [ASC[DESC]]</span><br></pre></td></tr></tbody></table></figure>
<p>注：</p>
<p>·你可以使用任何字段来作为排序的条件，从而返回排序后的查询结果</p>
<p>·你可以设定多个字段来排序</p>
<p>·你可以使用ASC或DESC关键字来设置查询结果是按升序或者降序排列。默认情况下，它是按升序排列</p>
<p>·你可以添加WHERE…LIKE子句来设置条件</p>
<p>举例：使用ORDER BY子句来读取MySQL数据表w3cschool_tbl中的数据：</p>
<p><code>SELECT * from w3cschool_tbl ORDER BY w3cschool_author ASC;</code></p>
<p><code>SELECT * from w3cschool_tbl ORDER BY w3cschool_author DESC;</code></p>
<h3 id="MySQL-分组"><a href="#MySQL-分组" class="headerlink" title="MySQL 分组"></a>MySQL 分组</h3><p>GROUP BY语句根据一个或者多个列对结果集进行分组，在分组的列上我们可以使用COUNT,SUM,AVG等函数。</p>
<h4 id="语法-9"><a href="#语法-9" class="headerlink" title="语法"></a>语法</h4><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT column_name, function(column_name)</span><br><span class="line">FROM table_name</span><br><span class="line">WHERE column_name operator value</span><br><span class="line">GROUP BY column_name;</span><br></pre></td></tr></tbody></table></figure>
<p>employee_tbl表格信息如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">id</th>
<th>name</th>
<th>date</th>
<th>singin</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td>小明</td>
<td>2016-04-22 15:25:33</td>
<td>1</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td>小王</td>
<td>2016-04-20 15:25:47</td>
<td>3</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td>小丽</td>
<td>2016-04-19 15:26:02</td>
<td>2</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td>小王</td>
<td>2016-04-07 15:26:14</td>
<td>4</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td>小明</td>
<td>2016-04-11 15:26:40</td>
<td>4</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td>小明</td>
<td>2016-04-04 15:26:54</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p>举例：将数据表按名字进行分组，并统计每个人有多少条记录：</p>
<p><code>SELECT name, COUNT(*) FROM employ_tbl GROUP BY name;</code></p>
<h3 id="使用WITH-ROLLUP"><a href="#使用WITH-ROLLUP" class="headerlink" title="使用WITH ROLLUP"></a>使用WITH ROLLUP</h3><p>WITH ROLLUP可以实现在分组统计数据基础上再进行相同的统计（SUM.AVG,COUNT).</p>
<p>举例：将以上的数据表按名字进行分组，再统计每个人登录的次数：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT name, SUM(singin) as singin_count</span><br><span class="line">FROM employee_tbl</span><br><span class="line">GROUP BY name WITH ROLLUP;</span><br></pre></td></tr></tbody></table></figure>
<p>其中结果如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>name</th>
<th>singin_out</th>
</tr>
</thead>
<tbody>
<tr>
<td>小丽</td>
<td>2</td>
</tr>
<tr>
<td>小明</td>
<td>7</td>
</tr>
<tr>
<td>小王</td>
<td>7</td>
</tr>
<tr>
<td>NULL</td>
<td>16</td>
</tr>
</tbody>
</table>
</div>
<p>其中记录NULL表示所有人的登录次数，我们可以使用coalesce来设置一个可以取代NULL的名称。</p>
<h4 id="语法-10"><a href="#语法-10" class="headerlink" title="语法"></a>语法</h4><p><code>select coalesce(a, b, c);</code></p>
<p>参数说明：如果a==null,则选择b;如果b==null,则选择c;如果a!=null,则选择a;如果abc都为null,则返回null(没意义)。</p>
<p>举例：如果名字为空，使用总数代替：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT coalesce(name, '总数'), SUM(singin) as singin_out</span><br><span class="line">FROM employee_tbl</span><br><span class="line">GROUP BY name WITH ROLLUP;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="MySQL连接的使用"><a href="#MySQL连接的使用" class="headerlink" title="MySQL连接的使用"></a>MySQL连接的使用</h3><p>JOIN按照功能大致分为如下三类：</p>
<p>·INNER JOIN(内连接，或等值连接)：获取两个表中字段匹配关系的记录</p>
<p>·LEFT JOIN（左连接）：获取左表所有记录，即使右表没有对应匹配的记录</p>
<p>·RIGHT JOIN(右连接)：用于获取右表所有记录，即使左表没有对应匹配的记录</p>
<p><a href="https://raw.githubusercontent.com/pistachio0812/pistachio0812.github.io/main/images/SQL_JOINS.png">SQL JOINS</a></p>
<p>假设W3CSCHOOL数据库中有两张表tcount_tbl和w3cschool_tbl,两张数据表数据如下：</p>
<p>1.tcount_tbl</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">w3cschool_author</th>
<th>w3cschool_count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">mahran</td>
<td>20</td>
</tr>
<tr>
<td style="text-align:center">mahnaz</td>
<td>NULL</td>
</tr>
<tr>
<td style="text-align:center">Jen</td>
<td>NULL</td>
</tr>
<tr>
<td style="text-align:center">Gill</td>
<td>20</td>
</tr>
<tr>
<td style="text-align:center">John Poul</td>
<td>1</td>
</tr>
<tr>
<td style="text-align:center">Sanjay</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>2.w3cschool_tbl</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>w3cschool_id</th>
<th>w3cschool_title</th>
<th>w3cschool_author</th>
<th>submission_date</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Learn PHP</td>
<td>John Poul</td>
<td>2007-05-24</td>
</tr>
<tr>
<td>2</td>
<td>Learn MyQL</td>
<td>Abdul S</td>
<td>2007-05-24</td>
</tr>
<tr>
<td>3</td>
<td>JAVA Tutorial</td>
<td>Sanjay</td>
<td>2007-05-06</td>
</tr>
</tbody>
</table>
</div>
<p>举例：使用INNER JOIN来连接以上两张表来读取w3cschool_tbl表中所有w3cschool_author字段在tcount_tbl表中对应的w3cschool_count字段值。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT a.w3cschool_id, a.w3cschool_author, b.w3cschool_count</span><br><span class="line">FROM w3cschool_tbl a INNER JOIN tcount_tbl b</span><br><span class="line">ON a.w3cschool_author = b.w3cschool_author</span><br></pre></td></tr></tbody></table></figure>
<p>等价于：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT a.w3cschool_id, a_w3cschool_author, b.w3cschool_count</span><br><span class="line">FROM w3cschool_tbl a, tcount_tbl b</span><br><span class="line">WHERE a.w3cschool_author = b.w3cschool_author;</span><br></pre></td></tr></tbody></table></figure>
<p>举例：以w3cschool_tbl为左表， t_count_tbl为右表，理解MySQL LEFT JOIN的应用：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT a.w3cschool_id, a.w3cschool_author, b.w3cschool_count</span><br><span class="line">FROM w3cschool_tbl a LEFT JOIN tcount_tbl b</span><br><span class="line">ON a.w3cschool_author = b.w3cschool_author;</span><br></pre></td></tr></tbody></table></figure>
<p>举例：以tcount_tbl为左表， w3cschool_tbl为右表，理解MySQL RIGHT JOIN的应用：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT b.w3cschool_id, b.w3cschool_author, a.w3cschool_count</span><br><span class="line">FROM tcount_tbl a RIGHT JOIN w3cschool_tbl b</span><br><span class="line">ON a.w3cschool_author = b.w3cschool_author;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="MySQL-NULL值处理"><a href="#MySQL-NULL值处理" class="headerlink" title="MySQL NULL值处理"></a>MySQL NULL值处理</h3><p>查询条件字段为NULL时，该命令可能无法正常工作，为了处理这种情况，MySQL提供了三大运算符：</p>
<p>·IS NULL:当列的值为NULL,此运算符返回True</p>
<p>·IS NOT NULL:当列的值不为NULL,运算符返回True</p>
<p>·&lt;=&gt;:比较运算符，当比较的两个值为NULL时返回True</p>
<p>关于NULL的条件比较运算是比较特殊的，你不能够使用=NULL或！=NULL在列中查找NULL值，在MySQL中，NULL值与任何其他值的比较永远返回false,即NULL=NULL返回false。</p>
<h4 id="在命令提示符中使用NULL值"><a href="#在命令提示符中使用NULL值" class="headerlink" title="在命令提示符中使用NULL值"></a>在命令提示符中使用NULL值</h4><p>假设数据库W3CSCHOOL中的表tcount_tbl含有两列w3cschool_author和w3cschool_count, w3cschool_count中设置插入NULL值。</p>
<p>假设表如下所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>w3cschool_author</th>
<th>w3cschool_count</th>
</tr>
</thead>
<tbody>
<tr>
<td>mahran</td>
<td>20</td>
</tr>
<tr>
<td>mahnaz</td>
<td>NULL</td>
</tr>
<tr>
<td>Jen</td>
<td>NULL</td>
</tr>
<tr>
<td>Gill</td>
<td>20</td>
</tr>
</tbody>
</table>
</div>
<p>查询数据表中w3cschool_count列是否为NULL,必须使用IS NULL和IS NOT NULL,如下实例：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT * FROM tcount_tbl</span><br><span class="line">WHERE w3cschool_count IS NULL;</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT * FROM tcount_tbl</span><br><span class="line">WHERE w3cschool_count IS NOT NULL;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="MySQL正则表达式"><a href="#MySQL正则表达式" class="headerlink" title="MySQL正则表达式"></a>MySQL正则表达式</h3><p>下表中的正则模式可应用于REGEXP操作符中。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>^</th>
<th>匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配’\n’或’\r’之后的位置。</th>
</tr>
</thead>
<tbody>
<tr>
<td>$</td>
<td>匹配输入字符串的结束位置。如果设置了RegExp对象的Multiline属性，^也匹配’\n’或’\r’之前的位置。</td>
</tr>
<tr>
<td>.</td>
<td>匹配除’\n’之外的任何单个字符。要匹配包括’\n’在内的任何字符，请使用’[.\n]’的模式。</td>
</tr>
<tr>
<td>[…]</td>
<td>字符集合。匹配所包含的任何一个字符。例如’[abc]’可以匹配’plain’中的’a’。</td>
</tr>
<tr>
<td><sup><a href="#fn_..." id="reffn_...">...</a></sup></td>
<td>负值字符集合。匹配未包含的任意字符。例如，’<sup><a href="#fn_abc" id="reffn_abc">abc</a></sup>‘可以匹配’plain’中的’p’。</td>
</tr>
<tr>
<td>p1\</td>
<td>p2\</td>
<td>p3</td>
<td>匹配p1或p2或p3.例如，’z\</td>
<td>food’能匹配’z’或’food’。，’(z\</td>
<td>f)food’能匹配’zood’或’food’</td>
</tr>
<tr>
<td>*</td>
<td>匹配前面的子表达式零次或多次。例如，zo能匹配’z’以及’zoo’.*等价于{0，}。</td>
</tr>
<tr>
<td>+</td>
<td>匹配前面的子表达式一次或多次。例如，’zo+’能匹配’zo’以及’zoo’,但不能匹配’z’。+等价于{1，}</td>
</tr>
<tr>
<td>{n}</td>
<td>n是一个非负整数。匹配确定的n次。例如，’o{2}’不能匹配’Bob’中的’o’,但是能匹配’food’中的两个o。</td>
</tr>
<tr>
<td>{n, m}</td>
<td>m和n均为非负整数，其中n&lt;=m。最少匹配n次且最多匹配m次。</td>
</tr>
</tbody>
</table>
</div>
<p>举例：</p>
<p>1.查找name字段中以’st’为开头的所有数据：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT name FROM person_tbl WHERE name REGEXP '^st';</span><br></pre></td></tr></tbody></table></figure>
<p>2.查找name字段中以’ok’为结尾的所有数据：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT name FROM person_tbl WHERE name REGEXP 'ok$';</span><br></pre></td></tr></tbody></table></figure>
<p>3.查找name字段中包含’mar’字符串的所有数据：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT name FROM person_tbl WHERE name REGEXP 'mar'</span><br></pre></td></tr></tbody></table></figure>
<p>4.查找name字段中以元音字符开头或以’ok’字符串结尾的所有数据：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">SELECT name FROM person_tbl WHERE name REGEXP '^[aeiou]|ok$'</span><br></pre></td></tr></tbody></table></figure>
<h3 id="MySQL事务"><a href="#MySQL事务" class="headerlink" title="MySQL事务"></a>MySQL事务</h3><p>MySQL 事务主要用于处理操作量大，复杂度高的数据。比如说，在人员管理系统中，你删除一个人员，你即需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等，这样，这些数据库操作语句就构成一个事务！</p>
<ul>
<li>在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。</li>
<li>事务处理可以用来维护数据库的完整性，保证成批的SQL语句要么全部执行，要么全部不执行。</li>
<li>事务用来管理 insert , update , delete 语句。</li>
</ul>
<p>一般来说，事务是必须满足4个条件（ACID）： Atomicity（原子性或不可分割性）、Consistency（一致性）、Isolation（隔离性或独立性）、Durability（持久性）</p>
<ul>
<li>1、<strong>原子性：</strong>一组事务，要么成功；要么撤回，即事务在执行过程中出错会回滚到事务开始前的状态。</li>
<li>2、<strong>一致性</strong> ： 一个事务不论是开始前还是结束后，数据库的完整性都没有被破坏。因此写入的数据必须完全符合所有预设规则（资料精确度、串联性以及后续数据库能够自发完成预定工作）。</li>
<li>3、<strong>隔离性：</strong>数据库允许多个事务并发的同时对其数据进行读写修改等操作，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离可分为：Read uncommitted（读未提交）、Read committed（读提交）、Repeatable read（可重复读）、Serializable（串行化）。</li>
<li>4、持久<strong>性：</strong>事务在处理结束后对数据做出的修改是永久的，无法丢失</li>
</ul>
<h4 id="事务控制语句"><a href="#事务控制语句" class="headerlink" title="事务控制语句"></a>事务控制语句</h4><p>1.显式的开始一个事务：</p>
<p><code>start transaction</code>或者<code>begin</code></p>
<p>2.做保存点，一个事务中可以有多个保存点：</p>
<p><code>savepoint  [savepoint_name]</code></p>
<p>3.提交事务，并使数据库中进行的修改成为永久性的：</p>
<p><code>commit</code>或<code>commit work</code></p>
<p>4.回滚结束用户的事务，并撤销正在进行的所有未提交的修改：</p>
<p><code>rollback</code>或<code>rollback work</code></p>
<p>5.删除一个事务的保存点，若没有指定保存点，执行该语句操作则会抛错：</p>
<p><code>release savepoint [savepoint_name]</code></p>
<p>6.将事务滚回标记点：</p>
<p><code>rollback to 标记点</code></p>
<p>7.设置事务的隔离级别。InnoDB 存储引擎提供事务的隔离级别有READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ 和 SERIALIZABLE。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">set transaction</span><br></pre></td></tr></tbody></table></figure>
<h4 id="事务处理方法"><a href="#事务处理方法" class="headerlink" title="事务处理方法"></a>事务处理方法</h4><p>1.用 begin ， rollback ， commit 来实现事务处理。</p>
<p>2.用 set 来改变 MySQL 的自动提交模式。</p>
<ul>
<li>set autocommit = 0 （禁止自动提交）。</li>
<li>set autocommit = 1 （开启自动提交）。</li>
</ul>
<h3 id="MySQL-ALTER命令"><a href="#MySQL-ALTER命令" class="headerlink" title="MySQL ALTER命令"></a>MySQL ALTER命令</h3><p>当我们需要修改数据表名或者修改数据表字段时，就需要使用到MySQL ALTER命令。</p>
<p>开始本章教程前让我们先创建一张表，表名为：testalter_tbl。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">create table testalter_tbl(</span><br><span class="line">	i INT,</span><br><span class="line">	c CHAR(1)</span><br><span class="line">	);</span><br><span class="line"> </span><br></pre></td></tr></tbody></table></figure>
<p><code>SHOW COLUMNS FROM testalter_tbl;</code></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Null</th>
<th>Key</th>
<th>Default</th>
<th>Extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>i</td>
<td>int(11)</td>
<td>YES</td>
<td></td>
<td>NULL</td>
<td></td>
</tr>
<tr>
<td>c</td>
<td>char(1)</td>
<td>YES</td>
<td></td>
<td>NULL</td>
</tr>
</tbody>
</table>
</div>
<h4 id="删除、添加或修改表字段"><a href="#删除、添加或修改表字段" class="headerlink" title="删除、添加或修改表字段"></a>删除、添加或修改表字段</h4><p><em>如下命令使用了 ALTER 命令及 DROP 子句来删除以上创建表的 i 字段：</em></p>
<p><em><code>mysql&gt; ALTER TABLE testalter_tbl  DROP i;</code></em></p>
<p><em>如果数据表中只剩余一个字段则无法使用DROP来删除字段。</em></p>
<p><em>MySQL 中使用 ADD 子句来想数据表中添加列，如下实例在表 testalter_tbl 中添加 i 字段，并定义数据类型:</em></p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl ADD i INT;</code></p>
<p>执行以上命令后，i 字段会自动添加到数据表字段的末尾。</p>
<p>`mysql&gt; SHOW COLUMNS FROM testalter_tbl; </p>
<p> <code>+-------+---------+------+-----+---------+-------+</code></p>
<p> <code>| Field | Type    | Null | Key | Default | Extra |</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code></p>
<p> <code>| c     | char(1) | YES  |     | NULL    |       |</code></p>
<p> <code>| i     | int(11) | YES  |     | NULL    |       |</code> </p>
<p> <code>+-------+---------+------+-----+---------+-------+</code> </p>
<p> <code>2 rows in set (0.00 sec)</code></p>
<p>如果你需要指定新增字段的位置，可以使用MySQL提供的关键字 FIRST (设定位第一列)， AFTER 字段名（设定位于某个字段之后）。</p>
<p>尝试以下 ALTER TABLE 语句, 在执行成功后，使用 SHOW COLUMNS 查看表结构的变化：</p>
<p><code>ALTER TABLE testalter_tbl DROP i;</code></p>
<p> <code>ALTER TABLE testalter_tbl ADD i INT FIRST;</code></p>
<p> <code>ALTER TABLE testalter_tbl DROP i;</code> </p>
<p> <code>ALTER TABLE testalter_tbl ADD i INT AFTER c;</code></p>
<p>FIRST 和 AFTER 关键字只占用于 ADD 子句，所以如果你想重置数据表字段的位置就需要先使用 DROP 删除字段然后使用 ADD 来添加字段并设置位置。</p>
<h4 id="修改字段类型及名称"><a href="#修改字段类型及名称" class="headerlink" title="修改字段类型及名称"></a>修改字段类型及名称</h4><p>如果需要修改字段类型及名称, 你可以在ALTER命令中使用 MODIFY 或 CHANGE 子句 。</p>
<p>例如，把字段 c 的类型从 CHAR(1) 改为 CHAR(10)，可以执行以下命令:</p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl MODIFY c CHAR(10);</code></p>
<p> 使用 CHANGE 子句, 语法有很大的不同。 在 CHANGE 关键字之后，紧跟着的是你要修改的字段名，然后指定新字段的类型及名称。尝试如下实例：</p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl CHANGE i j BIGINT;</code></p>
<p>如果你现在想把字段 j 从 BIGINT 修改为 INT，SQL语句如下：</p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl CHANGE j j INT;</code></p>
<h4 id="ALTER-TABLE-对-Null-值和默认值的影响"><a href="#ALTER-TABLE-对-Null-值和默认值的影响" class="headerlink" title="ALTER TABLE 对 Null 值和默认值的影响"></a>ALTER TABLE 对 Null 值和默认值的影响</h4><p>当你修改字段时，你可以指定是否包含只或者是否设置默认值。以下实例，指定字段 j 为 NOT NULL 且默认值为100 。</p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl</code></p>
<p>​        <code>-&gt; MODIFY j BIGINT NOT NULL DEFAULT 100;</code></p>
<p>如果你不设置默认值，MySQL会自动设置该字段默认为 NULL。</p>
<h4 id="修改字段默认值"><a href="#修改字段默认值" class="headerlink" title="修改字段默认值"></a>修改字段默认值</h4><p>你可以使用 ALTER 来修改字段的默认值，尝试以下实例：</p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl ALTER i SET DEFAULT 1000;</code></p>
<p><code>mysql&gt; SHOW COLUMNS FROM testalter_tbl;</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code></p>
<p> <code>| Field | Type    | Null | Key | Default | Extra |</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code> </p>
<p> <code>| c     | char(1) | YES  |     | NULL    |       |</code></p>
<p> <code>| i     | int(11) | YES  |     | 1000    |       |</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code></p>
<p> <code>2 rows in set (0.00 sec)</code></p>
<p>你也可以使用 ALTER 命令及 DROP子句来删除字段的默认值，如下实例：</p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl ALTER i DROP DEFAULT;</code></p>
<p> <code>mysql&gt; SHOW COLUMNS FROM testalter_tbl;</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code></p>
<p> <code>| Field | Type    | Null | Key | Default | Extra |</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code> </p>
<p> <code>| c     | char(1) | YES  |     | NULL    |       |</code></p>
<p> <code>| i     | int(11) | YES  |     | NULL    |       |</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code></p>
<p> <code>2 rows in set (0.00 sec) Changing a Table Type:</code></p>
<p>修改数据表类型，可以使用 ALTER 命令及 TYPE 子句来完成。尝试以下实例，我们将表 testalter_tbl 的类型修改为 MYISAM ：<strong>注意：</strong>查看数据表类型可以使用 SHOW TABLE STATUS 语句。*</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">mysql&gt; ALTER TABLE testalter_tbl TYPE = MYISAM;</span><br><span class="line">mysql&gt;  SHOW TABLE STATUS LIKE 'testalter_tbl'\G</span><br><span class="line"> 1. row **</span><br><span class="line">           Name: testalter_tbl</span><br><span class="line">           Type: MyISAM</span><br><span class="line">     Row_format: Fixed</span><br><span class="line">           Rows: 0</span><br><span class="line"> Avg_row_length: 0</span><br><span class="line">    Data_length: 0</span><br><span class="line">Max_data_length: 25769803775</span><br><span class="line">   Index_length: 1024</span><br><span class="line">      Data_free: 0</span><br><span class="line"> Auto_increment: NULL</span><br><span class="line">    Create_time: 2007-06-03 08:04:36</span><br><span class="line">    Update_time: 2007-06-03 08:04:36</span><br><span class="line">     Check_time: NULL</span><br><span class="line"> Create_options:</span><br><span class="line">        Comment:</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="修改表名"><a href="#修改表名" class="headerlink" title="修改表名"></a>修改表名</h4><p>如果需要修改数据表的名称，可以在 ALTER TABLE 语句中使用 RENAME 子句来实现。</p>
<p>尝试以下实例将数据表 testalter_tbl 重命名为 alter_tbl：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">mysql&gt; ALTER TABLE testalter_tbl RENAME TO alter_tbl;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="MySQL索引"><a href="#MySQL索引" class="headerlink" title="MySQL索引"></a>MySQL索引</h3><p>MySQL索引的建立对于MySQL的高效运行是很重要的，索引可以大大提高MySQL的检索速度。</p>
<p>打个比方，如果合理的设计且使用索引的MySQL是一辆兰博基尼的话，那么没有设计和使用索引的MySQL就是一个人力三轮车。</p>
<p>拿汉语字典的目录页（索引）打比方，我们可以按拼音、笔画、偏旁部首等排序的目录（索引）快速查找到需要的字。</p>
<p>索引分单列索引和组合索引。单列索引，即一个索引只包含单个列，一个表可以有多个单列索引，但这不是组合索引。组合索引，即一个索引包含多个列。</p>
<p>创建索引时，你需要确保该索引是应用在 SQL 查询语句的条件(一般作为 WHERE 子句的条件)。</p>
<p>实际上，索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录。</p>
<p>上面都在说使用索引的好处，但过多的使用索引将会造成滥用。因此索引也会有它的缺点：虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。</p>
<p>建立索引会占用磁盘空间的索引文件。</p>
<h4 id="普通索引"><a href="#普通索引" class="headerlink" title="普通索引"></a>普通索引</h4><p>创建索引</p>
<p>这是最基本的索引，它没有任何限制。它有以下几种创建方式：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">CREATE INDEX indexName ON mytable(username(length)); </span><br></pre></td></tr></tbody></table></figure>
<p>如果是CHAR，VARCHAR类型，length可以小于字段实际长度；如果是BLOB和TEXT类型，必须指定 length。</p>
<p>修改表结构(添加索引)</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">ALTER table tableName ADD INDEX indexName(columnName)</span><br></pre></td></tr></tbody></table></figure>
<p>创建表的时候直接指定</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">CREATE TABLE mytable(  </span><br><span class="line"> </span><br><span class="line">ID INT NOT NULL,   </span><br><span class="line"> </span><br><span class="line">username VARCHAR(16) NOT NULL,  </span><br><span class="line"> </span><br><span class="line">INDEX [indexName] (username(length))  </span><br><span class="line"> </span><br><span class="line">);  </span><br></pre></td></tr></tbody></table></figure>
<p>删除索引的语法</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">DROP INDEX [indexName] ON mytable; </span><br></pre></td></tr></tbody></table></figure>
<h4 id="唯一索引"><a href="#唯一索引" class="headerlink" title="唯一索引"></a>唯一索引</h4><p>它与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。它有以下几种创建方式：</p>
<p>创建索引</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">CREATE UNIQUE INDEX indexName ON mytable(username(length)) </span><br></pre></td></tr></tbody></table></figure>
<p>修改表结构</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">ALTER table mytable ADD UNIQUE [indexName] (username(length))</span><br></pre></td></tr></tbody></table></figure>
<p>创建表的时候直接指定</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">CREATE TABLE mytable(  </span><br><span class="line"> </span><br><span class="line">ID INT NOT NULL,   </span><br><span class="line"> </span><br><span class="line">username VARCHAR(16) NOT NULL,  </span><br><span class="line"> </span><br><span class="line">UNIQUE [indexName] (username(length))  </span><br><span class="line"> </span><br><span class="line">);  </span><br></pre></td></tr></tbody></table></figure>
<p>使用ALTER 命令添加和删除索引</p>
<p>有四种方式来添加数据表的索引：</p>
<ul>
<li>ALTER TABLE tbl_name ADD PRIMARY KEY (column_list): 该语句添加一个主键，这意味着索引值必须是唯一的，且不能为NULL。</li>
<li>ALTER TABLE tbl_name ADD UNIQUE index_name (column_list): 这条语句创建索引的值必须是唯一的（除了NULL外，NULL可能会出现多次）。</li>
<li>ALTER TABLE tbl_name ADD INDEX index_name (column_list): 添加普通索引，索引值可出现多次。</li>
<li>ALTER TABLE tbl_name ADD FULLTEXT index_name (column_list):该语句指定了索引为 FULLTEXT ，用于全文索引。</li>
</ul>
<p>以下实例为在表中添加索引。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">mysql&gt; ALTER TABLE testalter_tbl ADD INDEX (c);</span><br></pre></td></tr></tbody></table></figure>
<p>你还可以在 ALTER 命令中使用 DROP 子句来删除索引。尝试以下实例删除索引:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">mysql&gt; ALTER TABLE testalter_tbl DROP INDEX c;</span><br></pre></td></tr></tbody></table></figure>
<p>使用 ALTER 命令添加和删除主键</p>
<p>主键只能作用于一个列上，添加主键索引时，你需要确保该主键默认不为空（NOT NULL）。实例如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">mysql&gt; ALTER TABLE testalter_tbl MODIFY i INT NOT NULL;</span><br><span class="line">mysql&gt; ALTER TABLE testalter_tbl ADD PRIMARY KEY (i);</span><br></pre></td></tr></tbody></table></figure>
<p>你也可以使用 ALTER 命令删除主键：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">mysql&gt; ALTER TABLE testalter_tbl DROP PRIMARY KEY;</span><br></pre></td></tr></tbody></table></figure>
<p>删除主键时只需指定PRIMARY KEY，但在删除索引时，你必须知道索引名。</p>
<p>显示索引信息</p>
<p>你可以使用 SHOW INDEX 命令来列出表中的相关的索引信息。可以通过添加 \G 来格式化输出信息。</p>
<p>尝试以下实例:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">mysql&gt; SHOW INDEX FROM table_name; \G</span><br><span class="line">........</span><br></pre></td></tr></tbody></table></figure>
<h3 id="MySQL临时表"><a href="#MySQL临时表" class="headerlink" title="MySQL临时表"></a>MySQL临时表</h3><p>MySQL 临时表在我们需要保存一些临时数据时是非常有用的。临时表只在当前连接可见，当关闭连接时，MySQL会自动删除表并释放所有空间。</p>
<p>临时表在MySQL 3.23版本中添加，如果你的MySQL版本低于 3.23版本就无法使用MySQL的临时表。不过现在一般很少有再使用这么低版本的MySQL数据库服务了。</p>
<p>MySQL临时表只在当前连接可见，如果你使用PHP脚本来创建MySQL临时表，那每当PHP脚本执行完成后，该临时表也会自动销毁。</p>
<p>如果你使用了其他MySQL客户端程序连接MySQL数据库服务器来创建临时表，那么只有在关闭客户端程序时才会销毁临时表，当然你也可以手动销毁。</p>
<p>实例</p>
<p>以下展示了使用MySQL 临时表的简单实例，以下的SQL代码可以适用于PHP脚本的mysql_query()函数。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">mysql&gt; CREATE TEMPORARY TABLE SalesSummary (</span><br><span class="line">    -&gt; product_name VARCHAR(50) NOT NULL</span><br><span class="line">    -&gt; , total_sales DECIMAL(12,2) NOT NULL DEFAULT 0.00</span><br><span class="line">    -&gt; , avg_unit_price DECIMAL(7,2) NOT NULL DEFAULT 0.00</span><br><span class="line">    -&gt; , total_units_sold INT UNSIGNED NOT NULL DEFAULT 0</span><br><span class="line">);</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; INSERT INTO SalesSummary</span><br><span class="line">    -&gt; (product_name, total_sales, avg_unit_price, total_units_sold)</span><br><span class="line">    -&gt; VALUES</span><br><span class="line">    -&gt; ('cucumber', 100.25, 90, 2);</span><br><span class="line"></span><br><span class="line">mysql&gt; SELECT * FROM SalesSummary;</span><br><span class="line">+--------------+-------------+----------------+------------------+</span><br><span class="line">| product_name | total_sales | avg_unit_price | total_units_sold |</span><br><span class="line">+--------------+-------------+----------------+------------------+</span><br><span class="line">| cucumber     |      100.25 |          90.00 |                2 |</span><br><span class="line">+--------------+-------------+----------------+------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></tbody></table></figure>
<p>当你使用 <strong>SHOW TABLES</strong>命令显示数据表列表时，你将无法看到 SalesSummary表。</p>
<p>如果你退出当前MySQL会话，再使用 <strong>SELECT</strong>命令来读取原先创建的临时表数据，那你会发现数据库中没有该表的存在，因为在你退出时该临时表已经被销毁了。</p>
<p>删除MySQL 临时表</p>
<p>默认情况下，当你断开与数据库的连接后，临时表就会自动被销毁。当然你也可以在当前MySQL会话使用 <strong>DROP TABLE</strong> 命令来手动删除临时表。</p>
<p>以下是手动删除临时表的实例：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">mysql&gt; CREATE TEMPORARY TABLE SalesSummary (</span><br><span class="line">    -&gt; product_name VARCHAR(50) NOT NULL</span><br><span class="line">    -&gt; , total_sales DECIMAL(12,2) NOT NULL DEFAULT 0.00</span><br><span class="line">    -&gt; , avg_unit_price DECIMAL(7,2) NOT NULL DEFAULT 0.00</span><br><span class="line">    -&gt; , total_units_sold INT UNSIGNED NOT NULL DEFAULT 0</span><br><span class="line">);</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; INSERT INTO SalesSummary</span><br><span class="line">    -&gt; (product_name, total_sales, avg_unit_price, total_units_sold)</span><br><span class="line">    -&gt; VALUES</span><br><span class="line">    -&gt; ('cucumber', 100.25, 90, 2);</span><br><span class="line"></span><br><span class="line">mysql&gt; SELECT * FROM SalesSummary;</span><br><span class="line">+--------------+-------------+----------------+------------------+</span><br><span class="line">| product_name | total_sales | avg_unit_price | total_units_sold |</span><br><span class="line">+--------------+-------------+----------------+------------------+</span><br><span class="line">| cucumber     |      100.25 |          90.00 |                2 |</span><br><span class="line">+--------------+-------------+----------------+------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line">mysql&gt; DROP TABLE SalesSummary;</span><br><span class="line">mysql&gt;  SELECT * FROM SalesSummary;</span><br><span class="line">ERROR 1146: Table 'W3CSCHOOL.SalesSummary' doesn't exist</span><br></pre></td></tr></tbody></table></figure>]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>rss学习笔记</title>
    <url>/zh-CN/RSS/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://www.runoob.com/rss/rss-tutorial.html">RSS 教程 | 菜鸟教程 (runoob.com)</a></p>
<p>通过使用 RSS，您可以有选择地浏览您感兴趣的以及与您的工作相关的新闻。</p>
<p>通过使用 RSS，您可以把需要的信息从不需要的信息（兜售信息，垃圾邮件等）中分离出来。</p>
<p>通过使用 RSS，您可以创建自己的新闻频道，并将之发布到因特网。</p>
<h3 id="rss工作原理"><a href="#rss工作原理" class="headerlink" title="rss工作原理"></a>rss工作原理</h3><p>RSS 用于在网站间分享信息。</p>
<p>使用 RSS，您在名为聚合器的公司注册您的内容。</p>
<p>步骤之一是，创建一个 RSS 文档，然后使用 .xml 后缀来保存它。然后把此文件上传到您的网站。接下来，通过一个 RSS 聚合器来注册。每天，聚合器都会到被注册的网站搜索 RSS 文档，校验其链接，并显示有关 feed 的信息，这样客户就能够链接到使他们产生兴趣的文档。</p>
<h3 id="rss实例"><a href="#rss实例" class="headerlink" title="rss实例"></a>rss实例</h3><figure class="highlight xml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span> ?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">rss</span> <span class="attr">version</span>=<span class="string">"2.0"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">channel</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span>&gt;</span>菜鸟教程首页<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">link</span>&gt;</span>http://www.runoob.com<span class="tag">&lt;/<span class="name">link</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>免费编程教程<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">item</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>RSS 教程<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span>&gt;</span>http://www.runoob.com/rss<span class="tag">&lt;/<span class="name">link</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>菜鸟教程 Rss 教程<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">item</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">item</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>XML 教程<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span>&gt;</span>http://www.runoob.com/xml<span class="tag">&lt;/<span class="name">link</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>菜鸟教程 XML 教程<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">item</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">channel</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">rss</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<p>文档中的第一行：XML 声明 - 定义了文档中使用的 XML 版本和字符编码。此例子遵守 1.0 规范，并使用 UTF-8 字符集(可支持中文)。</p>
<p>下一行是标识此文档是一个 RSS 文档的 RSS 声明（此例是 RSS version 2.0）。</p>
<p>下一行含有 <channel> 元素。此元素用于描述 RSS feed。</channel></p>
<p><channel> 元素有三个必需的子元素：</channel></p>
<ul>
<li><title> - 定义频道的标题。（比如 菜鸟教程首页）</title></li>
<li><link> - 定义到达频道的超链接。（比如 www.runoob.com）</li>
<li><description> - 描述此频道（比如 免费编程教程）</description></li>
</ul>
<p>每个 <channel> 元素可拥有一个或多个 <item> 元素。</item></channel></p>
<p>每个 <item> 元素可定义 RSS feed 中的一篇文章或 “story”。</item></p>
<p><item> 元素拥有三个必需的子元素：</item></p>
<ul>
<li><title> - 定义项目的标题。（比如 RSS 教程）</title></li>
<li><link> - 定义到达项目的超链接。（比如 <a href="http://www.runoob.com/rss）">http://www.runoob.com/rss）</a></li>
<li><description> - 描述此项目（比如 菜鸟教程 Rss 教程）</description></li>
</ul>
<p>最后，后面的两行关闭 <channel> 和 <rss> 元素。</rss></channel></p>
<h3 id="rss发布到web"><a href="#rss发布到web" class="headerlink" title="rss发布到web"></a>rss发布到web</h3><p>现在是时候把您的 RSS 文件上传到网上了。下面是具体的步骤：</p>
<p>1.为您的 RSS 命名。请注意文件必须有 .xml 的后缀。</p>
<ol>
<li><p>验证您的 RSS 文件。（可以在 <a href="http://www.feedvalidator.org/">http://www.feedvalidator.org</a> 找到很好的验证器）。</p>
</li>
<li><p>把 RSS 文件上传到您的 web 服务器上的 web 目录。</p>
</li>
<li><p>把这个小的橙色按钮 <style>.uxvenqgrhylh{zoom:80%;}</style><img src="/zh-CN/RSS/RSS/rss.gif" class="lazyload" data-srcset="/zh-CN/RSS/RSS/rss.gif" srcset="data:image/png;base64,666" class="uxvenqgrhylh lazyload" alt="RSS Logo">或 <style>.hqpltqbdxfrp{zoom:80%;}</style><img src="/zh-CN/RSS/RSS/xml.gif" class="lazyload" data-srcset="/zh-CN/RSS/RSS/xml.gif" srcset="data:image/png;base64,666" class="hqpltqbdxfrp lazyload" alt="XML Logo">拷贝到您的 web 目录。</p>
</li>
<li><p>在你希望向外界提供 RSS 的页面上放置这个小按钮。然后向这个按钮添加一个指向 RSS 文件的链接。代码应该类似这样：</p>
</li>
</ol>
<figure class="highlight xml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.runoob.com/feed"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">loading</span>=<span class="string">"lazy"</span> <span class="attr">src</span>=<span class="string">"http://www.runoob.com/images/rss.gif"</span> <span class="attr">width</span>=<span class="string">"36"</span> <span class="attr">height</span>=<span class="string">"14"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<ol>
<li>把你的 RSS feed 提交到 RSS Feed 目录。要注意！feed 的 URL 不是你的页面，而是您的指向您的 feed 的 URL，比如 “<a href="http://www.runoob.com/feed&quot;。">http://www.runoob.com/feed"。</a> 此处提供一些免费的 RSS 聚合服务：</li>
</ol>
<ul>
<li><a href="http://www.newsisfree.com/">Newsisfree</a>: <a href="http://www.newsisfree.com/user/new/">点我注册</a></li>
</ul>
<ol>
<li>在重要的搜索引擎注册您的 feed ：</li>
</ol>
<ul>
<li><a href="http://wordpress.org/">WordPress</a></li>
<li><a href="http://www.blogger.com/">Blogger</a></li>
<li><a href="http://radio.userland.com/">Radio</a></li>
</ul>
<ol>
<li>更新您的 feed - 现在您已获得了来自 Google、Yahoo、以及 MSN 的 RSS feed 按钮。请您务必经常更新您的内容，并保持 RSS feed 的长期可用。</li>
</ol>
<h3 id="元素"><a href="#元素" class="headerlink" title="元素"></a><channel>元素</channel></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">元素</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-category-channel.html">category</a></td>
<td style="text-align:left">可选的。为 feed 定义所属的一个或多个种类。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-cloud.html">cloud</a></td>
<td style="text-align:left">可选的。注册进程，以获得 feed 更新的立即通知。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-copyright.html">copyright</a></td>
<td style="text-align:left">可选。告知版权资料。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-title-link-description-channel.html">description</a></td>
<td style="text-align:left">必需的。描述频道。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-docs.html">docs</a></td>
<td style="text-align:left">可选的。规定指向当前 RSS 文件所用格式说明的 URL。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-generator.html">generator</a></td>
<td style="text-align:left">可选的。规定用于生成 feed 的程序。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-image.html">image</a></td>
<td style="text-align:left">可选的。在聚合器呈现某个 feed 时，显示一个图像。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-language.html">language</a></td>
<td style="text-align:left">可选的。规定编写 feed 所用的语言。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-lastbuilddate.html">lastBuildDate</a></td>
<td style="text-align:left">可选的。定义 feed 内容的最后修改日期。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-title-link-description-channel.html">link</a></td>
<td style="text-align:left">必需的。定义指向频道的超链接。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-managingeditor.html">manageingEditor</a></td>
<td style="text-align:left">可选的。定义 feed 内容编辑的电子邮件地址。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-pubdate.html">pubDate</a></td>
<td style="text-align:left">可选的。为 feed 的内容定义最后发布日期。</td>
</tr>
<tr>
<td style="text-align:left"><rating></rating></td>
<td style="text-align:left">可选的。feed 的 PICS 级别。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-skipDays.html">skipDays</a></td>
<td style="text-align:left">可选的。规定忽略 feed 更新的天。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-skipHours.html">skipHours</a></td>
<td style="text-align:left">可选的。规定忽略 feed 更新的小时。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-textinput.html">textInput</a></td>
<td style="text-align:left">可选的。规定应当与 feed 一同显示的文本输入域。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-title-link-description-channel.html">title</a></td>
<td style="text-align:left">必需的。定义频道的标题。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-ttl.html">ttl</a></td>
<td style="text-align:left">可选的。指定从 feed 源更新此 feed 之前，feed 可被缓存的分钟数。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-webmaster.html">webMaster</a></td>
<td style="text-align:left">可选的。定义此 feed 的 web 管理员的电子邮件地址。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="元素-1"><a href="#元素-1" class="headerlink" title="元素"></a><item>元素</item></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">元素</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-author.html">author</a></td>
<td style="text-align:left">可选的。规定项目作者的电子邮件地址。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-category-item.html">category</a></td>
<td style="text-align:left">可选的。定义项目所属的一个或多个类别。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-comments.html">comments</a></td>
<td style="text-align:left">可选的。允许项目连接到有关此项目的注释（文件）。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-title-link-description-item.html">description</a></td>
<td style="text-align:left">必需的。描述此项目。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-enclosure.html">enclosure</a></td>
<td style="text-align:left">可选的。允许将一个媒体文件导入一个项中。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-guid.html">guid</a></td>
<td style="text-align:left">可选的。为项目定义一个唯一的标识符。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-title-link-description-item.html">link</a></td>
<td style="text-align:left">必需的。定义指向此项目的超链接。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-pubdate-item.html">pubDate</a></td>
<td style="text-align:left">可选的。定义此项目的最后发布日期。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-source.html">source</a></td>
<td style="text-align:left">可选的。为此项目指定一个第三方来源。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.runoob.com/rss/rss-tag-title-link-description-item.html">title</a></td>
<td style="text-align:left">必需的。定义此项目的标题。</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>rss</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>rss</tag>
      </tags>
  </entry>
  <entry>
    <title>react学习笔记</title>
    <url>/zh-CN/React/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="react特点"><a href="#react特点" class="headerlink" title="react特点"></a>react特点</h3><ul>
<li><strong>1.声明式设计</strong> −React采用声明范式，可以轻松描述应用。</li>
<li><strong>2.高效</strong> −React通过对DOM的模拟，最大限度地减少与DOM的交互。</li>
<li><strong>3.灵活</strong> −React可以与已知的库或框架很好地配合。</li>
<li><strong>4.JSX</strong> − JSX 是 JavaScript 语法的扩展。React 开发不一定使用 JSX ，但我们建议使用它。</li>
<li><strong>5.组件</strong> − 通过 React 构建组件，使得代码更加容易得到复用，能够很好的应用在大项目的开发中。</li>
<li><strong>6.单向响应的数据流</strong> − React 实现了单向响应的数据流，从而减少了重复代码，这也是它为什么比传统数据绑定更简单。</li>
</ul>
<figure class="highlight html"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>Hello React!<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://cdn.staticfile.org/react/16.4.0/umd/react.development.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://cdn.staticfile.org/react-dom/16.4.0/umd/react-dom.development.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://cdn.staticfile.org/babel-standalone/6.26.0/babel.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"example"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/babel"</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"><span class="title class_">ReactDOM</span>.<span class="title function_">render</span>(</span></span><br><span class="line"><span class="language-javascript">	<span class="language-xml"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>Hello, world!<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span>,</span></span><br><span class="line"><span class="language-javascript">	<span class="variable language_">document</span>.<span class="title function_">getElementById</span>(<span class="string">'example'</span>)</span></span><br><span class="line"><span class="language-javascript">);</span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="react安装"><a href="#react安装" class="headerlink" title="react安装"></a>react安装</h3><p>本教程使用了 React 的版本为 16.4.0，你可以在官网 <a href="https://reactjs.org/">https://reactjs.org/</a> 下载最新版</p>
<p>官方提供的CDN</p>
<figure class="highlight html"><table><tbody><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://unpkg.com/react@16/umd/react.development.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://unpkg.com/react-dom@16/umd/react-dom.development.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 生产环境中不建议使用 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 在浏览器中使用 Babel 来编译 JSX 效率是非常低的 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://unpkg.com/babel-standalone@6.15.0/babel.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>React</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>React</tag>
      </tags>
  </entry>
  <entry>
    <title>Resnet论文笔记</title>
    <url>/zh-CN/Resnet/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://gitcode.net/mirrors/bubbliiiing/faster-rcnn-pytorch">faster-rcnn-pytorch代码</a></p>
<p>2.<a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">Deep Residual Learning for Image Recognition (thecvf.com)</a></p>
<p>3.<a href="https://www.cnblogs.com/wujianming-110117/p/14975136.html">ResNet50结构 </a></p>
<p>4.<a href="https://blog.csdn.net/weixin_44791964/article/details/105739918?ops_request_misc=%7B%22request%5Fid%22%3A%22165156747616781685390350%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fblog.%22%7D&amp;request_id=165156747616781685390350&amp;biz_id=0&amp;spm=1018.2226.3001.4450">Pytorch搭建Faster R-CNN目标检测平台</a></p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>该网络是从原论文搬过来的，也就是参考博文2里的内容。有兴趣的可以读读原论文。</p>
<p><style>.igivvnkoxnxr{}</style><img src="/zh-CN/Resnet/Resnet/image-20220503185916137.png" class="lazyload" data-srcset="/zh-CN/Resnet/Resnet/image-20220503185916137.png" srcset="data:image/png;base64,666" class="igivvnkoxnxr lazyload" alt="image-20220503185916137"></p>
<h2 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h2><p>具体网络结构：</p>
<p>1.Bottleneck</p>
<p><style>.ncycumpbuiwr{zoom:50%;}</style><img src="/zh-CN/Resnet/Resnet/image-20220503193754428.png" class="lazyload" data-srcset="/zh-CN/Resnet/Resnet/image-20220503193754428.png" srcset="data:image/png;base64,666" class="ncycumpbuiwr lazyload" alt="image-20220503193754428"></p>
<p>2.Resnet50中50的含义：（3+4+6+3)*3+2=50（卷积全连接层数之和）</p>
<p>2.resnet101同理，只要把<code>model = ResNet(Bottleneck, [3, 4, 6, 3])</code>改成<code>model=ResNet(Bottleneck, [3, 4, 23, 3])</code>,101=(3+4+23+3)*3+2</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.hub <span class="keyword">import</span> load_state_dict_from_url</span><br><span class="line"></span><br><span class="line"><span class="comment"># c0表示输入特征图通道数，c1表示输出特征图通道数，由此可见，该类作用为通道扩张4倍</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, stride=<span class="number">1</span>, downsample=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line">        <span class="comment"># (N, C0, H, W)-&gt;(N, C1, H, W)</span></span><br><span class="line">        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># (N, C1, H, W)-&gt;(N, C1, H, W)</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># (N, C1, H, W)-&gt;(N, C1, H, W)</span></span><br><span class="line">        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># (N, C1, H, W)-&gt;(N, C1, H, W)</span></span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">		<span class="comment"># (N, C1, H, W)-&gt;(N, 4*C1, H, W)</span></span><br><span class="line">        self.conv3 = nn.Conv2d(planes, planes * <span class="number">4</span>, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># (N, 4*C1, H, W)-&gt;(N, 4*C1, H, W)</span></span><br><span class="line">        self.bn3 = nn.BatchNorm2d(planes * <span class="number">4</span>)</span><br><span class="line">        </span><br><span class="line">		<span class="comment"># (N, 4*C1, H, W)-&gt;(N, 4*C1, H, W)</span></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># defalt is None</span></span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block, layers, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="comment">#-----------------------------------#</span></span><br><span class="line">        <span class="comment">#   假设输入进来的图片是600,600,3</span></span><br><span class="line">        <span class="comment">#-----------------------------------#</span></span><br><span class="line">        self.inplanes = <span class="number">64</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 600,600,3 -&gt; 300,300,64</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 300,300,64 -&gt; 150,150,64</span></span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 150,150,64 -&gt; 150,150,256</span></span><br><span class="line">        <span class="comment"># 此时stride=1,但通道和原来一样，进行下采样，通道变为4*64=256</span></span><br><span class="line">        self.layer1 = self._make_layer(block, <span class="number">64</span>, layers[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 150,150,256 -&gt; 75,75,512</span></span><br><span class="line">        <span class="comment"># 此时stride=2,进行下采样，宽高变为原来的1/2，通道变为4*128=512</span></span><br><span class="line">        self.layer2 = self._make_layer(block, <span class="number">128</span>, layers[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 75,75,512 -&gt; 38,38,1024 到这里可以获得一个38,38,1024的共享特征层</span></span><br><span class="line">        self.layer3 = self._make_layer(block, <span class="number">256</span>, layers[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># self.layer4被用在classifier模型中</span></span><br><span class="line">        <span class="comment"># 38,38,1024 -&gt; 19,19,2048</span></span><br><span class="line">        self.layer4 = self._make_layer(block, <span class="number">512</span>, layers[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 19,19,2048 -&gt; 2,2,2048</span></span><br><span class="line">        self.avgpool = nn.AvgPool2d(<span class="number">7</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span> * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                n = m.kernel_size[<span class="number">0</span>] * m.kernel_size[<span class="number">1</span>] * m.out_channels</span><br><span class="line">                m.weight.data.normal_(<span class="number">0</span>, math.sqrt(<span class="number">2.</span> / n))</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line">                </span><br><span class="line">	<span class="comment"># 定义每个层的操作，比如在resnet50里conv2_x重复了三次，那么就是block=Bottleck,blocks=3</span></span><br><span class="line">    <span class="comment"># 其实conv3_x,conv4_x,conv5_x中block都是Bottlenecck,就是重复的次数不一样</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, planes, blocks, stride=<span class="number">1</span></span>):</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="comment">#-------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment"># 当模型需要进行高和宽的压缩的时候，或者通道不为原来的4倍，就需要用到残差边的downsample</span></span><br><span class="line">        <span class="comment"># 个人认为这里通道变化不大（相比于2倍上层特征图通道数）也需要downsample是因为通道变化不</span></span><br><span class="line">        <span class="comment"># 大，信息不够细致</span></span><br><span class="line">        <span class="comment">#-------------------------------------------------------------------#</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.inplanes != planes * block.expansion:</span><br><span class="line">            <span class="comment"># (N, C0, H, W)-&gt;(N, 4*C1, H, W)</span></span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.inplanes, planes * block.expansion,kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(planes * block.expansion),</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># layers记录块结果</span></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.inplanes, planes, stride, downsample))</span><br><span class="line">        self.inplanes = planes * block.expansion</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, blocks):</span><br><span class="line">            layers.append(block(self.inplanes, planes))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># 接上面假设，此时x=(N, 2, 2, 2048)</span></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        <span class="comment"># (N, 2, 2, 2048)-&gt;(N, 4096)</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (N,4096)-&gt;(N, 1000)</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50</span>(<span class="params">pretrained = <span class="literal">False</span></span>):</span><br><span class="line">    model = ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        state_dict = load_state_dict_from_url(<span class="string">"https://download.pytorch.org/models/resnet50-19c8e357.pth"</span>, model_dir=<span class="string">"./model_data"</span>)</span><br><span class="line">        model.load_state_dict(state_dict)</span><br><span class="line">    <span class="comment">#----------------------------------------------------------------------------#</span></span><br><span class="line">    <span class="comment">#   获取特征提取部分，从conv1到model.layer3，最终获得一个38,38,1024的特征层</span></span><br><span class="line">    <span class="comment">#----------------------------------------------------------------------------#</span></span><br><span class="line">    features    = <span class="built_in">list</span>([model.conv1, model.bn1, model.relu, model.maxpool, model.layer1, model.layer2, model.layer3])</span><br><span class="line">    <span class="comment">#----------------------------------------------------------------------------#</span></span><br><span class="line">    <span class="comment">#   获取分类部分，从model.layer4到model.avgpool</span></span><br><span class="line">    <span class="comment">#----------------------------------------------------------------------------#</span></span><br><span class="line">    classifier  = <span class="built_in">list</span>([model.layer4, model.avgpool])</span><br><span class="line">    </span><br><span class="line">    features    = nn.Sequential(*features)</span><br><span class="line">    classifier  = nn.Sequential(*classifier)</span><br><span class="line">    <span class="keyword">return</span> features, classifier</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>resnet</tag>
      </tags>
  </entry>
  <entry>
    <title>RetinaNet论文笔记</title>
    <url>/zh-CN/RetinaNet/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文地址：<a href="https://arxiv.org/pdf/1708.02002.pdf">Focal loss for dense object detection</a></p>
<p>源码地址：<a href="https://github.com/facebookresearch/Detectron">RetinaNet</a></p>
<p>文章引用代码地址：<a href="https://github.com/bubbliiiing/retinanet-pytorch">https://github.com/bubbliiiing/retinanet-pytorch</a></p>
<p>文章出处：<a href="https://blog.csdn.net/weixin_44791964/article/details/108319189">https://blog.csdn.net/weixin_44791964/article/details/108319189</a></p>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>Retinanet是在何凯明大神提出Focal loss同时提出的一种新的目标检测方案，来验证Focal Loss的有效性。</p>
<p>One-Stage目标检测方法常常使用先验框提高预测性能，一张图像可能生成成千上万的候选框，但是其中只有很少一部分是包含目标的的，有目标的就是正样本，没有目标的就是负样本。这种情况造成了One-Stage目标检测方法的正负样本不平衡，也使得One-Stage目标检测方法的检测效果比不上Two-Stage目标检测方法。</p>
<p>Focal Loss是一种新的用于平衡One-Stage目标检测方法正负样本的Loss方案。</p>
<p>Retinane的结构非常简单，但是其存在非常多的先验框，以输入600x600x3的图片为例，就存在着67995个先验框，这些先验框里面大多包含的是背景，存在非常多的负样本。以Focal Loss训练的Retinanet可以有效的平衡正负样本，实现有效的训练。</p>
<h3 id="预测部分"><a href="#预测部分" class="headerlink" title="预测部分"></a>预测部分</h3><h4 id="主干网络"><a href="#主干网络" class="headerlink" title="主干网络"></a>主干网络</h4><p><style>.ljbakvwmofmj{}</style><img src="/zh-CN/RetinaNet/RetinaNet/image-20220418210603737.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/image-20220418210603737.png" srcset="data:image/png;base64,666" class="ljbakvwmofmj lazyload"></p>
<p>假设输入的图片大小为600x600x3。</p>
<p>ResNet50有两个基本的块，分别名为Conv Block和Identity Block，其中Conv Block输入和输出的维度是不一样的，所以不能连续串联，它的作用是改变网络的维度；Identity Block输入维度和输出维度相同，可以串联，用于加深网络的。</p>
<p><style>.hkbszhwevqdc{}</style><img src="/zh-CN/RetinaNet/RetinaNet/image-20220419095619530.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/image-20220419095619530.png" srcset="data:image/png;base64,666" class="hkbszhwevqdc lazyload"></p>
<p>当输入的图片为600x600x3的时候，shape变化与总的网络结构如下：</p>
<p><style>.ntfulxbnutci{zoom:50%;}</style><img src="/zh-CN/RetinaNet/RetinaNet/20200215151533258.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/20200215151533258.png" srcset="data:image/png;base64,666" class="ntfulxbnutci lazyload" alt="img"></p>
<p>我们取出长宽压缩了三次、四次、五次的结果来进行网络金字塔结构的构造</p>
<p>实现代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.utils.model_zoo <span class="keyword">as</span> model_zoo</span><br><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model_urls = {</span><br><span class="line"><span class="string">'resnet18'</span>: <span class="string">'https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth'</span>,</span><br><span class="line"><span class="string">'resnet34'</span>: <span class="string">'https://s3.amazonaws.com/pytorch/models/resnet34-333f7ec4.pth'</span>,</span><br><span class="line"><span class="string">'resnet50'</span>: <span class="string">'https://s3.amazonaws.com/pytorch/models/resnet50-19c8e357.pth'</span>,</span><br><span class="line"><span class="string">'resnet101'</span>: <span class="string">'https://s3.amazonaws.com/pytorch/models/resnet101-5d3b4d8f.pth'</span>,</span><br><span class="line"><span class="string">'resnet152'</span>: <span class="string">'https://s3.amazonaws.com/pytorch/models/resnet152-b121ed2d.pth'</span>,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv3x3</span>(<span class="params">in_planes, out_planes, stride=<span class="number">1</span>, groups=<span class="number">1</span>, dilation=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">"""3x3 convolution with padding"""</span></span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(in_planes, out_planes, kernel_size=<span class="number">3</span>, stride=stride,</span><br><span class="line">                     padding=dilation, groups=groups, bias=<span class="literal">False</span>, dilation=dilation)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv1x1</span>(<span class="params">in_planes, out_planes, stride=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">"""1x1 convolution"""</span></span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(in_planes, out_planes, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>, groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 base_width=<span class="number">64</span>, dilation=<span class="number">1</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        <span class="keyword">if</span> groups != <span class="number">1</span> <span class="keyword">or</span> base_width != <span class="number">64</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'BasicBlock only supports groups=1 and base_width=64'</span>)</span><br><span class="line">        <span class="keyword">if</span> dilation &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(<span class="string">"Dilation &gt; 1 not supported in BasicBlock"</span>)</span><br><span class="line">        <span class="comment"># Both self.conv1 and self.downsample layers downsample the input when stride != 1</span></span><br><span class="line">        self.conv1 = conv3x3(inplanes, planes, stride)</span><br><span class="line">        self.bn1 = norm_layer(planes)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = conv3x3(planes, planes)</span><br><span class="line">        self.bn2 = norm_layer(planes)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>, groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 base_width=<span class="number">64</span>, dilation=<span class="number">1</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        width = <span class="built_in">int</span>(planes * (base_width / <span class="number">64.</span>)) * groups</span><br><span class="line">        <span class="comment"># Both self.conv2 and self.downsample layers downsample the input when stride != 1</span></span><br><span class="line">        self.conv1 = conv1x1(inplanes, width)</span><br><span class="line">        self.bn1 = norm_layer(width)</span><br><span class="line">        self.conv2 = conv3x3(width, width, stride, groups, dilation)</span><br><span class="line">        self.bn2 = norm_layer(width)</span><br><span class="line">        self.conv3 = conv1x1(width, planes * self.expansion)</span><br><span class="line">        self.bn3 = norm_layer(planes * self.expansion)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block, layers, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        self.inplanes = <span class="number">64</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>,</span><br><span class="line">                    bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, ceil_mode=<span class="literal">True</span>) <span class="comment"># change</span></span><br><span class="line">        self.layer1 = self._make_layer(block, <span class="number">64</span>, layers[<span class="number">0</span>])</span><br><span class="line">        self.layer2 = self._make_layer(block, <span class="number">128</span>, layers[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer(block, <span class="number">256</span>, layers[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer(block, <span class="number">512</span>, layers[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        self.avgpool = nn.AvgPool2d(<span class="number">7</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span> * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                n = m.kernel_size[<span class="number">0</span>] * m.kernel_size[<span class="number">1</span>] * m.out_channels</span><br><span class="line">                m.weight.data.normal_(<span class="number">0</span>, math.sqrt(<span class="number">2.</span> / n))</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, planes, blocks, stride=<span class="number">1</span></span>):</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.inplanes != planes * block.expansion:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.inplanes, planes * block.expansion,</span><br><span class="line">                    kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(planes * block.expansion),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.inplanes, planes, stride, downsample))</span><br><span class="line">        self.inplanes = planes * block.expansion</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, blocks):</span><br><span class="line">            layers.append(block(self.inplanes, planes))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet18</span>(<span class="params">pretrained=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">    <span class="string">"""Constructs a ResNet-18 model.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    model = ResNet(BasicBlock, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], **kwargs)</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        model.load_state_dict(model_zoo.load_url(model_urls[<span class="string">'resnet18'</span>], model_dir=<span class="string">'model_data'</span>), strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet34</span>(<span class="params">pretrained=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">    <span class="string">"""Constructs a ResNet-34 model.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    model = ResNet(BasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], **kwargs)</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        model.load_state_dict(model_zoo.load_url(model_urls[<span class="string">'resnet34'</span>], model_dir=<span class="string">'model_data'</span>), strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50</span>(<span class="params">pretrained=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">    <span class="string">"""Constructs a ResNet-50 model.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    model = ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], **kwargs)</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        model.load_state_dict(model_zoo.load_url(model_urls[<span class="string">'resnet50'</span>], model_dir=<span class="string">'model_data'</span>), strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet101</span>(<span class="params">pretrained=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">    <span class="string">"""Constructs a ResNet-101 model.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    model = ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], **kwargs)</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        model.load_state_dict(model_zoo.load_url(model_urls[<span class="string">'resnet101'</span>], model_dir=<span class="string">'model_data'</span>), strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet152</span>(<span class="params">pretrained=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">    <span class="string">"""Constructs a ResNet-152 model.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    model = ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">3</span>], **kwargs)</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        model.load_state_dict(model_zoo.load_url(model_urls[<span class="string">'resnet152'</span>], model_dir=<span class="string">'model_data'</span>), strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></tbody></table></figure>
<h4 id="从特征获取预测结果"><a href="#从特征获取预测结果" class="headerlink" title="从特征获取预测结果"></a>从特征获取预测结果</h4><p><style>.youauosyinuk{}</style><img src="/zh-CN/RetinaNet/RetinaNet/image-20220419100322079.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/image-20220419100322079.png" srcset="data:image/png;base64,666" class="youauosyinuk lazyload"></p>
<p>由抽象的结构图可知，获得到的特征还需要经过图像金字塔的处理，这样的结构可以融合多尺度的特征，实现更有效的预测。</p>
<p>图像金字塔的具体结构如下：</p>
<p><style>.ocbtclcrdejx{}</style><img src="/zh-CN/RetinaNet/RetinaNet/2020021613520193.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/2020021613520193.png" srcset="data:image/png;base64,666" class="ocbtclcrdejx lazyload"></p>
<p>通过图像金字塔我们可以获得五个有效的特征层，分别是P3、P4、P5、P6、P7，<br>为了和普通特征层区分，我们称之为有效特征层，将这五个有效的特征层传输过class+box subnets就可以获得预测结果了。</p>
<p>class subnet采用4次256通道的卷积和1次num_anchors x num_classes的卷积，num_anchors指的是该特征层所拥有的先验框数量，num_classes指的是网络一共对多少类的目标进行检测。</p>
<p>box subnet采用4次256通道的卷积和1次num_anchors x 4的卷积，num_anchors指的是该特征层所拥有的先验框数量，4指的是先验框的调整情况。</p>
<p>需要注意的是，每个特征层所用的class subnet是同一个class subnet；每个特征层所用的box subnet是同一个box subnet。</p>
<p><style>.rywvxsshmcjd{}</style><img src="/zh-CN/RetinaNet/RetinaNet/image-20220419100832971.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/image-20220419100832971.png" srcset="data:image/png;base64,666" class="rywvxsshmcjd lazyload"></p>
<p>其中：<br>1.num_anchors x 4的卷积 用于预测 该特征层上 每一个网格点上 每一个先验框的变化情况。（为什么说是变化情况呢，这是因为ssd的预测结果需要结合先验框获得预测框，预测结果就是先验框的变化情况。）</p>
<p>2.num_anchors x num_classes的卷积 用于预测 该特征层上 每一个网格点上 每一个预测框对应的种类。<br>实现代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F  </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> nets.resnet <span class="keyword">import</span> resnet18,resnet34,resnet50,resnet101,resnet152</span><br><span class="line"><span class="keyword">from</span> utils.anchors <span class="keyword">import</span> Anchors</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PyramidFeatures</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, C3_size, C4_size, C5_size, feature_size=<span class="number">256</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PyramidFeatures, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.P7_1 = nn.ReLU()</span><br><span class="line">        self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        C3, C4, C5 = inputs</span><br><span class="line">        _, _, h4, w4 = C4.size()</span><br><span class="line">        _, _, h3, w3 = C3.size()</span><br><span class="line"></span><br><span class="line">        P5_x = self.P5_1(C5)</span><br><span class="line">        P5_upsampled_x = F.interpolate(P5_x, size=(h4, w4))</span><br><span class="line">        P5_x = self.P5_2(P5_x)</span><br><span class="line"></span><br><span class="line">        P4_x = self.P4_1(C4)</span><br><span class="line">        P4_x = P5_upsampled_x + P4_x</span><br><span class="line">        P4_upsampled_x = F.interpolate(P4_x, size=(h3, w3))</span><br><span class="line">        P4_x = self.P4_2(P4_x)</span><br><span class="line"></span><br><span class="line">        P3_x = self.P3_1(C3)</span><br><span class="line">        P3_x = P3_x + P4_upsampled_x</span><br><span class="line">        P3_x = self.P3_2(P3_x)</span><br><span class="line"></span><br><span class="line">        P6_x = self.P6(C5)</span><br><span class="line"></span><br><span class="line">        P7_x = self.P7_1(P6_x)</span><br><span class="line">        P7_x = self.P7_2(P7_x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [P3_x, P4_x, P5_x, P6_x, P7_x]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RegressionModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features_in, num_anchors=<span class="number">9</span>, feature_size=<span class="number">256</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(RegressionModel, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act1 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act2 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act3 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act4 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.output = nn.Conv2d(feature_size, num_anchors * <span class="number">4</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.act1(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.act2(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.act3(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv4(out)</span><br><span class="line">        out = self.act4(out)</span><br><span class="line"></span><br><span class="line">        out = self.output(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># out is B x C x W x H, with C = 4*num_anchors</span></span><br><span class="line">        out = out.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out.contiguous().view(out.shape[<span class="number">0</span>], -<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ClassificationModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features_in, num_anchors=<span class="number">9</span>, num_classes=<span class="number">80</span>, anchor=<span class="number">0.01</span>, feature_size=<span class="number">256</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ClassificationModel, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.num_anchors = num_anchors</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act1 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act2 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act3 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act4 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.output_act = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.act1(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.act2(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.act3(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv4(out)</span><br><span class="line">        out = self.act4(out)</span><br><span class="line"></span><br><span class="line">        out = self.output(out)</span><br><span class="line">        out = self.output_act(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># out is B x C x W x H, with C = n_classes + n_anchors</span></span><br><span class="line">        out1 = out.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        batch_size, width, height, channels = out1.shape</span><br><span class="line"></span><br><span class="line">        out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out2.contiguous().view(x.shape[<span class="number">0</span>], -<span class="number">1</span>, self.num_classes)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Resnet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, phi, load_weights=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Resnet, self).__init__()</span><br><span class="line">        self.edition = [resnet18,resnet34,resnet50,resnet101,resnet152]</span><br><span class="line">        model = self.edition[phi](load_weights)</span><br><span class="line">        <span class="keyword">del</span> model.avgpool</span><br><span class="line">        <span class="keyword">del</span> model.fc</span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model.conv1(x)</span><br><span class="line">        x = self.model.bn1(x)</span><br><span class="line">        x = self.model.relu(x)</span><br><span class="line">        x = self.model.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.model.layer1(x)</span><br><span class="line">        feat1 = self.model.layer2(x)</span><br><span class="line">        feat2 = self.model.layer3(feat1)</span><br><span class="line">        feat3 = self.model.layer4(feat2)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [feat1,feat2,feat3]</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Retinanet</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, phi, pretrain_weights=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Retinanet, self).__init__()</span><br><span class="line">        self.pretrain_weights = pretrain_weights</span><br><span class="line">        self.backbone_net = Resnet(phi,pretrain_weights)</span><br><span class="line">        fpn_sizes = {</span><br><span class="line">            <span class="number">0</span>: [<span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>],</span><br><span class="line">            <span class="number">1</span>: [<span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>],</span><br><span class="line">            <span class="number">2</span>: [<span class="number">512</span>, <span class="number">1024</span>, <span class="number">2048</span>],</span><br><span class="line">            <span class="number">3</span>: [<span class="number">512</span>, <span class="number">1024</span>, <span class="number">2048</span>],</span><br><span class="line">            <span class="number">4</span>: [<span class="number">512</span>, <span class="number">1024</span>, <span class="number">2048</span>],</span><br><span class="line">        }[phi]</span><br><span class="line"></span><br><span class="line">        self.fpn = PyramidFeatures(fpn_sizes[<span class="number">0</span>], fpn_sizes[<span class="number">1</span>], fpn_sizes[<span class="number">2</span>])</span><br><span class="line">        self.regressionModel = RegressionModel(<span class="number">256</span>)</span><br><span class="line">        self.classificationModel = ClassificationModel(<span class="number">256</span>, num_classes=num_classes)</span><br><span class="line">        self.anchors = Anchors()</span><br><span class="line">        self._init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.pretrain_weights:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"_init_weights"</span>)</span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                    n = m.kernel_size[<span class="number">0</span>] * m.kernel_size[<span class="number">1</span>] * m.out_channels</span><br><span class="line">                    m.weight.data.normal_(<span class="number">0</span>, math.sqrt(<span class="number">2.</span> / n))</span><br><span class="line">                <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                    m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                    m.bias.data.zero_()</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"_init_classificationModel"</span>)</span><br><span class="line">        anchor = <span class="number">0.01</span></span><br><span class="line">        self.classificationModel.output.weight.data.fill_(<span class="number">0</span>)</span><br><span class="line">        self.classificationModel.output.bias.data.fill_(-math.log((<span class="number">1.0</span> - anchor) / anchor))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"_init_regressionModel"</span>)</span><br><span class="line">        self.regressionModel.output.weight.data.fill_(<span class="number">0</span>)</span><br><span class="line">        self.regressionModel.output.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line"></span><br><span class="line">        p3, p4, p5 = self.backbone_net(inputs)</span><br><span class="line"></span><br><span class="line">        features = self.fpn([p3, p4, p5])</span><br><span class="line"></span><br><span class="line">        regression = torch.cat([self.regressionModel(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> features], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        classification = torch.cat([self.classificationModel(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> features], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        anchors = self.anchors(features)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> features, regression, classification, anchors</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="预测结果的解码"><a href="#预测结果的解码" class="headerlink" title="预测结果的解码"></a>预测结果的解码</h4><p>我们通过对每一个特征层的处理，可以获得三个内容，分别是：</p>
<p>num_anchors x 4的卷积 用于预测 该特征层上 每一个网格点上 每一个先验框的变化情况。</p>
<p>num_anchors x num_classes的卷积 用于预测 该特征层上 每一个网格点上 每一个预测框对应的种类。</p>
<p>每一个有效特征层对应的先验框对应着该特征层上 每一个网格点上 预先设定好的9个框。</p>
<p>我们利用 num_anchors x 4的卷积 与 每一个有效特征层对应的先验框 获得框的真实位置。</p>
<p>每一个有效特征层对应的先验框就是，如图所示的作用：<br>每一个有效特征层将整个图片分成与其长宽对应的网格，如P3的特征层就是将整个图像分成75x75个网格；然后从每个网格中心建立9个先验框，一共75x75x9个，50625个先验框</p>
<p><style>.nidgndjlqlix{}</style><img src="/zh-CN/RetinaNet/RetinaNet/20200129210050147.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/20200129210050147.png" srcset="data:image/png;base64,666" class="nidgndjlqlix lazyload"></p>
<p>先验框虽然可以代表一定的框的位置信息与框的大小信息，但是其是有限的，无法表示任意情况，因此还需要调整，Retinanet利用4次256通道的卷积+num_anchors x 4的卷积的结果对先验框进行调整。</p>
<p>num_anchors x 4中的num_anchors表示了这个网格点所包含的先验框数量，其中的4表示了框的左上角xy轴，右下角xy的调整情况。</p>
<p>Retinanet解码过程就是将对应的先验框的左上角和右下角进行位置的调整，调整完的结果就是预测框的位置了。</p>
<p>当然得到最终的预测结构后还要进行得分排序与非极大抑制筛选这一部分基本上是所有目标检测通用的部分。<br>1、取出每一类得分大于confidence_threshold的框和得分。<br>2、利用框的位置和得分进行非极大抑制。<br>实现代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decodebox</span>(<span class="params">regression, anchors, img</span>):</span><br><span class="line">    dtype = regression.dtype</span><br><span class="line">    anchors = anchors.to(dtype)</span><br><span class="line">    y_centers_a = (anchors[..., <span class="number">0</span>] + anchors[..., <span class="number">2</span>]) / <span class="number">2</span></span><br><span class="line">    x_centers_a = (anchors[..., <span class="number">1</span>] + anchors[..., <span class="number">3</span>]) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    ha = anchors[..., <span class="number">2</span>] - anchors[..., <span class="number">0</span>]</span><br><span class="line">    wa = anchors[..., <span class="number">3</span>] - anchors[..., <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    w = regression[..., <span class="number">3</span>].exp() * wa</span><br><span class="line">    h = regression[..., <span class="number">2</span>].exp() * ha</span><br><span class="line"></span><br><span class="line">    y_centers = regression[..., <span class="number">0</span>] * ha + y_centers_a</span><br><span class="line">    x_centers = regression[..., <span class="number">1</span>] * wa + x_centers_a</span><br><span class="line"></span><br><span class="line">    ymin = y_centers - h / <span class="number">2.</span></span><br><span class="line">    xmin = x_centers - w / <span class="number">2.</span></span><br><span class="line">    ymax = y_centers + h / <span class="number">2.</span></span><br><span class="line">    xmax = x_centers + w / <span class="number">2.</span></span><br><span class="line"></span><br><span class="line">    boxes = torch.stack([xmin, ymin, xmax, ymax], dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    _, _, height, width = np.shape(img)</span><br><span class="line"></span><br><span class="line">    boxes[:, :, <span class="number">0</span>] = torch.clamp(boxes[:, :, <span class="number">0</span>], <span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">    boxes[:, :, <span class="number">1</span>] = torch.clamp(boxes[:, :, <span class="number">1</span>], <span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    boxes[:, :, <span class="number">2</span>] = torch.clamp(boxes[:, :, <span class="number">2</span>], <span class="built_in">max</span>=width - <span class="number">1</span>)</span><br><span class="line">    boxes[:, :, <span class="number">3</span>] = torch.clamp(boxes[:, :, <span class="number">3</span>], <span class="built_in">max</span>=height - <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># fig = plt.figure()</span></span><br><span class="line">    <span class="comment"># ax = fig.add_subplot(121)</span></span><br><span class="line">    <span class="comment"># grid_x = x_centers_a[0,-4*4*9:]</span></span><br><span class="line">    <span class="comment"># grid_y = y_centers_a[0,-4*4*9:]</span></span><br><span class="line">    <span class="comment"># plt.ylim(-600,1200)</span></span><br><span class="line">    <span class="comment"># plt.xlim(-600,1200)</span></span><br><span class="line">    <span class="comment"># plt.gca().invert_yaxis()</span></span><br><span class="line">    <span class="comment"># plt.scatter(grid_x.cpu(),grid_y.cpu())</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># anchor_left = anchors[0,-4*4*9:,1]</span></span><br><span class="line">    <span class="comment"># anchor_top = anchors[0,-4*4*9:,0]</span></span><br><span class="line">    <span class="comment"># anchor_w = wa[0,-4*4*9:]</span></span><br><span class="line">    <span class="comment"># anchor_h = ha[0,-4*4*9:]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># for i in range(9,18):</span></span><br><span class="line">    <span class="comment">#     rect1 = plt.Rectangle([anchor_left[i],anchor_top[i]],anchor_w[i],anchor_h[i],color="r",fill=False)</span></span><br><span class="line">    <span class="comment">#     ax.add_patch(rect1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ax = fig.add_subplot(122)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># grid_x = x_centers_a[0,-4*4*9:]</span></span><br><span class="line">    <span class="comment"># grid_y = y_centers_a[0,-4*4*9:]</span></span><br><span class="line">    <span class="comment"># plt.scatter(grid_x.cpu(),grid_y.cpu())</span></span><br><span class="line">    <span class="comment"># plt.ylim(-600,1200)</span></span><br><span class="line">    <span class="comment"># plt.xlim(-600,1200)</span></span><br><span class="line">    <span class="comment"># plt.gca().invert_yaxis()</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># y_centers = y_centers[0,-4*4*9:]</span></span><br><span class="line">    <span class="comment"># x_centers = x_centers[0,-4*4*9:]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># pre_left = xmin[0,-4*4*9:]</span></span><br><span class="line">    <span class="comment"># pre_top = ymin[0,-4*4*9:]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># pre_w = xmax[0,-4*4*9:]-xmin[0,-4*4*9:]</span></span><br><span class="line">    <span class="comment"># pre_h = ymax[0,-4*4*9:]-ymin[0,-4*4*9:]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># for i in range(9,18):</span></span><br><span class="line">    <span class="comment">#     plt.scatter(x_centers[i].cpu(),y_centers[i].cpu(),c='r')</span></span><br><span class="line">    <span class="comment">#     rect1 = plt.Rectangle([pre_left[i],pre_top[i]],pre_w[i],pre_h[i],color="r",fill=False)</span></span><br><span class="line">    <span class="comment">#     ax.add_patch(rect1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line">    <span class="keyword">return</span> boxes</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">retinanet_correct_boxes</span>(<span class="params">box_xy, box_wh, input_shape, image_shape, letterbox_image</span>):</span><br><span class="line">    <span class="comment">#-----------------------------------------------------------------#</span></span><br><span class="line">    <span class="comment">#   把y轴放前面是因为方便预测框和图像的宽高进行相乘</span></span><br><span class="line">    <span class="comment">#-----------------------------------------------------------------#</span></span><br><span class="line">    box_yx = box_xy[..., ::-<span class="number">1</span>]</span><br><span class="line">    box_hw = box_wh[..., ::-<span class="number">1</span>]</span><br><span class="line">    input_shape = np.array(input_shape)</span><br><span class="line">    image_shape = np.array(image_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> letterbox_image:</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   这里求出来的offset是图像有效区域相对于图像左上角的偏移情况</span></span><br><span class="line">        <span class="comment">#   new_shape指的是宽高缩放情况</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------#</span></span><br><span class="line">        new_shape = np.<span class="built_in">round</span>(image_shape * np.<span class="built_in">min</span>(input_shape/image_shape))</span><br><span class="line">        offset  = (input_shape - new_shape)/<span class="number">2.</span>/input_shape</span><br><span class="line">        scale   = input_shape/new_shape</span><br><span class="line"></span><br><span class="line">        box_yx  = (box_yx - offset) * scale</span><br><span class="line">        box_hw *= scale</span><br><span class="line"></span><br><span class="line">    box_mins    = box_yx - (box_hw / <span class="number">2.</span>)</span><br><span class="line">    box_maxes   = box_yx + (box_hw / <span class="number">2.</span>)</span><br><span class="line">    boxes  = np.concatenate([box_mins[..., <span class="number">0</span>:<span class="number">1</span>], box_mins[..., <span class="number">1</span>:<span class="number">2</span>], box_maxes[..., <span class="number">0</span>:<span class="number">1</span>], box_maxes[..., <span class="number">1</span>:<span class="number">2</span>]], axis=-<span class="number">1</span>)</span><br><span class="line">    boxes *= np.concatenate([image_shape, image_shape], axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> boxes</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">non_max_suppression</span>(<span class="params">prediction, input_shape, image_shape, letterbox_image, conf_thres=<span class="number">0.5</span>, nms_thres=<span class="number">0.4</span></span>):</span><br><span class="line">    output = [<span class="literal">None</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prediction))]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">    <span class="comment">#   预测只用一张图片，只会进行一次</span></span><br><span class="line">    <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">    <span class="keyword">for</span> i, image_pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(prediction):</span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   对种类预测部分取max。</span></span><br><span class="line">        <span class="comment">#   class_conf  [num_anchors, 1]    种类置信度</span></span><br><span class="line">        <span class="comment">#   class_pred  [num_anchors, 1]    种类</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        class_conf, class_pred = torch.<span class="built_in">max</span>(image_pred[:, <span class="number">4</span>:], <span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   利用置信度进行第一轮筛选</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        conf_mask = (class_conf[:, <span class="number">0</span>] &gt;= conf_thres).squeeze()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   根据置信度进行预测结果的筛选</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        image_pred = image_pred[conf_mask]</span><br><span class="line">        class_conf = class_conf[conf_mask]</span><br><span class="line">        class_pred = class_pred[conf_mask]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_pred.size(<span class="number">0</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment">#-------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   detections  [num_anchors, 6]</span></span><br><span class="line">        <span class="comment">#   6的内容为：x1, y1, x2, y2, class_conf, class_pred</span></span><br><span class="line">        <span class="comment">#-------------------------------------------------------------------------#</span></span><br><span class="line">        detections = torch.cat((image_pred[:, :<span class="number">4</span>], class_conf.<span class="built_in">float</span>(), class_pred.<span class="built_in">float</span>()), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   获得预测结果中包含的所有种类</span></span><br><span class="line">        <span class="comment">#------------------------------------------#</span></span><br><span class="line">        unique_labels = detections[:, -<span class="number">1</span>].cpu().unique()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> prediction.is_cuda:</span><br><span class="line">            unique_labels = unique_labels.cuda()</span><br><span class="line">            detections = detections.cuda()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> unique_labels:</span><br><span class="line">            <span class="comment">#------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   获得某一类得分筛选后全部的预测结果</span></span><br><span class="line">            <span class="comment">#------------------------------------------#</span></span><br><span class="line">            detections_class = detections[detections[:, -<span class="number">1</span>] == c]</span><br><span class="line"></span><br><span class="line">            <span class="comment">#------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   使用官方自带的非极大抑制会速度更快一些！</span></span><br><span class="line">            <span class="comment">#------------------------------------------#</span></span><br><span class="line">            keep = nms(</span><br><span class="line">                detections_class[:, :<span class="number">4</span>],</span><br><span class="line">                detections_class[:, <span class="number">4</span>],</span><br><span class="line">                nms_thres</span><br><span class="line">            )</span><br><span class="line">            max_detections = detections_class[keep]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># #------------------------------------------#</span></span><br><span class="line">            <span class="comment"># #   按照存在物体的置信度排序</span></span><br><span class="line">            <span class="comment"># #------------------------------------------#</span></span><br><span class="line">            <span class="comment"># _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)</span></span><br><span class="line">            <span class="comment"># detections_class = detections_class[conf_sort_index]</span></span><br><span class="line">            <span class="comment"># #------------------------------------------#</span></span><br><span class="line">            <span class="comment"># #   进行非极大抑制</span></span><br><span class="line">            <span class="comment"># #------------------------------------------#</span></span><br><span class="line">            <span class="comment"># max_detections = []</span></span><br><span class="line">            <span class="comment"># while detections_class.size(0):</span></span><br><span class="line">            <span class="comment">#     #---------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#     #   取出这一类置信度最高的，一步一步往下判断。</span></span><br><span class="line">            <span class="comment">#     #   判断重合程度是否大于nms_thres，如果是则去除掉</span></span><br><span class="line">            <span class="comment">#     #---------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#     max_detections.append(detections_class[0].unsqueeze(0))</span></span><br><span class="line">            <span class="comment">#     if len(detections_class) == 1:</span></span><br><span class="line">            <span class="comment">#         break</span></span><br><span class="line">            <span class="comment">#     ious = bbox_iou(max_detections[-1], detections_class[1:])</span></span><br><span class="line">            <span class="comment">#     detections_class = detections_class[1:][ious &lt; nms_thres]</span></span><br><span class="line">            <span class="comment"># #------------------------------------------#</span></span><br><span class="line">            <span class="comment"># #   堆叠</span></span><br><span class="line">            <span class="comment"># #------------------------------------------#</span></span><br><span class="line">            <span class="comment"># max_detections = torch.cat(max_detections).data</span></span><br><span class="line">            </span><br><span class="line">            output[i] = max_detections <span class="keyword">if</span> output[i] <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> torch.cat((output[i], max_detections))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output[i] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output[i]           = output[i].cpu().numpy()</span><br><span class="line">            box_xy, box_wh      = (output[i][:, <span class="number">0</span>:<span class="number">2</span>] + output[i][:, <span class="number">2</span>:<span class="number">4</span>])/<span class="number">2</span>, output[i][:, <span class="number">2</span>:<span class="number">4</span>] - output[i][:, <span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">            output[i][:, :<span class="number">4</span>]    = retinanet_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image)</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="原图上进行绘制"><a href="#原图上进行绘制" class="headerlink" title="原图上进行绘制"></a>原图上进行绘制</h4><p>通过第三步，我们可以获得预测框在原图上的位置，而且这些预测框都是经过筛选的。这些筛选后的框可以直接绘制在图片上，就可以获得结果了。</p>
<h3 id="训练部分"><a href="#训练部分" class="headerlink" title="训练部分"></a>训练部分</h3><h4 id="真实框的处理"><a href="#真实框的处理" class="headerlink" title="真实框的处理"></a>真实框的处理</h4><p>从预测部分我们知道，每个特征层的预测结果，num_anchors x 4的卷积 用于预测 该特征层上 每一个网格点上 每一个先验框的变化情况。</p>
<p>也就是说，我们直接利用retinanet网络预测到的结果，并不是预测框在图片上的真实位置，需要解码才能得到真实位置。</p>
<p>而在训练的时候，我们需要计算loss函数，这个loss函数是相对于Retinanet网络的预测结果的。我们需要把图片输入到当前的Retinanet网络中，得到预测结果；同时还需要把真实框的信息，进行编码，这个编码是把真实框的位置信息格式转化为Retinanet预测结果的格式信息。</p>
<p>也就是，我们需要找到 每一张用于训练的图片的每一个真实框对应的先验框，并求出如果想要得到这样一个真实框，我们的预测结果应该是怎么样的。</p>
<p>从预测结果获得真实框的过程被称作解码，而从真实框获得预测结果的过程就是编码的过程。</p>
<p>因此我们只需要将解码过程逆过来就是编码过程了。</p>
<p>在进行编码的时候，我们需要找到每一个真实框对应的先验框，我们把和真实框重合程度在0.5以上的作为正样本，在0.4以下的作为负样本，在0.4和0.5之间的作为忽略样本。<br>实现代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_target</span>(<span class="params">anchor, bbox_annotation, classification, cuda</span>):</span><br><span class="line">    IoU = calc_iou(anchor[:, :], bbox_annotation[:, :<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">    IoU_max, IoU_argmax = torch.<span class="built_in">max</span>(IoU, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the loss for classification</span></span><br><span class="line">    targets = torch.ones_like(classification) * -<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> cuda:</span><br><span class="line">        targets = targets.cuda()</span><br><span class="line"></span><br><span class="line">    targets[torch.lt(IoU_max, <span class="number">0.4</span>), :] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    positive_indices = torch.ge(IoU_max, <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    num_positive_anchors = positive_indices.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    assigned_annotations = bbox_annotation[IoU_argmax, :]</span><br><span class="line"></span><br><span class="line">    targets[positive_indices, :] = <span class="number">0</span></span><br><span class="line">    targets[positive_indices, assigned_annotations[positive_indices, <span class="number">4</span>].long()] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> targets, num_positive_anchors, positive_indices, assigned_annotations</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode_bbox</span>(<span class="params">assigned_annotations, positive_indices, anchor_widths, anchor_heights, anchor_ctr_x, anchor_ctr_y</span>):</span><br><span class="line">    assigned_annotations = assigned_annotations[positive_indices, :]</span><br><span class="line"></span><br><span class="line">    anchor_widths_pi = anchor_widths[positive_indices]</span><br><span class="line">    anchor_heights_pi = anchor_heights[positive_indices]</span><br><span class="line">    anchor_ctr_x_pi = anchor_ctr_x[positive_indices]</span><br><span class="line">    anchor_ctr_y_pi = anchor_ctr_y[positive_indices]</span><br><span class="line"></span><br><span class="line">    gt_widths = assigned_annotations[:, <span class="number">2</span>] - assigned_annotations[:, <span class="number">0</span>]</span><br><span class="line">    gt_heights = assigned_annotations[:, <span class="number">3</span>] - assigned_annotations[:, <span class="number">1</span>]</span><br><span class="line">    gt_ctr_x = assigned_annotations[:, <span class="number">0</span>] + <span class="number">0.5</span> * gt_widths</span><br><span class="line">    gt_ctr_y = assigned_annotations[:, <span class="number">1</span>] + <span class="number">0.5</span> * gt_heights</span><br><span class="line"></span><br><span class="line">    <span class="comment"># efficientdet style</span></span><br><span class="line">    gt_widths = torch.clamp(gt_widths, <span class="built_in">min</span>=<span class="number">1</span>)</span><br><span class="line">    gt_heights = torch.clamp(gt_heights, <span class="built_in">min</span>=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi</span><br><span class="line">    targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi</span><br><span class="line">    targets_dw = torch.log(gt_widths / anchor_widths_pi)</span><br><span class="line">    targets_dh = torch.log(gt_heights / anchor_heights_pi)</span><br><span class="line"></span><br><span class="line">    targets = torch.stack((targets_dy, targets_dx, targets_dh, targets_dw))</span><br><span class="line">    targets = targets.t()</span><br><span class="line">    <span class="keyword">return</span> targets</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="loss计算"><a href="#loss计算" class="headerlink" title="loss计算"></a>loss计算</h4><p>loss的计算分为两个部分：<br>1、Smooth Loss：获取所有正标签的框的预测结果的回归loss。<br>2、Focal Loss：获取所有未被忽略的种类的预测结果的交叉熵loss。</p>
<p>由于在Retinanet的训练过程中，正负样本极其不平衡，即 存在对应真实框的先验框可能只有若干个，但是不存在对应真实框的负样本却有上万个，这就会导致负样本的loss值极大，因此引入了Focal Loss进行正负样本的平衡。</p>
<p>Focal loss是何恺明大神提出的一种新的loss计算方案。其具有两个重要的特点。</p>
<p>a)控制正负样本的权重<br>控制容易分类和难分类样本的权重<br>正负样本的概念如下：<br>一张图像可能生成成千上万的候选框，但是其中只有很少一部分是包含目标的的，有目标的就是正样本，没有目标的就是负样本。</p>
<p>容易分类和难分类样本的概念如下：<br>假设存在一个二分类，样本1属于类别1的pt=0.9，样本2属于类别1的pt=0.6，显然前者更可能是类别1，其就是容易分类的样本；后者有可能是类别1，所以其为难分类样本。</p>
<p>如何实现权重控制呢：<br>以二分类为例，常用交叉熵loss:</p>
<p><style>.veneasljnbyi{zoom:50%;}</style><img src="/zh-CN/RetinaNet/RetinaNet/20191101111610548.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/20191101111610548.png" srcset="data:image/png;base64,666" class="veneasljnbyi lazyload" alt="img"></p>
<p>利用pt简化交叉熵损失：</p>
<p><style>.hrpxkftlapjp{zoom:50%;}</style><img src="/zh-CN/RetinaNet/RetinaNet/20191101111700950.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/20191101111700950.png" srcset="data:image/png;base64,666" class="hrpxkftlapjp lazyload" alt="在这里插入图片描述"></p>
<p>因此得到：</p>
<p><style>.vdjrjqvgphkp{zoom:50%;}</style><img src="/zh-CN/RetinaNet/RetinaNet/20191101111712727.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/20191101111712727.png" srcset="data:image/png;base64,666" class="vdjrjqvgphkp lazyload" alt="在这里插入图片描述"></p>
<p><strong>想要降低负样本的影响，可以在常规的损失函数前增加一个系数αt。与Pt类似，当label=1的时候，αt=α；当label=otherwise的时候，αt=1 - α，a的范围也是0到1。此时我们便可以通过设置α实现控制正负样本对loss的贡献</strong><style>.pmcnbkecjjnj{zoom:50%;}</style><img src="/zh-CN/RetinaNet/RetinaNet/2019110111235956.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/2019110111235956.png" srcset="data:image/png;base64,666" class="pmcnbkecjjnj lazyload" alt="在这里插入图片描述"></p>
<p>其中：</p>
<p><style>.hscjugltvgla{zoom:50%;}</style><img src="/zh-CN/RetinaNet/RetinaNet/20200214164001867.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/20200214164001867.png" srcset="data:image/png;base64,666" class="hscjugltvgla lazyload" alt="在这里插入图片描述"></p>
<p>分解：</p>
<p><style>.hxeuecmnplja{zoom:50%;}</style><img src="/zh-CN/RetinaNet/RetinaNet/20200214164354541.jpg" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/20200214164354541.jpg" srcset="data:image/png;base64,666" class="hxeuecmnplja lazyload" alt="在这里插入图片描述"></p>
<p>b)控制容易分类和难分类样本的权重</p>
<p>按照刚才的思路，一个二分类，样本1属于类别1的pt=0.9，样本2属于类别1的pt=0.6，也就是 <strong>是某个类的概率越大，其越容易分类</strong> 所以利用1-Pt就可以计算出其属于容易分类或者难分类。<br>具体实现方式如下。</p>
<p><style>.hbmxltemptat{zoom:50%;}</style><img src="/zh-CN/RetinaNet/RetinaNet/20191101113143598.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/20191101113143598.png" srcset="data:image/png;base64,666" class="hbmxltemptat lazyload" alt="在这里插入图片描述"></p>
<p>其中调制系数为：</p>
<p><style>.clvlpgtdvbex{zoom:50%;}</style><img src="/zh-CN/RetinaNet/RetinaNet/image-20220419103637923.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/image-20220419103637923.png" srcset="data:image/png;base64,666" class="clvlpgtdvbex lazyload" alt="image-20220419103637923"></p>
<p>1、当pt趋于0的时候，调制系数趋于1，对于总的loss的贡献很大。当pt趋于1的时候，调制系数趋于0，也就是对于总的loss的贡献很小。<br>2、当γ=0的时候，focal loss就是传统的交叉熵损失，可以通过调整γ实现调制系数的改变。</p>
<p>c）两种权重控制方法合并<br>通过如下公式就可以实现控制正负样本的权重和控制容易分类和难分类样本的权重。</p>
<p><style>.ptmfeuybewwl{zoom:50%;}</style><img src="/zh-CN/RetinaNet/RetinaNet/20191101114056207.png" class="lazyload" data-srcset="/zh-CN/RetinaNet/RetinaNet/20191101114056207.png" srcset="data:image/png;base64,666" class="ptmfeuybewwl lazyload" alt="在这里插入图片描述"></p>
<p>实现代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FocalLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(FocalLoss, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, classifications, regressions, anchors, annotations, alpha = <span class="number">0.25</span>, gamma = <span class="number">2.0</span>, cuda = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="comment"># 设置</span></span><br><span class="line">        dtype = regressions.dtype</span><br><span class="line">        batch_size = classifications.shape[<span class="number">0</span>]</span><br><span class="line">        classification_losses = []</span><br><span class="line">        regression_losses = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得先验框，将先验框转换成中心宽高的形势</span></span><br><span class="line">        anchor = anchors[<span class="number">0</span>, :, :].to(dtype)</span><br><span class="line">        <span class="comment"># 转换成中心，宽高的形式</span></span><br><span class="line">        anchor_widths = anchor[:, <span class="number">3</span>] - anchor[:, <span class="number">1</span>]</span><br><span class="line">        anchor_heights = anchor[:, <span class="number">2</span>] - anchor[:, <span class="number">0</span>]</span><br><span class="line">        anchor_ctr_x = anchor[:, <span class="number">1</span>] + <span class="number">0.5</span> * anchor_widths</span><br><span class="line">        anchor_ctr_y = anchor[:, <span class="number">0</span>] + <span class="number">0.5</span> * anchor_heights</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            <span class="comment"># 取出真实框</span></span><br><span class="line">            bbox_annotation = annotations[j]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 获得每张图片的分类结果和回归预测结果</span></span><br><span class="line">            classification = classifications[j, :, :]</span><br><span class="line">            regression = regressions[j, :, :]</span><br><span class="line">            <span class="comment"># 平滑标签</span></span><br><span class="line">            classification = torch.clamp(classification, <span class="number">1e-4</span>, <span class="number">1.0</span> - <span class="number">1e-4</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(bbox_annotation) == <span class="number">0</span>:</span><br><span class="line">                alpha_factor = torch.ones_like(classification) * alpha</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> cuda:</span><br><span class="line">                    alpha_factor = alpha_factor.cuda()</span><br><span class="line">                alpha_factor = <span class="number">1.</span> - alpha_factor</span><br><span class="line">                focal_weight = classification</span><br><span class="line">                focal_weight = alpha_factor * torch.<span class="built_in">pow</span>(focal_weight, gamma)</span><br><span class="line">                </span><br><span class="line">                bce = -(torch.log(<span class="number">1.0</span> - classification))</span><br><span class="line">                </span><br><span class="line">                cls_loss = focal_weight * bce</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> cuda:</span><br><span class="line">                    regression_losses.append(torch.tensor(<span class="number">0</span>).to(dtype).cuda())</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    regression_losses.append(torch.tensor(<span class="number">0</span>).to(dtype))</span><br><span class="line">                classification_losses.append(cls_loss.<span class="built_in">sum</span>())</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 获得目标预测结果</span></span><br><span class="line">            targets, num_positive_anchors, positive_indices, assigned_annotations = get_target(anchor, bbox_annotation, classification, cuda)</span><br><span class="line">            </span><br><span class="line">            alpha_factor = torch.ones_like(targets) * alpha</span><br><span class="line">            <span class="keyword">if</span> cuda:</span><br><span class="line">                alpha_factor = alpha_factor.cuda()</span><br><span class="line">            alpha_factor = torch.where(torch.eq(targets, <span class="number">1.</span>), alpha_factor, <span class="number">1.</span> - alpha_factor)</span><br><span class="line">            focal_weight = torch.where(torch.eq(targets, <span class="number">1.</span>), <span class="number">1.</span> - classification, classification)</span><br><span class="line">            focal_weight = alpha_factor * torch.<span class="built_in">pow</span>(focal_weight, gamma)</span><br><span class="line"></span><br><span class="line">            bce = -(targets * torch.log(classification) + (<span class="number">1.0</span> - targets) * torch.log(<span class="number">1.0</span> - classification))</span><br><span class="line"></span><br><span class="line">            cls_loss = focal_weight * bce</span><br><span class="line"></span><br><span class="line">            zeros = torch.zeros_like(cls_loss)</span><br><span class="line">            <span class="keyword">if</span> cuda:</span><br><span class="line">                zeros = zeros.cuda()</span><br><span class="line">            cls_loss = torch.where(torch.ne(targets, -<span class="number">1.0</span>), cls_loss, zeros)</span><br><span class="line">            classification_losses.append(cls_loss.<span class="built_in">sum</span>() / torch.clamp(num_positive_anchors.to(dtype), <span class="built_in">min</span>=<span class="number">1.0</span>))</span><br><span class="line">            <span class="comment"># smoooth_l1</span></span><br><span class="line">            <span class="keyword">if</span> positive_indices.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">                targets = encode_bbox(assigned_annotations, positive_indices, anchor_widths, anchor_heights, anchor_ctr_x, anchor_ctr_y)</span><br><span class="line">               </span><br><span class="line">                regression_diff = torch.<span class="built_in">abs</span>(targets - regression[positive_indices, :])</span><br><span class="line"></span><br><span class="line">                regression_loss = torch.where(</span><br><span class="line">                    torch.le(regression_diff, <span class="number">1.0</span> / <span class="number">9.0</span>),</span><br><span class="line">                    <span class="number">0.5</span> * <span class="number">9.0</span> * torch.<span class="built_in">pow</span>(regression_diff, <span class="number">2</span>),</span><br><span class="line">                    regression_diff - <span class="number">0.5</span> / <span class="number">9.0</span></span><br><span class="line">                )</span><br><span class="line">                regression_losses.append(regression_loss.mean())</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> cuda:</span><br><span class="line">                    regression_losses.append(torch.tensor(<span class="number">0</span>).to(dtype).cuda())</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    regression_losses.append(torch.tensor(<span class="number">0</span>).to(dtype))</span><br><span class="line">        c_loss = torch.stack(classification_losses).mean()</span><br><span class="line">        r_loss = torch.stack(regression_losses).mean()</span><br><span class="line">        loss = c_loss + r_loss</span><br><span class="line">        <span class="keyword">return</span> loss, c_loss, r_loss</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>retinanet</tag>
      </tags>
  </entry>
  <entry>
    <title>SSD300转换到SSD512</title>
    <url>/zh-CN/SSD300_to_SSD512/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://gitcode.net/mirrors/bubbliiiing/ssd-pytorch">mirrors / bubbliiiing / ssd-pytorch · GitCode</a></p>
<p>2.<a href="https://github.com/midasklr/SSD.Pytorch">https://github.com/midasklr/SSD.Pytorch</a></p>
<p>由于我使用的第一篇参考链接里的代码，只有SSD300，而我又想升级改造成SSD512，因此找了一些参考，比如第二个链接，现在，让我们尝试着改造它吧。</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>SSD</tag>
      </tags>
  </entry>
  <entry>
    <title>SSD损失函数详解</title>
    <url>/zh-CN/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://www.freesion.com/article/9127332548/">目标识别：SSD pytorch代码学习笔记</a></p>
<p>2.<a href="https://linkspringer.53yu.com/content/pdf/10.1007/978-3-319-46448-0_2.pdf">SSD论文</a></p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>SSD损失函数包括两部分：位置损失函数<script type="math/tex">L_{loc}</script>和置信度损失函数<script type="math/tex">L_{conf}</script></p>
<p>整个损失函数为：</p>
<p><style>.kqwovdtfgobv{zoom:80%;}</style><img src="/zh-CN/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/image-20220506200608617.png" class="lazyload" data-srcset="/zh-CN/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/image-20220506200608617.png" srcset="data:image/png;base64,666" class="kqwovdtfgobv lazyload" alt="image-20220506200608617"></p>
<p>说明：</p>
<p>·N是先验框的正样本数量</p>
<p>·c是类别置信度预测值</p>
<p>·l为先验框所对应边界框的位置预测值</p>
<p>·g为ground truth的位置参数</p>
<h4 id="位置损失函数"><a href="#位置损失函数" class="headerlink" title="位置损失函数"></a>位置损失函数</h4><p>针对所有正样本，采用Smooth L1损失，位置信息都是编码之后的信息。</p>
<p><style>.frfyxmdbefuq{zoom:50%;}</style><img src="/zh-CN/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/390ed4e1f5fc0a137b65dea937b907a5.png" class="lazyload" data-srcset="/zh-CN/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/390ed4e1f5fc0a137b65dea937b907a5.png" srcset="data:image/png;base64,666" class="frfyxmdbefuq lazyload" alt="在这里插入图片描述"></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_l1_smooth_loss</span>(<span class="params">self, y_true, y_pred</span>):</span><br><span class="line">    abs_loss = torch.<span class="built_in">abs</span>(y_true - y_pred)</span><br><span class="line">    sq_loss = <span class="number">0.5</span> * (y_true - y_pred)**<span class="number">2</span></span><br><span class="line">    l1_loss = torch.where(abs_loss &lt; <span class="number">1.0</span>, sq_loss, abs_loss - <span class="number">0.5</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(l1_loss, -<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="置信度损失函数"><a href="#置信度损失函数" class="headerlink" title="置信度损失函数"></a>置信度损失函数</h4><p>首先需要使用 <strong>hard negative mining</strong> 将正负样本按照 <strong>1:3</strong> 的比例把负样本抽样出来，抽样的方法是：</p>
<p><strong>思想</strong>： 针对所有batch的confidence，按照置信度误差进行降序排列，取出前<strong>top_k</strong>个负样本。</p>
<p>编程:</p>
<p>·reshape所有batch中的conf</p>
<p><code>batch_conf = conf_data.view(-1, self.num_classes)</code></p>
<p>·置信度误差越大，实际上就是预测背景的置信度越小</p>
<p>·把所有置信度进行log_softmax处理（均为负值），预测的置信度越小，则log_softmax越小，取绝对值，则|log_softmax|越大，降序排列后，取前top_k的负样本。</p>
<p>softmax代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_softmax_loss</span>(<span class="params">self, y_true, y_pred</span>):</span><br><span class="line">    y_pred = torch.clamp(y_pred, <span class="built_in">min</span>=<span class="number">1e-7</span>)</span><br><span class="line">    softmax_loss = -torch.<span class="built_in">sum</span>(y_true * torch.log(y_pred),</span><br><span class="line">                              axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> softmax_loss</span><br></pre></td></tr></tbody></table></figure>
<h4 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h4><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiboxLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, alpha=<span class="number">1.0</span>, neg_pos_ratio=<span class="number">3.0</span>,</span></span><br><span class="line"><span class="params">                 background_label_id=<span class="number">0</span>, negatives_for_hard=<span class="number">100.0</span></span>):</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.neg_pos_ratio = neg_pos_ratio</span><br><span class="line">        <span class="keyword">if</span> background_label_id != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">'Only 0 as background label id is supported'</span>)</span><br><span class="line">        self.background_label_id = background_label_id</span><br><span class="line">        self.negatives_for_hard = torch.FloatTensor([negatives_for_hard])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_l1_smooth_loss</span>(<span class="params">self, y_true, y_pred</span>):</span><br><span class="line">        abs_loss = torch.<span class="built_in">abs</span>(y_true - y_pred)</span><br><span class="line">        sq_loss = <span class="number">0.5</span> * (y_true - y_pred)**<span class="number">2</span></span><br><span class="line">        l1_loss = torch.where(abs_loss &lt; <span class="number">1.0</span>, sq_loss, abs_loss - <span class="number">0.5</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">sum</span>(l1_loss, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_softmax_loss</span>(<span class="params">self, y_true, y_pred</span>):</span><br><span class="line">        y_pred = torch.clamp(y_pred, <span class="built_in">min</span>=<span class="number">1e-7</span>)</span><br><span class="line">        softmax_loss = -torch.<span class="built_in">sum</span>(y_true * torch.log(y_pred),</span><br><span class="line">                                  axis=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> softmax_loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, y_true, y_pred</span>):</span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        <span class="comment">#   y_true batch_size, 8732, 4 + self.num_classes + 1</span></span><br><span class="line">        <span class="comment">#   y_pred batch_size, 8732, 4 + self.num_classes</span></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        num_boxes = y_true.size()[<span class="number">1</span>]</span><br><span class="line">        y_pred = torch.cat([y_pred[<span class="number">0</span>], nn.Softmax(-<span class="number">1</span>)(y_pred[<span class="number">1</span>])], dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        <span class="comment">#   分类的loss</span></span><br><span class="line">        <span class="comment">#   batch_size,8732,21 -&gt; batch_size,8732</span></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        conf_loss = self._softmax_loss(y_true[:, :, <span class="number">4</span>:-<span class="number">1</span>], y_pred[:, :, <span class="number">4</span>:])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        <span class="comment">#   框的位置的loss</span></span><br><span class="line">        <span class="comment">#   batch_size,8732,4 -&gt; batch_size,8732</span></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        loc_loss = self._l1_smooth_loss(y_true[:, :, :<span class="number">4</span>],</span><br><span class="line">                                        y_pred[:, :, :<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        <span class="comment">#   获取所有的正标签的loss</span></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        pos_loc_loss = torch.<span class="built_in">sum</span>(loc_loss * y_true[:, :, -<span class="number">1</span>],</span><br><span class="line">                                 axis=<span class="number">1</span>)</span><br><span class="line">        pos_conf_loss = torch.<span class="built_in">sum</span>(conf_loss * y_true[:, :, -<span class="number">1</span>],</span><br><span class="line">                                  axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        <span class="comment">#   每一张图的正样本的个数</span></span><br><span class="line">        <span class="comment">#   num_pos     [batch_size,]</span></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        num_pos = torch.<span class="built_in">sum</span>(y_true[:, :, -<span class="number">1</span>], axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        <span class="comment">#   每一张图的负样本的个数</span></span><br><span class="line">        <span class="comment">#   num_neg     [batch_size,]</span></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        num_neg = torch.<span class="built_in">min</span>(self.neg_pos_ratio * num_pos, num_boxes - num_pos)</span><br><span class="line">        <span class="comment"># 找到了哪些值是大于0的</span></span><br><span class="line">        pos_num_neg_mask = num_neg &gt; <span class="number">0</span></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        <span class="comment">#   如果所有的图，正样本的数量均为0</span></span><br><span class="line">        <span class="comment">#   那么则默认选取100个先验框作为负样本</span></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        has_min = torch.<span class="built_in">sum</span>(pos_num_neg_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        <span class="comment">#   从这里往后，与视频中看到的代码有些许不同。</span></span><br><span class="line">        <span class="comment">#   由于以前的负样本选取方式存在一些问题，</span></span><br><span class="line">        <span class="comment">#   我对该部分代码进行重构。</span></span><br><span class="line">        <span class="comment">#   求整个batch应该的负样本数量总和</span></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        num_neg_batch = torch.<span class="built_in">sum</span>(</span><br><span class="line">            num_neg) <span class="keyword">if</span> has_min &gt; <span class="number">0</span> <span class="keyword">else</span> self.negatives_for_hard</span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        <span class="comment">#   对预测结果进行判断，如果该先验框没有包含物体</span></span><br><span class="line">        <span class="comment">#   那么它的不属于背景的预测概率过大的话</span></span><br><span class="line">        <span class="comment">#   就是难分类样本</span></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        confs_start = <span class="number">4</span> + self.background_label_id + <span class="number">1</span></span><br><span class="line">        confs_end = confs_start + self.num_classes - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        <span class="comment">#   batch_size,8732</span></span><br><span class="line">        <span class="comment">#   把不是背景的概率求和，求和后的概率越大</span></span><br><span class="line">        <span class="comment">#   代表越难分类。</span></span><br><span class="line">        <span class="comment"># --------------------------------------------- #</span></span><br><span class="line">        max_confs = torch.<span class="built_in">sum</span>(y_pred[:, :, confs_start:confs_end], dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># --------------------------------------------------- #</span></span><br><span class="line">        <span class="comment">#   只有没有包含物体的先验框才得到保留</span></span><br><span class="line">        <span class="comment">#   我们在整个batch里面选取最难分类的num_neg_batch个</span></span><br><span class="line">        <span class="comment">#   先验框作为负样本。</span></span><br><span class="line">        <span class="comment"># --------------------------------------------------- #</span></span><br><span class="line">        max_confs = (max_confs * (<span class="number">1</span> - y_true[:, :, -<span class="number">1</span>])).view([-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        _, indices = torch.topk(max_confs, k=<span class="built_in">int</span>(</span><br><span class="line">            num_neg_batch.cpu().numpy().tolist()))</span><br><span class="line"></span><br><span class="line">        neg_conf_loss = torch.gather(conf_loss.view([-<span class="number">1</span>]), <span class="number">0</span>, indices)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行归一化</span></span><br><span class="line">        num_pos = torch.where(num_pos != <span class="number">0</span>, num_pos, torch.ones_like(num_pos))</span><br><span class="line">        total_loss = torch.<span class="built_in">sum</span>(</span><br><span class="line">            pos_conf_loss) + torch.<span class="built_in">sum</span>(neg_conf_loss) + torch.<span class="built_in">sum</span>(self.alpha * pos_loc_loss)</span><br><span class="line">        total_loss = total_loss / torch.<span class="built_in">sum</span>(num_pos)</span><br><span class="line">        <span class="keyword">return</span> total_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">weights_init</span>(<span class="params">net, init_type=<span class="string">'normal'</span>, init_gain=<span class="number">0.02</span></span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_func</span>(<span class="params">m</span>):</span><br><span class="line">        classname = m.__class__.__name__</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(m, <span class="string">'weight'</span>) <span class="keyword">and</span> classname.find(<span class="string">'Conv'</span>) != -<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> init_type == <span class="string">'normal'</span>:</span><br><span class="line">                torch.nn.init.normal_(m.weight.data, <span class="number">0.0</span>, init_gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'xavier'</span>:</span><br><span class="line">                torch.nn.init.xavier_normal_(m.weight.data, gain=init_gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'kaiming'</span>:</span><br><span class="line">                torch.nn.init.kaiming_normal_(</span><br><span class="line">                    m.weight.data, a=<span class="number">0</span>, mode=<span class="string">'fan_in'</span>)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'orthogonal'</span>:</span><br><span class="line">                torch.nn.init.orthogonal_(m.weight.data, gain=init_gain)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> NotImplementedError(</span><br><span class="line">                    <span class="string">'initialization method [%s] is not implemented'</span> % init_type)</span><br><span class="line">        <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm2d'</span>) != -<span class="number">1</span>:</span><br><span class="line">            torch.nn.init.normal_(m.weight.data, <span class="number">1.0</span>, <span class="number">0.02</span>)</span><br><span class="line">            torch.nn.init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'initialize network with %s type'</span> % init_type)</span><br><span class="line">    net.apply(init_func)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_lr_scheduler</span>(<span class="params">lr_decay_type, lr, min_lr, total_iters, warmup_iters_ratio=<span class="number">0.1</span>, warmup_lr_ratio=<span class="number">0.1</span>, no_aug_iter_ratio=<span class="number">0.3</span>, step_num=<span class="number">10</span></span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">yolox_warm_cos_lr</span>(<span class="params">lr, min_lr, total_iters, warmup_total_iters, warmup_lr_start, no_aug_iter, iters</span>):</span><br><span class="line">        <span class="keyword">if</span> iters &lt;= warmup_total_iters:</span><br><span class="line">            <span class="comment"># lr = (lr - warmup_lr_start) * iters / float(warmup_total_iters) + warmup_lr_start</span></span><br><span class="line">            lr = (lr - warmup_lr_start) * <span class="built_in">pow</span>(iters /</span><br><span class="line">                                              <span class="built_in">float</span>(warmup_total_iters), <span class="number">2</span>) + warmup_lr_start</span><br><span class="line">        <span class="keyword">elif</span> iters &gt;= total_iters - no_aug_iter:</span><br><span class="line">            lr = min_lr</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            lr = min_lr + <span class="number">0.5</span> * (lr - min_lr) * (</span><br><span class="line">                <span class="number">1.0</span> + math.cos(math.pi * (iters - warmup_total_iters) /</span><br><span class="line">                               (total_iters - warmup_total_iters - no_aug_iter))</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> lr</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step_lr</span>(<span class="params">lr, decay_rate, step_size, iters</span>):</span><br><span class="line">        <span class="keyword">if</span> step_size &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"step_size must above 1."</span>)</span><br><span class="line">        n = iters // step_size</span><br><span class="line">        out_lr = lr * decay_rate ** n</span><br><span class="line">        <span class="keyword">return</span> out_lr</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> lr_decay_type == <span class="string">"cos"</span>:</span><br><span class="line">        warmup_total_iters = <span class="built_in">min</span>(<span class="built_in">max</span>(warmup_iters_ratio * total_iters, <span class="number">1</span>), <span class="number">3</span>)</span><br><span class="line">        warmup_lr_start = <span class="built_in">max</span>(warmup_lr_ratio * lr, <span class="number">1e-6</span>)</span><br><span class="line">        no_aug_iter = <span class="built_in">min</span>(<span class="built_in">max</span>(no_aug_iter_ratio * total_iters, <span class="number">1</span>), <span class="number">15</span>)</span><br><span class="line">        func = partial(yolox_warm_cos_lr, lr, min_lr, total_iters,</span><br><span class="line">                       warmup_total_iters, warmup_lr_start, no_aug_iter)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        decay_rate = (min_lr / lr) ** (<span class="number">1</span> / (step_num - <span class="number">1</span>))</span><br><span class="line">        step_size = total_iters / step_num</span><br><span class="line">        func = partial(step_lr, lr, decay_rate, step_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> func</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_optimizer_lr</span>(<span class="params">optimizer, lr_scheduler_func, epoch</span>):</span><br><span class="line">    lr = lr_scheduler_func(epoch)</span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">'lr'</span>] = lr</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>SSD</tag>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title>Sanderson the Man 1</title>
    <url>/zh-CN/The_story_of_a_great_schoolmaster/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Of all the men I have met—and I have now had a fairly long and active life and have met a very great variety of interesting people—one only has stirred me to a biographical effort. This one exception is F. W. Sanderson, for many years the headmaster of Oundle School. I think him beyond question the greatest man I have ever known with any degree of <a href="http://dict.qsbdc.com/intimacy"><strong>intimacy</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_1">1</a>, and it is in the hope of conveying to others something of my sense not merely of his importance, but of his <a href="http://dict.qsbdc.com/peculiar"><strong>peculiar</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_2">2</a> genius and the rich humanity of his character, that I am setting out to write this book. He was in himself a very delightful[Pg 2] mixture of <a href="http://dict.qsbdc.com/subtlety"><strong>subtlety</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_3">3</a> and <a href="http://dict.qsbdc.com/simplicity"><strong>simplicity</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_4">4</a>, <a href="http://dict.qsbdc.com/generosity"><strong>generosity</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_5">5</a>, <a href="http://dict.qsbdc.com/adventurousness"><strong>adventurousness</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_6">6</a>, imagination and <a href="http://dict.qsbdc.com/steadfast"><strong>steadfast</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_7">7</a> purpose, and he approached the general life of our time at such an angle as to reflect the most curious and profitable lights upon it. To tell his story is to reflect upon all the main educational ideas of the last half-century, and to revise our conception of the process and purpose of the modern community in relation to education. For Sanderson had a mind like an <a href="http://dict.qsbdc.com/octopus"><strong>octopus</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_8">8</a>, it seemed always to have a <a href="http://dict.qsbdc.com/tentacle"><strong>tentacle</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_9">9</a> free to reach out beyond what was already held, and his <a href="http://dict.qsbdc.com/tentacles"><strong>tentacles</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_10">10</a> grew and radiated farther and farther. Before his end he had come to a vision of the school as a centre for the complete reorganisation of civilised life.</p>
]]></content>
      <categories>
        <category>novel</category>
      </categories>
  </entry>
  <entry>
    <title>vision transformer论文笔记</title>
    <url>/zh-CN/VIT/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文地址：<a href="https://arxiv.org/pdf/2010.11929.pdf">https://arxiv.org/pdf/2010.11929.pdf</a></p>
<p>源码地址：<a href="https://github.com/google-research/vision_transformer">google-research/vision_transformer (github.com)</a></p>
<p>文章引用源码：<a href="https://github.com/bubbliiiing/classification-pytorch">https://github.com/bubbliiiing/classification-pytorch</a></p>
<p>文章出处：<a href="https://blog.csdn.net/weixin_44791964/article/details/122637701">https://blog.csdn.net/weixin_44791964/article/details/122637701</a></p>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>Vision Transformer是Transformer的视觉版本，Transformer基本上已经成为了自然语言处理的标配，但是在视觉中的运用还受到限制。</p>
<p>Vision Transformer打破了这种NLP与CV的隔离，将Transformer应用于图像图块（patch）序列上，进一步完成图像分类任务。简单来理解，Vision Transformer就是将输入进来的图片，每隔一定的区域大小划分图片块。然后将划分后的图片块组合成序列，将组合后的结果传入Transformer特有的Multi-head Self-attention进行特征提取。最后利用Cls Token进行分类。</p>
<h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p><style>.lmfyroylmuuh{}</style><img src="/zh-CN/VIT/VIT/image-20220418091306571.png" class="lazyload" data-srcset="/zh-CN/VIT/VIT/image-20220418091306571.png" srcset="data:image/png;base64,666" class="lmfyroylmuuh lazyload"></p>
<p>与寻常的分类网络类似，整个Vision Transformer可以分为两部分，一部分是特征提取部分，另一部分是分类部分。</p>
<p>在特征提取部分，VIT所做的工作是特征提取。特征提取部分在图片中的对应区域是Patch+Position Embedding和Transformer Encoder。Patch+Position Embedding的作用主要是对输入进来的图片进行分块处理，每隔一定的区域大小划分图片块。然后将划分后的图片块组合成序列。在获得序列信息后，传入Transformer Encoder进行特征提取，这是Transformer特有的Multi-head Self-attention结构，通过自注意力机制，关注每个图片块的重要程度。</p>
<p>在分类部分，VIT所做的工作是利用提取到的特征进行分类。在进行特征提取的时候，我们会在图片序列中添加上Cls Token，该Token会作为一个单位的序列信息一起进行特征提取，提取的过程中，该Cls Token会与其它的特征进行特征交互，融合其它图片序列的特征。最终，我们利用Multi-head Self-attention结构提取特征后的Cls Token进行全连接分类。</p>
<h3 id="网络结构详解"><a href="#网络结构详解" class="headerlink" title="网络结构详解"></a>网络结构详解</h3><h4 id="特征提取部分"><a href="#特征提取部分" class="headerlink" title="特征提取部分"></a>特征提取部分</h4><p>a）Patch+Position Embedding</p>
<p><style>.jkdvcgobyyum{}</style><img src="/zh-CN/VIT/VIT/image-20220418091855460.png" class="lazyload" data-srcset="/zh-CN/VIT/VIT/image-20220418091855460.png" srcset="data:image/png;base64,666" class="jkdvcgobyyum lazyload"></p>
<p>该部分作用：对输入进来的图片进行分块处理，每隔一定的区域大小划分图片块。然后将划分后的图片块组合成序列。</p>
<p>该部分首先对输入进来的图片进行分块处理，处理方式其实很简单，使用的是现成的卷积。由于卷积使用的是滑动窗口的思想，我们只需要设定特定的步长，就可以输入进来的图片进行分块处理了。</p>
<p>在VIT中，我们常设置这个卷积的卷积核大小为16x16，步长也为16x16，此时卷积就会每隔16个像素点进行一次特征提取，由于卷积核大小为16x16，两个图片区域的特征提取过程就不会有重叠。当我们输入的图片是[224, 224, 3]的时候，我们可以获得一个[14, 14, 768]的特征层。</p>
<p><style>.pihsjxhjlgvv{}</style><img src="/zh-CN/VIT/VIT/58cc10deb7dc45ae90ae606966d7c724.gif" class="lazyload" data-srcset="/zh-CN/VIT/VIT/58cc10deb7dc45ae90ae606966d7c724.gif" srcset="data:image/png;base64,666" class="pihsjxhjlgvv lazyload"></p>
<p>下一步就是将这个特征层组合成序列，组合的方式非常简单，就是将高宽维度进行平铺，[14, 14, 768]在高宽维度平铺后，获得一个196, 768的特征层。平铺完成后，我们会在图片序列中添加上Cls Token，该Token会作为一个单位的序列信息一起进行特征提取，图中的这个0*就是Cls Token，我们此时获得一个197, 768的特征层。</p>
<p><style>.dunuiecavxmk{}</style><img src="/zh-CN/VIT/VIT/image-20220418092240916.png" class="lazyload" data-srcset="/zh-CN/VIT/VIT/image-20220418092240916.png" srcset="data:image/png;base64,666" class="dunuiecavxmk lazyload"></p>
<p>添加完成Cls Token后，再为所有特征添加上位置信息，这样网络才有区分不同区域的能力。添加方式其实也非常简单，我们生成一个197, 768的参数矩阵，这个参数矩阵是可训练的，把这个矩阵加上197, 768的特征层即可。</p>
<p>到这里，Patch+Position Embedding就构建完成了，构建代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># [224, 224, 3]-&gt;[14, 14, 768]</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_shape=[<span class="number">224</span>, <span class="number">224</span>], patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, num_features=<span class="number">768</span>, norm_layer=<span class="literal">None</span>, flatten=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 196 = 14 * 14</span></span><br><span class="line">        self.num_patches    = (input_shape[<span class="number">0</span>] // patch_size) * (input_shape[<span class="number">1</span>] // patch_size)</span><br><span class="line">        self.flatten        = flatten</span><br><span class="line"></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, num_features, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        self.norm = norm_layer(num_features) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        <span class="keyword">if</span> self.flatten:</span><br><span class="line">            x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># BCHW -&gt; BNC</span></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="comment"># x = [b, 196, 768]</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self, input_shape=[<span class="number">224</span>, <span class="number">224</span>], patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>, num_features=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">            depth=<span class="number">12</span>, num_heads=<span class="number">12</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, drop_rate=<span class="number">0.1</span>, attn_drop_rate=<span class="number">0.1</span>, drop_path_rate=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">            norm_layer=partial(<span class="params">nn.LayerNorm, eps=<span class="number">1e-6</span></span>), act_layer=GELU</span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   224, 224, 3 -&gt; 196, 768</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        self.patch_embed    = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features)</span><br><span class="line">        num_patches         = (<span class="number">224</span> // patch_size) * (<span class="number">224</span> // patch_size)</span><br><span class="line">        self.num_features   = num_features</span><br><span class="line">        self.new_feature_shape = [<span class="built_in">int</span>(input_shape[<span class="number">0</span>] // patch_size), <span class="built_in">int</span>(input_shape[<span class="number">1</span>] // patch_size)]</span><br><span class="line">        self.old_feature_shape = [<span class="built_in">int</span>(<span class="number">224</span> // patch_size), <span class="built_in">int</span>(<span class="number">224</span> // patch_size)]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">#   在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。</span></span><br><span class="line">        <span class="comment">#   此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。</span></span><br><span class="line">        <span class="comment">#   在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。</span></span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   196, 768 -&gt; 197, 768</span></span><br><span class="line">        self.cls_token      = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, num_features))</span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   为网络提取到的特征添加上位置信息。</span></span><br><span class="line">        <span class="comment">#   以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768</span></span><br><span class="line">        <span class="comment">#   此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。</span></span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   197, 768 -&gt; 197, 768</span></span><br><span class="line">        self.pos_embed      = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, num_features))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = [b, 196, 768]</span></span><br><span class="line">        x = self.patch_embed(x)</span><br><span class="line">        <span class="comment"># cls_token = [b, 1, 768]</span></span><br><span class="line">        cls_token = self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>) </span><br><span class="line">        <span class="comment"># x = [b, 197, 768]</span></span><br><span class="line">        x = torch.cat((cls_token, x), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [1, 1, 768]</span></span><br><span class="line">        cls_token_pe = self.pos_embed[:, <span class="number">0</span>:<span class="number">1</span>, :]</span><br><span class="line">        <span class="comment"># [1, 196, 768]</span></span><br><span class="line">        img_token_pe = self.pos_embed[:, <span class="number">1</span>: , :]</span><br><span class="line">		<span class="comment"># [1, 196, 768]-&gt;[1, 14, 14, 768]-&gt;[1, 768, 14, 14]</span></span><br><span class="line">        img_token_pe = img_token_pe.view(<span class="number">1</span>, *self.old_feature_shape, -<span class="number">1</span>).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 做插值，以防输入图片不是224*224</span></span><br><span class="line">        img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode=<span class="string">'bicubic'</span>, align_corners=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># [1, 768, 14, 14]-&gt;[1, 14, 14, 768]-&gt;[1, 196, 768]</span></span><br><span class="line">        img_token_pe = img_token_pe.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).flatten(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># [1, 197, 768]</span></span><br><span class="line">        pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = self.pos_drop(x + pos_embed)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>b）transformer encoder</p>
<p><style>.pwmflxfhwiym{}</style><img src="/zh-CN/VIT/VIT/8ff82ad32b994a12bfc2356718ac9683.gif" class="lazyload" data-srcset="/zh-CN/VIT/VIT/8ff82ad32b994a12bfc2356718ac9683.gif" srcset="data:image/png;base64,666" class="pwmflxfhwiym lazyload"></p>
<p>在上一步<strong>获得shape为197, 768的序列信息</strong>后，将<strong>序列信息传入Transformer Encoder进行特征提取</strong>，这是Transformer特有的Multi-head Self-attention结构，<strong>通过自注意力机制，关注每个图片块的重要程度。</strong></p>
<p>1)self-attention结构解析</p>
<p>看懂Self-attention结构，其实看懂下面这个动图就可以了，动图中存在<strong>一个序列的三个单位输入</strong>，<strong>每一个序列单位的输入</strong>都可以通过<strong>三个处理（比如全连接）获得Query、Key、Value</strong>，Query是查询向量、Key是键向量、Value值向量。</p>
<p><style>.hecipdttujgo{}</style><img src="/zh-CN/VIT/VIT/32c551decdb64331a1c4ec0471cc1f3d.gif" class="lazyload" data-srcset="/zh-CN/VIT/VIT/32c551decdb64331a1c4ec0471cc1f3d.gif" srcset="data:image/png;base64,666" class="hecipdttujgo lazyload"></p>
<p>如果我们想要获得input-1的输出，那么我们进行如下几步：<br>1、利用input-1的查询向量，分别乘上input-1、input-2、input-3的键向量，此时我们获得了三个score。<br>2、然后对这三个score取softmax，获得了input-1、input-2、input-3各自的重要程度。<br>3、然后将这个重要程度乘上input-1、input-2、input-3的值向量，求和。<br>4、此时我们获得了input-1的输出。</p>
<p>如图所示，我们进行如下几步：<br>1、input-1的查询向量为[1, 0, 2]，分别乘上input-1、input-2、input-3的键向量，获得三个score为2，4，4。<br>2、然后对这三个score取softmax，获得了input-1、input-2、input-3各自的重要程度，获得三个重要程度为0.0，0.5，0.5。<br>3、然后将这个重要程度乘上input-1、input-2、input-3的值向量，求和，即<br>0.0 ∗ [ 1 , 2 , 3 ] + 0.5 ∗ [ 2 , 8 , 0 ] + 0.5 ∗ [ 2 , 6 , 3 ] = [ 2.0 , 7.0 , 1.5 ]<br>4、此时我们获得了input-1的输出 [2.0, 7.0, 1.5]。</p>
<p>上述的例子中，序列长度仅为3，每个单位序列的特征长度仅为3，在VIT的Transformer Encoder中，序列长度为197，每个单位序列的特征长度为768 // num_heads。但计算过程是一样的。在实际运算时，我们采用矩阵进行运算。<br>2)self-attention的矩阵运算</p>
<p>实际的矩阵运算过程如下图所示。我以实际矩阵为例子给大家解析：</p>
<p><style>.prwoqibaecrv{}</style><img src="/zh-CN/VIT/VIT/19f323060f1f41ba99e743cea1fa5174.png" class="lazyload" data-srcset="/zh-CN/VIT/VIT/19f323060f1f41ba99e743cea1fa5174.png" srcset="data:image/png;base64,666" class="prwoqibaecrv lazyload"></p>
<p>输入的Query、Key、Value如下图所示：</p>
<p><style>.rqnuhyqwjfcw{}</style><img src="/zh-CN/VIT/VIT/2500484f29ae4671944a06543ad3e026.png" class="lazyload" data-srcset="/zh-CN/VIT/VIT/2500484f29ae4671944a06543ad3e026.png" srcset="data:image/png;base64,666" class="rqnuhyqwjfcw lazyload"></p>
<p>首先利用 查询向量query 叉乘 转置后的键向量key，这一步可以通俗的理解为，利用查询向量去查询序列的特征，获得序列每个部分的重要程度score。</p>
<p>输出的每一行，都代表input-1、input-2、input-3，对当前input的贡献，我们对这个贡献值取一个softmax。</p>
<p><style>.mfphassqitnd{}</style><img src="/zh-CN/VIT/VIT/image-20220418104816862.png" class="lazyload" data-srcset="/zh-CN/VIT/VIT/image-20220418104816862.png" srcset="data:image/png;base64,666" class="mfphassqitnd lazyload"></p>
<p>然后利用 score 叉乘 value，<strong>这一步可以通俗的理解为，将序列每个部分的重要程度重新施加到序列的值上去。</strong></p>
<p><style>.nsbojfdjysrd{}</style><img src="/zh-CN/VIT/VIT/c41d889912a64057ab571bdfd5458910.png" class="lazyload" data-srcset="/zh-CN/VIT/VIT/c41d889912a64057ab571bdfd5458910.png" srcset="data:image/png;base64,666" class="nsbojfdjysrd lazyload"></p>
<p>矩阵代码运算如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">soft_max</span>(<span class="params">z</span>):</span><br><span class="line">    t = np.exp(z)</span><br><span class="line">    a = np.exp(z) / np.expand_dims(np.<span class="built_in">sum</span>(t, axis=<span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">Query = np.array([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">Key = np.array([</span><br><span class="line">    [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">    [<span class="number">4</span>,<span class="number">4</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">Value = np.array([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">8</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">6</span>,<span class="number">3</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">scores = Query @ Key.T</span><br><span class="line"><span class="built_in">print</span>(scores)</span><br><span class="line">scores = soft_max(scores)</span><br><span class="line"><span class="built_in">print</span>(scores)</span><br><span class="line">out = scores @ Value</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>3）Multihead多头注意力机制</p>
<p>多头注意力机制的示意图如图所示：</p>
<p><style>.khsxpexysmru{}</style><img src="/zh-CN/VIT/VIT/430e12e75fd44c82ac95e504b5da0d50.png" class="lazyload" data-srcset="/zh-CN/VIT/VIT/430e12e75fd44c82ac95e504b5da0d50.png" srcset="data:image/png;base64,666" class="khsxpexysmru lazyload"></p>
<p>这幅图给人的感觉略显迷茫，我们跳脱出这个图，直接从矩阵的shape入手会清晰很多。</p>
<p>在第一步进行图像的分割后，我们获得的特征层为197, 768。</p>
<p>在施加多头的时候，我们直接对196, 768的最后一维度进行分割，比如我们想分割成12个头，那么矩阵的shape就变成了196, 12, 64。</p>
<p>然后我们将196, 12, 64进行转置，将12放到前面去，获得的特征层为12, 196, 64。之后我们忽略这个12，把它和batch维度同等对待，只对196, 64进行处理，其实也就是上面的注意力机制的过程了。</p>
<p><style>.rfgxbcpfuwqb{zoom:50%;}</style><img src="/zh-CN/VIT/VIT/90787898063c45fe888c136ba4b32e64.png" class="lazyload" data-srcset="/zh-CN/VIT/VIT/90787898063c45fe888c136ba4b32e64.png" srcset="data:image/png;base64,666" class="rfgxbcpfuwqb lazyload" alt="img"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#--------------------------------------------------------------------------#</span></span><br><span class="line"><span class="comment">#   Attention机制</span></span><br><span class="line"><span class="comment">#   将输入的特征qkv特征进行划分，首先生成query, key, value。query是查询向量、key是键向量、v是值向量。</span></span><br><span class="line"><span class="comment">#   然后利用 查询向量query 叉乘 转置后的键向量key，这一步可以通俗的理解为，利用查询向量去查询序列的特征，获得序列每个部分的重要程度score。</span></span><br><span class="line"><span class="comment">#   然后利用 score 叉乘 value，这一步可以通俗的理解为，将序列每个部分的重要程度重新施加到序列的值上去。</span></span><br><span class="line"><span class="comment">#--------------------------------------------------------------------------#</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads=<span class="number">8</span>, qkv_bias=<span class="literal">False</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_heads  = num_heads</span><br><span class="line">        self.scale      = (dim // num_heads) ** -<span class="number">0.5</span></span><br><span class="line">		<span class="comment"># 768-&gt;768*3</span></span><br><span class="line">        self.qkv        = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop  = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj       = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop  = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># batch, 196, 768</span></span><br><span class="line">        B, N, C     = x.shape</span><br><span class="line">        <span class="comment"># batch, 196, 768 -&gt; batch, 196, 768*3 -&gt; batch, 196, 3, 8, 768/8=96 -&gt; 3, batch, 8, 196, 96</span></span><br><span class="line">        qkv         = self.qkv(x).reshape(B, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 3 * 1, batch, 8, 196, 96  -&gt; q, k, v = batch: 16, head: 8, patch: 196, each_head_attention_channels: 96</span></span><br><span class="line">        q, k, v     = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]</span><br><span class="line">		<span class="comment"># batch, 8, 196, 96 @ batch, 8, 96, 196 -&gt; batch, 8, 196, 196</span></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale</span><br><span class="line">        attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line">		<span class="comment"># batch, 8, 196, 196 @ batch, 8, 196, 96 -&gt; batch, 8, 196, 96 -&gt; batch, 196, 8, 96 -&gt; batch, 196, 768</span></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B, N, C)</span><br><span class="line">        <span class="comment"># batch, 196, 768 -&gt; batch, 196, 768</span></span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        <span class="comment"># Dropout(batch, 196, 768)</span></span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>4）TransformerBlock的构建</p>
<p><style>.hiowydecszfm{}</style><img src="/zh-CN/VIT/VIT/4036cdfc91a6477d91009d574788a78b.png" class="lazyload" data-srcset="/zh-CN/VIT/VIT/4036cdfc91a6477d91009d574788a78b.png" srcset="data:image/png;base64,666" class="hiowydecszfm lazyload"></p>
<p><strong>在完成MultiHeadSelfAttention的构建后，我们需要在其后加上两个全连接。就构建了整个TransformerBlock</strong></p>
<p>block流程见下图：</p>
<p><style>.dzwugrvghlaj{zoom: 80%;}</style><img src="/zh-CN/VIT/VIT/e3bf360d541c4eb1a243e100f17a48b6.png" class="lazyload" data-srcset="/zh-CN/VIT/VIT/e3bf360d541c4eb1a243e100f17a48b6.png" srcset="data:image/png;base64,666" class="dzwugrvghlaj lazyload" alt="img"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="string">""" MLP as used in Vision Transformer, MLP-Mixer and related networks</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=GELU, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features    = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        drop_probs      = (drop, drop)</span><br><span class="line"></span><br><span class="line">        self.fc1    = nn.Linear(in_features, hidden_features)</span><br><span class="line">        self.act    = act_layer()</span><br><span class="line">        self.drop1  = nn.Dropout(drop_probs[<span class="number">0</span>])</span><br><span class="line">        self.fc2    = nn.Linear(hidden_features, out_features)</span><br><span class="line">        self.drop2  = nn.Dropout(drop_probs[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># batch, 196, 768 -&gt; batch, 196, 768</span></span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        <span class="comment"># batch, 196, 768 -&gt; batch, 196, 768</span></span><br><span class="line">        x = self.act(x)</span><br><span class="line">        <span class="comment"># batch, 196, 768 -&gt; batch, 196, 768</span></span><br><span class="line">        x = self.drop1(x)</span><br><span class="line">        <span class="comment"># batch, 196, 768 -&gt; batch, 196, 768</span></span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="comment"># batch, 196, 768 -&gt; batch, 196, 768</span></span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">#  a transoformer encoder block</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">False</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path=<span class="number">0.</span>, act_layer=GELU, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm1      = norm_layer(dim)</span><br><span class="line">        self.attn       = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line">        self.norm2      = norm_layer(dim)</span><br><span class="line">        self.mlp        = Mlp(in_features=dim, hidden_features=<span class="built_in">int</span>(dim * mlp_ratio), act_layer=act_layer, drop=drop)</span><br><span class="line">        self.drop_path  = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + self.drop_path(self.attn(self.norm1(x)))</span><br><span class="line">        x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>c）VIT模型构建</p>
<p><style>.ueivxtcniphu{}</style><img src="/zh-CN/VIT/VIT/image-20220418105537648.png" class="lazyload" data-srcset="/zh-CN/VIT/VIT/image-20220418105537648.png" srcset="data:image/png;base64,666" class="ueivxtcniphu lazyload"></p>
<p>整个VIT模型由一个Patch+Position Embedding加上多个TransformerBlock组成。典型的TransforerBlock的数量为12个</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self, input_shape=[<span class="number">224</span>, <span class="number">224</span>], patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>, num_features=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">            depth=<span class="number">12</span>, num_heads=<span class="number">12</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, drop_rate=<span class="number">0.1</span>, attn_drop_rate=<span class="number">0.1</span>, drop_path_rate=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">            norm_layer=partial(<span class="params">nn.LayerNorm, eps=<span class="number">1e-6</span></span>), act_layer=GELU</span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">        <span class="built_in">super</span>(VisionTransformer, self).__init__()</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   224, 224, 3 -&gt; batch, 196, 768</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        self.patch_embed    = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features)</span><br><span class="line">        num_patches         = (<span class="number">224</span> // patch_size) * (<span class="number">224</span> // patch_size)</span><br><span class="line">        self.num_features   = num_features</span><br><span class="line">        self.new_feature_shape = [<span class="built_in">int</span>(input_shape[<span class="number">0</span>] // patch_size), <span class="built_in">int</span>(input_shape[<span class="number">1</span>] // patch_size)]</span><br><span class="line">        self.old_feature_shape = [<span class="built_in">int</span>(<span class="number">224</span> // patch_size), <span class="built_in">int</span>(<span class="number">224</span> // patch_size)]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">#   在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。</span></span><br><span class="line">        <span class="comment">#   此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。</span></span><br><span class="line">        <span class="comment">#   在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。</span></span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   1, 1, 768</span></span><br><span class="line">        self.cls_token      = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, num_features))</span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   为网络提取到的特征添加上位置信息。</span></span><br><span class="line">        <span class="comment">#   以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768</span></span><br><span class="line">        <span class="comment">#   此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。</span></span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   1, 197, 768</span></span><br><span class="line">        self.pos_embed      = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, num_features))</span><br><span class="line">        <span class="comment"># 1, 197, 768</span></span><br><span class="line">        self.pos_drop       = nn.Dropout(p=drop_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   197, 768 -&gt; 197, 768  12次</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, depth)]</span><br><span class="line">        self.blocks = nn.Sequential(</span><br><span class="line">            *[</span><br><span class="line">                Block(</span><br><span class="line">                    dim         = num_features, </span><br><span class="line">                    num_heads   = num_heads, </span><br><span class="line">                    mlp_ratio   = mlp_ratio, </span><br><span class="line">                    qkv_bias    = qkv_bias, </span><br><span class="line">                    drop        = drop_rate,</span><br><span class="line">                    attn_drop   = attn_drop_rate, </span><br><span class="line">                    drop_path   = dpr[i], </span><br><span class="line">                    norm_layer  = norm_layer, </span><br><span class="line">                    act_layer   = act_layer</span><br><span class="line">                )<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">        self.norm = norm_layer(num_features)</span><br><span class="line">        self.head = nn.Linear(num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.patch_embed(x)</span><br><span class="line">        cls_token = self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>) </span><br><span class="line">        x = torch.cat((cls_token, x), dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        cls_token_pe = self.pos_embed[:, <span class="number">0</span>:<span class="number">1</span>, :]</span><br><span class="line">        img_token_pe = self.pos_embed[:, <span class="number">1</span>: , :]</span><br><span class="line"></span><br><span class="line">        img_token_pe = img_token_pe.view(<span class="number">1</span>, *self.old_feature_shape, -<span class="number">1</span>).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode=<span class="string">'bicubic'</span>, align_corners=<span class="literal">False</span>)</span><br><span class="line">        img_token_pe = img_token_pe.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).flatten(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = self.pos_drop(x + pos_embed)</span><br><span class="line">        x = self.blocks(x)</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># # 整个Transformer Encoder = batch, 768</span></span><br><span class="line">        x = self.forward_features(x)</span><br><span class="line">        <span class="comment"># 最后的MLP Header = batch, 768 -&gt; 768 -&gt; 1000 -&gt; batch, 1000</span></span><br><span class="line">        x = self.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">freeze_backbone</span>(<span class="params">self</span>):</span><br><span class="line">        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:<span class="number">8</span>]]</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> backbone:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> module.parameters():</span><br><span class="line">                    param.requires_grad = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                module.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Unfreeze_backbone</span>(<span class="params">self</span>):</span><br><span class="line">        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:<span class="number">8</span>]]</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> backbone:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> module.parameters():</span><br><span class="line">                    param.requires_grad = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                module.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="分类部分"><a href="#分类部分" class="headerlink" title="分类部分"></a>分类部分</h4><p><style>.kkeonvzdnwjj{}</style><img src="/zh-CN/VIT/VIT/image-20220418105537648.png" class="lazyload" data-srcset="/zh-CN/VIT/VIT/image-20220418105537648.png" srcset="data:image/png;base64,666" class="kkeonvzdnwjj lazyload"></p>
<p>在分类部分，VIT所做的工作是利用提取到的特征进行分类。</p>
<p>在进行特征提取的时候，我们会在图片序列中添加上Cls Token，该Token会作为一个单位的序列信息一起进行特征提取，提取的过程中，该Cls Token会与其它的特征进行特征交互，融合其它图片序列的特征。</p>
<p>最终，我们利用Multi-head Self-attention结构提取特征后的Cls Token进行全连接分类。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self, input_shape=[<span class="number">224</span>, <span class="number">224</span>], patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>, num_features=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">            depth=<span class="number">12</span>, num_heads=<span class="number">12</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, drop_rate=<span class="number">0.1</span>, attn_drop_rate=<span class="number">0.1</span>, drop_path_rate=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">            norm_layer=partial(<span class="params">nn.LayerNorm, eps=<span class="number">1e-6</span></span>), act_layer=GELU</span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   224, 224, 3 -&gt; 196, 768</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        self.patch_embed    = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features)</span><br><span class="line">        num_patches         = (<span class="number">224</span> // patch_size) * (<span class="number">224</span> // patch_size)</span><br><span class="line">        self.num_features   = num_features</span><br><span class="line">        self.new_feature_shape = [<span class="built_in">int</span>(input_shape[<span class="number">0</span>] // patch_size), <span class="built_in">int</span>(input_shape[<span class="number">1</span>] // patch_size)]</span><br><span class="line">        self.old_feature_shape = [<span class="built_in">int</span>(<span class="number">224</span> // patch_size), <span class="built_in">int</span>(<span class="number">224</span> // patch_size)]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">#   在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。</span></span><br><span class="line">        <span class="comment">#   此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。</span></span><br><span class="line">        <span class="comment">#   在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。</span></span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   196, 768 -&gt; 197, 768</span></span><br><span class="line">        self.cls_token      = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, num_features))</span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   为网络提取到的特征添加上位置信息。</span></span><br><span class="line">        <span class="comment">#   以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768</span></span><br><span class="line">        <span class="comment">#   此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。</span></span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   197, 768 -&gt; 197, 768</span></span><br><span class="line">        self.pos_embed      = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, num_features))</span><br><span class="line">        self.pos_drop       = nn.Dropout(p=drop_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   197, 768 -&gt; 197, 768  12次</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, depth)]</span><br><span class="line">        self.blocks = nn.Sequential(</span><br><span class="line">            *[</span><br><span class="line">                Block(</span><br><span class="line">                    dim         = num_features, </span><br><span class="line">                    num_heads   = num_heads, </span><br><span class="line">                    mlp_ratio   = mlp_ratio, </span><br><span class="line">                    qkv_bias    = qkv_bias, </span><br><span class="line">                    drop        = drop_rate,</span><br><span class="line">                    attn_drop   = attn_drop_rate, </span><br><span class="line">                    drop_path   = dpr[i], </span><br><span class="line">                    norm_layer  = norm_layer, </span><br><span class="line">                    act_layer   = act_layer</span><br><span class="line">                )<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">        self.norm = norm_layer(num_features)</span><br><span class="line">        self.head = nn.Linear(num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.patch_embed(x)</span><br><span class="line">        cls_token = self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>) </span><br><span class="line">        x = torch.cat((cls_token, x), dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        cls_token_pe = self.pos_embed[:, <span class="number">0</span>:<span class="number">1</span>, :]</span><br><span class="line">        img_token_pe = self.pos_embed[:, <span class="number">1</span>: , :]</span><br><span class="line"></span><br><span class="line">        img_token_pe = img_token_pe.view(<span class="number">1</span>, *self.old_feature_shape, -<span class="number">1</span>).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode=<span class="string">'bicubic'</span>, align_corners=<span class="literal">False</span>)</span><br><span class="line">        img_token_pe = img_token_pe.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).flatten(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = self.pos_drop(x + pos_embed)</span><br><span class="line">        x = self.blocks(x)</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.forward_features(x)</span><br><span class="line">        x = self.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">freeze_backbone</span>(<span class="params">self</span>):</span><br><span class="line">        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:<span class="number">8</span>]]</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> backbone:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> module.parameters():</span><br><span class="line">                    param.requires_grad = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                module.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Unfreeze_backbone</span>(<span class="params">self</span>):</span><br><span class="line">        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:<span class="number">8</span>]]</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> backbone:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> module.parameters():</span><br><span class="line">                    param.requires_grad = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                module.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="VIT构建代码"><a href="#VIT构建代码" class="headerlink" title="VIT构建代码"></a>VIT构建代码</h2><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment">#--------------------------------------#</span></span><br><span class="line"><span class="comment">#   Gelu激活函数的实现</span></span><br><span class="line"><span class="comment">#   利用近似的数学公式</span></span><br><span class="line"><span class="comment">#--------------------------------------#</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GELU</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(GELU, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span> * x * (<span class="number">1</span> + F.tanh(np.sqrt(<span class="number">2</span> / np.pi) * (x + <span class="number">0.044715</span> * torch.<span class="built_in">pow</span>(x,<span class="number">3</span>))))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">drop_path</span>(<span class="params">x, drop_prob: <span class="built_in">float</span> = <span class="number">0.</span>, training: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> drop_prob == <span class="number">0.</span> <span class="keyword">or</span> <span class="keyword">not</span> training:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    keep_prob       = <span class="number">1</span> - drop_prob</span><br><span class="line">    shape           = (x.shape[<span class="number">0</span>],) + (<span class="number">1</span>,) * (x.ndim - <span class="number">1</span>)</span><br><span class="line">    random_tensor   = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)</span><br><span class="line">    random_tensor.floor_() </span><br><span class="line">    output          = x.div(keep_prob) * random_tensor</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DropPath</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop_prob=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DropPath, self).__init__()</span><br><span class="line">        self.drop_prob = drop_prob</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> drop_path(x, self.drop_prob, self.training)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_shape=[<span class="number">224</span>, <span class="number">224</span>], patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, num_features=<span class="number">768</span>, norm_layer=<span class="literal">None</span>, flatten=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_patches    = (input_shape[<span class="number">0</span>] // patch_size) * (input_shape[<span class="number">1</span>] // patch_size)</span><br><span class="line">        self.flatten        = flatten</span><br><span class="line"></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, num_features, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        self.norm = norm_layer(num_features) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        <span class="keyword">if</span> self.flatten:</span><br><span class="line">            x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># BCHW -&gt; BNC</span></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line"><span class="comment">#   Attention机制</span></span><br><span class="line"><span class="comment">#   将输入的特征qkv特征进行划分，首先生成query, key, value。query是查询向量、key是键向量、v是值向量。</span></span><br><span class="line"><span class="comment">#   然后利用 查询向量query 叉乘 转置后的键向量key，这一步可以通俗的理解为，利用查询向量去查询序列的特征，获得序列每个部分的重要程度score。</span></span><br><span class="line"><span class="comment">#   然后利用 score 叉乘 value，这一步可以通俗的理解为，将序列每个部分的重要程度重新施加到序列的值上去。</span></span><br><span class="line"><span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads=<span class="number">8</span>, qkv_bias=<span class="literal">False</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_heads  = num_heads</span><br><span class="line">        self.scale      = (dim // num_heads) ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.qkv        = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop  = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj       = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop  = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, N, C     = x.shape</span><br><span class="line">        qkv         = self.qkv(x).reshape(B, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        q, k, v     = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale</span><br><span class="line">        attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="string">""" MLP as used in Vision Transformer, MLP-Mixer and related networks</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=GELU, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features    = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        drop_probs      = (drop, drop)</span><br><span class="line"></span><br><span class="line">        self.fc1    = nn.Linear(in_features, hidden_features)</span><br><span class="line">        self.act    = act_layer()</span><br><span class="line">        self.drop1  = nn.Dropout(drop_probs[<span class="number">0</span>])</span><br><span class="line">        self.fc2    = nn.Linear(hidden_features, out_features)</span><br><span class="line">        self.drop2  = nn.Dropout(drop_probs[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">False</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path=<span class="number">0.</span>, act_layer=GELU, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm1      = norm_layer(dim)</span><br><span class="line">        self.attn       = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line">        self.norm2      = norm_layer(dim)</span><br><span class="line">        self.mlp        = Mlp(in_features=dim, hidden_features=<span class="built_in">int</span>(dim * mlp_ratio), act_layer=act_layer, drop=drop)</span><br><span class="line">        self.drop_path  = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + self.drop_path(self.attn(self.norm1(x)))</span><br><span class="line">        x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self, input_shape=[<span class="number">224</span>, <span class="number">224</span>], patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>, num_features=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">            depth=<span class="number">12</span>, num_heads=<span class="number">12</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, drop_rate=<span class="number">0.1</span>, attn_drop_rate=<span class="number">0.1</span>, drop_path_rate=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">            norm_layer=partial(<span class="params">nn.LayerNorm, eps=<span class="number">1e-6</span></span>), act_layer=GELU</span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   224, 224, 3 -&gt; 196, 768</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        self.patch_embed    = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features)</span><br><span class="line">        num_patches         = (<span class="number">224</span> // patch_size) * (<span class="number">224</span> // patch_size)</span><br><span class="line">        self.num_features   = num_features</span><br><span class="line">        self.new_feature_shape = [<span class="built_in">int</span>(input_shape[<span class="number">0</span>] // patch_size), <span class="built_in">int</span>(input_shape[<span class="number">1</span>] // patch_size)]</span><br><span class="line">        self.old_feature_shape = [<span class="built_in">int</span>(<span class="number">224</span> // patch_size), <span class="built_in">int</span>(<span class="number">224</span> // patch_size)]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">#   在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。</span></span><br><span class="line">        <span class="comment">#   此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。</span></span><br><span class="line">        <span class="comment">#   在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。</span></span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   196, 768 -&gt; 197, 768</span></span><br><span class="line">        self.cls_token      = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, num_features))</span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   为网络提取到的特征添加上位置信息。</span></span><br><span class="line">        <span class="comment">#   以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768</span></span><br><span class="line">        <span class="comment">#   此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。</span></span><br><span class="line">        <span class="comment">#--------------------------------------------------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   197, 768 -&gt; 197, 768</span></span><br><span class="line">        self.pos_embed      = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, num_features))</span><br><span class="line">        self.pos_drop       = nn.Dropout(p=drop_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   197, 768 -&gt; 197, 768  12次</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, depth)]</span><br><span class="line">        self.blocks = nn.Sequential(</span><br><span class="line">            *[</span><br><span class="line">                Block(</span><br><span class="line">                    dim         = num_features, </span><br><span class="line">                    num_heads   = num_heads, </span><br><span class="line">                    mlp_ratio   = mlp_ratio, </span><br><span class="line">                    qkv_bias    = qkv_bias, </span><br><span class="line">                    drop        = drop_rate,</span><br><span class="line">                    attn_drop   = attn_drop_rate, </span><br><span class="line">                    drop_path   = dpr[i], </span><br><span class="line">                    norm_layer  = norm_layer, </span><br><span class="line">                    act_layer   = act_layer</span><br><span class="line">                )<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">        self.norm = norm_layer(num_features)</span><br><span class="line">        self.head = nn.Linear(num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.patch_embed(x)</span><br><span class="line">        cls_token = self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>) </span><br><span class="line">        x = torch.cat((cls_token, x), dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        cls_token_pe = self.pos_embed[:, <span class="number">0</span>:<span class="number">1</span>, :]</span><br><span class="line">        img_token_pe = self.pos_embed[:, <span class="number">1</span>: , :]</span><br><span class="line"></span><br><span class="line">        img_token_pe = img_token_pe.view(<span class="number">1</span>, *self.old_feature_shape, -<span class="number">1</span>).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode=<span class="string">'bicubic'</span>, align_corners=<span class="literal">False</span>)</span><br><span class="line">        img_token_pe = img_token_pe.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).flatten(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = self.pos_drop(x + pos_embed)</span><br><span class="line">        x = self.blocks(x)</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.forward_features(x)</span><br><span class="line">        x = self.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">freeze_backbone</span>(<span class="params">self</span>):</span><br><span class="line">        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:<span class="number">8</span>]]</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> backbone:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> module.parameters():</span><br><span class="line">                    param.requires_grad = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                module.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Unfreeze_backbone</span>(<span class="params">self</span>):</span><br><span class="line">        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:<span class="number">8</span>]]</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> backbone:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> module.parameters():</span><br><span class="line">                    param.requires_grad = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                module.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vit</span>(<span class="params">input_shape=[<span class="number">224</span>, <span class="number">224</span>], pretrained=<span class="literal">False</span>, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">    model = VisionTransformer(input_shape)</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        model.load_state_dict(torch.load(<span class="string">"model_data/vit-patch_16.pth"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> num_classes!=<span class="number">1000</span>:</span><br><span class="line">        model.head = nn.Linear(model.num_features, num_classes)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>yolov3论文笔记</title>
    <url>/zh-CN/YOLOV3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文地址：<a href="https://arxiv.org/pdf/1804.02767.pdf">https://arxiv.org/pdf/1804.02767.pdf</a></p>
<p>源码地址：<a href="https://github.com/ultralytics/yolov3">ultralytics/yolov3</a></p>
<p>文章引用源码：<a href="https://github.com/bubbliiiing/yolo3-pytorch">https://github.com/bubbliiiing/yolo3-pytorch</a></p>
<p>文章出处：<a href="https://blog.csdn.net/weixin_44791964/article/details/105310627">https://blog.csdn.net/weixin_44791964/article/details/105310627</a></p>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><h3 id="预测部分"><a href="#预测部分" class="headerlink" title="预测部分"></a>预测部分</h3><h4 id="主干网络Darknet53"><a href="#主干网络Darknet53" class="headerlink" title="主干网络Darknet53"></a>主干网络Darknet53</h4><p><style>.nbhidgzowutu{}</style><img src="/zh-CN/YOLOV3/YOLOV3/image-20220416194945035.png" class="lazyload" data-srcset="/zh-CN/YOLOV3/YOLOV3/image-20220416194945035.png" srcset="data:image/png;base64,666" class="nbhidgzowutu lazyload"></p>
<p>YoloV3所使用的主干特征提取网络为Darknet53，它具有两个重要特点：<br>1、Darknet53具有一个重要特点是使用了残差网络Residual，Darknet53中的残差卷积就是首先进行一次卷积核大小为3X3、步长为2的卷积，该卷积会压缩输入进来的特征层的宽和高，此时我们可以获得一个特征层，我们将该特征层命名为layer。之后我们再对该特征层进行一次1X1的卷积和一次3X3的卷积，并把这个结果加上layer，此时我们便构成了残差结构。通过不断的1X1卷积和3X3卷积以及残差边的叠加，我们便大幅度的加深了网络。残差网络的特点是容易优化，并且能够通过增加相当的深度来提高准确率。其内部的残差块使用了跳跃连接，缓解了在深度神经网络中增加深度带来的梯度消失问题。</p>
<p>2、Darknet53的每一个卷积部分使用了特有的DarknetConv2D结构，每一次卷积的时候进行l2正则化，完成卷积后进行BatchNormalization标准化与LeakyReLU。普通的ReLU是将所有的负值都设为零，Leaky ReLU则是给所有负值赋予一个非零斜率。以数学的方式我们可以表示为：</p>
<p><style>.hhdxkvhenwvr{}</style><img src="/zh-CN/YOLOV3/YOLOV3/image-20220419102411716.png" class="lazyload" data-srcset="/zh-CN/YOLOV3/YOLOV3/image-20220419102411716.png" srcset="data:image/png;base64,666" class="hhdxkvhenwvr lazyload"></p>
<p>实现代码：<br></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">python</span><br><span class="line"># 详情见nets/darknet.py</span><br><span class="line">#---------------------------------------------------------------------#</span><br><span class="line">#   残差结构</span><br><span class="line">#   利用一个1x1卷积下降通道数，然后利用一个3x3卷积提取特征并且上升通道数</span><br><span class="line">#   最后接上一个残差边</span><br><span class="line">#---------------------------------------------------------------------#</span><br><span class="line">class BasicBlock(nn.Module):</span><br><span class="line">    def __init__(self, inplanes, planes):</span><br><span class="line">        super(BasicBlock, self).__init__()</span><br><span class="line">        self.conv1  = nn.Conv2d(inplanes, planes[0], kernel_size=1, stride=1, padding=0, bias=False)</span><br><span class="line">        self.bn1    = nn.BatchNorm2d(planes[0])</span><br><span class="line">        self.relu1  = nn.LeakyReLU(0.1)</span><br><span class="line">        </span><br><span class="line">        self.conv2  = nn.Conv2d(planes[0], planes[1], kernel_size=3, stride=1, padding=1, bias=False)</span><br><span class="line">        self.bn2    = nn.BatchNorm2d(planes[1])</span><br><span class="line">        self.relu2  = nn.LeakyReLU(0.1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu1(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu2(out)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line">class DarkNet(nn.Module):</span><br><span class="line">    def __init__(self, layers):</span><br><span class="line">        super(DarkNet, self).__init__()</span><br><span class="line">        self.inplanes = 32</span><br><span class="line">        # 416,416,3 -&gt; 416,416,32</span><br><span class="line">        self.conv1  = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)</span><br><span class="line">        self.bn1    = nn.BatchNorm2d(self.inplanes)</span><br><span class="line">        self.relu1  = nn.LeakyReLU(0.1)</span><br><span class="line"></span><br><span class="line">        # 416,416,32 -&gt; 208,208,64</span><br><span class="line">        self.layer1 = self._make_layer([32, 64], layers[0])</span><br><span class="line">        # 208,208,64 -&gt; 104,104,128</span><br><span class="line">        self.layer2 = self._make_layer([64, 128], layers[1])</span><br><span class="line">        # 104,104,128 -&gt; 52,52,256</span><br><span class="line">        self.layer3 = self._make_layer([128, 256], layers[2])</span><br><span class="line">        # 52,52,256 -&gt; 26,26,512</span><br><span class="line">        self.layer4 = self._make_layer([256, 512], layers[3])</span><br><span class="line">        # 26,26,512 -&gt; 13,13,1024</span><br><span class="line">        self.layer5 = self._make_layer([512, 1024], layers[4])</span><br><span class="line"></span><br><span class="line">        self.layers_out_filters = [64, 128, 256, 512, 1024]</span><br><span class="line"></span><br><span class="line">        # 进行权值初始化</span><br><span class="line">        for m in self.modules():</span><br><span class="line">            if isinstance(m, nn.Conv2d):</span><br><span class="line">                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels</span><br><span class="line">                m.weight.data.normal_(0, math.sqrt(2. / n))</span><br><span class="line">            elif isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(1)</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line"></span><br><span class="line">    #---------------------------------------------------------------------#</span><br><span class="line">    #   在每一个layer里面，首先利用一个步长为2的3x3卷积进行下采样</span><br><span class="line">    #   然后进行残差结构的堆叠</span><br><span class="line">    #---------------------------------------------------------------------#</span><br><span class="line">    def _make_layer(self, planes, blocks):</span><br><span class="line">        layers = []</span><br><span class="line">        # 下采样，步长为2，卷积核大小为3</span><br><span class="line">        layers.append(("ds_conv", nn.Conv2d(self.inplanes, planes[1], kernel_size=3, stride=2, padding=1, bias=False)))</span><br><span class="line">        layers.append(("ds_bn", nn.BatchNorm2d(planes[1])))</span><br><span class="line">        layers.append(("ds_relu", nn.LeakyReLU(0.1)))</span><br><span class="line">        # 加入残差结构</span><br><span class="line">        self.inplanes = planes[1]</span><br><span class="line">        for i in range(0, blocks):</span><br><span class="line">            layers.append(("residual_{}".format(i), BasicBlock(self.inplanes, planes)))</span><br><span class="line">        return nn.Sequential(OrderedDict(layers))</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu1(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        out3 = self.layer3(x)</span><br><span class="line">        out4 = self.layer4(out3)</span><br><span class="line">        out5 = self.layer5(out4)</span><br><span class="line"></span><br><span class="line">        return out3, out4, out5</span><br><span class="line"></span><br><span class="line">def darknet53():</span><br><span class="line">    model = DarkNet([1, 2, 8, 8, 4])</span><br><span class="line">    return model</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p></p>
<h4 id="从特征层获取预测结果"><a href="#从特征层获取预测结果" class="headerlink" title="从特征层获取预测结果"></a>从特征层获取预测结果</h4><p><style>.pebshjobcjan{}</style><img src="/zh-CN/YOLOV3/YOLOV3/image-20220416194945035.png" class="lazyload" data-srcset="/zh-CN/YOLOV3/YOLOV3/image-20220416194945035.png" srcset="data:image/png;base64,666" class="pebshjobcjan lazyload"></p>
<p>从特征获取预测结果的过程可以分为两个部分，分别是：</p>
<p>·构建FPN特征金字塔进行加强特征提取。<br>·利用Yolo Head对三个有效特征层进行预测。<br>a）构建FPN特征金字塔进行加强特征提取<br>在特征利用部分，YoloV3提取多特征层进行目标检测，一共提取三个特征层。三个特征层位于主干部分Darknet53的不同位置，分别位于中间层，中下层，底层，三个特征层的shape分别为(52,52,256)、(26,26,512)、(13,13,1024)。</p>
<p>在获得三个有效特征层后，我们利用这三个有效特征层进行FPN层的构建，构建方式为：</p>
<p>·13x13x1024的特征层进行5次卷积处理，处理完后利用YoloHead获得预测结果，一部分用于进行上采样UmSampling2d后与26x26x512特征层进行结合，结合特征层的shape为(26,26,768)。<br>·结合特征层再次进行5次卷积处理，处理完后利用YoloHead获得预测结果，一部分用于进行上采样UmSampling2d后与52x52x256特征层进行结合，结合特征层的shape为(52,52,384)。<br>·结合特征层再次进行5次卷积处理，处理完后利用YoloHead获得预测结果。<br>特征金字塔可以将不同shape的特征层进行特征融合，有利于提取出更好的特征。</p>
<p>b）利用Yolo Head获得预测结果<br>利用FPN特征金字塔，我们可以获得三个加强特征，这三个加强特征的shape分别为(13,13,512)、(26,26,256)、(52,52,128)，然后我们利用这三个shape的特征层传入Yolo Head获得预测结果。</p>
<p>Yolo Head本质上是一次3x3卷积加上一次1x1卷积，3x3卷积的作用是特征整合，1x1卷积的作用是调整通道数。</p>
<p>对三个特征层分别进行处理，假设我们预测是的VOC数据集，我们的输出层的shape分别为(13,13,75)，(26,26,75)，(52,52,75)，最后一个维度为75是因为该图是基于voc数据集的，它的类为20种，YoloV3针对每一个特征层的每一个特征点存在3个先验框，所以预测结果的通道数为3x25；<br>如果使用的是coco训练集，类则为80种，最后的维度应该为255 = 3x85，三个特征层的shape为(13,13,255)，(26,26,255)，(52,52,255)</p>
<p>其实际情况就是，输入N张416x416的图片，在经过多层的运算后，会输出三个shape分别为(N,13,13,255)，(N,26,26,255)，(N,52,52,255)的数据，对应每个图分为13x13、26x26、52x52的网格上3个先验框的位置。</p>
<p>实现代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 详情见nets/yolo.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d</span>(<span class="params">filter_in, filter_out, kernel_size</span>):</span><br><span class="line">    pad = (kernel_size - <span class="number">1</span>) // <span class="number">2</span> <span class="keyword">if</span> kernel_size <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(OrderedDict([</span><br><span class="line">        (<span class="string">"conv"</span>, nn.Conv2d(filter_in, filter_out, kernel_size=kernel_size, stride=<span class="number">1</span>, padding=pad, bias=<span class="literal">False</span>)),</span><br><span class="line">        (<span class="string">"bn"</span>, nn.BatchNorm2d(filter_out)),</span><br><span class="line">        (<span class="string">"relu"</span>, nn.LeakyReLU(<span class="number">0.1</span>)),</span><br><span class="line">    ]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#------------------------------------------------------------------------#</span></span><br><span class="line"><span class="comment">#   make_last_layers里面一共有七个卷积，前五个用于提取特征。</span></span><br><span class="line"><span class="comment">#   后两个用于获得yolo网络的预测结果</span></span><br><span class="line"><span class="comment">#------------------------------------------------------------------------#</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_last_layers</span>(<span class="params">filters_list, in_filters, out_filter</span>):</span><br><span class="line">    m = nn.Sequential(</span><br><span class="line">        conv2d(in_filters, filters_list[<span class="number">0</span>], <span class="number">1</span>),</span><br><span class="line">        conv2d(filters_list[<span class="number">0</span>], filters_list[<span class="number">1</span>], <span class="number">3</span>),</span><br><span class="line">        conv2d(filters_list[<span class="number">1</span>], filters_list[<span class="number">0</span>], <span class="number">1</span>),</span><br><span class="line">        conv2d(filters_list[<span class="number">0</span>], filters_list[<span class="number">1</span>], <span class="number">3</span>),</span><br><span class="line">        conv2d(filters_list[<span class="number">1</span>], filters_list[<span class="number">0</span>], <span class="number">1</span>),</span><br><span class="line">        conv2d(filters_list[<span class="number">0</span>], filters_list[<span class="number">1</span>], <span class="number">3</span>),</span><br><span class="line">        nn.Conv2d(filters_list[<span class="number">1</span>], out_filter, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> m</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YoloBody</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, anchors_mask, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(YoloBody, self).__init__()</span><br><span class="line">        <span class="comment">#---------------------------------------------------#   </span></span><br><span class="line">        <span class="comment">#   生成darknet53的主干模型</span></span><br><span class="line">        <span class="comment">#   获得三个有效特征层，他们的shape分别是：</span></span><br><span class="line">        <span class="comment">#   52,52,256</span></span><br><span class="line">        <span class="comment">#   26,26,512</span></span><br><span class="line">        <span class="comment">#   13,13,1024</span></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        self.backbone = darknet53()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   out_filters : [64, 128, 256, 512, 1024]</span></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        out_filters = self.backbone.layers_out_filters</span><br><span class="line"></span><br><span class="line">        <span class="comment">#------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算yolo_head的输出通道数，对于voc数据集而言</span></span><br><span class="line">        <span class="comment">#   final_out_filter0 = final_out_filter1 = final_out_filter2 = 75</span></span><br><span class="line">        <span class="comment">#------------------------------------------------------------------------#</span></span><br><span class="line">        self.last_layer0            = make_last_layers([<span class="number">512</span>, <span class="number">1024</span>], out_filters[-<span class="number">1</span>], <span class="built_in">len</span>(anchors_mask[<span class="number">0</span>]) * (num_classes + <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">        self.last_layer1_conv       = conv2d(<span class="number">512</span>, <span class="number">256</span>, <span class="number">1</span>)</span><br><span class="line">        self.last_layer1_upsample   = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">'nearest'</span>)</span><br><span class="line">        self.last_layer1            = make_last_layers([<span class="number">256</span>, <span class="number">512</span>], out_filters[-<span class="number">2</span>] + <span class="number">256</span>, <span class="built_in">len</span>(anchors_mask[<span class="number">1</span>]) * (num_classes + <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">        self.last_layer2_conv       = conv2d(<span class="number">256</span>, <span class="number">128</span>, <span class="number">1</span>)</span><br><span class="line">        self.last_layer2_upsample   = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">'nearest'</span>)</span><br><span class="line">        self.last_layer2            = make_last_layers([<span class="number">128</span>, <span class="number">256</span>], out_filters[-<span class="number">3</span>] + <span class="number">128</span>, <span class="built_in">len</span>(anchors_mask[<span class="number">2</span>]) * (num_classes + <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#---------------------------------------------------#   </span></span><br><span class="line">        <span class="comment">#   获得三个有效特征层，他们的shape分别是：</span></span><br><span class="line">        <span class="comment">#   52,52,256；26,26,512；13,13,1024</span></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        x2, x1, x0 = self.backbone(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   第一个特征层</span></span><br><span class="line">        <span class="comment">#   out0 = (batch_size,255,13,13)</span></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment"># 13,13,1024 -&gt; 13,13,512 -&gt; 13,13,1024 -&gt; 13,13,512 -&gt; 13,13,1024 -&gt; 13,13,512</span></span><br><span class="line">        out0_branch = self.last_layer0[:<span class="number">5</span>](x0)</span><br><span class="line">        out0        = self.last_layer0[<span class="number">5</span>:](out0_branch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 13,13,512 -&gt; 13,13,256 -&gt; 26,26,256</span></span><br><span class="line">        x1_in = self.last_layer1_conv(out0_branch)</span><br><span class="line">        x1_in = self.last_layer1_upsample(x1_in)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 26,26,256 + 26,26,512 -&gt; 26,26,768</span></span><br><span class="line">        x1_in = torch.cat([x1_in, x1], <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   第二个特征层</span></span><br><span class="line">        <span class="comment">#   out1 = (batch_size,255,26,26)</span></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment"># 26,26,768 -&gt; 26,26,256 -&gt; 26,26,512 -&gt; 26,26,256 -&gt; 26,26,512 -&gt; 26,26,256</span></span><br><span class="line">        out1_branch = self.last_layer1[:<span class="number">5</span>](x1_in)</span><br><span class="line">        out1        = self.last_layer1[<span class="number">5</span>:](out1_branch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 26,26,256 -&gt; 26,26,128 -&gt; 52,52,128</span></span><br><span class="line">        x2_in = self.last_layer2_conv(out1_branch)</span><br><span class="line">        x2_in = self.last_layer2_upsample(x2_in)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 52,52,128 + 52,52,256 -&gt; 52,52,384</span></span><br><span class="line">        x2_in = torch.cat([x2_in, x2], <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   第一个特征层</span></span><br><span class="line">        <span class="comment">#   out3 = (batch_size,255,52,52)</span></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment"># 52,52,384 -&gt; 52,52,128 -&gt; 52,52,256 -&gt; 52,52,128 -&gt; 52,52,256 -&gt; 52,52,128</span></span><br><span class="line">        out2 = self.last_layer2(x2_in)</span><br><span class="line">        <span class="keyword">return</span> out0, out1, out2</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="预测结果的解码"><a href="#预测结果的解码" class="headerlink" title="预测结果的解码"></a>预测结果的解码</h4><p>由第二步我们可以获得三个特征层的预测结果，shape分别为：</p>
<p>·(N,13,13,255)<br>·(N,26,26,255)<br>·(N,52,52,255)<br>在这里我们简单了解一下每个有效特征层到底做了什么：<br>每一个有效特征层将整个图片分成与其长宽对应的网格，如(N,13,13,255)的特征层就是将整个图像分成13x13个网格；然后从每个网格中心建立多个先验框，这些框是网络预先设定好的框，网络的预测结果会判断这些框内是否包含物体，以及这个物体的种类。</p>
<p>由于每一个网格点都具有三个先验框，所以上述的预测结果可以reshape为：</p>
<p>(N,13,13,3,85)<br>(N,26,26,3,85)<br>(N,52,52,3,85)<br>其中的85可以拆分为4+1+80，其中的4代表先验框的调整参数，1代表先验框内是否包含物体，80代表的是这个先验框的种类，由于coco分了80类，所以这里是80。如果YoloV3只检测两类物体，那么这个85就变为了4+1+2 = 7。</p>
<p>即85包含了4+1+80，分别代表x_offset、y_offset、h和w、置信度、分类结果。</p>
<p>但是这个预测结果并不对应着最终的预测框在图片上的位置，还需要解码才可以完成。</p>
<p>YoloV3的解码过程分为两步：</p>
<p>·先将每个网格点加上它对应的x_offset和y_offset，加完后的结果就是预测框的中心。<br>·然后再利用 先验框和h、w结合 计算出预测框的宽高。这样就能得到整个预测框的位置了。</p>
<p>得到最终的预测结果后还要进行<strong>得分排序与非极大抑制筛选</strong>。</p>
<p>这一部分基本上是所有目标检测通用的部分。其对于每一个类进行判别：<br><strong>1、取出每一类得分大于self.obj_threshold的框和得分。<br>2、利用框的位置和得分进行非极大抑制。</strong></p>
<p>实现代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 详情见utils/utils_bbox.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecodeBox</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, anchors, num_classes, input_shape, anchors_mask = [[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>], [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>], [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]]</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecodeBox, self).__init__()</span><br><span class="line">        self.anchors        = anchors</span><br><span class="line">        self.num_classes    = num_classes</span><br><span class="line">        self.bbox_attrs     = <span class="number">5</span> + num_classes</span><br><span class="line">        self.input_shape    = input_shape</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   13x13的特征层对应的anchor是[116,90],[156,198],[373,326]</span></span><br><span class="line">        <span class="comment">#   26x26的特征层对应的anchor是[30,61],[62,45],[59,119]</span></span><br><span class="line">        <span class="comment">#   52x52的特征层对应的anchor是[10,13],[16,30],[33,23]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        self.anchors_mask   = anchors_mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode_box</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        outputs = []</span><br><span class="line">        <span class="keyword">for</span> i, <span class="built_in">input</span> <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs):</span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   输入的input一共有三个，他们的shape分别是</span></span><br><span class="line">            <span class="comment">#   batch_size, 255, 13, 13</span></span><br><span class="line">            <span class="comment">#   batch_size, 255, 26, 26</span></span><br><span class="line">            <span class="comment">#   batch_size, 255, 52, 52</span></span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            batch_size      = <span class="built_in">input</span>.size(<span class="number">0</span>)</span><br><span class="line">            input_height    = <span class="built_in">input</span>.size(<span class="number">2</span>)</span><br><span class="line">            input_width     = <span class="built_in">input</span>.size(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   输入为416x416时</span></span><br><span class="line">            <span class="comment">#   stride_h = stride_w = 32、16、8</span></span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            stride_h = self.input_shape[<span class="number">0</span>] / input_height</span><br><span class="line">            stride_w = self.input_shape[<span class="number">1</span>] / input_width</span><br><span class="line">            <span class="comment">#-------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   此时获得的scaled_anchors大小是相对于特征层的</span></span><br><span class="line">            <span class="comment">#-------------------------------------------------#</span></span><br><span class="line">            scaled_anchors = [(anchor_width / stride_w, anchor_height / stride_h) <span class="keyword">for</span> anchor_width, anchor_height <span class="keyword">in</span> self.anchors[self.anchors_mask[i]]]</span><br><span class="line"></span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   输入的input一共有三个，他们的shape分别是</span></span><br><span class="line">            <span class="comment">#   batch_size, 3, 13, 13, 85</span></span><br><span class="line">            <span class="comment">#   batch_size, 3, 26, 26, 85</span></span><br><span class="line">            <span class="comment">#   batch_size, 3, 52, 52, 85</span></span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            prediction = <span class="built_in">input</span>.view(batch_size, <span class="built_in">len</span>(self.anchors_mask[i]),</span><br><span class="line">                                    self.bbox_attrs, input_height, input_width).permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>).contiguous()</span><br><span class="line"></span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   先验框的中心位置的调整参数</span></span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            x = torch.sigmoid(prediction[..., <span class="number">0</span>])  </span><br><span class="line">            y = torch.sigmoid(prediction[..., <span class="number">1</span>])</span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   先验框的宽高调整参数</span></span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            w = prediction[..., <span class="number">2</span>]</span><br><span class="line">            h = prediction[..., <span class="number">3</span>]</span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   获得置信度，是否有物体</span></span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            conf        = torch.sigmoid(prediction[..., <span class="number">4</span>])</span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   种类置信度</span></span><br><span class="line">            <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">            pred_cls    = torch.sigmoid(prediction[..., <span class="number">5</span>:])</span><br><span class="line"></span><br><span class="line">            FloatTensor = torch.cuda.FloatTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line">            LongTensor  = torch.cuda.LongTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.LongTensor</span><br><span class="line"></span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   生成网格，先验框中心，网格左上角 </span></span><br><span class="line">            <span class="comment">#   batch_size,3,13,13</span></span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            grid_x = torch.linspace(<span class="number">0</span>, input_width - <span class="number">1</span>, input_width).repeat(input_height, <span class="number">1</span>).repeat(</span><br><span class="line">                batch_size * <span class="built_in">len</span>(self.anchors_mask[i]), <span class="number">1</span>, <span class="number">1</span>).view(x.shape).<span class="built_in">type</span>(FloatTensor)</span><br><span class="line">            grid_y = torch.linspace(<span class="number">0</span>, input_height - <span class="number">1</span>, input_height).repeat(input_width, <span class="number">1</span>).t().repeat(</span><br><span class="line">                batch_size * <span class="built_in">len</span>(self.anchors_mask[i]), <span class="number">1</span>, <span class="number">1</span>).view(y.shape).<span class="built_in">type</span>(FloatTensor)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   按照网格格式生成先验框的宽高</span></span><br><span class="line">            <span class="comment">#   batch_size,3,13,13</span></span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            anchor_w = FloatTensor(scaled_anchors).index_select(<span class="number">1</span>, LongTensor([<span class="number">0</span>]))</span><br><span class="line">            anchor_h = FloatTensor(scaled_anchors).index_select(<span class="number">1</span>, LongTensor([<span class="number">1</span>]))</span><br><span class="line">            anchor_w = anchor_w.repeat(batch_size, <span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, input_height * input_width).view(w.shape)</span><br><span class="line">            anchor_h = anchor_h.repeat(batch_size, <span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, input_height * input_width).view(h.shape)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   利用预测结果对先验框进行调整</span></span><br><span class="line">            <span class="comment">#   首先调整先验框的中心，从先验框中心向右下角偏移</span></span><br><span class="line">            <span class="comment">#   再调整先验框的宽高。</span></span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            pred_boxes          = FloatTensor(prediction[..., :<span class="number">4</span>].shape)</span><br><span class="line">            pred_boxes[..., <span class="number">0</span>]  = x.data + grid_x</span><br><span class="line">            pred_boxes[..., <span class="number">1</span>]  = y.data + grid_y</span><br><span class="line">            pred_boxes[..., <span class="number">2</span>]  = torch.exp(w.data) * anchor_w</span><br><span class="line">            pred_boxes[..., <span class="number">3</span>]  = torch.exp(h.data) * anchor_h</span><br><span class="line"></span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   将输出结果归一化成小数的形式</span></span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            _scale = torch.Tensor([input_width, input_height, input_width, input_height]).<span class="built_in">type</span>(FloatTensor)</span><br><span class="line">            output = torch.cat((pred_boxes.view(batch_size, -<span class="number">1</span>, <span class="number">4</span>) / _scale,</span><br><span class="line">                                conf.view(batch_size, -<span class="number">1</span>, <span class="number">1</span>), pred_cls.view(batch_size, -<span class="number">1</span>, self.num_classes)), -<span class="number">1</span>)</span><br><span class="line">            outputs.append(output.data)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">yolo_correct_boxes</span>(<span class="params">self, box_xy, box_wh, input_shape, image_shape, letterbox_image</span>):</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   把y轴放前面是因为方便预测框和图像的宽高进行相乘</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------#</span></span><br><span class="line">        box_yx = box_xy[..., ::-<span class="number">1</span>]</span><br><span class="line">        box_hw = box_wh[..., ::-<span class="number">1</span>]</span><br><span class="line">        input_shape = np.array(input_shape)</span><br><span class="line">        image_shape = np.array(image_shape)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> letterbox_image:</span><br><span class="line">            <span class="comment">#-----------------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   这里求出来的offset是图像有效区域相对于图像左上角的偏移情况</span></span><br><span class="line">            <span class="comment">#   new_shape指的是宽高缩放情况</span></span><br><span class="line">            <span class="comment">#-----------------------------------------------------------------#</span></span><br><span class="line">            new_shape = np.<span class="built_in">round</span>(image_shape * np.<span class="built_in">min</span>(input_shape/image_shape))</span><br><span class="line">            offset  = (input_shape - new_shape)/<span class="number">2.</span>/input_shape</span><br><span class="line">            scale   = input_shape/new_shape</span><br><span class="line"></span><br><span class="line">            box_yx  = (box_yx - offset) * scale</span><br><span class="line">            box_hw *= scale</span><br><span class="line"></span><br><span class="line">        box_mins    = box_yx - (box_hw / <span class="number">2.</span>)</span><br><span class="line">        box_maxes   = box_yx + (box_hw / <span class="number">2.</span>)</span><br><span class="line">        boxes  = np.concatenate([box_mins[..., <span class="number">0</span>:<span class="number">1</span>], box_mins[..., <span class="number">1</span>:<span class="number">2</span>], box_maxes[..., <span class="number">0</span>:<span class="number">1</span>], box_maxes[..., <span class="number">1</span>:<span class="number">2</span>]], axis=-<span class="number">1</span>)</span><br><span class="line">        boxes *= np.concatenate([image_shape, image_shape], axis=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> boxes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">non_max_suppression</span>(<span class="params">self, prediction, num_classes, input_shape, image_shape, letterbox_image, conf_thres=<span class="number">0.5</span>, nms_thres=<span class="number">0.4</span></span>):</span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   将预测结果的格式转换成左上角右下角的格式。</span></span><br><span class="line">        <span class="comment">#   prediction  [batch_size, num_anchors, 85]</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        box_corner          = prediction.new(prediction.shape)</span><br><span class="line">        box_corner[:, :, <span class="number">0</span>] = prediction[:, :, <span class="number">0</span>] - prediction[:, :, <span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">        box_corner[:, :, <span class="number">1</span>] = prediction[:, :, <span class="number">1</span>] - prediction[:, :, <span class="number">3</span>] / <span class="number">2</span></span><br><span class="line">        box_corner[:, :, <span class="number">2</span>] = prediction[:, :, <span class="number">0</span>] + prediction[:, :, <span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">        box_corner[:, :, <span class="number">3</span>] = prediction[:, :, <span class="number">1</span>] + prediction[:, :, <span class="number">3</span>] / <span class="number">2</span></span><br><span class="line">        prediction[:, :, :<span class="number">4</span>] = box_corner[:, :, :<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">        output = [<span class="literal">None</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prediction))]</span><br><span class="line">        <span class="keyword">for</span> i, image_pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(prediction):</span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   对种类预测部分取max。</span></span><br><span class="line">            <span class="comment">#   class_conf  [num_anchors, 1]    种类置信度</span></span><br><span class="line">            <span class="comment">#   class_pred  [num_anchors, 1]    种类</span></span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            class_conf, class_pred = torch.<span class="built_in">max</span>(image_pred[:, <span class="number">5</span>:<span class="number">5</span> + num_classes], <span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   利用置信度进行第一轮筛选</span></span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            conf_mask = (image_pred[:, <span class="number">4</span>] * class_conf[:, <span class="number">0</span>] &gt;= conf_thres).squeeze()</span><br><span class="line"></span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   根据置信度进行预测结果的筛选</span></span><br><span class="line">            <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">            image_pred = image_pred[conf_mask]</span><br><span class="line">            class_conf = class_conf[conf_mask]</span><br><span class="line">            class_pred = class_pred[conf_mask]</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> image_pred.size(<span class="number">0</span>):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment">#-------------------------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   detections  [num_anchors, 7]</span></span><br><span class="line">            <span class="comment">#   7的内容为：x1, y1, x2, y2, obj_conf, class_conf, class_pred</span></span><br><span class="line">            <span class="comment">#-------------------------------------------------------------------------#</span></span><br><span class="line">            detections = torch.cat((image_pred[:, :<span class="number">5</span>], class_conf.<span class="built_in">float</span>(), class_pred.<span class="built_in">float</span>()), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   获得预测结果中包含的所有种类</span></span><br><span class="line">            <span class="comment">#------------------------------------------#</span></span><br><span class="line">            unique_labels = detections[:, -<span class="number">1</span>].cpu().unique()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> prediction.is_cuda:</span><br><span class="line">                unique_labels = unique_labels.cuda()</span><br><span class="line">                detections = detections.cuda()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> unique_labels:</span><br><span class="line">                <span class="comment">#------------------------------------------#</span></span><br><span class="line">                <span class="comment">#   获得某一类得分筛选后全部的预测结果</span></span><br><span class="line">                <span class="comment">#------------------------------------------#</span></span><br><span class="line">                detections_class = detections[detections[:, -<span class="number">1</span>] == c]</span><br><span class="line"></span><br><span class="line">                <span class="comment">#------------------------------------------#</span></span><br><span class="line">                <span class="comment">#   使用官方自带的非极大抑制会速度更快一些！</span></span><br><span class="line">                <span class="comment">#------------------------------------------#</span></span><br><span class="line">                keep = nms(</span><br><span class="line">                    detections_class[:, :<span class="number">4</span>],</span><br><span class="line">                    detections_class[:, <span class="number">4</span>] * detections_class[:, <span class="number">5</span>],</span><br><span class="line">                    nms_thres</span><br><span class="line">                )</span><br><span class="line">                max_detections = detections_class[keep]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># # 按照存在物体的置信度排序</span></span><br><span class="line">                <span class="comment"># _, conf_sort_index = torch.sort(detections_class[:, 4]*detections_class[:, 5], descending=True)</span></span><br><span class="line">                <span class="comment"># detections_class = detections_class[conf_sort_index]</span></span><br><span class="line">                <span class="comment"># # 进行非极大抑制</span></span><br><span class="line">                <span class="comment"># max_detections = []</span></span><br><span class="line">                <span class="comment"># while detections_class.size(0):</span></span><br><span class="line">                <span class="comment">#     # 取出这一类置信度最高的，一步一步往下判断，判断重合程度是否大于nms_thres，如果是则去除掉</span></span><br><span class="line">                <span class="comment">#     max_detections.append(detections_class[0].unsqueeze(0))</span></span><br><span class="line">                <span class="comment">#     if len(detections_class) == 1:</span></span><br><span class="line">                <span class="comment">#         break</span></span><br><span class="line">                <span class="comment">#     ious = bbox_iou(max_detections[-1], detections_class[1:])</span></span><br><span class="line">                <span class="comment">#     detections_class = detections_class[1:][ious &lt; nms_thres]</span></span><br><span class="line">                <span class="comment"># # 堆叠</span></span><br><span class="line">                <span class="comment"># max_detections = torch.cat(max_detections).data</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Add max detections to outputs</span></span><br><span class="line">                output[i] = max_detections <span class="keyword">if</span> output[i] <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> torch.cat((output[i], max_detections))</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> output[i] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                output[i]           = output[i].cpu().numpy()</span><br><span class="line">                box_xy, box_wh      = (output[i][:, <span class="number">0</span>:<span class="number">2</span>] + output[i][:, <span class="number">2</span>:<span class="number">4</span>])/<span class="number">2</span>, output[i][:, <span class="number">2</span>:<span class="number">4</span>] - output[i][:, <span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">                output[i][:, :<span class="number">4</span>]    = self.yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="原图上进行绘制"><a href="#原图上进行绘制" class="headerlink" title="原图上进行绘制"></a>原图上进行绘制</h4><p>通过第三步，我们可以获得预测框在原图上的位置，而且这些预测框都是经过筛选的。这些筛选后的框可以直接绘制在图片上，就可以获得结果了。</p>
<h3 id="训练部分"><a href="#训练部分" class="headerlink" title="训练部分"></a>训练部分</h3><h4 id="计算loss所需参数"><a href="#计算loss所需参数" class="headerlink" title="计算loss所需参数"></a>计算loss所需参数</h4><p>在计算loss的时候，实际上是pred和target之间的对比：<br>pred就是网络的预测结果。<br>target就是网络的真实框情况。</p>
<h4 id="pred是什么"><a href="#pred是什么" class="headerlink" title="pred是什么"></a>pred是什么</h4><p>对于yolo3的模型来说，网络最后输出的内容就是三个特征层每个网格点对应的预测框及其种类，即三个特征层分别对应着图片被分为不同size的网格后，每个网格点上三个先验框对应的位置、置信度及其种类。</p>
<p>输出层的shape分别为(13,13,75)，(26,26,75)，(52,52,75)，最后一个维度为75是因为是基于voc数据集的，它的类为20种，yolo3只有针对每一个特征层存在3个先验框，所以最后维度为3x25；<br>如果使用的是coco训练集，类则为80种，最后的维度应该为255 = 3x85，三个特征层的shape为(13,13,255)，(26,26,255)，(52,52,255)</p>
<p>现在的y_pre还是没有解码的，解码了之后才是真实图像上的情况。</p>
<h4 id="target是什么。"><a href="#target是什么。" class="headerlink" title="target是什么。"></a>target是什么。</h4><p>target就是一个真实图像中，真实框的情况。<br>第一个维度是batch_size，第二个维度是每一张图片里面真实框的数量，第三个维度内部是真实框的信息，包括位置以及种类。</p>
<h4 id="loss的计算过程"><a href="#loss的计算过程" class="headerlink" title="loss的计算过程"></a>loss的计算过程</h4><p>拿到pred和target后，不可以简单的减一下作为对比，需要进行如下步骤。</p>
<p>判断真实框在图片中的位置，判断其属于哪一个网格点去检测。判断真实框和这个特征点的哪个先验框重合程度最高。计算该网格点应该有怎么样的预测结果才能获得真实框，与真实框重合度最高的先验框被用于作为正样本。<br>根据网络的预测结果获得预测框，计算预测框和所有真实框的重合程度，如果重合程度大于一定门限，则将该预测框对应的先验框忽略。其余作为负样本。<br>最终损失由三个部分组成：a、正样本，编码后的长宽与xy轴偏移量与预测值的差距。b、正样本，预测结果中置信度的值与1对比；负样本，预测结果中置信度的值与0对比。c、实际存在的框，种类预测结果与实际结果的对比。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 详情见nets/yolo_training.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YOLOLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, anchors, num_classes, input_shape, cuda, anchors_mask = [[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>], [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>], [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]]</span>):</span><br><span class="line">        <span class="built_in">super</span>(YOLOLoss, self).__init__()</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   13x13的特征层对应的anchor是[116,90],[156,198],[373,326]</span></span><br><span class="line">        <span class="comment">#   26x26的特征层对应的anchor是[30,61],[62,45],[59,119]</span></span><br><span class="line">        <span class="comment">#   52x52的特征层对应的anchor是[10,13],[16,30],[33,23]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        self.anchors        = anchors</span><br><span class="line">        self.num_classes    = num_classes</span><br><span class="line">        self.bbox_attrs     = <span class="number">5</span> + num_classes</span><br><span class="line">        self.input_shape    = input_shape</span><br><span class="line">        self.anchors_mask   = anchors_mask</span><br><span class="line"></span><br><span class="line">        self.ignore_threshold = <span class="number">0.7</span></span><br><span class="line">        self.cuda = cuda</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">clip_by_tensor</span>(<span class="params">self, t, t_min, t_max</span>):</span><br><span class="line">        t = t.<span class="built_in">float</span>()</span><br><span class="line">        result = (t &gt;= t_min).<span class="built_in">float</span>() * t + (t &lt; t_min).<span class="built_in">float</span>() * t_min</span><br><span class="line">        result = (result &lt;= t_max).<span class="built_in">float</span>() * result + (result &gt; t_max).<span class="built_in">float</span>() * t_max</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">MSELoss</span>(<span class="params">self, pred, target</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">pow</span>(pred - target, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">BCELoss</span>(<span class="params">self, pred, target</span>):</span><br><span class="line">        epsilon = <span class="number">1e-7</span></span><br><span class="line">        pred    = self.clip_by_tensor(pred, epsilon, <span class="number">1.0</span> - epsilon)</span><br><span class="line">        output  = - target * torch.log(pred) - (<span class="number">1.0</span> - target) * torch.log(<span class="number">1.0</span> - pred)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, l, <span class="built_in">input</span>, targets=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   l代表的是，当前输入进来的有效特征层，是第几个有效特征层</span></span><br><span class="line">        <span class="comment">#   input的shape为  bs, 3*(5+num_classes), 13, 13</span></span><br><span class="line">        <span class="comment">#                   bs, 3*(5+num_classes), 26, 26</span></span><br><span class="line">        <span class="comment">#                   bs, 3*(5+num_classes), 52, 52</span></span><br><span class="line">        <span class="comment">#   targets代表的是真实框。</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#--------------------------------#</span></span><br><span class="line">        <span class="comment">#   获得图片数量，特征层的高和宽</span></span><br><span class="line">        <span class="comment">#   13和13</span></span><br><span class="line">        <span class="comment">#--------------------------------#</span></span><br><span class="line">        bs      = <span class="built_in">input</span>.size(<span class="number">0</span>)</span><br><span class="line">        in_h    = <span class="built_in">input</span>.size(<span class="number">2</span>)</span><br><span class="line">        in_w    = <span class="built_in">input</span>.size(<span class="number">3</span>)</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算步长</span></span><br><span class="line">        <span class="comment">#   每一个特征点对应原来的图片上多少个像素点</span></span><br><span class="line">        <span class="comment">#   如果特征层为13x13的话，一个特征点就对应原来的图片上的32个像素点</span></span><br><span class="line">        <span class="comment">#   如果特征层为26x26的话，一个特征点就对应原来的图片上的16个像素点</span></span><br><span class="line">        <span class="comment">#   如果特征层为52x52的话，一个特征点就对应原来的图片上的8个像素点</span></span><br><span class="line">        <span class="comment">#   stride_h = stride_w = 32、16、8</span></span><br><span class="line">        <span class="comment">#   stride_h和stride_w都是32。</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------------#</span></span><br><span class="line">        stride_h = self.input_shape[<span class="number">0</span>] / in_h</span><br><span class="line">        stride_w = self.input_shape[<span class="number">1</span>] / in_w</span><br><span class="line">        <span class="comment">#-------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   此时获得的scaled_anchors大小是相对于特征层的</span></span><br><span class="line">        <span class="comment">#-------------------------------------------------#</span></span><br><span class="line">        scaled_anchors  = [(a_w / stride_w, a_h / stride_h) <span class="keyword">for</span> a_w, a_h <span class="keyword">in</span> self.anchors]</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   输入的input一共有三个，他们的shape分别是</span></span><br><span class="line">        <span class="comment">#   bs, 3*(5+num_classes), 13, 13 =&gt; batch_size, 3, 13, 13, 5 + num_classes</span></span><br><span class="line">        <span class="comment">#   batch_size, 3, 26, 26, 5 + num_classes</span></span><br><span class="line">        <span class="comment">#   batch_size, 3, 52, 52, 5 + num_classes</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        prediction = <span class="built_in">input</span>.view(bs, <span class="built_in">len</span>(self.anchors_mask[l]), self.bbox_attrs, in_h, in_w).permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>).contiguous()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   先验框的中心位置的调整参数</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        x = torch.sigmoid(prediction[..., <span class="number">0</span>])</span><br><span class="line">        y = torch.sigmoid(prediction[..., <span class="number">1</span>])</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   先验框的宽高调整参数</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        w = prediction[..., <span class="number">2</span>]</span><br><span class="line">        h = prediction[..., <span class="number">3</span>]</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   获得置信度，是否有物体</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        conf = torch.sigmoid(prediction[..., <span class="number">4</span>])</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   种类置信度</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        pred_cls = torch.sigmoid(prediction[..., <span class="number">5</span>:])</span><br><span class="line"></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   获得网络应该有的预测结果</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        y_true, noobj_mask, box_loss_scale = self.get_target(l, targets, scaled_anchors, in_h, in_w)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#---------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   将预测结果进行解码，判断预测结果和真实值的重合程度</span></span><br><span class="line">        <span class="comment">#   如果重合程度过大则忽略，因为这些特征点属于预测比较准确的特征点</span></span><br><span class="line">        <span class="comment">#   作为负样本不合适</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------------#</span></span><br><span class="line">        noobj_mask = self.get_ignore(l, x, y, h, w, targets, scaled_anchors, in_h, in_w, noobj_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            y_true          = y_true.cuda()</span><br><span class="line">            noobj_mask      = noobj_mask.cuda()</span><br><span class="line">            box_loss_scale  = box_loss_scale.cuda()</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   reshape_y_true[...,2:3]和reshape_y_true[...,3:4]</span></span><br><span class="line">        <span class="comment">#   表示真实框的宽高，二者均在0-1之间</span></span><br><span class="line">        <span class="comment">#   真实框越大，比重越小，小框的比重更大。</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        box_loss_scale = <span class="number">2</span> - box_loss_scale</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算中心偏移情况的loss，使用BCELoss效果好一些</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        loss_x = torch.<span class="built_in">sum</span>(self.BCELoss(x, y_true[..., <span class="number">0</span>]) * box_loss_scale * y_true[..., <span class="number">4</span>])</span><br><span class="line">        loss_y = torch.<span class="built_in">sum</span>(self.BCELoss(y, y_true[..., <span class="number">1</span>]) * box_loss_scale * y_true[..., <span class="number">4</span>])</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算宽高调整值的loss</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        loss_w = torch.<span class="built_in">sum</span>(self.MSELoss(w, y_true[..., <span class="number">2</span>]) * <span class="number">0.5</span> * box_loss_scale * y_true[..., <span class="number">4</span>])</span><br><span class="line">        loss_h = torch.<span class="built_in">sum</span>(self.MSELoss(h, y_true[..., <span class="number">3</span>]) * <span class="number">0.5</span> * box_loss_scale * y_true[..., <span class="number">4</span>])</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算置信度的loss</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        loss_conf   = torch.<span class="built_in">sum</span>(self.BCELoss(conf, y_true[..., <span class="number">4</span>]) * y_true[..., <span class="number">4</span>]) + \</span><br><span class="line">                      torch.<span class="built_in">sum</span>(self.BCELoss(conf, y_true[..., <span class="number">4</span>]) * noobj_mask)</span><br><span class="line"></span><br><span class="line">        loss_cls    = torch.<span class="built_in">sum</span>(self.BCELoss(pred_cls[y_true[..., <span class="number">4</span>] == <span class="number">1</span>], y_true[..., <span class="number">5</span>:][y_true[..., <span class="number">4</span>] == <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        loss        = loss_x  + loss_y + loss_w + loss_h + loss_conf + loss_cls</span><br><span class="line">        num_pos = torch.<span class="built_in">sum</span>(y_true[..., <span class="number">4</span>])</span><br><span class="line">        num_pos = torch.<span class="built_in">max</span>(num_pos, torch.ones_like(num_pos))</span><br><span class="line">        <span class="keyword">return</span> loss, num_pos</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">calculate_iou</span>(<span class="params">self, _box_a, _box_b</span>):</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算真实框的左上角和右下角</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        b1_x1, b1_x2 = _box_a[:, <span class="number">0</span>] - _box_a[:, <span class="number">2</span>] / <span class="number">2</span>, _box_a[:, <span class="number">0</span>] + _box_a[:, <span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">        b1_y1, b1_y2 = _box_a[:, <span class="number">1</span>] - _box_a[:, <span class="number">3</span>] / <span class="number">2</span>, _box_a[:, <span class="number">1</span>] + _box_a[:, <span class="number">3</span>] / <span class="number">2</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算先验框获得的预测框的左上角和右下角</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        b2_x1, b2_x2 = _box_b[:, <span class="number">0</span>] - _box_b[:, <span class="number">2</span>] / <span class="number">2</span>, _box_b[:, <span class="number">0</span>] + _box_b[:, <span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">        b2_y1, b2_y2 = _box_b[:, <span class="number">1</span>] - _box_b[:, <span class="number">3</span>] / <span class="number">2</span>, _box_b[:, <span class="number">1</span>] + _box_b[:, <span class="number">3</span>] / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   将真实框和预测框都转化成左上角右下角的形式</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        box_a = torch.zeros_like(_box_a)</span><br><span class="line">        box_b = torch.zeros_like(_box_b)</span><br><span class="line">        box_a[:, <span class="number">0</span>], box_a[:, <span class="number">1</span>], box_a[:, <span class="number">2</span>], box_a[:, <span class="number">3</span>] = b1_x1, b1_y1, b1_x2, b1_y2</span><br><span class="line">        box_b[:, <span class="number">0</span>], box_b[:, <span class="number">1</span>], box_b[:, <span class="number">2</span>], box_b[:, <span class="number">3</span>] = b2_x1, b2_y1, b2_x2, b2_y2</span><br><span class="line"></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   A为真实框的数量，B为先验框的数量</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        A = box_a.size(<span class="number">0</span>)</span><br><span class="line">        B = box_b.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算交的面积</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        max_xy  = torch.<span class="built_in">min</span>(box_a[:, <span class="number">2</span>:].unsqueeze(<span class="number">1</span>).expand(A, B, <span class="number">2</span>), box_b[:, <span class="number">2</span>:].unsqueeze(<span class="number">0</span>).expand(A, B, <span class="number">2</span>))</span><br><span class="line">        min_xy  = torch.<span class="built_in">max</span>(box_a[:, :<span class="number">2</span>].unsqueeze(<span class="number">1</span>).expand(A, B, <span class="number">2</span>), box_b[:, :<span class="number">2</span>].unsqueeze(<span class="number">0</span>).expand(A, B, <span class="number">2</span>))</span><br><span class="line">        inter   = torch.clamp((max_xy - min_xy), <span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">        inter   = inter[:, :, <span class="number">0</span>] * inter[:, :, <span class="number">1</span>]</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算预测框和真实框各自的面积</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        area_a = ((box_a[:, <span class="number">2</span>]-box_a[:, <span class="number">0</span>]) * (box_a[:, <span class="number">3</span>]-box_a[:, <span class="number">1</span>])).unsqueeze(<span class="number">1</span>).expand_as(inter)  <span class="comment"># [A,B]</span></span><br><span class="line">        area_b = ((box_b[:, <span class="number">2</span>]-box_b[:, <span class="number">0</span>]) * (box_b[:, <span class="number">3</span>]-box_b[:, <span class="number">1</span>])).unsqueeze(<span class="number">0</span>).expand_as(inter)  <span class="comment"># [A,B]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   求IOU</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        union = area_a + area_b - inter</span><br><span class="line">        <span class="keyword">return</span> inter / union  <span class="comment"># [A,B]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_target</span>(<span class="params">self, l, targets, anchors, in_h, in_w</span>):</span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算一共有多少张图片</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        bs              = <span class="built_in">len</span>(targets)</span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   用于选取哪些先验框不包含物体</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        noobj_mask      = torch.ones(bs, <span class="built_in">len</span>(self.anchors_mask[l]), in_h, in_w, requires_grad = <span class="literal">False</span>)</span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   让网络更加去关注小目标</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        box_loss_scale  = torch.zeros(bs, <span class="built_in">len</span>(self.anchors_mask[l]), in_h, in_w, requires_grad = <span class="literal">False</span>)</span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   batch_size, 3, 13, 13, 5 + num_classes</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        y_true          = torch.zeros(bs, <span class="built_in">len</span>(self.anchors_mask[l]), in_h, in_w, self.bbox_attrs, requires_grad = <span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(bs):            </span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(targets[b])==<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            batch_target = torch.zeros_like(targets[b])</span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   计算出正样本在特征层上的中心点</span></span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            batch_target[:, [<span class="number">0</span>,<span class="number">2</span>]] = targets[b][:, [<span class="number">0</span>,<span class="number">2</span>]] * in_w</span><br><span class="line">            batch_target[:, [<span class="number">1</span>,<span class="number">3</span>]] = targets[b][:, [<span class="number">1</span>,<span class="number">3</span>]] * in_h</span><br><span class="line">            batch_target[:, <span class="number">4</span>] = targets[b][:, <span class="number">4</span>]</span><br><span class="line">            batch_target = batch_target.cpu()</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   将真实框转换一个形式</span></span><br><span class="line">            <span class="comment">#   num_true_box, 4</span></span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            gt_box          = torch.FloatTensor(torch.cat((torch.zeros((batch_target.size(<span class="number">0</span>), <span class="number">2</span>)), batch_target[:, <span class="number">2</span>:<span class="number">4</span>]), <span class="number">1</span>))</span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   将先验框转换一个形式</span></span><br><span class="line">            <span class="comment">#   9, 4</span></span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            anchor_shapes   = torch.FloatTensor(torch.cat((torch.zeros((<span class="built_in">len</span>(anchors), <span class="number">2</span>)), torch.FloatTensor(anchors)), <span class="number">1</span>))</span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   计算交并比</span></span><br><span class="line">            <span class="comment">#   self.calculate_iou(gt_box, anchor_shapes) = [num_true_box, 9]每一个真实框和9个先验框的重合情况</span></span><br><span class="line">            <span class="comment">#   best_ns:</span></span><br><span class="line">            <span class="comment">#   [每个真实框最大的重合度max_iou, 每一个真实框最重合的先验框的序号]</span></span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            best_ns = torch.argmax(self.calculate_iou(gt_box, anchor_shapes), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> t, best_n <span class="keyword">in</span> <span class="built_in">enumerate</span>(best_ns):</span><br><span class="line">                <span class="keyword">if</span> best_n <span class="keyword">not</span> <span class="keyword">in</span> self.anchors_mask[l]:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="comment">#----------------------------------------#</span></span><br><span class="line">                <span class="comment">#   判断这个先验框是当前特征点的哪一个先验框</span></span><br><span class="line">                <span class="comment">#----------------------------------------#</span></span><br><span class="line">                k = self.anchors_mask[l].index(best_n)</span><br><span class="line">                <span class="comment">#----------------------------------------#</span></span><br><span class="line">                <span class="comment">#   获得真实框属于哪个网格点</span></span><br><span class="line">                <span class="comment">#----------------------------------------#</span></span><br><span class="line">                i = torch.floor(batch_target[t, <span class="number">0</span>]).long()</span><br><span class="line">                j = torch.floor(batch_target[t, <span class="number">1</span>]).long()</span><br><span class="line">                <span class="comment">#----------------------------------------#</span></span><br><span class="line">                <span class="comment">#   取出真实框的种类</span></span><br><span class="line">                <span class="comment">#----------------------------------------#</span></span><br><span class="line">                c = batch_target[t, <span class="number">4</span>].long()</span><br><span class="line"></span><br><span class="line">                <span class="comment">#----------------------------------------#</span></span><br><span class="line">                <span class="comment">#   noobj_mask代表无目标的特征点</span></span><br><span class="line">                <span class="comment">#----------------------------------------#</span></span><br><span class="line">                noobj_mask[b, k, j, i] = <span class="number">0</span></span><br><span class="line">                <span class="comment">#----------------------------------------#</span></span><br><span class="line">                <span class="comment">#   tx、ty代表中心调整参数的真实值</span></span><br><span class="line">                <span class="comment">#----------------------------------------#</span></span><br><span class="line">                y_true[b, k, j, i, <span class="number">0</span>] = batch_target[t, <span class="number">0</span>] - i.<span class="built_in">float</span>()</span><br><span class="line">                y_true[b, k, j, i, <span class="number">1</span>] = batch_target[t, <span class="number">1</span>] - j.<span class="built_in">float</span>()</span><br><span class="line">                y_true[b, k, j, i, <span class="number">2</span>] = math.log(batch_target[t, <span class="number">2</span>] / anchors[best_n][<span class="number">0</span>])</span><br><span class="line">                y_true[b, k, j, i, <span class="number">3</span>] = math.log(batch_target[t, <span class="number">3</span>] / anchors[best_n][<span class="number">1</span>])</span><br><span class="line">                y_true[b, k, j, i, <span class="number">4</span>] = <span class="number">1</span></span><br><span class="line">                y_true[b, k, j, i, c + <span class="number">5</span>] = <span class="number">1</span></span><br><span class="line">                <span class="comment">#----------------------------------------#</span></span><br><span class="line">                <span class="comment">#   用于获得xywh的比例</span></span><br><span class="line">                <span class="comment">#   大目标loss权重小，小目标loss权重大</span></span><br><span class="line">                <span class="comment">#----------------------------------------#</span></span><br><span class="line">                box_loss_scale[b, k, j, i] = batch_target[t, <span class="number">2</span>] * batch_target[t, <span class="number">3</span>] / in_w / in_h</span><br><span class="line">        <span class="keyword">return</span> y_true, noobj_mask, box_loss_scale</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_ignore</span>(<span class="params">self, l, x, y, h, w, targets, scaled_anchors, in_h, in_w, noobj_mask</span>):</span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算一共有多少张图片</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        bs = <span class="built_in">len</span>(targets)</span><br><span class="line"></span><br><span class="line">        FloatTensor = torch.cuda.FloatTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line">        LongTensor  = torch.cuda.LongTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.LongTensor</span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   生成网格，先验框中心，网格左上角</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        grid_x = torch.linspace(<span class="number">0</span>, in_w - <span class="number">1</span>, in_w).repeat(in_h, <span class="number">1</span>).repeat(</span><br><span class="line">            <span class="built_in">int</span>(bs * <span class="built_in">len</span>(self.anchors_mask[l])), <span class="number">1</span>, <span class="number">1</span>).view(x.shape).<span class="built_in">type</span>(FloatTensor)</span><br><span class="line">        grid_y = torch.linspace(<span class="number">0</span>, in_h - <span class="number">1</span>, in_h).repeat(in_w, <span class="number">1</span>).t().repeat(</span><br><span class="line">            <span class="built_in">int</span>(bs * <span class="built_in">len</span>(self.anchors_mask[l])), <span class="number">1</span>, <span class="number">1</span>).view(y.shape).<span class="built_in">type</span>(FloatTensor)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成先验框的宽高</span></span><br><span class="line">        scaled_anchors_l = np.array(scaled_anchors)[self.anchors_mask[l]]</span><br><span class="line">        anchor_w = FloatTensor(scaled_anchors_l).index_select(<span class="number">1</span>, LongTensor([<span class="number">0</span>]))</span><br><span class="line">        anchor_h = FloatTensor(scaled_anchors_l).index_select(<span class="number">1</span>, LongTensor([<span class="number">1</span>]))</span><br><span class="line">        </span><br><span class="line">        anchor_w = anchor_w.repeat(bs, <span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, in_h * in_w).view(w.shape)</span><br><span class="line">        anchor_h = anchor_h.repeat(bs, <span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, in_h * in_w).view(h.shape)</span><br><span class="line">        <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算调整后的先验框中心与宽高</span></span><br><span class="line">        <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">        pred_boxes_x    = torch.unsqueeze(x.data + grid_x, -<span class="number">1</span>)</span><br><span class="line">        pred_boxes_y    = torch.unsqueeze(y.data + grid_y, -<span class="number">1</span>)</span><br><span class="line">        pred_boxes_w    = torch.unsqueeze(torch.exp(w.data) * anchor_w, -<span class="number">1</span>)</span><br><span class="line">        pred_boxes_h    = torch.unsqueeze(torch.exp(h.data) * anchor_h, -<span class="number">1</span>)</span><br><span class="line">        pred_boxes      = torch.cat([pred_boxes_x, pred_boxes_y, pred_boxes_w, pred_boxes_h], dim = -<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(bs):           </span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   将预测结果转换一个形式</span></span><br><span class="line">            <span class="comment">#   pred_boxes_for_ignore      num_anchors, 4</span></span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            pred_boxes_for_ignore = pred_boxes[b].view(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   计算真实框，并把真实框转换成相对于特征层的大小</span></span><br><span class="line">            <span class="comment">#   gt_box      num_true_box, 4</span></span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(targets[b]) &gt; <span class="number">0</span>:</span><br><span class="line">                batch_target = torch.zeros_like(targets[b])</span><br><span class="line">                <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">                <span class="comment">#   计算出正样本在特征层上的中心点</span></span><br><span class="line">                <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">                batch_target[:, [<span class="number">0</span>,<span class="number">2</span>]] = targets[b][:, [<span class="number">0</span>,<span class="number">2</span>]] * in_w</span><br><span class="line">                batch_target[:, [<span class="number">1</span>,<span class="number">3</span>]] = targets[b][:, [<span class="number">1</span>,<span class="number">3</span>]] * in_h</span><br><span class="line">                batch_target = batch_target[:, :<span class="number">4</span>]</span><br><span class="line">                <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">                <span class="comment">#   计算交并比</span></span><br><span class="line">                <span class="comment">#   anch_ious       num_true_box, num_anchors</span></span><br><span class="line">                <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">                anch_ious = self.calculate_iou(batch_target, pred_boxes_for_ignore)</span><br><span class="line">                <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">                <span class="comment">#   每个先验框对应真实框的最大重合度</span></span><br><span class="line">                <span class="comment">#   anch_ious_max   num_anchors</span></span><br><span class="line">                <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">                anch_ious_max, _    = torch.<span class="built_in">max</span>(anch_ious, dim = <span class="number">0</span>)</span><br><span class="line">                anch_ious_max       = anch_ious_max.view(pred_boxes[b].size()[:<span class="number">3</span>])</span><br><span class="line">                noobj_mask[b][anch_ious_max &gt; self.ignore_threshold] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> noobj_mask</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>yolov3</tag>
      </tags>
  </entry>
  <entry>
    <title>CenterNet论文笔记</title>
    <url>/zh-CN/centernet/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文地址：<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Duan_CenterNet_Keypoint_Triplets_for_Object_Detection_ICCV_2019_paper.pdf">CenterNet: Keypoint Triplets for Object Detection (thecvf.com)</a></p>
<p>源码地址： <a href="https://github.com/Duankaiwen/CenterNet">CenterNet: Keypoint Triplets for Object Detection</a></p>
<p>文章引用代码地址：<a href="https://github.com/bubbliiiing/centernet-pytorch">https://github.com/bubbliiiing/centernet-pytorch</a></p>
<p>文章出处：<a href="https://blog.csdn.net/weixin_44791964/article/details/107748542?spm=1001.2014.3001.5502">Pytorch搭建自己的Centernet目标检测平台</a></p>
<h3 id="主干网络"><a href="#主干网络" class="headerlink" title="主干网络"></a>主干网络</h3><p>主干网络选用resnet,ResNet50有两个基本的块，<strong>分别名为Conv Block和Identity Block，其中Conv Block输入和输出的维度是不一样的，所以不能连续串联，它的作用是改变网络的维度；Identity Block输入维度和输出维度相同，可以串联，用于加深网络的。</strong></p>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Concat和add的区别</title>
    <url>/zh-CN/concat_add/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://openreview.net/pdf?id=q2ZaVU6bEsT">https://openreview.net/pdf?id=q2ZaVU6bEsT</a></p>
<p>今天偶然看到了这篇论文，论文里有一部分讲到了concat,add还有自适应的关系，很感兴趣，记录下来。</p>
<p><style>.knapdxziidqh{zoom:50%;}</style><img src="/zh-CN/concat_add/concat_add/image-20220508171742020.png" class="lazyload" data-srcset="/zh-CN/concat_add/concat_add/image-20220508171742020.png" srcset="data:image/png;base64,666" class="knapdxziidqh lazyload" alt="image-20220508171742020"></p>
<p>方法（a）和（c）分别是加权融合和连接操作。即在空间和通道维度上直接添加特征图。方法（b）是一种自适应融合方法。具体来说，假设输入的大小可以表示为（bs，C，H，W），我们可以通过卷积运算得到（bs，3，H，W）的空间自适应权重，连接和 Softmax。三个通道与三个输入一一对应，通过计算加权和可以将上下文信息聚合到输出。</p>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>Concat&amp;add</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOV5论文笔记</title>
    <url>/zh-CN/YOLOV5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/weixin_44791964/article/details/121626848">pytorch搭建yolov5</a></p>
<p>2.<a href="https://github.com/bubbliiiing/yolov5-pytorch">yolov5-pytorch</a></p>
<p>3.<a href="https://blog.csdn.net/zhuangyuan7838/article/details/122672465">以YOLOV4为例详解anchor_based目标检测训练过程</a></p>
<h2 id="整体结构解析"><a href="#整体结构解析" class="headerlink" title="整体结构解析"></a>整体结构解析</h2><p><style>.exlfbbbrqksv{zoom:50%;}</style><img src="/zh-CN/YOLOV5/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" srcset="data:image/png;base64,666" class="exlfbbbrqksv lazyload" alt="img"></p>
<p>在学习YoloV5之前，我们需要对YoloV5所作的工作有一定的了解，这有助于我们后面去了解网络的细节。</p>
<p>和之前版本的Yolo类似，整个YoloV5可以依然可以分为三个部分，分别是Backbone，FPN以及Yolo Head。</p>
<p>Backbone可以被称作YoloV5的主干特征提取网络，根据它的结构以及之前Yolo主干的叫法，我一般叫它CSPDarknet，输入的图片首先会在CSPDarknet里面进行特征提取，提取到的特征可以被称作特征层，是输入图片的特征集合。在主干部分，我们获取了三个特征层进行下一步网络的构建，这三个特征层我称它为有效特征层。</p>
<p>FPN可以被称作YoloV5的加强特征提取网络，在主干部分获得的三个有效特征层会在这一部分进行特征融合，特征融合的目的是结合不同尺度的特征信息。在FPN部分，已经获得的有效特征层被用于继续提取特征。在YoloV5里依然使用到了Panet的结构，我们不仅会对特征进行上采样实现特征融合，还会对特征再次进行下采样实现特征融合。</p>
<p>Yolo Head是YoloV5的分类器与回归器，通过CSPDarknet和FPN，我们已经可以获得三个加强过的有效特征层。每一个特征层都有宽、高和通道数，此时我们可以将特征图看作一个又一个特征点的集合，每一个特征点都有通道数个特征。Yolo Head实际上所做的工作就是对特征点进行判断，判断特征点是否有物体与其对应。与以前版本的Yolo一样，YoloV5所用的解耦头是一起的，也就是分类和回归在一个1X1卷积里实现。</p>
<p>因此，整个YoloV5网络所作的工作就是 特征提取-特征加强-预测特征点对应的物体情况。</p>
<h2 id="网络结构解析"><a href="#网络结构解析" class="headerlink" title="网络结构解析"></a>网络结构解析</h2><h3 id="主干网络backbone"><a href="#主干网络backbone" class="headerlink" title="主干网络backbone"></a>主干网络backbone</h3><p><style>.vdrvniozmvvc{zoom:50%;}</style><img src="/zh-CN/YOLOV5/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" srcset="data:image/png;base64,666" class="vdrvniozmvvc lazyload" alt="在这里插入图片描述"></p>
<p>这部分在nets/CSPdarknet.py文件下，YoloV5所使用的主干特征提取网络为CSPDarknet，接下来就是各个组件实现:</p>
<h4 id="Conv2D-BN-SiLU"><a href="#Conv2D-BN-SiLU" class="headerlink" title="Conv2D_BN_SiLU"></a>Conv2D_BN_SiLU</h4><p>在代码里把这卷积、正则化和激活进行封装，封装成类Conv</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SiLU</span>(nn.Module):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">return</span> x * torch.sigmoid(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动填充,padding=1/2*kernel_size,根据计算卷积计算宽高公式自然懂的</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">autopad</span>(<span class="params">k, p=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> p <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        p = k // <span class="number">2</span> <span class="keyword">if</span> <span class="built_in">isinstance</span>(k, <span class="built_in">int</span>) <span class="keyword">else</span> [x // <span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> k] </span><br><span class="line">    <span class="keyword">return</span> p</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, k=<span class="number">1</span>, s=<span class="number">1</span>, p=<span class="literal">None</span>, g=<span class="number">1</span>, act=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Conv, self).__init__()</span><br><span class="line">        self.conv   = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn     = nn.BatchNorm2d(c2, eps=<span class="number">0.001</span>, momentum=<span class="number">0.03</span>)</span><br><span class="line">        self.act    = SiLU() <span class="keyword">if</span> act <span class="keyword">is</span> <span class="literal">True</span> <span class="keyword">else</span> (act <span class="keyword">if</span> <span class="built_in">isinstance</span>(act, nn.Module) <span class="keyword">else</span> nn.Identity())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.act(self.bn(self.conv(x)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fuseforward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.act(self.conv(x))</span><br></pre></td></tr></tbody></table></figure>
<h4 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h4><p>使用了<strong>残差网络Residual</strong>，CSPDarknet中的残差卷积可以分为两个部分，主干部分是一次1X1的卷积和一次3X3的卷积；残差边部分不做任何处理，直接将主干的输入与输出结合。整个YoloV5的主干部分都由残差卷积构成：</p>
<p>如下图所示：</p>
<p><style>.dogdonkfpnue{zoom:50%;}</style><img src="/zh-CN/YOLOV5/YOLOV5/d232d9ef19cb4b3bbb7b492aaf6ae097.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/d232d9ef19cb4b3bbb7b492aaf6ae097.png" srcset="data:image/png;base64,666" class="dogdonkfpnue lazyload" alt="在这里插入图片描述"></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="comment"># Standard bottleneck</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, shortcut=<span class="literal">True</span>, g=<span class="number">1</span>, e=<span class="number">0.5</span></span>):  <span class="comment"># 参数分别代表ch_in, ch_out, shortcut, groups, expansion</span></span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line">        c_ = <span class="built_in">int</span>(c2 * e)  <span class="comment"># hidden channels</span></span><br><span class="line">        self.cv1 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cv2 = Conv(c_, c2, <span class="number">3</span>, <span class="number">1</span>, g=g)</span><br><span class="line">        self.add = shortcut <span class="keyword">and</span> c1 == c2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x + self.cv2(self.cv1(x)) <span class="keyword">if</span> self.add <span class="keyword">else</span> self.cv2(self.cv1(x))</span><br></pre></td></tr></tbody></table></figure>
<p>残差网络的特点是<strong>容易优化</strong>，并且能够通过增加相当的<strong>深度来提高准确率</strong>。其内部的<strong>残差块使用了跳跃连接，缓解了在深度神经网络中增加深度带来的梯度消失问题。</strong></p>
<h4 id="CSPnet"><a href="#CSPnet" class="headerlink" title="CSPnet"></a>CSPnet</h4><p>使用<strong>CSPnet</strong>网络结构，CSPnet结构并不算复杂，就是将原来的残差块的堆叠进行了一个拆分，拆成左右两部分：<strong>主干部分继续进行原来的残差块的堆叠</strong>；另一部分则像一个残差边一样，经过少量处理直接连接到最后。因此可以认为CSP中存在一个大的残差边。</p>
<p>如下图所示：</p>
<p><style>.fqlsfsscpkca{zoom:50%;}</style><img src="/zh-CN/YOLOV5/YOLOV5/20200509113651540.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/20200509113651540.png" srcset="data:image/png;base64,666" class="fqlsfsscpkca lazyload" alt="CSPLayer"></p>
<p>代码实现：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 这里是backbone里CSPLayer的实现</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">C3</span>(nn.Module):</span><br><span class="line">    <span class="comment"># CSP Bottleneck with 3 convolutions</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, n=<span class="number">1</span>, shortcut=<span class="literal">True</span>, g=<span class="number">1</span>, e=<span class="number">0.5</span></span>):  <span class="comment"># ch_in, ch_out, number, shortcut, groups, expansion</span></span><br><span class="line">        <span class="built_in">super</span>(C3, self).__init__()</span><br><span class="line">        c_ = <span class="built_in">int</span>(c2 * e)  <span class="comment"># hidden channels</span></span><br><span class="line">        self.cv1 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cv2 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cv3 = Conv(<span class="number">2</span> * c_, c2, <span class="number">1</span>)  <span class="comment"># act=FReLU(c2)</span></span><br><span class="line">        self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=<span class="number">1.0</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)])</span><br><span class="line">        <span class="comment"># self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=<span class="number">1</span>))</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="Focus网络结构"><a href="#Focus网络结构" class="headerlink" title="Focus网络结构"></a>Focus网络结构</h4><p>使用了Focus网络结构，这个网络结构是在YoloV5里面使用到比较有趣的网络结构，具体操作是在一张图片中每隔一个像素拿到一个值，这个时候获得了四个独立的特征层，然后将四个独立的特征层进行堆叠，此时宽高信息就集中到了通道信息，输入通道扩充了四倍。拼接起来的特征层相对于原先的三通道变成了十二个通道，下图很好的展示了Focus结构，一看就能明白。</p>
<p><style>.isgzotttuxbw{}</style><img src="/zh-CN/YOLOV5/YOLOV5/7d1f567fc97140d4b9492b5e28cd0ebc.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/7d1f567fc97140d4b9492b5e28cd0ebc.png" srcset="data:image/png;base64,666" class="isgzotttuxbw lazyload" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Focus</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, k=<span class="number">1</span>, s=<span class="number">1</span>, p=<span class="literal">None</span>, g=<span class="number">1</span>, act=<span class="literal">True</span></span>):  <span class="comment"># ch_in, ch_out, kernel, stride, padding, groups</span></span><br><span class="line">        <span class="built_in">super</span>(Focus, self).__init__()</span><br><span class="line">        self.conv = Conv(c1 * <span class="number">4</span>, c2, k, s, p, g, act)</span><br><span class="line">	</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.conv(torch.cat(</span><br><span class="line">        	[x[..., ::<span class="number">2</span>, ::<span class="number">2</span>], <span class="comment"># 1</span></span><br><span class="line">        	x[..., <span class="number">1</span>::<span class="number">2</span>, ::<span class="number">2</span>], <span class="comment"># 2</span></span><br><span class="line">        	x[..., ::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>], <span class="comment"># 3</span></span><br><span class="line">        	x[..., <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>] <span class="comment"># 4</span></span><br><span class="line">        	], <span class="number">1</span>))</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="SiLU函数"><a href="#SiLU函数" class="headerlink" title="SiLU函数"></a>SiLU函数</h4><p>使用了SiLU激活函数，SiLU是Sigmoid和ReLU的改进版。SiLU具备无上界有下界、平滑、非单调的特性。SiLU在深层模型上的效果优于 ReLU。可以看做是平滑的ReLU激活函数。</p>
<p><style>.vqiznrulcckq{zoom: 80%;}</style><img src="/zh-CN/YOLOV5/YOLOV5/image-20220505154442317.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/image-20220505154442317.png" srcset="data:image/png;base64,666" class="vqiznrulcckq lazyload" alt="image-20220505154442317"></p>
<p><style>.sweowzgeynvc{zoom:50%;}</style><img src="/zh-CN/YOLOV5/YOLOV5/e6cb6d3c11db4510a034ca9b1a0ca339.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/e6cb6d3c11db4510a034ca9b1a0ca339.png" srcset="data:image/png;base64,666" class="sweowzgeynvc lazyload" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SiLU</span>(nn.Module):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">return</span> x * torch.sigmoid(x)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="SPP"><a href="#SPP" class="headerlink" title="SPP"></a>SPP</h4><p>使用了SPP结构，通过不同池化核大小的最大池化进行特征提取，提高网络的感受野。在YoloV4中，SPP是用在FPN里面的，在YoloV5中，SPP模块被用在了主干特征提取网络中。</p>
<p>如下图所示：</p>
<p><style>.iefnlcubzrqh{zoom:50%;}</style><img src="/zh-CN/YOLOV5/YOLOV5/image-20220505154713232.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/image-20220505154713232.png" srcset="data:image/png;base64,666" class="iefnlcubzrqh lazyload" alt="image-20220505154713232"></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SPP</span>(nn.Module):</span><br><span class="line">    <span class="comment"># Spatial pyramid pooling layer used in YOLOv3-SPP</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, k=(<span class="params"><span class="number">5</span>, <span class="number">9</span>, <span class="number">13</span></span>)</span>):</span><br><span class="line">        <span class="built_in">super</span>(SPP, self).__init__()</span><br><span class="line">        c_ = c1 // <span class="number">2</span>  <span class="comment"># hidden channels</span></span><br><span class="line">        self.cv1 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cv2 = Conv(c_ * (<span class="built_in">len</span>(k) + <span class="number">1</span>), c2, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=<span class="number">1</span>, padding=x // <span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> k])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.cv1(x)</span><br><span class="line">        <span class="keyword">return</span> self.cv2(torch.cat([x] + [m(x) <span class="keyword">for</span> m <span class="keyword">in</span> self.m], <span class="number">1</span>))</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="CSPDarknet"><a href="#CSPDarknet" class="headerlink" title="CSPDarknet"></a>CSPDarknet</h4><p>在完成各个部件后，整个的CSPDarknet实现如下（左边部分）：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CSPDarknet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_channels, base_depth</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   输入图片是640, 640, 3</span></span><br><span class="line">        <span class="comment">#   初始的基本通道是64</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   利用focus网络结构进行特征提取</span></span><br><span class="line">        <span class="comment">#   640, 640, 3 -&gt; 320, 320, 12 -&gt; 320, 320, 64</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        self.stem       = Focus(<span class="number">3</span>, base_channels, k=<span class="number">3</span>)</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   完成卷积之后，320, 320, 64 -&gt; 160, 160, 128</span></span><br><span class="line">        <span class="comment">#   完成CSPlayer之后，160, 160, 128 -&gt; 160, 160, 128</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        self.dark2 = nn.Sequential(</span><br><span class="line">            Conv(base_channels, base_channels * <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            C3(base_channels * <span class="number">2</span>, base_channels * <span class="number">2</span>, base_depth),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   完成卷积之后，160, 160, 128 -&gt; 80, 80, 256</span></span><br><span class="line">        <span class="comment">#   完成CSPlayer之后，80, 80, 256 -&gt; 80, 80, 256</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        self.dark3 = nn.Sequential(</span><br><span class="line">            Conv(base_channels * <span class="number">2</span>, base_channels * <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            C3(base_channels * <span class="number">4</span>, base_channels * <span class="number">4</span>, base_depth * <span class="number">3</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   完成卷积之后，80, 80, 256 -&gt; 40, 40, 512</span></span><br><span class="line">        <span class="comment">#   完成CSPlayer之后，40, 40, 512 -&gt; 40, 40, 512</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        self.dark4 = nn.Sequential(</span><br><span class="line">            Conv(base_channels * <span class="number">4</span>, base_channels * <span class="number">8</span>, <span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            C3(base_channels * <span class="number">8</span>, base_channels * <span class="number">8</span>, base_depth * <span class="number">3</span>),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   完成卷积之后，40, 40, 512 -&gt; 20, 20, 1024</span></span><br><span class="line">        <span class="comment">#   完成SPP之后，20, 20, 1024 -&gt; 20, 20, 1024</span></span><br><span class="line">        <span class="comment">#   完成CSPlayer之后，20, 20, 1024 -&gt; 20, 20, 1024</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        self.dark5 = nn.Sequential(</span><br><span class="line">            Conv(base_channels * <span class="number">8</span>, base_channels * <span class="number">16</span>, <span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            SPP(base_channels * <span class="number">16</span>, base_channels * <span class="number">16</span>),</span><br><span class="line">            C3(base_channels * <span class="number">16</span>, base_channels * <span class="number">16</span>, base_depth, shortcut=<span class="literal">False</span>),</span><br><span class="line">        )</span><br><span class="line">	<span class="comment"># feat1, feat2, feat3是用来目标检测的三个特征层，后续还要进行FPN层的构建，也就是中间那部分</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.stem(x)</span><br><span class="line">        x = self.dark2(x)</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   dark3的输出为80, 80, 256，是一个有效特征层</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        x = self.dark3(x)</span><br><span class="line">        feat1 = x</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   dark4的输出为40, 40, 512，是一个有效特征层</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        x = self.dark4(x)</span><br><span class="line">        feat2 = x</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   dark5的输出为20, 20, 1024，是一个有效特征层</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        x = self.dark5(x)</span><br><span class="line">        feat3 = x</span><br><span class="line">        <span class="keyword">return</span> feat1, feat2, feat3</span><br></pre></td></tr></tbody></table></figure>
<p>好了，我们现在已经提取到了构建FPN层的三个特征层，接下来就是构建FPN</p>
<h3 id="构建FPN加强特征提取"><a href="#构建FPN加强特征提取" class="headerlink" title="构建FPN加强特征提取"></a>构建FPN加强特征提取</h3><p><style>.forrldhfhswm{zoom:50%;}</style><img src="/zh-CN/YOLOV5/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" srcset="data:image/png;base64,666" class="forrldhfhswm lazyload" alt="在这里插入图片描述"></p>
<p>再次回到这幅图，现在我们需要进行中间部分的实现了,这部分在yolo.py文件。</p>
<p>在特征利用部分，YoloV5提取多特征层进行目标检测，一共提取三个特征层。<br>三个特征层位于主干部分CSPdarknet的不同位置，分别位于中间层，中下层，底层，当输入为(640,640,3)的时候，三个特征层的shape分别为feat1=(80,80,256)、feat2=(40,40,512)、feat3=(20,20,1024)。</p>
<p>在获得三个有效特征层后，我们利用这三个有效特征层进行FPN层的构建，构建方式为：</p>
<p>1.feat3=(20,20,1024)的特征层进行1次1X1卷积调整通道后获得P5，P5进行上采样UmSampling2d后与feat2=(40,40,512)特征层进行结合，然后使用CSPLayer进行特征提取获得P5_upsample，此时获得的特征层为(40,40,512)。</p>
<p>2.P5_upsample=(40,40,512)的特征层进行1次1X1卷积调整通道后获得P4，P4进行上采样UmSampling2d后与feat1=(80,80,256)特征层进行结合，然后使用CSPLayer进行特征提取<strong>P3_out</strong>，此时获得的特征层为(80,80,256)。</p>
<p>3.P3_out=(80,80,256)的特征层进行一次3x3卷积进行下采样，下采样后与P4堆叠，然后使用CSPLayer进行特征提取<strong>P4_out</strong>，此时获得的特征层为(40,40,512)。</p>
<p>4.P4_out=(40,40,512)的特征层进行一次3x3卷积进行下采样，下采样后与P5堆叠，然后使用CSPLayer进行特征提取<strong>P5_out</strong>，此时获得的特征层为(20,20,1024)。</p>
<p>注：p3_out, p4_out,p5_out即为经过FPN输出的三个特征层，用于检测。</p>
<p>特征金字塔可以将不同shape的特征层进行特征融合，有利于提取出更好的特征。</p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nets.ConvNext <span class="keyword">import</span> ConvNeXt_Small, ConvNeXt_Tiny</span><br><span class="line"><span class="keyword">from</span> nets.CSPdarknet <span class="keyword">import</span> C3, Conv, CSPDarknet</span><br><span class="line"><span class="keyword">from</span> nets.Swin_transformer <span class="keyword">import</span> Swin_transformer_Tiny</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------------------------------------#</span></span><br><span class="line"><span class="comment">#   yolo_body</span></span><br><span class="line"><span class="comment">#---------------------------------------------------#</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YoloBody</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, anchors_mask, num_classes, phi, backbone=<span class="string">'cspdarknet'</span>, pretrained=<span class="literal">False</span>, input_shape=[<span class="number">640</span>, <span class="number">640</span>]</span>):</span><br><span class="line">        <span class="built_in">super</span>(YoloBody, self).__init__()</span><br><span class="line">        depth_dict          = {<span class="string">'s'</span> : <span class="number">0.33</span>, <span class="string">'m'</span> : <span class="number">0.67</span>, <span class="string">'l'</span> : <span class="number">1.00</span>, <span class="string">'x'</span> : <span class="number">1.33</span>,}</span><br><span class="line">        width_dict          = {<span class="string">'s'</span> : <span class="number">0.50</span>, <span class="string">'m'</span> : <span class="number">0.75</span>, <span class="string">'l'</span> : <span class="number">1.00</span>, <span class="string">'x'</span> : <span class="number">1.25</span>,}</span><br><span class="line">        dep_mul, wid_mul    = depth_dict[phi], width_dict[phi]</span><br><span class="line"></span><br><span class="line">        base_channels       = <span class="built_in">int</span>(wid_mul * <span class="number">64</span>)  <span class="comment"># 64</span></span><br><span class="line">        base_depth          = <span class="built_in">max</span>(<span class="built_in">round</span>(dep_mul * <span class="number">3</span>), <span class="number">1</span>)  <span class="comment"># 3</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   输入图片是640, 640, 3</span></span><br><span class="line">        <span class="comment">#   初始的基本通道是64</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        self.backbone_name  = backbone</span><br><span class="line">        <span class="keyword">if</span> backbone == <span class="string">"cspdarknet"</span>:</span><br><span class="line">            <span class="comment">#---------------------------------------------------#   </span></span><br><span class="line">            <span class="comment">#   生成CSPdarknet53的主干模型</span></span><br><span class="line">            <span class="comment">#   获得三个有效特征层，他们的shape分别是：</span></span><br><span class="line">            <span class="comment">#   80,80,256</span></span><br><span class="line">            <span class="comment">#   40,40,512</span></span><br><span class="line">            <span class="comment">#   20,20,1024</span></span><br><span class="line">            <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">            self.backbone   = CSPDarknet(base_channels, base_depth, phi, pretrained)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#---------------------------------------------------#   </span></span><br><span class="line">            <span class="comment">#   如果输入不为cspdarknet，则调整通道数</span></span><br><span class="line">            <span class="comment">#   使其符合YoloV5的格式</span></span><br><span class="line">            <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">            self.backbone       = {</span><br><span class="line">                <span class="string">'convnext_tiny'</span>         : ConvNeXt_Tiny,</span><br><span class="line">                <span class="string">'convnext_small'</span>        : ConvNeXt_Small,</span><br><span class="line">                <span class="string">'swin_transfomer_tiny'</span>  : Swin_transformer_Tiny,</span><br><span class="line">            }[backbone](pretrained=pretrained, input_shape=input_shape)</span><br><span class="line">            in_channels         = {</span><br><span class="line">                <span class="string">'convnext_tiny'</span>         : [<span class="number">192</span>, <span class="number">384</span>, <span class="number">768</span>],</span><br><span class="line">                <span class="string">'convnext_small'</span>        : [<span class="number">192</span>, <span class="number">384</span>, <span class="number">768</span>],</span><br><span class="line">                <span class="string">'swin_transfomer_tiny'</span>  : [<span class="number">192</span>, <span class="number">384</span>, <span class="number">768</span>],</span><br><span class="line">            }[backbone]</span><br><span class="line">            feat1_c, feat2_c, feat3_c = in_channels </span><br><span class="line">            self.conv_1x1_feat1 = Conv(feat1_c, base_channels * <span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            self.conv_1x1_feat2 = Conv(feat2_c, base_channels * <span class="number">8</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            self.conv_1x1_feat3 = Conv(feat3_c, base_channels * <span class="number">16</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 上采样操作，采用最近邻插值法</span></span><br><span class="line">        self.upsample   = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">"nearest"</span>)</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># Conv1×1,获得p5</span></span><br><span class="line">        self.conv_for_feat3         = Conv(base_channels * <span class="number">16</span>, base_channels * <span class="number">8</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># CSPLayer操作，进行特征提取，获得p5_unsample</span></span><br><span class="line">        self.conv3_for_upsample1    = C3(base_channels * <span class="number">16</span>, base_channels * <span class="number">8</span>, base_depth, shortcut=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.conv_for_feat2         = Conv(base_channels * <span class="number">8</span>, base_channels * <span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv3_for_upsample2    = C3(base_channels * <span class="number">8</span>, base_channels * <span class="number">4</span>, base_depth, shortcut=<span class="literal">False</span>)</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># 下采样操作，采用卷积步长为2的方法，通道不变，宽高压缩一半</span></span><br><span class="line">        self.down_sample1           = Conv(base_channels * <span class="number">4</span>, base_channels * <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv3_for_downsample1  = C3(base_channels * <span class="number">8</span>, base_channels * <span class="number">8</span>, base_depth, shortcut=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.down_sample2           = Conv(base_channels * <span class="number">8</span>, base_channels * <span class="number">8</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv3_for_downsample2  = C3(base_channels * <span class="number">16</span>, base_channels * <span class="number">16</span>, base_depth, shortcut=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 80, 80, 256 =&gt; 80, 80, 3 * (5 + num_classes) =&gt; 80, 80, 3 * (4 + 1 + num_classes)</span></span><br><span class="line">        self.yolo_head_P3 = nn.Conv2d(base_channels * <span class="number">4</span>, <span class="built_in">len</span>(anchors_mask[<span class="number">2</span>]) * (<span class="number">5</span> + num_classes), <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 40, 40, 512 =&gt; 40, 40, 3 * (5 + num_classes) =&gt; 40, 40, 3 * (4 + 1 + num_classes)</span></span><br><span class="line">        self.yolo_head_P4 = nn.Conv2d(base_channels * <span class="number">8</span>, <span class="built_in">len</span>(anchors_mask[<span class="number">1</span>]) * (<span class="number">5</span> + num_classes), <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 20, 20, 1024 =&gt; 20, 20, 3 * (5 + num_classes) =&gt; 20, 20, 3 * (4 + 1 + num_classes)</span></span><br><span class="line">        self.yolo_head_P5 = nn.Conv2d(base_channels * <span class="number">16</span>, <span class="built_in">len</span>(anchors_mask[<span class="number">0</span>]) * (<span class="number">5</span> + num_classes), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#  backbone</span></span><br><span class="line">        feat1, feat2, feat3 = self.backbone(x)</span><br><span class="line">        <span class="keyword">if</span> self.backbone_name != <span class="string">"cspdarknet"</span>:</span><br><span class="line">            feat1 = self.conv_1x1_feat1(feat1)</span><br><span class="line">            feat2 = self.conv_1x1_feat2(feat2)</span><br><span class="line">            feat3 = self.conv_1x1_feat3(feat3)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 20, 20, 1024 -&gt; 20, 20, 512</span></span><br><span class="line">        P5          = self.conv_for_feat3(feat3)</span><br><span class="line">        <span class="comment"># 20, 20, 512 -&gt; 40, 40, 512</span></span><br><span class="line">        P5_upsample = self.upsample(P5)</span><br><span class="line">        <span class="comment"># 40, 40, 512 -&gt; 40, 40, 1024</span></span><br><span class="line">        P4          = torch.cat([P5_upsample, feat2], <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 40, 40, 1024 -&gt; 40, 40, 512</span></span><br><span class="line">        P4          = self.conv3_for_upsample1(P4)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 40, 40, 512 -&gt; 40, 40, 256</span></span><br><span class="line">        P4          = self.conv_for_feat2(P4)</span><br><span class="line">        <span class="comment"># 40, 40, 256 -&gt; 80, 80, 256</span></span><br><span class="line">        P4_upsample = self.upsample(P4)</span><br><span class="line">        <span class="comment"># 80, 80, 256 cat 80, 80, 256 -&gt; 80, 80, 512</span></span><br><span class="line">        P3          = torch.cat([P4_upsample, feat1], <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 80, 80, 512 -&gt; 80, 80, 256</span></span><br><span class="line">        P3          = self.conv3_for_upsample2(P3)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 80, 80, 256 -&gt; 40, 40, 256</span></span><br><span class="line">        P3_downsample = self.down_sample1(P3)</span><br><span class="line">        <span class="comment"># 40, 40, 256 cat 40, 40, 256 -&gt; 40, 40, 512</span></span><br><span class="line">        P4 = torch.cat([P3_downsample, P4], <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 40, 40, 512 -&gt; 40, 40, 512</span></span><br><span class="line">        P4 = self.conv3_for_downsample1(P4)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 40, 40, 512 -&gt; 20, 20, 512</span></span><br><span class="line">        P4_downsample = self.down_sample2(P4)</span><br><span class="line">        <span class="comment"># 20, 20, 512 cat 20, 20, 512 -&gt; 20, 20, 1024</span></span><br><span class="line">        P5 = torch.cat([P4_downsample, P5], <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 20, 20, 1024 -&gt; 20, 20, 1024</span></span><br><span class="line">        P5 = self.conv3_for_downsample2(P5)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   第三个特征层</span></span><br><span class="line">        <span class="comment">#   y3=(batch_size,75,80,80)</span></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        out2 = self.yolo_head_P3(P3)</span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   第二个特征层</span></span><br><span class="line">        <span class="comment">#   y2=(batch_size,75,40,40)</span></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        out1 = self.yolo_head_P4(P4)</span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   第一个特征层</span></span><br><span class="line">        <span class="comment">#   y1=(batch_size,75,20,20)</span></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        out0 = self.yolo_head_P5(P5)</span><br><span class="line">        <span class="keyword">return</span> out0, out1, out2</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>注：特征金字塔输出来的是p3,p4,p5。代码里输出的是yolohead输出来的结果，即out0,out1,out2</p>
<h3 id="利用YoloHead获得预测结果"><a href="#利用YoloHead获得预测结果" class="headerlink" title="利用YoloHead获得预测结果"></a>利用YoloHead获得预测结果</h3><p><style>.lxvcvxnlyijt{zoom:50%;}</style><img src="/zh-CN/YOLOV5/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" srcset="data:image/png;base64,666" class="lxvcvxnlyijt lazyload" alt="在这里插入图片描述"></p>
<p>利用FPN特征金字塔，我们可以获得三个加强特征，这三个加强特征的shape分别为(20,20,1024)、(40,40,512)、(80,80,256)，然后我们利用这三个shape的特征层传入Yolo Head获得预测结果。</p>
<p>对于每一个特征层，我们可以获得利用一个卷积调整通道数，最终的通道数和需要区分的种类个数相关，在YoloV5里，每一个特征层上每一个特征点存在3个先验框。</p>
<p>如果使用的是voc训练集，类则为20种，最后的维度应该为75 = 3x25，三个特征层的shape为(20,20,75)，(40,40,75)，(80,80,75)。<br>最后的75可以拆分成3个25，对应3个先验框的25个参数，25可以拆分成4+1+20。<br>前4个参数用于判断每一个特征点的回归参数，回归参数调整后可以获得预测框；<br>第5个参数用于判断每一个特征点是否包含物体；<br>最后20个参数用于判断每一个特征点所包含的物体种类。</p>
<p>如果使用的是coco训练集，类则为80种，最后的维度应该为255 = 3x85，三个特征层的shape为(20,20,255)，(40,40,255)，(80,80,255)<br>最后的255可以拆分成3个85，对应3个先验框的85个参数，85可以拆分成4+1+80。<br>前4个参数用于判断每一个特征点的回归参数，回归参数调整后可以获得预测框；<br>第5个参数用于判断每一个特征点是否包含物体；<br>最后80个参数用于判断每一个特征点所包含的物体种类。</p>
<p>在上一部分其实已经写到了预测部分的代码，现在我们把它单独拿出来看看,或者直接看上面FPN那里的代码。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">YoloBody</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, anchor_mask, num_classes, phi</span>):</span><br><span class="line">        <span class="built_in">super</span>(YoloBody, self).__init__()</span><br><span class="line">        <span class="comment"># 80, 80, 256 =&gt; 80, 80, 3 * (5 + num_classes) =&gt; 80, 80, 3 * (4 + 1 + num_classes)</span></span><br><span class="line">        self.yolo_head_P3 = nn.Conv2d(base_channels * <span class="number">4</span>, <span class="built_in">len</span>(anchors_mask[<span class="number">2</span>]) * (<span class="number">5</span> + num_classes), <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 40, 40, 512 =&gt; 40, 40, 3 * (5 + num_classes) =&gt; 40, 40, 3 * (4 + 1 + num_classes)</span></span><br><span class="line">        self.yolo_head_P4 = nn.Conv2d(base_channels * <span class="number">8</span>, <span class="built_in">len</span>(anchors_mask[<span class="number">1</span>]) * (<span class="number">5</span> + num_classes), <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 20, 20, 1024 =&gt; 20, 20, 3 * (5 + num_classes) =&gt; 20, 20, 3 * (4 + 1 + num_classes)</span></span><br><span class="line">        self.yolo_head_P5 = nn.Conv2d(base_channels * <span class="number">16</span>, <span class="built_in">len</span>(anchors_mask[<span class="number">0</span>]) * (<span class="number">5</span> + num_classes), <span class="number">1</span>)</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   第三个特征层</span></span><br><span class="line">        <span class="comment">#   y3=(batch_size,75,80,80)</span></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        out2 = self.yolo_head_P3(P3)</span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   第二个特征层</span></span><br><span class="line">        <span class="comment">#   y2=(batch_size,75,40,40)</span></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        out1 = self.yolo_head_P4(P4)</span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   第一个特征层</span></span><br><span class="line">        <span class="comment">#   y1=(batch_size,75,20,20)</span></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        out0 = self.yolo_head_P5(P5)</span><br><span class="line">        <span class="keyword">return</span> out0, out1, out2</span><br></pre></td></tr></tbody></table></figure>
<p>注：</p>
<p><code>anchor_mask=[[6, 7, 8], [3, 4, 5], [0, 1, 2]]</code>, 表示三个特征图的9个先验框，因此每一个特征图上每一个特点上存在3个先验框。</p>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><h3 id="获得预测框和得分"><a href="#获得预测框和得分" class="headerlink" title="获得预测框和得分"></a>获得预测框和得分</h3><p>假设我们使用coco数据集进行训练，由第二步我们可以获得三个特征层的预测结果，shape分别为(N,20,20,255)，(N,40,40,255)，(N,80,80,255)的数据。</p>
<p>但是这个预测结果并不对应着最终的预测框在图片上的位置，还需要解码才可以完成。在YoloV5里，每一个特征层上每一个特征点存在3个先验框。</p>
<p>每个特征层最后的255可以拆分成3个85，对应3个先验框的85个参数，我们先将其reshape一下，其结果为(N,20,20,3,85)，(N,40.40,3,85)，(N,80,80,3,85)。</p>
<p>其中的85可以拆分成4+1+80。<br>前4个参数用于判断每一个特征点的回归参数，回归参数调整后可以获得预测框；<br>第5个参数用于判断每一个特征点是否包含物体；<br>最后80个参数用于判断每一个特征点所包含的物体种类。</p>
<p>以(N,20,20,3,85)这个特征层为例，该特征层相当于将图像划分成20x20个特征点，如果某个特征点落在物体的对应框内，就用于预测该物体。</p>
<p>如图所示，蓝色的点为20x20的特征点，此时我们对左图黑色点的三个先验框进行解码操作演示：<br>1、进行中心预测点的计算，利用Regression预测结果前两个序号的内容对特征点的三个先验框中心坐标进行偏移，偏移后是右图红色的三个点；<br>2、进行预测框宽高的计算，利用Regression预测结果后两个序号的内容求指数后获得预测框的宽高；<br>3、此时获得的预测框就可以绘制在图片上了。</p>
<p><style>.zenycmxlssfj{zoom:50%;}</style><img src="/zh-CN/YOLOV5/YOLOV5/93fc40f7a47f46bbb37819115826ec1a.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/93fc40f7a47f46bbb37819115826ec1a.png" srcset="data:image/png;base64,666" class="zenycmxlssfj lazyload" alt="在这里插入图片描述"></p>
<p>除去这样的解码操作，还有非极大抑制的操作需要进行，防止同一种类的框的堆积。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># utils/utils_bbox.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">decode_box</span>(<span class="params">self, inputs</span>):</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> i, <span class="built_in">input</span> <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs):</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   输入的input一共有三个，他们的shape分别是</span></span><br><span class="line">        <span class="comment">#   batch_size, 255, 20, 20</span></span><br><span class="line">        <span class="comment">#   batch_size, 255, 40, 40</span></span><br><span class="line">        <span class="comment">#   batch_size, 255, 80, 80</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        batch_size      = <span class="built_in">input</span>.size(<span class="number">0</span>)</span><br><span class="line">        input_height    = <span class="built_in">input</span>.size(<span class="number">2</span>)</span><br><span class="line">        input_width     = <span class="built_in">input</span>.size(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   输入为416x416时</span></span><br><span class="line">        <span class="comment">#   stride_h = stride_w = 32、16、8</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        stride_h = self.input_shape[<span class="number">0</span>] / input_height</span><br><span class="line">        stride_w = self.input_shape[<span class="number">1</span>] / input_width</span><br><span class="line">        <span class="comment">#-------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   此时获得的scaled_anchors大小是相对于特征层的</span></span><br><span class="line">        <span class="comment">#-------------------------------------------------#</span></span><br><span class="line">        scaled_anchors = [(anchor_width / stride_w, anchor_height / stride_h) <span class="keyword">for</span> anchor_width, anchor_height <span class="keyword">in</span> self.anchors[self.anchors_mask[i]]]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   输入的input一共有三个，他们的shape分别是</span></span><br><span class="line">        <span class="comment">#   batch_size, 3, 20, 20, 85</span></span><br><span class="line">        <span class="comment">#   batch_size, 3, 40, 40, 85</span></span><br><span class="line">        <span class="comment">#   batch_size, 3, 80, 80, 85</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        prediction = <span class="built_in">input</span>.view(batch_size, <span class="built_in">len</span>(self.anchors_mask[i]),</span><br><span class="line">                                self.bbox_attrs, input_height, input_width).permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>).contiguous()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   先验框的中心位置的调整参数</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        x = torch.sigmoid(prediction[..., <span class="number">0</span>])  </span><br><span class="line">        y = torch.sigmoid(prediction[..., <span class="number">1</span>])</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   先验框的宽高调整参数</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        w = torch.sigmoid(prediction[..., <span class="number">2</span>]) </span><br><span class="line">        h = torch.sigmoid(prediction[..., <span class="number">3</span>]) </span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   获得置信度，是否有物体</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        conf        = torch.sigmoid(prediction[..., <span class="number">4</span>])</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   种类置信度</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        pred_cls    = torch.sigmoid(prediction[..., <span class="number">5</span>:])</span><br><span class="line"></span><br><span class="line">        FloatTensor = torch.cuda.FloatTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line">        LongTensor  = torch.cuda.LongTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.LongTensor</span><br><span class="line"></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   生成网格，先验框中心，网格左上角 </span></span><br><span class="line">        <span class="comment">#   batch_size,3,20,20</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        grid_x = torch.linspace(<span class="number">0</span>, input_width - <span class="number">1</span>, input_width).repeat(input_height, <span class="number">1</span>).repeat(</span><br><span class="line">            batch_size * <span class="built_in">len</span>(self.anchors_mask[i]), <span class="number">1</span>, <span class="number">1</span>).view(x.shape).<span class="built_in">type</span>(FloatTensor)</span><br><span class="line">        grid_y = torch.linspace(<span class="number">0</span>, input_height - <span class="number">1</span>, input_height).repeat(input_width, <span class="number">1</span>).t().repeat(</span><br><span class="line">            batch_size * <span class="built_in">len</span>(self.anchors_mask[i]), <span class="number">1</span>, <span class="number">1</span>).view(y.shape).<span class="built_in">type</span>(FloatTensor)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   按照网格格式生成先验框的宽高</span></span><br><span class="line">        <span class="comment">#   batch_size,3,20,20</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        anchor_w = FloatTensor(scaled_anchors).index_select(<span class="number">1</span>, LongTensor([<span class="number">0</span>]))</span><br><span class="line">        anchor_h = FloatTensor(scaled_anchors).index_select(<span class="number">1</span>, LongTensor([<span class="number">1</span>]))</span><br><span class="line">        anchor_w = anchor_w.repeat(batch_size, <span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, input_height * input_width).view(w.shape)</span><br><span class="line">        anchor_h = anchor_h.repeat(batch_size, <span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, input_height * input_width).view(h.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   利用预测结果对先验框进行调整</span></span><br><span class="line">        <span class="comment">#   首先调整先验框的中心，从先验框中心向右下角偏移</span></span><br><span class="line">        <span class="comment">#   再调整先验框的宽高。</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        pred_boxes          = FloatTensor(prediction[..., :<span class="number">4</span>].shape)</span><br><span class="line">        pred_boxes[..., <span class="number">0</span>]  = x.data * <span class="number">2.</span> - <span class="number">0.5</span> + grid_x</span><br><span class="line">        pred_boxes[..., <span class="number">1</span>]  = y.data * <span class="number">2.</span> - <span class="number">0.5</span> + grid_y</span><br><span class="line">        pred_boxes[..., <span class="number">2</span>]  = (w.data * <span class="number">2</span>) ** <span class="number">2</span> * anchor_w</span><br><span class="line">        pred_boxes[..., <span class="number">3</span>]  = (h.data * <span class="number">2</span>) ** <span class="number">2</span> * anchor_h</span><br><span class="line"></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   将输出结果归一化成小数的形式</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        _scale = torch.Tensor([input_width, input_height, input_width, input_height]).<span class="built_in">type</span>(FloatTensor)</span><br><span class="line">        output = torch.cat((pred_boxes.view(batch_size, -<span class="number">1</span>, <span class="number">4</span>) / _scale,</span><br><span class="line">                            conf.view(batch_size, -<span class="number">1</span>, <span class="number">1</span>), pred_cls.view(batch_size, -<span class="number">1</span>, self.num_classes)), -<span class="number">1</span>)</span><br><span class="line">        outputs.append(output.data)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="得分筛选和非极大值抑制"><a href="#得分筛选和非极大值抑制" class="headerlink" title="得分筛选和非极大值抑制"></a>得分筛选和非极大值抑制</h3><p>得到最终的预测结果后还要进行得分排序与非极大抑制筛选。</p>
<p>得分筛选就是筛选出得分满足confidence置信度的预测框。<br>非极大抑制就是筛选出一定区域内属于同一种类得分最大的框。</p>
<p>得分筛选与非极大抑制的过程可以概括如下：<br>1、找出该图片中得分大于门限函数的框。在进行重合框筛选前就进行得分的筛选可以大幅度减少框的数量。<br>2、对种类进行循环，非极大抑制的作用是筛选出一定区域内属于同一种类得分最大的框，对种类进行循环可以帮助我们对每一个类分别进行非极大抑制。<br>3、根据得分对该种类进行从大到小排序。<br>4、每次取出得分最大的框，计算其与其它所有预测框的重合程度，重合程度过大的则剔除。</p>
<p>得分筛选与非极大抑制后的结果就可以用于绘制预测框了。</p>
<p>下图是经过非极大抑制的：</p>
<p><style>.tyhozsuxexsf{zoom:50%;}</style><img src="/zh-CN/YOLOV5/YOLOV5/20200526140617608.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/20200526140617608.png" srcset="data:image/png;base64,666" class="tyhozsuxexsf lazyload" alt="在这里插入图片描述"></p>
<p>下图是未经过非极大值抑制的：</p>
<p><style>.whtkpmkncbve{zoom:50%;}</style><img src="/zh-CN/YOLOV5/YOLOV5/20200519211538419.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/20200519211538419.png" srcset="data:image/png;base64,666" class="whtkpmkncbve lazyload" alt="在这里插入图片描述"></p>
<p>实现代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">non_max_suppression</span>(<span class="params">self, prediction, num_classes, input_shape, image_shape, letterbox_image, conf_thres=<span class="number">0.5</span>, nms_thres=<span class="number">0.4</span></span>):</span><br><span class="line">    <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">    <span class="comment">#   将预测结果的格式转换成左上角右下角的格式。</span></span><br><span class="line">    <span class="comment">#   prediction  [batch_size, num_anchors, 85]</span></span><br><span class="line">    <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">    box_corner          = prediction.new(prediction.shape)</span><br><span class="line">    box_corner[:, :, <span class="number">0</span>] = prediction[:, :, <span class="number">0</span>] - prediction[:, :, <span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">    box_corner[:, :, <span class="number">1</span>] = prediction[:, :, <span class="number">1</span>] - prediction[:, :, <span class="number">3</span>] / <span class="number">2</span></span><br><span class="line">    box_corner[:, :, <span class="number">2</span>] = prediction[:, :, <span class="number">0</span>] + prediction[:, :, <span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">    box_corner[:, :, <span class="number">3</span>] = prediction[:, :, <span class="number">1</span>] + prediction[:, :, <span class="number">3</span>] / <span class="number">2</span></span><br><span class="line">    prediction[:, :, :<span class="number">4</span>] = box_corner[:, :, :<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">    output = [<span class="literal">None</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prediction))]</span><br><span class="line">    <span class="keyword">for</span> i, image_pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(prediction):</span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   对种类预测部分取max。</span></span><br><span class="line">        <span class="comment">#   class_conf  [num_anchors, 1]    种类置信度</span></span><br><span class="line">        <span class="comment">#   class_pred  [num_anchors, 1]    种类</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        class_conf, class_pred = torch.<span class="built_in">max</span>(image_pred[:, <span class="number">5</span>:<span class="number">5</span> + num_classes], <span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   利用置信度进行第一轮筛选</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        conf_mask = (image_pred[:, <span class="number">4</span>] * class_conf[:, <span class="number">0</span>] &gt;= conf_thres).squeeze()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   根据置信度进行预测结果的筛选</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------#</span></span><br><span class="line">        image_pred = image_pred[conf_mask]</span><br><span class="line">        class_conf = class_conf[conf_mask]</span><br><span class="line">        class_pred = class_pred[conf_mask]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_pred.size(<span class="number">0</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment">#-------------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   detections  [num_anchors, 7]</span></span><br><span class="line">        <span class="comment">#   7的内容为：x1, y1, x2, y2, obj_conf, class_conf, class_pred</span></span><br><span class="line">        <span class="comment">#-------------------------------------------------------------------------#</span></span><br><span class="line">        detections = torch.cat((image_pred[:, :<span class="number">5</span>], class_conf.<span class="built_in">float</span>(), class_pred.<span class="built_in">float</span>()), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   获得预测结果中包含的所有种类</span></span><br><span class="line">        <span class="comment">#------------------------------------------#</span></span><br><span class="line">        unique_labels = detections[:, -<span class="number">1</span>].cpu().unique()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> prediction.is_cuda:</span><br><span class="line">            unique_labels = unique_labels.cuda()</span><br><span class="line">            detections = detections.cuda()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> unique_labels:</span><br><span class="line">            <span class="comment">#------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   获得某一类得分筛选后全部的预测结果</span></span><br><span class="line">            <span class="comment">#------------------------------------------#</span></span><br><span class="line">            detections_class = detections[detections[:, -<span class="number">1</span>] == c]</span><br><span class="line"></span><br><span class="line">            <span class="comment">#------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   使用官方自带的非极大抑制会速度更快一些！</span></span><br><span class="line">            <span class="comment">#------------------------------------------#</span></span><br><span class="line">            keep = nms(</span><br><span class="line">                detections_class[:, :<span class="number">4</span>],</span><br><span class="line">                detections_class[:, <span class="number">4</span>] * detections_class[:, <span class="number">5</span>],</span><br><span class="line">                nms_thres</span><br><span class="line">            )</span><br><span class="line">            max_detections = detections_class[keep]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># # 按照存在物体的置信度排序</span></span><br><span class="line">            <span class="comment"># _, conf_sort_index = torch.sort(detections_class[:, 4]*detections_class[:, 5], descending=True)</span></span><br><span class="line">            <span class="comment"># detections_class = detections_class[conf_sort_index]</span></span><br><span class="line">            <span class="comment"># # 进行非极大抑制</span></span><br><span class="line">            <span class="comment"># max_detections = []</span></span><br><span class="line">            <span class="comment"># while detections_class.size(0):</span></span><br><span class="line">            <span class="comment">#     # 取出这一类置信度最高的，一步一步往下判断，判断重合程度是否大于nms_thres，如果是则去除掉</span></span><br><span class="line">            <span class="comment">#     max_detections.append(detections_class[0].unsqueeze(0))</span></span><br><span class="line">            <span class="comment">#     if len(detections_class) == 1:</span></span><br><span class="line">            <span class="comment">#         break</span></span><br><span class="line">            <span class="comment">#     ious = bbox_iou(max_detections[-1], detections_class[1:])</span></span><br><span class="line">            <span class="comment">#     detections_class = detections_class[1:][ious &lt; nms_thres]</span></span><br><span class="line">            <span class="comment"># # 堆叠</span></span><br><span class="line">            <span class="comment"># max_detections = torch.cat(max_detections).data</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Add max detections to outputs</span></span><br><span class="line">            output[i] = max_detections <span class="keyword">if</span> output[i] <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> torch.cat((output[i], max_detections))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> output[i] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output[i]           = output[i].cpu().numpy()</span><br><span class="line">            box_xy, box_wh      = (output[i][:, <span class="number">0</span>:<span class="number">2</span>] + output[i][:, <span class="number">2</span>:<span class="number">4</span>])/<span class="number">2</span>, output[i][:, <span class="number">2</span>:<span class="number">4</span>] - output[i][:, <span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">            output[i][:, :<span class="number">4</span>]    = self.yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image)</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="训练部分"><a href="#训练部分" class="headerlink" title="训练部分"></a>训练部分</h2><h3 id="loss组成"><a href="#loss组成" class="headerlink" title="loss组成"></a>loss组成</h3><p>计算loss实际上是网络的预测结果和网络的真实结果的对比。<br>和网络的预测结果一样，网络的损失也由三个部分组成，分别是Reg部分、Obj部分、Cls部分。Reg部分是特征点的回归参数判断、Obj部分是特征点是否包含物体判断、Cls部分是特征点包含的物体的种类。</p>
<h3 id="正样本的匹配过程"><a href="#正样本的匹配过程" class="headerlink" title="正样本的匹配过程"></a>正样本的匹配过程</h3><p>在YoloV5中，训练时正样本的匹配过程可以分为两部分。<br>a、匹配先验框。<br>b、匹配特征点。</p>
<p>所谓<strong>正样本匹配</strong>，就是<strong>寻找哪些先验框被认为有对应的真实框，并且负责这个真实框的预测</strong>。</p>
<h4 id="匹配先验框"><a href="#匹配先验框" class="headerlink" title="匹配先验框"></a>匹配先验框</h4><p>在YoloV5网络中，一共设计了9个不同大小的先验框。每个输出的特征层对应3个先验框。</p>
<p>对于任何一个真实框gt，YoloV5不再使用iou进行正样本的匹配，而是直接采用高宽比进行匹配，即使用真实框和9个不同大小的先验框计算宽高比。</p>
<p>如果真实框与某个先验框的宽高比例大于设定阈值，则说明该真实框和该先验框匹配度不够，将该先验框认为是负样本。</p>
<p>比如此时有一个真实框，它的宽高为[200, 200]，是一个正方形。YoloV5默认设置的9个先验框为[10,13], [16,30], [33,23], [30,61], [62,45], [59,119], [116,90], [156,198], [373,326]。设定阈值门限为4。</p>
<p>此时我们需要计算该真实框和9个先验框的宽高比例。比较宽高时存在两个情况，一个是真实框的宽高比先验框大，一个是先验框的宽高比真实框大。因此我们需要同时计算：真实框的宽高/先验框的宽高；先验框的宽高/真实框的宽高。然后在这其中选取最大值。</p>
<p>下个列表就是比较结果，这是一个shape为[9, 4]的矩阵，9代表9个先验框，4代表真实框的宽高/先验框的宽高；先验框的宽高/真实框的宽高。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">[[<span class="number">20.</span>         <span class="number">15.38461538</span>  <span class="number">0.05</span>        <span class="number">0.065</span>     ]</span><br><span class="line"> [<span class="number">12.5</span>         <span class="number">6.66666667</span>  <span class="number">0.08</span>        <span class="number">0.15</span>      ]</span><br><span class="line"> [ <span class="number">6.06060606</span>  <span class="number">8.69565217</span>  <span class="number">0.165</span>       <span class="number">0.115</span>     ]</span><br><span class="line"> [ <span class="number">6.66666667</span>  <span class="number">3.27868852</span>  <span class="number">0.15</span>        <span class="number">0.305</span>     ]</span><br><span class="line"> [ <span class="number">3.22580645</span>  <span class="number">4.44444444</span>  <span class="number">0.31</span>        <span class="number">0.225</span>     ]</span><br><span class="line"> [ <span class="number">3.38983051</span>  <span class="number">1.68067227</span>  <span class="number">0.295</span>       <span class="number">0.595</span>     ]</span><br><span class="line"> [ <span class="number">1.72413793</span>  <span class="number">2.22222222</span>  <span class="number">0.58</span>        <span class="number">0.45</span>      ]</span><br><span class="line"> [ <span class="number">1.28205128</span>  <span class="number">1.01010101</span>  <span class="number">0.78</span>        <span class="number">0.99</span>      ]</span><br><span class="line"> [ <span class="number">0.53619303</span>  <span class="number">0.61349693</span>  <span class="number">1.865</span>       <span class="number">1.63</span>      ]]</span><br></pre></td></tr></tbody></table></figure>
<p>然后对每个先验框的比较结果取最大值。获得下述矩阵：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">[<span class="number">20.</span>         <span class="number">12.5</span>         <span class="number">8.69565217</span>  <span class="number">6.66666667</span>  <span class="number">4.44444444</span>  <span class="number">3.38983051</span></span><br><span class="line">   <span class="number">2.22222222</span>  <span class="number">1.28205128</span>  <span class="number">1.865</span>     ]</span><br></pre></td></tr></tbody></table></figure>
<p>之后我们判断，哪些先验框的比较结果的值小于门限。可以知道[59,119], [116,90], [156,198], [373,326]四个先验框均满足需求。</p>
<p>[116,90], [156,198], [373,326]属于20,20的特征层。<br>[59,119]属于40,40的特征层。</p>
<p>此时我们已经可以判断哪些大小的先验框可用于该真实框的预测。</p>
<h4 id="匹配特征点"><a href="#匹配特征点" class="headerlink" title="匹配特征点"></a>匹配特征点</h4><p>在过去的Yolo系列中，每个真实框由其中心点所在的网格内的左上角特征点来负责预测。</p>
<p>对于被选中的特征层，首先计算真实框落在哪个网格内，此时<strong>该网格左上角特征点便是一个负责预测的特征点。</strong></p>
<p><strong>同时利用四舍五入规则，找出最近的两个网格，将这三个网格</strong>都认为是负责预测该真实框的。</p>
<p><style>.ocbqdonwuwtk{zoom:50%;}</style><img src="/zh-CN/YOLOV5/YOLOV5/ddf3729aef4240b3aa6e5aa914ffff52.png" class="lazyload" data-srcset="/zh-CN/YOLOV5/YOLOV5/ddf3729aef4240b3aa6e5aa914ffff52.png" srcset="data:image/png;base64,666" class="ocbqdonwuwtk lazyload" alt="在这里插入图片描述"></p>
<p><strong>红色点表示该真实框的中心</strong>，除了当前所处的网格外，其2个最近的邻域网格也被选中。从这里就可以发现预测框的XY轴偏移部分的取值范围不再是0-1，而是0.5-1.5。</p>
<p><strong>找到对应特征点后，对应特征点在a中被选中的先验框负责该真实框的预测。</strong></p>
<h3 id="计算loss"><a href="#计算loss" class="headerlink" title="计算loss"></a>计算loss</h3><p>由第一部分可知，YoloV5的损失由三个部分组成：<br>1、Reg部分，由第2部分可知道每个真实框对应的先验框，获取到每个框对应的先验框后，取出该先验框对应的预测框，利用真实框和预测框计算CIOU损失，作为Reg部分的Loss组成。<br>2、Obj部分，由第2部分可知道每个真实框对应的先验框，所有真实框对应的先验框都是正样本，剩余的先验框均为负样本，根据正负样本和特征点的是否包含物体的预测结果计算交叉熵损失，作为Obj部分的Loss组成。<br>3、Cls部分，由第三部分可知道每个真实框对应的先验框，获取到每个框对应的先验框后，取出该先验框的种类预测结果，根据真实框的种类和先验框的种类预测结果计算交叉熵损失，作为Cls部分的Loss组成。<br>实现代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YOLOLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, anchors, num_classes, input_shape, cuda, anchors_mask = [[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>], [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>], [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]], label_smoothing = <span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(YOLOLoss, self).__init__()</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   13x13的特征层对应的anchor是[142, 110],[192, 243],[459, 401]</span></span><br><span class="line">        <span class="comment">#   26x26的特征层对应的anchor是[36, 75],[76, 55],[72, 146]</span></span><br><span class="line">        <span class="comment">#   52x52的特征层对应的anchor是[12, 16],[19, 36],[40, 28]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        self.anchors        = anchors</span><br><span class="line">        self.num_classes    = num_classes</span><br><span class="line">        self.bbox_attrs     = <span class="number">5</span> + num_classes</span><br><span class="line">        self.input_shape    = input_shape</span><br><span class="line">        self.anchors_mask   = anchors_mask</span><br><span class="line">        self.label_smoothing = label_smoothing</span><br><span class="line"></span><br><span class="line">        self.threshold      = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">        self.balance        = [<span class="number">0.4</span>, <span class="number">1.0</span>, <span class="number">4</span>]</span><br><span class="line">        self.box_ratio      = <span class="number">5</span></span><br><span class="line">        self.cls_ratio      = <span class="number">0.5</span></span><br><span class="line">        self.obj_ratio      = <span class="number">1</span></span><br><span class="line">        self.cuda = cuda</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">clip_by_tensor</span>(<span class="params">self, t, t_min, t_max</span>):</span><br><span class="line">        t = t.<span class="built_in">float</span>()</span><br><span class="line">        result = (t &gt;= t_min).<span class="built_in">float</span>() * t + (t &lt; t_min).<span class="built_in">float</span>() * t_min</span><br><span class="line">        result = (result &lt;= t_max).<span class="built_in">float</span>() * result + (result &gt; t_max).<span class="built_in">float</span>() * t_max</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">MSELoss</span>(<span class="params">self, pred, target</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">pow</span>(pred - target, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">BCELoss</span>(<span class="params">self, pred, target</span>):</span><br><span class="line">        epsilon = <span class="number">1e-7</span></span><br><span class="line">        pred    = self.clip_by_tensor(pred, epsilon, <span class="number">1.0</span> - epsilon)</span><br><span class="line">        output  = - target * torch.log(pred) - (<span class="number">1.0</span> - target) * torch.log(<span class="number">1.0</span> - pred)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">box_giou</span>(<span class="params">self, b1, b2</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        输入为：</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        b1: tensor, shape=(batch, feat_w, feat_h, anchor_num, 4), xywh</span></span><br><span class="line"><span class="string">        b2: tensor, shape=(batch, feat_w, feat_h, anchor_num, 4), xywh</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回为：</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        giou: tensor, shape=(batch, feat_w, feat_h, anchor_num, 1)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   求出预测框左上角右下角</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        b1_xy       = b1[..., :<span class="number">2</span>]</span><br><span class="line">        b1_wh       = b1[..., <span class="number">2</span>:<span class="number">4</span>]</span><br><span class="line">        b1_wh_half  = b1_wh/<span class="number">2.</span></span><br><span class="line">        b1_mins     = b1_xy - b1_wh_half</span><br><span class="line">        b1_maxes    = b1_xy + b1_wh_half</span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   求出真实框左上角右下角</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        b2_xy       = b2[..., :<span class="number">2</span>]</span><br><span class="line">        b2_wh       = b2[..., <span class="number">2</span>:<span class="number">4</span>]</span><br><span class="line">        b2_wh_half  = b2_wh/<span class="number">2.</span></span><br><span class="line">        b2_mins     = b2_xy - b2_wh_half</span><br><span class="line">        b2_maxes    = b2_xy + b2_wh_half</span><br><span class="line"></span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   求真实框和预测框所有的iou</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        intersect_mins  = torch.<span class="built_in">max</span>(b1_mins, b2_mins)</span><br><span class="line">        intersect_maxes = torch.<span class="built_in">min</span>(b1_maxes, b2_maxes)</span><br><span class="line">        intersect_wh    = torch.<span class="built_in">max</span>(intersect_maxes - intersect_mins, torch.zeros_like(intersect_maxes))</span><br><span class="line">        intersect_area  = intersect_wh[..., <span class="number">0</span>] * intersect_wh[..., <span class="number">1</span>]</span><br><span class="line">        b1_area         = b1_wh[..., <span class="number">0</span>] * b1_wh[..., <span class="number">1</span>]</span><br><span class="line">        b2_area         = b2_wh[..., <span class="number">0</span>] * b2_wh[..., <span class="number">1</span>]</span><br><span class="line">        union_area      = b1_area + b2_area - intersect_area</span><br><span class="line">        iou             = intersect_area / union_area</span><br><span class="line"></span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   找到包裹两个框的最小框的左上角和右下角</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        enclose_mins    = torch.<span class="built_in">min</span>(b1_mins, b2_mins)</span><br><span class="line">        enclose_maxes   = torch.<span class="built_in">max</span>(b1_maxes, b2_maxes)</span><br><span class="line">        enclose_wh      = torch.<span class="built_in">max</span>(enclose_maxes - enclose_mins, torch.zeros_like(intersect_maxes))</span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算对角线距离</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        enclose_area    = enclose_wh[..., <span class="number">0</span>] * enclose_wh[..., <span class="number">1</span>]</span><br><span class="line">        giou            = iou - (enclose_area - union_area) / enclose_area</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> giou</span><br><span class="line"></span><br><span class="line">    <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">    <span class="comment">#   平滑标签</span></span><br><span class="line">    <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">smooth_labels</span>(<span class="params">self, y_true, label_smoothing, num_classes</span>):</span><br><span class="line">        <span class="keyword">return</span> y_true * (<span class="number">1.0</span> - label_smoothing) + label_smoothing / num_classes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, l, <span class="built_in">input</span>, targets=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   l 代表使用的是第几个有效特征层</span></span><br><span class="line">        <span class="comment">#   input的shape为  bs, 3*(5+num_classes), 13, 13</span></span><br><span class="line">        <span class="comment">#                   bs, 3*(5+num_classes), 26, 26</span></span><br><span class="line">        <span class="comment">#                   bs, 3*(5+num_classes), 52, 52</span></span><br><span class="line">        <span class="comment">#   targets 真实框的标签情况 [batch_size, num_gt, 5]</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#--------------------------------#</span></span><br><span class="line">        <span class="comment">#   获得图片数量，特征层的高和宽</span></span><br><span class="line">        <span class="comment">#--------------------------------#</span></span><br><span class="line">        bs      = <span class="built_in">input</span>.size(<span class="number">0</span>)</span><br><span class="line">        in_h    = <span class="built_in">input</span>.size(<span class="number">2</span>)</span><br><span class="line">        in_w    = <span class="built_in">input</span>.size(<span class="number">3</span>)</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算步长</span></span><br><span class="line">        <span class="comment">#   每一个特征点对应原来的图片上多少个像素点</span></span><br><span class="line">        <span class="comment">#   </span></span><br><span class="line">        <span class="comment">#   如果特征层为13x13的话，一个特征点就对应原来的图片上的32个像素点</span></span><br><span class="line">        <span class="comment">#   如果特征层为26x26的话，一个特征点就对应原来的图片上的16个像素点</span></span><br><span class="line">        <span class="comment">#   如果特征层为52x52的话，一个特征点就对应原来的图片上的8个像素点</span></span><br><span class="line">        <span class="comment">#   stride_h = stride_w = 32、16、8</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------------#</span></span><br><span class="line">        stride_h = self.input_shape[<span class="number">0</span>] / in_h</span><br><span class="line">        stride_w = self.input_shape[<span class="number">1</span>] / in_w</span><br><span class="line">        <span class="comment">#-------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   此时获得的scaled_anchors大小是相对于特征层的</span></span><br><span class="line">        <span class="comment">#-------------------------------------------------#</span></span><br><span class="line">        scaled_anchors  = [(a_w / stride_w, a_h / stride_h) <span class="keyword">for</span> a_w, a_h <span class="keyword">in</span> self.anchors]</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   输入的input一共有三个，他们的shape分别是</span></span><br><span class="line">        <span class="comment">#   bs, 3 * (5+num_classes), 13, 13 =&gt; bs, 3, 5 + num_classes, 13, 13 =&gt; batch_size, 3, 13, 13, 5 + num_classes</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#   batch_size, 3, 13, 13, 5 + num_classes</span></span><br><span class="line">        <span class="comment">#   batch_size, 3, 26, 26, 5 + num_classes</span></span><br><span class="line">        <span class="comment">#   batch_size, 3, 52, 52, 5 + num_classes</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        prediction = <span class="built_in">input</span>.view(bs, <span class="built_in">len</span>(self.anchors_mask[l]), self.bbox_attrs, in_h, in_w).permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>).contiguous()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   先验框的中心位置的调整参数</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        x = torch.sigmoid(prediction[..., <span class="number">0</span>])</span><br><span class="line">        y = torch.sigmoid(prediction[..., <span class="number">1</span>])</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   先验框的宽高调整参数</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        w = torch.sigmoid(prediction[..., <span class="number">2</span>]) </span><br><span class="line">        h = torch.sigmoid(prediction[..., <span class="number">3</span>]) </span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   获得置信度，是否有物体</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        conf = torch.sigmoid(prediction[..., <span class="number">4</span>])</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   种类置信度</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        pred_cls = torch.sigmoid(prediction[..., <span class="number">5</span>:])</span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   获得网络应该有的预测结果</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------#</span></span><br><span class="line">        y_true, noobj_mask, box_loss_scale = self.get_target(l, targets, scaled_anchors, in_h, in_w)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#---------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   将预测结果进行解码，判断预测结果和真实值的重合程度</span></span><br><span class="line">        <span class="comment">#   如果重合程度过大则忽略，因为这些特征点属于预测比较准确的特征点</span></span><br><span class="line">        <span class="comment">#   作为负样本不合适</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------------#</span></span><br><span class="line">        pred_boxes = self.get_pred_boxes(l, x, y, h, w, targets, scaled_anchors, in_h, in_w)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            y_true          = y_true.cuda()</span><br><span class="line">            noobj_mask      = noobj_mask.cuda()</span><br><span class="line">            box_loss_scale  = box_loss_scale.cuda()</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   reshape_y_true[...,2:3]和reshape_y_true[...,3:4]</span></span><br><span class="line">        <span class="comment">#   表示真实框的宽高，二者均在0-1之间</span></span><br><span class="line">        <span class="comment">#   真实框越大，比重越小，小框的比重更大。</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        box_loss_scale = <span class="number">2</span> - box_loss_scale</span><br><span class="line"></span><br><span class="line">        <span class="comment">#---------------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算预测结果和真实结果的giou</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------------#</span></span><br><span class="line">        giou        = self.box_giou(pred_boxes[y_true[..., <span class="number">4</span>] == <span class="number">1</span>], y_true[..., :<span class="number">4</span>][y_true[..., <span class="number">4</span>] == <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        loss_loc    = torch.<span class="built_in">sum</span>((<span class="number">1</span> - giou) * box_loss_scale[y_true[..., <span class="number">4</span>] == <span class="number">1</span>])</span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算置信度的loss</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------#</span></span><br><span class="line">        loss_conf   = torch.<span class="built_in">sum</span>(self.BCELoss(conf[y_true[..., <span class="number">4</span>] == <span class="number">1</span>], giou.detach().clamp(<span class="number">0</span>))) + \</span><br><span class="line">                      torch.<span class="built_in">sum</span>(self.BCELoss(conf, y_true[..., <span class="number">4</span>]) * noobj_mask)</span><br><span class="line">        loss_cls    = torch.<span class="built_in">sum</span>(self.BCELoss(pred_cls[y_true[..., <span class="number">4</span>] == <span class="number">1</span>], self.smooth_labels(y_true[..., <span class="number">5</span>:][y_true[..., <span class="number">4</span>] == <span class="number">1</span>], self.label_smoothing, self.num_classes)))</span><br><span class="line"></span><br><span class="line">        loss        = loss_loc * self.box_ratio + loss_conf * self.balance[l] * self.obj_ratio + loss_cls * self.cls_ratio</span><br><span class="line">        num_pos = torch.<span class="built_in">sum</span>(y_true[..., <span class="number">4</span>])</span><br><span class="line">        num_pos = torch.<span class="built_in">max</span>(num_pos, torch.ones_like(num_pos))</span><br><span class="line">        <span class="keyword">return</span> loss, num_pos</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_near_points</span>(<span class="params">self, x, y, i, j</span>):</span><br><span class="line">        sub_x = x - i</span><br><span class="line">        sub_y = y - j</span><br><span class="line">        <span class="keyword">if</span> sub_x &gt; <span class="number">0.5</span> <span class="keyword">and</span> sub_y &gt; <span class="number">0.5</span>:</span><br><span class="line">            <span class="keyword">return</span> [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">elif</span> sub_x &lt; <span class="number">0.5</span> <span class="keyword">and</span> sub_y &gt; <span class="number">0.5</span>:</span><br><span class="line">            <span class="keyword">return</span> [[<span class="number">0</span>, <span class="number">0</span>], [-<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">elif</span> sub_x &lt; <span class="number">0.5</span> <span class="keyword">and</span> sub_y &lt; <span class="number">0.5</span>:</span><br><span class="line">            <span class="keyword">return</span> [[<span class="number">0</span>, <span class="number">0</span>], [-<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, -<span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, -<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_target</span>(<span class="params">self, l, targets, anchors, in_h, in_w</span>):</span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算一共有多少张图片</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        bs              = <span class="built_in">len</span>(targets)</span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   用于选取哪些先验框不包含物体</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        noobj_mask      = torch.ones(bs, <span class="built_in">len</span>(self.anchors_mask[l]), in_h, in_w, requires_grad = <span class="literal">False</span>)</span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   让网络更加去关注小目标</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        box_loss_scale  = torch.zeros(bs, <span class="built_in">len</span>(self.anchors_mask[l]), in_h, in_w, requires_grad = <span class="literal">False</span>)</span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   anchors_best_ratio</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        box_best_ratio = torch.zeros(bs, <span class="built_in">len</span>(self.anchors_mask[l]), in_h, in_w, requires_grad = <span class="literal">False</span>)</span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   batch_size, 3, 13, 13, 5 + num_classes</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        y_true          = torch.zeros(bs, <span class="built_in">len</span>(self.anchors_mask[l]), in_h, in_w, self.bbox_attrs, requires_grad = <span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(bs):            </span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(targets[b])==<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            batch_target = torch.zeros_like(targets[b])</span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   计算出正样本在特征层上的中心点</span></span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            batch_target[:, [<span class="number">0</span>,<span class="number">2</span>]] = targets[b][:, [<span class="number">0</span>,<span class="number">2</span>]] * in_w</span><br><span class="line">            batch_target[:, [<span class="number">1</span>,<span class="number">3</span>]] = targets[b][:, [<span class="number">1</span>,<span class="number">3</span>]] * in_h</span><br><span class="line">            batch_target[:, <span class="number">4</span>] = targets[b][:, <span class="number">4</span>]</span><br><span class="line">            batch_target = batch_target.cpu()</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            <span class="comment">#   batch_target            : num_true_box, 4</span></span><br><span class="line">            <span class="comment">#   anchors                 : 9, 2</span></span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            <span class="comment">#   ratios_of_gt_anchors    : num_true_box, 9, 2</span></span><br><span class="line">            <span class="comment">#   ratios_of_anchors_gt    : num_true_box, 9, 2</span></span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            <span class="comment">#   ratios                  : num_true_box, 9, 4</span></span><br><span class="line">            <span class="comment">#   max_ratios              : num_true_box, 9</span></span><br><span class="line">            <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">            ratios_of_gt_anchors = torch.unsqueeze(batch_target[:, <span class="number">2</span>:<span class="number">4</span>], <span class="number">1</span>) / torch.unsqueeze(torch.FloatTensor(anchors), <span class="number">0</span>)</span><br><span class="line">            ratios_of_anchors_gt = torch.unsqueeze(torch.FloatTensor(anchors), <span class="number">0</span>) /  torch.unsqueeze(batch_target[:, <span class="number">2</span>:<span class="number">4</span>], <span class="number">1</span>)</span><br><span class="line">            ratios               = torch.cat([ratios_of_gt_anchors, ratios_of_anchors_gt], dim = -<span class="number">1</span>)</span><br><span class="line">            max_ratios, _        = torch.<span class="built_in">max</span>(ratios, dim = -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> t, ratio <span class="keyword">in</span> <span class="built_in">enumerate</span>(max_ratios):</span><br><span class="line">                <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">                <span class="comment">#   ratio : 9</span></span><br><span class="line">                <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">                over_threshold = ratio &lt; self.threshold</span><br><span class="line">                over_threshold[torch.argmin(ratio)] = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">for</span> k, mask <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.anchors_mask[l]):</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> over_threshold[mask]:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="comment">#----------------------------------------#</span></span><br><span class="line">                    <span class="comment">#   获得真实框属于哪个网格点</span></span><br><span class="line">                    <span class="comment">#----------------------------------------#</span></span><br><span class="line">                    i = torch.floor(batch_target[t, <span class="number">0</span>]).long()</span><br><span class="line">                    j = torch.floor(batch_target[t, <span class="number">1</span>]).long()</span><br><span class="line">                    </span><br><span class="line">                    offsets = self.get_near_points(batch_target[t, <span class="number">0</span>], batch_target[t, <span class="number">1</span>], i, j)</span><br><span class="line">                    <span class="keyword">for</span> offset <span class="keyword">in</span> offsets:</span><br><span class="line">                        local_i = i + offset[<span class="number">0</span>]</span><br><span class="line">                        local_j = j + offset[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> local_i &gt;= in_w <span class="keyword">or</span> local_i &lt; <span class="number">0</span> <span class="keyword">or</span> local_j &gt;= in_h <span class="keyword">or</span> local_j &lt; <span class="number">0</span>:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> box_best_ratio[b, k, local_j, local_i] != <span class="number">0</span>:</span><br><span class="line">                            <span class="keyword">if</span> box_best_ratio[b, k, local_j, local_i] &gt; ratio[mask]:</span><br><span class="line">                                y_true[b, k, local_j, local_i, :] = <span class="number">0</span></span><br><span class="line">                            <span class="keyword">else</span>:</span><br><span class="line">                                <span class="keyword">continue</span></span><br><span class="line">                            </span><br><span class="line">                        <span class="comment">#----------------------------------------#</span></span><br><span class="line">                        <span class="comment">#   取出真实框的种类</span></span><br><span class="line">                        <span class="comment">#----------------------------------------#</span></span><br><span class="line">                        c = batch_target[t, <span class="number">4</span>].long()</span><br><span class="line"></span><br><span class="line">                        <span class="comment">#----------------------------------------#</span></span><br><span class="line">                        <span class="comment">#   noobj_mask代表无目标的特征点</span></span><br><span class="line">                        <span class="comment">#----------------------------------------#</span></span><br><span class="line">                        noobj_mask[b, k, local_j, local_i] = <span class="number">0</span></span><br><span class="line">                        <span class="comment">#----------------------------------------#</span></span><br><span class="line">                        <span class="comment">#   tx、ty代表中心调整参数的真实值</span></span><br><span class="line">                        <span class="comment">#----------------------------------------#</span></span><br><span class="line">                        y_true[b, k, local_j, local_i, <span class="number">0</span>] = batch_target[t, <span class="number">0</span>]</span><br><span class="line">                        y_true[b, k, local_j, local_i, <span class="number">1</span>] = batch_target[t, <span class="number">1</span>]</span><br><span class="line">                        y_true[b, k, local_j, local_i, <span class="number">2</span>] = batch_target[t, <span class="number">2</span>]</span><br><span class="line">                        y_true[b, k, local_j, local_i, <span class="number">3</span>] = batch_target[t, <span class="number">3</span>]</span><br><span class="line">                        y_true[b, k, local_j, local_i, <span class="number">4</span>] = <span class="number">1</span></span><br><span class="line">                        y_true[b, k, local_j, local_i, c + <span class="number">5</span>] = <span class="number">1</span></span><br><span class="line">                        <span class="comment">#----------------------------------------#</span></span><br><span class="line">                        <span class="comment">#   用于获得xywh的比例</span></span><br><span class="line">                        <span class="comment">#   大目标loss权重小，小目标loss权重大</span></span><br><span class="line">                        <span class="comment">#----------------------------------------#</span></span><br><span class="line">                        box_loss_scale[b, k, local_j, local_i] = batch_target[t, <span class="number">2</span>] * batch_target[t, <span class="number">3</span>] / in_w / in_h</span><br><span class="line">                        <span class="comment">#----------------------------------------#</span></span><br><span class="line">                        <span class="comment">#   获得当前先验框最好的比例</span></span><br><span class="line">                        <span class="comment">#----------------------------------------#</span></span><br><span class="line">                        box_best_ratio[b, k, local_j, local_i] = ratio[mask]</span><br><span class="line">                        </span><br><span class="line">        <span class="keyword">return</span> y_true, noobj_mask, box_loss_scale</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_pred_boxes</span>(<span class="params">self, l, x, y, h, w, targets, scaled_anchors, in_h, in_w</span>):</span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算一共有多少张图片</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        bs = <span class="built_in">len</span>(targets)</span><br><span class="line"></span><br><span class="line">        FloatTensor = torch.cuda.FloatTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.FloatTensor</span><br><span class="line">        LongTensor  = torch.cuda.LongTensor <span class="keyword">if</span> x.is_cuda <span class="keyword">else</span> torch.LongTensor</span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   生成网格，先验框中心，网格左上角</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------#</span></span><br><span class="line">        grid_x = torch.linspace(<span class="number">0</span>, in_w - <span class="number">1</span>, in_w).repeat(in_h, <span class="number">1</span>).repeat(</span><br><span class="line">            <span class="built_in">int</span>(bs * <span class="built_in">len</span>(self.anchors_mask[l])), <span class="number">1</span>, <span class="number">1</span>).view(x.shape).<span class="built_in">type</span>(FloatTensor)</span><br><span class="line">        grid_y = torch.linspace(<span class="number">0</span>, in_h - <span class="number">1</span>, in_h).repeat(in_w, <span class="number">1</span>).t().repeat(</span><br><span class="line">            <span class="built_in">int</span>(bs * <span class="built_in">len</span>(self.anchors_mask[l])), <span class="number">1</span>, <span class="number">1</span>).view(y.shape).<span class="built_in">type</span>(FloatTensor)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成先验框的宽高</span></span><br><span class="line">        scaled_anchors_l = np.array(scaled_anchors)[self.anchors_mask[l]]</span><br><span class="line">        anchor_w = FloatTensor(scaled_anchors_l).index_select(<span class="number">1</span>, LongTensor([<span class="number">0</span>]))</span><br><span class="line">        anchor_h = FloatTensor(scaled_anchors_l).index_select(<span class="number">1</span>, LongTensor([<span class="number">1</span>]))</span><br><span class="line">        </span><br><span class="line">        anchor_w = anchor_w.repeat(bs, <span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, in_h * in_w).view(w.shape)</span><br><span class="line">        anchor_h = anchor_h.repeat(bs, <span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, in_h * in_w).view(h.shape)</span><br><span class="line">        <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   计算调整后的先验框中心与宽高</span></span><br><span class="line">        <span class="comment">#-------------------------------------------------------#</span></span><br><span class="line">        pred_boxes_x    = torch.unsqueeze(x * <span class="number">2.</span> - <span class="number">0.5</span> + grid_x, -<span class="number">1</span>)</span><br><span class="line">        pred_boxes_y    = torch.unsqueeze(y * <span class="number">2.</span> - <span class="number">0.5</span> + grid_y, -<span class="number">1</span>)</span><br><span class="line">        pred_boxes_w    = torch.unsqueeze((w * <span class="number">2</span>) ** <span class="number">2</span> * anchor_w, -<span class="number">1</span>)</span><br><span class="line">        pred_boxes_h    = torch.unsqueeze((h * <span class="number">2</span>) ** <span class="number">2</span> * anchor_h, -<span class="number">1</span>)</span><br><span class="line">        pred_boxes      = torch.cat([pred_boxes_x, pred_boxes_y, pred_boxes_w, pred_boxes_h], dim = -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> pred_boxes</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>yolov5</tag>
      </tags>
  </entry>
  <entry>
    <title>epoch和iteration的区别</title>
    <url>/zh-CN/epoch%E5%92%8Citeration%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/u011731135/article/details/81481868">epoch和iteration的区别</a></p>
<p>（1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；<br>（2）iteration：1个iteration等于使用batchsize个样本训练一次；一个迭代 = 一个正向通过+一个反向通过<br>（3）epoch：1个epoch等于使用训练集中的全部样本训练一次；一个epoch = 所有训练样本的一个正向传递和一个反向传递</p>
<p>举个例子，训练集有1000个样本，batchsize=10，那么：<br>训练完整个样本集需要：<br>100次iteration，1次epoch。</p>
<p>但是也有人说在有些文章中epoch和iteration是一个概念。</p>
<p>还有就是随机取batchsize，那么并不能保证所有的样本都被抽到，那么epoch的概念不明确。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>epoch&amp;iteration</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch中contiguous函数的用法</title>
    <url>/zh-CN/contiguous/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p><a href="http://www.ddrfans.com/Html/1/177713.html#:~:text=contiguous tensor变量调用contiguous (">Pytorch之contiguous的用法 </a>函数会使tensor变量在内存中的存储变得连续。 contiguous,()：view只能用在contiguous的variable上。如果在view之前用了transpose%2C permute等，需要用contiguous ()来返回一个contiguous copy。)</p>
<p><strong>contiguous</strong></p>
<p>tensor变量调用contiguous()函数会使tensor变量在内存中的存储变得连续。</p>
<p>contiguous()：view只能用在contiguous的variable上。如果在view之前用了transpose, permute等，需要用contiguous()来返回一个contiguous copy。</p>
<p><strong>一种可能的解释是：</strong></p>
<p>有些tensor并不是占用一整块内存，而是由不同的数据块组成，而tensor的view()操作依赖于内存是整块的，这时只需要执行contiguous()这个函数，把tensor变成在内存中连续分布的形式。</p>
<p><strong>is_contiguous</strong></p>
<p><strong>判断是否contiguous用torch.Tensor.is_contiguous()函数。</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">x = torch.ones(10, 10)</span><br><span class="line">x.is_contiguous() # True</span><br><span class="line">x.transpose(0, 1).is_contiguous() # False</span><br><span class="line">x.transpose(0, 1).contiguous().is_contiguous() # True</span><br></pre></td></tr></tbody></table></figure>
<p>在pytorch的最新版本0.4版本中，增加了torch.reshape(), 这与 numpy.reshape 的功能类似。它大致相当于 tensor.contiguous().view()</p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>函数库</tag>
        <tag>contiguous</tag>
      </tags>
  </entry>
  <entry>
    <title>github使用指南</title>
    <url>/zh-CN/github%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h4 id="git安装"><a href="#git安装" class="headerlink" title="git安装"></a>git安装</h4><p>1.安装git OSX版</p>
<p>下载地址：<a href="http://git-scm.com/download/mac">http://git-scm.com/download/mac</a></p>
<p>2.安装git Windows版</p>
<p>下载地址：<a href="http://book.git-scm.com/download/win">http://book.git-scm.com/download/win</a></p>
<p>3.安装git Linux版</p>
<p>下载地址：<a href="http://book.git-scm.com/download/linux">http://book.git-scm.com/download/linux</a></p>
<h4 id="创建新仓库"><a href="#创建新仓库" class="headerlink" title="创建新仓库"></a>创建新仓库</h4><p>创建新文件夹，打开，然后执行<code>git init</code>以创建新的git仓库</p>
<h4 id="检出仓库"><a href="#检出仓库" class="headerlink" title="检出仓库"></a>检出仓库</h4><p>执行如下命令以创建一个本地仓库的克隆版本：</p>
<p><code>git clone /path/to/repository</code></p>
<p>如果是远端服务器上的仓库，则使用如下命令：</p>
<p><code>git clone username@host:/path/to/repository</code></p>
<h4 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h4><p>你的本地仓库由git维护的三棵“树”组成。第一个是你的<strong>工作目录</strong>，它持有实际文件；第二个是<strong>暂存区（index)</strong>,它像个缓存区域，临时保存你的改动；最后是<strong>HEAD</strong>,它指向你最后一次提交的结果。</p>
<h4 id="添加和提交"><a href="#添加和提交" class="headerlink" title="添加和提交"></a>添加和提交</h4><p>你可以提出更改（把它们添加到暂存区），使用如下命令：</p>
<p><code>git add &lt;filename&gt;</code></p>
<p><code>git add *</code></p>
<p>这是git基本工作流程的第一步；使用如下命令以实际提交改动：</p>
<p><code>git commit -m "代码提交信息"</code></p>
<p>现在，你的改动已经提交到了HEAD,但是还没到你的远端仓库。</p>
<h4 id="推送改动"><a href="#推送改动" class="headerlink" title="推送改动"></a>推送改动</h4><p>你的改动现在已经在本地仓库的HEAD中了。执行如下命令以将这些改动提交到远端仓库：</p>
<p><code>git push origin master</code></p>
<p>可以把<em>master</em>换成你想要推送的任何分支。</p>
<p>如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加：</p>
<p><code>git remote add origin &lt;server&gt;</code></p>
<p>如此你就能够将你的改动推送到所添加的服务器上去了。</p>
<h4 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h4><p>分支是用来将特性开发绝缘开来的。在你创建仓库的时候，<em>master</em>是默认的分支。在其他分支上进行开发，完成后再将它们合并到主分支上。</p>
<p>创建一个叫做“feature_x”的分支，并切换过去：</p>
<p><code>git checkout -b feature_x</code></p>
<p>切换回主分支：</p>
<p><code>git checkout master</code></p>
<p>再把新建的分支删掉：</p>
<p><code>git branch -d feature_x</code></p>
<p>除非你将分支推送到远端仓库，不然该分支就是不为他人所见的：</p>
<p><code>git push origin &lt;branch&gt;</code></p>
<h4 id="更新与合并"><a href="#更新与合并" class="headerlink" title="更新与合并"></a>更新与合并</h4><p>要更新你的本地仓库至最新改动，执行：<br><code>git pull</code><br>以在你的工作目录中 <em>获取（fetch）</em> 并 <em>合并（merge）</em> 远端的改动。<br>要合并其他分支到你的当前分支（例如 master），执行：<br><code>git merge &lt;branch&gt;</code><br>在这两种情况下，git 都会尝试去自动合并改动。遗憾的是，这可能并非每次都成功，并可能出现<em>冲突（conflicts）</em>。 这时候就需要你修改这些文件来手动合并这些<em>冲突（conflicts）</em>。改完之后，你需要执行如下命令以将它们标记为合并成功：</p>
<p><code>git add &lt;filename&gt;</code><br>在合并改动之前，你可以使用如下命令预览差异：<br><code>git diff &lt;source_branch&gt; &lt;target_branch&gt;</code></p>
<h4 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h4><p>为软件发布创建标签是推荐的。这个概念早已存在，在 SVN 中也有。你可以执行如下命令创建一个叫做 <em>1.0.0</em> 的标签：<br><code>git tag 1.0.0 1b2e1d63ff</code><br><em>1b2e1d63ff</em> 是你想要标记的提交 ID 的前 10 位字符。可以使用下列命令获取提交 ID：<br><code>git log</code><br>你也可以使用少一点的提交 ID 前几位，只要它的指向具有唯一性。</p>
<h4 id="log"><a href="#log" class="headerlink" title="log"></a>log</h4><p>如果你想了解本地仓库的历史记录，最简单的命令就是使用:<br><code>git log</code><br>你可以添加一些参数来修改他的输出，从而得到自己想要的结果。 只看某一个人的提交记录:<br><code>git log --author=bob</code><br>一个压缩后的每一条提交记录只占一行的输出:<br><code>git log --pretty=oneline</code><br>或者你想通过 ASCII 艺术的树形结构来展示所有的分支, 每个分支都标示了他的名字和标签:<br><code>git log --graph --oneline --decorate --all</code><br>看看哪些文件改变了:<br><code>git log --name-status</code><br>这些只是你可以使用的参数中很小的一部分。更多的信息，参考：<br><code>git log --help</code></p>
<h4 id="替换本地改动"><a href="#替换本地改动" class="headerlink" title="替换本地改动"></a>替换本地改动</h4><p>假如你操作失误（当然，这最好永远不要发生），你可以使用如下命令替换掉本地改动：<br><code>git checkout -- &lt;filename&gt;</code><br>此命令会使用 HEAD 中的最新内容替换掉你的工作目录中的文件。已添加到暂存区的改动以及新文件都不会受到影响。</p>
<p>假如你想丢弃你在本地的所有改动与提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它：<br><code>git fetch origin</code><br><code>git reset --hard origin/master</code></p>
<h4 id="实用小贴士"><a href="#实用小贴士" class="headerlink" title="实用小贴士"></a>实用小贴士</h4><p>内建的图形化 git：<br><code>gitk</code><br>彩色的 git 输出：<br><code>git config color.ui true</code><br>显示历史记录时，每个提交的信息只显示一行：<br><code>git config format.pretty oneline</code><br>交互式添加文件到暂存区：<br><code>git add -i</code></p>
<p>更多内容请参考：<a href="http://rogerdudler.github.io/git-guide/index.zh.html">git - 简明指南</a></p>
]]></content>
      <categories>
        <category>github</category>
      </categories>
      <tags>
        <tag>使用指南</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo标签插件测试</title>
    <url>/zh-CN/hexo%E6%A0%87%E7%AD%BE%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>本文语法参考<a href="https://volantis.js.org/v4/tag-plugins/">volantis4.0</a>,博客正在使用的版本为4.3.1</p>
<p>问题总结如下：</p>
<p>1.在复选框checkbox下，选中状态比不选中状态框提前，对不齐。</p>
<p>2.button基础按钮那里本文写的参考有点问题，不知道哪里出了问题，但源码没错。</p>
<h3 id="text"><a href="#text" class="headerlink" title="text"></a>text</h3><p>带 <u>下划线</u> 的文本；带 <emp>着重号</emp> 的文本；带 <wavy>波浪线</wavy> 的文本；带 <del>删除线</del> 的文本</p>
<p>键盘样式的文本：<kbd>⌘</kbd> + <kbd>D</kbd></p>
<p>密码样式的文本：<psw>这里没有验证码</psw></p>
<p>源码：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">带 {% u 下划线 %} 的文本；带 {% emp 着重号 %} 的文本；带 {% wavy 波浪线 %} 的文本；带 {% del 删除线 %} 的文本</span><br><span class="line"></span><br><span class="line">键盘样式的文本：{% kbd ⌘ %} + {% kbd D %}</span><br><span class="line"></span><br><span class="line">密码样式的文本：{% psw 这里没有验证码 %}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="span"><a href="#span" class="headerlink" title="span"></a>span</h3><p>语法：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% span 样式参数, 文本内容 %}</span><br></pre></td></tr></tbody></table></figure>
<p>效果：</p>
<h4 id="彩色文字"><a href="#彩色文字" class="headerlink" title="彩色文字"></a>彩色文字</h4><p>在一段话中方便插入各种颜色的标签，包括：<span class="p red">红色</span>、<span class="p yellow">黄色</span>、<span class="p green">绿色</span>、<span class="p cyan">青色</span>、<span class="p blue">蓝色</span>、<span class="p gray">灰色</span>。</p>
<h4 id="超大号文字"><a href="#超大号文字" class="headerlink" title="超大号文字"></a>超大号文字</h4><p>文档「开始」页面中的标题部分就是超大号文字。</p>
<span class="p center logo large">Volantis</span>
<span class="p center small">A Wonderful Theme for Hexo</span>
<p>源码:</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="section">#### 彩色文字</span></span><br><span class="line"></span><br><span class="line">在一段话中方便插入各种颜色的标签，包括：{% span red, 红色 %}、{% span yellow, 黄色 %}、{% span green, 绿色 %}、{% span cyan, 青色 %}、{% span blue, 蓝色 %}、{% span gray, 灰色 %}。</span><br><span class="line"></span><br><span class="line"><span class="section">#### 超大号文字</span></span><br><span class="line"></span><br><span class="line">文档「开始」页面中的标题部分就是超大号文字。</span><br><span class="line"></span><br><span class="line">{% span center logo large, Volantis %}</span><br><span class="line">{% span center small, A Wonderful Theme for Hexo %}</span><br></pre></td></tr></tbody></table></figure>
<p>参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>可选值</th>
</tr>
</thead>
<tbody>
<tr>
<td>字体</td>
<td><code>logo</code>, <code>code</code></td>
</tr>
<tr>
<td>颜色</td>
<td><code>red</code>, <code>yellow</code>, <code>green</code>, <code>cyan</code>, <code>blue</code>, <code>gray</code></td>
</tr>
<tr>
<td>大小</td>
<td><code>small</code>, <code>h4</code>, <code>h3</code>, <code>h2</code>, <code>h1</code>, <code>large</code>, <code>huge</code>, <code>ultra</code></td>
</tr>
<tr>
<td>对齐方向</td>
<td><code>left</code>, <code>center</code>, <code>right</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="p"><a href="#p" class="headerlink" title="p"></a>p</h3><p>语法：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% p 样式参数, 文本内容 %}</span><br></pre></td></tr></tbody></table></figure>
<p>效果：</p>
<h4 id="彩色文字-1"><a href="#彩色文字-1" class="headerlink" title="彩色文字"></a>彩色文字</h4><p>在一段话中方便插入各种颜色的标签，包括：</p><p class="p red">红色</p><p class="p yellow">黄色</p><p class="p green">绿色</p><p class="p cyan">青色</p><p class="p blue">蓝色</p><p class="p gray">灰色</p><p></p>
<h4 id="超大号文字-1"><a href="#超大号文字-1" class="headerlink" title="超大号文字"></a>超大号文字</h4><p>文档「开始」页面中的标题部分就是超大号文字。</p>
<p class="p center logo large">Volantis</p>
<p class="p center small">A Wonderful Theme for Hexo</p>
<p>源码：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="section">#### 彩色文字</span></span><br><span class="line"></span><br><span class="line">在一段话中方便插入各种颜色的标签，包括：{% p red, 红色 %}{% p yellow, 黄色 %}{% p green, 绿色 %}{% p cyan, 青色 %}{% p blue, 蓝色 %}{% p gray, 灰色 %}</span><br><span class="line"></span><br><span class="line"><span class="section">#### 超大号文字</span></span><br><span class="line"></span><br><span class="line">文档「开始」页面中的标题部分就是超大号文字。</span><br><span class="line"></span><br><span class="line">{% p center logo large, Volantis %}</span><br><span class="line">{% p center small, A Wonderful Theme for Hexo %}</span><br></pre></td></tr></tbody></table></figure>
<p>参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>可选值</th>
</tr>
</thead>
<tbody>
<tr>
<td>字体</td>
<td><code>logo</code>, <code>code</code></td>
</tr>
<tr>
<td>颜色</td>
<td><code>red</code>, <code>yellow</code>, <code>green</code>, <code>cyan</code>, <code>blue</code>, <code>gray</code></td>
</tr>
<tr>
<td>大小</td>
<td><code>small</code>, <code>h4</code>, <code>h3</code>, <code>h2</code>, <code>h1</code>, <code>large</code>, <code>huge</code>, <code>ultra</code></td>
</tr>
<tr>
<td>对齐方向</td>
<td><code>left</code>, <code>center</code>, <code>right</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="note"><a href="#note" class="headerlink" title="note"></a>note</h3><p>NoteBlock 是 Blockquote 的增强版，在左边显示图标，并且可以自定颜色。而 Note 是 NoteBlock 的简便写法。</p>
<p>语法：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% note 样式参数, 文本内容 %}</span><br></pre></td></tr></tbody></table></figure>
<p>效果：</p>
<h4 id="经典用法"><a href="#经典用法" class="headerlink" title="经典用法"></a>经典用法</h4><div class="note "><p>可以在配置文件中设置默认样式，为简单的一句话提供最的简便写法。</p></div>
<div class="note quote"><p>note quote 适合引用一段话</p></div>
<div class="note info"><p>note info 默认主题色，适合中性的信息</p></div>
<div class="note warning"><p>note warning 默认黄色，适合警告性的信息</p></div>
<div class="note danger"><p>note error/danger 默认红色，适合危险性的信息</p></div>
<div class="note success"><p>note done/success 默认绿色，适合正确操作的信息</p></div>

#### 更多图标

这些都是默认样式，可以手动加上颜色：

<div class="note radiation"><p>note radiation 默认样式</p></div>
<div class="note radiation yellow"><p>note radiation yellow 可以加上颜色</p></div>
<div class="note bug red"><p>note bug red 说明还存在的一些故障</p></div>
<div class="note link green"><p>note link green 可以放置一些链接</p></div>
<div class="note paperclip blue"><p>note paperclip blue 放置一些附件链接</p></div>
<div class="note todo"><p>note todo 待办事项</p></div>
<div class="note guide clear"><p>note guide clear 可以加上一段向导</p></div>
<div class="note download"><p>note download 可以放置下载链接</p></div>
<div class="note message gray"><p>note message gray 一段消息</p></div>
<div class="note up"><p>note up 可以说明如何进行更新</p></div>
<div class="note undo light"><p>note undo light 可以说明如何撤销或者回退</p></div>

源码：

<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="section">#### 经典用法</span></span><br><span class="line"></span><br><span class="line">{% note, 可以在配置文件中设置默认样式，为简单的一句话提供最的简便写法。 %}</span><br><span class="line">{% note quote, note quote 适合引用一段话 %}</span><br><span class="line">{% note info, note info 默认主题色，适合中性的信息 %}</span><br><span class="line">{% note warning, note warning 默认黄色，适合警告性的信息 %}</span><br><span class="line">{% note danger, note error/danger 默认红色，适合危险性的信息 %}</span><br><span class="line">{% note success, note done/success 默认绿色，适合正确操作的信息 %}</span><br><span class="line"></span><br><span class="line"><span class="section">#### 更多图标</span></span><br><span class="line"></span><br><span class="line">这些都是默认样式，可以手动加上颜色：</span><br><span class="line"></span><br><span class="line">{% note radiation, note radiation 默认样式 %}</span><br><span class="line">{% note radiation yellow, note radiation yellow 可以加上颜色 %}</span><br><span class="line">{% note bug red, note bug red 说明还存在的一些故障 %}</span><br><span class="line">{% note link green, note link green 可以放置一些链接 %}</span><br><span class="line">{% note paperclip blue, note paperclip blue 放置一些附件链接 %}</span><br><span class="line">{% note todo, note todo 待办事项 %}</span><br><span class="line">{% note guide clear, note guide clear 可以加上一段向导 %}</span><br><span class="line">{% note download, note download 可以放置下载链接 %}</span><br><span class="line">{% note message gray, note message gray 一段消息 %}</span><br><span class="line">{% note up, note up 可以说明如何进行更新 %}</span><br><span class="line">{% note undo light, note undo light 可以说明如何撤销或者回退 %}</span><br></pre></td></tr></tbody></table></figure>

参数:

| 属性 | 可选值                                                       |
| ---- | :----------------------------------------------------------- |
| 图标 | # 彩色的      quote, info, warning, done/success, error/danger                                                                                                                                                                                                 # 灰色的，也可以指定颜色      radiation, bug, idea, link, paperclip, todo, message, guide, download, up, undo |
| 颜色 | clear, light, gray, red, yellow, green, cyan, blue           |

### noteblock

NoteBlock 是 Blockquote 的增强版，在左边显示图标，并且可以自定颜色。而 Note 是 NoteBlock 的简便写法。

语法：

<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% noteblock 样式参数（可选）, 标题（可选） %}</span><br><span class="line">文本段落</span><br><span class="line">{% endnoteblock %}</span><br></pre></td></tr></tbody></table></figure>

演示效果：

<div class="note "><p><strong>标题（可选）</strong></p><p>Windows 10不是為所有人設計,而是為每個人設計</p><p></p><div class="note done"><p>嵌套测试： 请坐和放宽，我正在帮你搞定一切…</p></div><p></p><details yellow=""><summary> Folding 测试： 点击查看更多 </summary>              <div class="content">              <div class="note warning"><p>不要说我们没有警告过你</p></div><div class="note bug red"><p>我们都有不顺利的时候</p></div>              </div>            </details></div>
<p>源码：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% noteblock, 标题（可选） %}</span><br><span class="line"></span><br><span class="line">Windows 10不是為所有人設計,而是為每個人設計</span><br><span class="line"></span><br><span class="line">{% noteblock done %}</span><br><span class="line">嵌套测试： 请坐和放宽，我正在帮你搞定一切...</span><br><span class="line">{% endnoteblock %}</span><br><span class="line"></span><br><span class="line">{% folding yellow, Folding 测试： 点击查看更多 %}</span><br><span class="line"></span><br><span class="line">{% note warning, 不要说我们没有警告过你 %}</span><br><span class="line">{% noteblock bug red %}</span><br><span class="line">我们都有不顺利的时候</span><br><span class="line">{% endnoteblock %}</span><br><span class="line"></span><br><span class="line">{% endfolding %}</span><br><span class="line">{% endnoteblock %}</span><br></pre></td></tr></tbody></table></figure>
<p>参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th style="text-align:left">可选值</th>
</tr>
</thead>
<tbody>
<tr>
<td>图标</td>
<td style="text-align:left"># 彩色的      quote, info, warning, done/success, error/danger                                                                                                                                                                                                 # 灰色的，也可以指定颜色      radiation, bug, idea, link, paperclip, todo, message, guide, download, up, undo</td>
</tr>
<tr>
<td>颜色</td>
<td style="text-align:left">clear, light, gray, red, yellow, green, cyan, blue</td>
</tr>
</tbody>
</table>
</div>
<h3 id="checkbox"><a href="#checkbox" class="headerlink" title="checkbox"></a>checkbox</h3><p>语法：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% checkbox 样式参数（可选）, 文本（支持简单md） %}</span><br></pre></td></tr></tbody></table></figure>
<p>演示效果：</p>
<div class="checkbox"><input type="checkbox">
            <p>纯文本测试</p>
            </div>
<div class="checkbox checked"><input type="checkbox" checked="checked">
            <p>支持简单的 <a href="https://guides.github.com/features/mastering-markdown/">markdown</a> 语法</p>
            </div>
<div class="checkbox red"><input type="checkbox">
            <p>支持自定义颜色</p>
            </div>
<div class="checkbox green checked"><input type="checkbox" checked="checked">
            <p>绿色 + 默认选中</p>
            </div>
<div class="checkbox yellow checked"><input type="checkbox" checked="checked">
            <p>黄色 + 默认选中</p>
            </div>
<div class="checkbox cyan checked"><input type="checkbox" checked="checked">
            <p>青色 + 默认选中</p>
            </div>
<div class="checkbox blue checked"><input type="checkbox" checked="checked">
            <p>蓝色 + 默认选中</p>
            </div>
<div class="checkbox plus green checked"><input type="checkbox" checked="checked">
            <p>增加</p>
            </div>
<div class="checkbox minus yellow checked"><input type="checkbox" checked="checked">
            <p>减少</p>
            </div>
<div class="checkbox times red checked"><input type="checkbox" checked="checked">
            <p>叉</p>
            </div>
<p>源码：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="section"># 复选框</span></span><br><span class="line">{% checkbox 纯文本测试 %}</span><br><span class="line">{% checkbox checked, 支持简单的 [<span class="string">markdown</span>](<span class="link">https://guides.github.com/features/mastering-markdown/</span>) 语法 %}</span><br><span class="line">{% checkbox red, 支持自定义颜色 %}</span><br><span class="line">{% checkbox green checked, 绿色 + 默认选中 %}</span><br><span class="line">{% checkbox yellow checked, 黄色 + 默认选中 %}</span><br><span class="line">{% checkbox cyan checked, 青色 + 默认选中 %}</span><br><span class="line">{% checkbox blue checked, 蓝色 + 默认选中 %}</span><br><span class="line">{% checkbox plus green checked, 增加 %}</span><br><span class="line">{% checkbox minus yellow checked, 减少 %}</span><br><span class="line">{% checkbox times red checked, 叉 %}</span><br></pre></td></tr></tbody></table></figure>
<p>参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>可选值</th>
</tr>
</thead>
<tbody>
<tr>
<td>颜色</td>
<td>red, yellow, green, cyan, blue</td>
</tr>
<tr>
<td>样式</td>
<td>plus, minus, times</td>
</tr>
<tr>
<td>选中状态</td>
<td>checked</td>
</tr>
</tbody>
</table>
</div>
<h3 id="radio"><a href="#radio" class="headerlink" title="radio"></a>radio</h3><p>语法：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="section"># 单选框</span></span><br><span class="line">{% radio 样式参数（可选）, 文本（支持简单md） %}</span><br></pre></td></tr></tbody></table></figure>
<p>演示效果：</p>
<div class="checkbox"><input type="radio">
            <p>纯文本测试</p>
            </div>
<div class="checkbox checked"><input type="radio" checked="checked">
            <p>支持简单的 <a href="https://guides.github.com/features/mastering-markdown/">markdown</a> 语法</p>
            </div>
<div class="checkbox red"><input type="radio">
            <p>支持自定义颜色</p>
            </div>
<div class="checkbox green"><input type="radio">
            <p>绿色</p>
            </div>
<div class="checkbox yellow"><input type="radio">
            <p>黄色</p>
            </div>
<div class="checkbox cyan"><input type="radio">
            <p>青色</p>
            </div>
<div class="checkbox blue"><input type="radio">
            <p>蓝色</p>
            </div>
<p>源码：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% radio 纯文本测试 %}</span><br><span class="line">{% radio checked, 支持简单的 [<span class="string">markdown</span>](<span class="link">https://guides.github.com/features/mastering-markdown/</span>) 语法 %}</span><br><span class="line">{% radio red, 支持自定义颜色 %}</span><br><span class="line">{% radio green, 绿色 %}</span><br><span class="line">{% radio yellow, 黄色 %}</span><br><span class="line">{% radio cyan, 青色 %}</span><br><span class="line">{% radio blue, 蓝色 %}</span><br></pre></td></tr></tbody></table></figure>
<p>参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>可选值</th>
</tr>
</thead>
<tbody>
<tr>
<td>颜色</td>
<td>red, yellow, green, cyan, blue</td>
</tr>
<tr>
<td>选中状态</td>
<td>checked</td>
</tr>
</tbody>
</table>
</div>
<h3 id="timeline"><a href="#timeline" class="headerlink" title="timeline"></a>timeline</h3><p>语法：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% timeline 时间线标题（可选） %}</span><br><span class="line"></span><br><span class="line">{% timenode 时间节点（标题） %}</span><br><span class="line"></span><br><span class="line">正文内容</span><br><span class="line"></span><br><span class="line">{% endtimenode %}</span><br><span class="line"></span><br><span class="line">{% timenode 时间节点（标题） %}</span><br><span class="line"></span><br><span class="line">正文内容</span><br><span class="line"></span><br><span class="line">{% endtimenode %}</span><br><span class="line"></span><br><span class="line">{% endtimeline %}</span><br></pre></td></tr></tbody></table></figure>
<p>效果：</p>
<div class="timeline">
<div class="timenode"><div class="meta"><p></p><p>2020-07-24 <a href="https://github.com/volantis-x/hexo-theme-volantis/releases">2.6.6 -&gt; 3.0</a></p>
<p></p></div><div class="body"><ol><li>如果有 <code>hexo-lazyload-image</code> 插件，需要删除并重新安装最新版本，设置 <code>lazyload.isSPA: true</code>。</li><li>2.x 版本的 css 和 js 不适用于 3.x 版本，如果使用了 <code>use_cdn: true</code> 则需要删除。</li><li>2.x 版本的 fancybox 标签在 3.x 版本中被重命名为 gallery 。</li><li>2.x 版本的置顶 <code>top: true</code> 改为了 <code>pin: true</code>，并且同样适用于 <code>layout: page</code> 的页面。</li><li>如果使用了 <code>hexo-offline</code> 插件，建议卸载，3.0 版本默认开启了 pjax 服务。</li></ol></div></div>

<div class="timenode"><div class="meta"><p></p><p>2020-05-15 <a href="https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.6">2.6.3 -&gt; 2.6.6</a></p>
<p></p></div><div class="body"><p>不需要额外处理。</p></div></div>

<div class="timenode"><div class="meta"><p></p><p>2020-04-20 <a href="https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.3">2.6.2 -&gt; 2.6.3</a></p>
<p></p></div><div class="body"><ol><li>全局搜索 <code>seotitle</code> 并替换为 <code>seo_title</code>。</li><li>group 组件的索引规则有变，使用 group 组件的文章内，<code>group: group_name</code> 对应的组件名必须是 <code>group_name</code>。</li><li>group 组件的列表名优先显示文章的 <code>short_title</code> 其次是 <code>title</code>。</li></ol></div></div>
</div>
<p>源码：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% timeline %}</span><br><span class="line"></span><br><span class="line">{% timenode 2020-07-24 [<span class="string">2.6.6 -&gt; 3.0</span>](<span class="link">https://github.com/volantis-x/hexo-theme-volantis/releases</span>) %}</span><br><span class="line"></span><br><span class="line"><span class="bullet">1.</span> 如果有 <span class="code">`hexo-lazyload-image`</span> 插件，需要删除并重新安装最新版本，设置 <span class="code">`lazyload.isSPA: true`</span>。</span><br><span class="line"><span class="bullet">2.</span> 2.x 版本的 css 和 js 不适用于 3.x 版本，如果使用了 <span class="code">`use_cdn: true`</span> 则需要删除。</span><br><span class="line"><span class="bullet">3.</span> 2.x 版本的 fancybox 标签在 3.x 版本中被重命名为 gallery 。</span><br><span class="line"><span class="bullet">4.</span> 2.x 版本的置顶 <span class="code">`top: true`</span> 改为了 <span class="code">`pin: true`</span>，并且同样适用于 <span class="code">`layout: page`</span> 的页面。</span><br><span class="line"><span class="bullet">5.</span> 如果使用了 <span class="code">`hexo-offline`</span> 插件，建议卸载，3.0 版本默认开启了 pjax 服务。</span><br><span class="line"></span><br><span class="line">{% endtimenode %}</span><br><span class="line"></span><br><span class="line">{% timenode 2020-05-15 [<span class="string">2.6.3 -&gt; 2.6.6</span>](<span class="link">https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.6</span>) %}</span><br><span class="line"></span><br><span class="line">不需要额外处理。</span><br><span class="line"></span><br><span class="line">{% endtimenode %}</span><br><span class="line"></span><br><span class="line">{% timenode 2020-04-20 [<span class="string">2.6.2 -&gt; 2.6.3</span>](<span class="link">https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.3</span>) %}</span><br><span class="line"></span><br><span class="line"><span class="bullet">1.</span> 全局搜索 <span class="code">`seotitle`</span> 并替换为 <span class="code">`seo_title`</span>。</span><br><span class="line"><span class="bullet">2.</span> group 组件的索引规则有变，使用 group 组件的文章内，<span class="code">`group: group_name`</span> 对应的组件名必须是 <span class="code">`group_name`</span>。</span><br><span class="line"><span class="bullet">2.</span> group 组件的列表名优先显示文章的 <span class="code">`short_title`</span> 其次是 <span class="code">`title`</span>。</span><br><span class="line"></span><br><span class="line">{% endtimenode %}</span><br><span class="line"></span><br><span class="line">{% endtimeline %}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="link"><a href="#link" class="headerlink" title="link"></a>link</h3><p>语法：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">{% link 标题, 链接, 图片链接（可选） %}</span><br></pre></td></tr></tbody></table></figure>
<p>演示效果：</p>
<div class="tag link"><a class="link-card" title="如何参与项目" href="https://volantis.js.org/contributors/"><div class="left"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets@master/logo/256/safari.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets@master/logo/256/safari.png" srcset="data:image/png;base64,666"></div><div class="right"><p class="text">如何参与项目</p><p class="url">https://volantis.js.org/contributors/</p></div></a></div>
<p>源码：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% link 如何参与项目, https://volantis.js.org/contributors/, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets@master/logo/256/safari.png %}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="button"><a href="#button" class="headerlink" title="button"></a>button</h3><h4 id="基础按钮"><a href="#基础按钮" class="headerlink" title="基础按钮"></a>基础按钮</h4><p>语法：`<span class="btn 样式参数（可选）"><a class="button" href="链接" title="标题"><i class="图标（可选）"></i>标题</a></span>`

参数如下：

样式参数：

`regular, large, center`

图标：

第1个或者第2个参数包含 `fa-` 的那个被识别为图标。

效果：

不设置任何参数的 <span class="btn"><a class="button" href="/" title="按钮">按钮</a></span> 适合融入段落中。

regular 按钮适合独立于段落之外：

<span class="btn regular"><a class="button" href="https://xaoxuu.com" title="示例博客"><i class="fas fa-play-circle"></i>示例博客</a></span>

large 按钮更具有强调作用，建议搭配 center 使用：

<span class="btn center large"><a class="button" href="https://volantis.js.org/v3/getting-started/" title="开始使用"><i class="fas fa-download"></i>开始使用</a></span>

源码：

</p><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">不设置任何参数的 {% btn 按钮, / %} 适合融入段落中。</span><br><span class="line"></span><br><span class="line">regular 按钮适合独立于段落之外：</span><br><span class="line"></span><br><span class="line">{% btn regular, 示例博客, https://xaoxuu.com, fas fa-play-circle %}</span><br><span class="line"></span><br><span class="line">large 按钮更具有强调作用，建议搭配 center 使用：</span><br><span class="line"></span><br><span class="line">{% btn center large, 开始使用, https://volantis.js.org/v3/getting-started/, fas fa-download %}</span><br></pre></td></tr></tbody></table></figure>

#### 富文本按钮

语法：

<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% btns 样式参数 %}</span><br><span class="line">{% cell 标题, 链接, 图片或者图标 %}</span><br><span class="line">{% cell 标题, 链接, 图片或者图标 %}</span><br><span class="line">{% endbtns %}</span><br></pre></td></tr></tbody></table></figure>

参数列表：

样式参数位置可以写图片样式、布局方式，多个样式参数用空格隔开。

圆角样式：

<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="section"># 默认为方形</span></span><br><span class="line">rounded, circle</span><br></pre></td></tr></tbody></table></figure>

布局方式：

默认为自动宽度，适合视野内只有一两个的情况。

| 参数   | 含义                                     |
| ------ | ---------------------------------------- |
| wide   | 宽一点的按钮                             |
| fill   | 填充布局，自动铺满至少一行，多了会换行。 |
| center | 居中，按钮之间是固定间距。               |
| around | 居中分散                                 |
| grid2  | 等宽最多2列，屏幕变窄会适当减少列数。    |
| grid3  | 等宽最多3列，屏幕变窄会适当减少列数。    |
| grid4  | 等宽最多4列，屏幕变窄会适当减少列数。    |
| grid5  | 等宽最多5列，屏幕变窄会适当减少列数。    |

增加文字样式：

可以在容器内增加 `<b>标题</b>` 和 `<p></p><p>描述文字</p>`

效果：

如果需要显示类似「团队成员」之类的一组含有头像的链接：

<div class="btns circle grid5">
            <a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" srcset="data:image/png;base64,666">xaoxuu</a>
<a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" srcset="data:image/png;base64,666">xaoxuu</a>
<a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" srcset="data:image/png;base64,666">xaoxuu</a>
<a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" srcset="data:image/png;base64,666">xaoxuu</a>
<a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" srcset="data:image/png;base64,666">xaoxuu</a>
          </div><p></p>
<p>或者含有图标的按钮：</p>
<div class="btns rounded grid5">
            <a class="button" href="/" title="下载源码"><i class="fas fa-download"></i>下载源码</a>
<a class="button" href="/" title="查看文档"><i class="fas fa-book-open"></i>查看文档</a>
          </div>
<p>圆形图标 + 标题 + 描述 + 图片 + 网格5列 + 居中</p>
<div class="btns circle center grid5">
            <a href="https://apps.apple.com/cn/app/heart-mate-pro-hrm-utility/id1463348922?ls=1">
  <i class="fab fa-apple"></i>
  <b>心率管家</b>
  <p class="p red">专业版</p>
  <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_pro.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_pro.png" srcset="data:image/png;base64,666">
</a>
<a href="https://apps.apple.com/cn/app/heart-mate-lite-hrm-utility/id1475747930?ls=1">
  <i class="fab fa-apple"></i>
  <b>心率管家</b>
  <p class="p green">免费版</p>
  <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_lite.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_lite.png" srcset="data:image/png;base64,666">
</a>
          </div>
<p>源码：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="section"># 如果需要显示类似「团队成员」之类的一组含有头像的链接：</span></span><br><span class="line">{% btns circle grid5 %}</span><br><span class="line">{% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %}</span><br><span class="line">{% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %}</span><br><span class="line">{% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %}</span><br><span class="line">{% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %}</span><br><span class="line">{% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %}</span><br><span class="line">{% endbtns %}</span><br><span class="line"><span class="section"># 或者含有图标的按钮：</span></span><br><span class="line">{% btns rounded grid5 %}</span><br><span class="line">{% cell 下载源码, /, fas fa-download %}</span><br><span class="line">{% cell 查看文档, /, fas fa-book-open %}</span><br><span class="line">{% endbtns %}</span><br><span class="line"><span class="section"># 圆形图标 + 标题 + 描述 + 图片 + 网格5列 + 居中</span></span><br><span class="line">{% btns circle center grid5 %}</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'https://apps.apple.com/cn/app/heart-mate-pro-hrm-utility/id1463348922?ls=1'</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">'fab fa-apple'</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">b</span>&gt;</span></span>心率管家<span class="language-xml"><span class="tag">&lt;/<span class="name">b</span>&gt;</span></span></span><br><span class="line">  {% p red, 专业版 %}</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_pro.png'</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'https://apps.apple.com/cn/app/heart-mate-lite-hrm-utility/id1475747930?ls=1'</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">'fab fa-apple'</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">b</span>&gt;</span></span>心率管家<span class="language-xml"><span class="tag">&lt;/<span class="name">b</span>&gt;</span></span></span><br><span class="line">  {% p green, 免费版 %}</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_lite.png'</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span></span><br><span class="line">{% endbtns %}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="ghcard"><a href="#ghcard" class="headerlink" title="ghcard"></a>ghcard</h3><p>样式：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% ghcard 用户名, 其它参数（可选） %}</span><br><span class="line">{% ghcard 用户名/仓库, 其它参数（可选） %}</span><br></pre></td></tr></tbody></table></figure>
<p>效果示例：</p>
<p>如下所示，其实就是一个4×2的表格，只不过每个格子里放的是用户信息卡片</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;show_owner=true" srcset="data:image/png;base64,666"></a></th>
<th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistacho0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistacho0812&amp;theme=vue&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistacho0812&amp;theme=vue&amp;show_owner=true" srcset="data:image/png;base64,666"></a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;theme=buefy&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;theme=buefy&amp;show_owner=true" srcset="data:image/png;base64,666"></a></td>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;theme=solarized-light&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;theme=solarized-light&amp;show_owner=true" srcset="data:image/png;base64,666"></a></td>
</tr>
<tr>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;theme=onedark&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;theme=onedark&amp;show_owner=true" srcset="data:image/png;base64,666"></a></td>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;theme=solarized-dark&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;theme=solarized-dark&amp;show_owner=true" srcset="data:image/png;base64,666"></a></td>
</tr>
<tr>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;theme=algolia&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;theme=algolia&amp;show_owner=true" srcset="data:image/png;base64,666"></a></td>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;theme=calm&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&amp;theme=calm&amp;show_owner=true" srcset="data:image/png;base64,666"></a></td>
</tr>
</tbody>
</table>
</div>
<p>源码：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="section"># 用户信息卡片</span></span><br><span class="line">| {% ghcard pistachio0812 %} | {% ghcard pistacho0812, theme=vue %} |</span><br><span class="line">| -- | -- |</span><br><span class="line">| {% ghcard pistachio0812, theme=buefy %} | {% ghcard pistachio0812, theme=solarized-light %} |</span><br><span class="line">| {% ghcard pistachio0812, theme=onedark %} | {% ghcard pistachio0812, theme=solarized-dark %} |</span><br><span class="line">| {% ghcard pistachio0812, theme=algolia %} | {% ghcard pistachio0812, theme=calm %} |</span><br></pre></td></tr></tbody></table></figure>
<p>效果示例：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;show_owner=true" srcset="data:image/png;base64,666"></a></th>
<th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=vue&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=vue&amp;show_owner=true" srcset="data:image/png;base64,666"></a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=buefy&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=buefy&amp;show_owner=true" srcset="data:image/png;base64,666"></a></td>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=solarized-light&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=solarized-light&amp;show_owner=true" srcset="data:image/png;base64,666"></a></td>
</tr>
<tr>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=onedark&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=onedark&amp;show_owner=true" srcset="data:image/png;base64,666"></a></td>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=solarized-dark&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=solarized-dark&amp;show_owner=true" srcset="data:image/png;base64,666"></a></td>
</tr>
<tr>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=algolia&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=algolia&amp;show_owner=true" srcset="data:image/png;base64,666"></a></td>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=calm&amp;show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&amp;repo=hexo-theme-volantis&amp;theme=calm&amp;show_owner=true" srcset="data:image/png;base64,666"></a></td>
</tr>
</tbody>
</table>
</div>
<p>源码：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="section"># 仓库信息卡片</span></span><br><span class="line">| {% ghcard volantis-x/hexo-theme-volantis %} | {% ghcard volantis-x/hexo-theme-volantis, theme=vue %} |</span><br><span class="line">| -- | -- |</span><br><span class="line">| {% ghcard volantis-x/hexo-theme-volantis, theme=buefy %} | {% ghcard volantis-x/hexo-theme-volantis, theme=solarized-light %} |</span><br><span class="line">| {% ghcard volantis-x/hexo-theme-volantis, theme=onedark %} | {% ghcard volantis-x/hexo-theme-volantis, theme=solarized-dark %} |</span><br><span class="line">| {% ghcard volantis-x/hexo-theme-volantis, theme=algolia %} | {% ghcard volantis-x/hexo-theme-volantis, theme=calm %} |</span><br></pre></td></tr></tbody></table></figure>
<p>更多参数选择：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<div class="tag link"><a class="link-card" title="GitHub卡片API参数" href="https://github.com/anuraghazra/github-readme-stats"><div class="left"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets@master/logo/256/safari.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets@master/logo/256/safari.png" srcset="data:image/png;base64,666"></div><div class="right"><p class="text">GitHub卡片API参数</p><p class="url">https://github.com/anuraghazra/github-readme-stats</p></div></a></div>
<h3 id="site"><a href="#site" class="headerlink" title="site"></a>site</h3><p>网站卡片可以显示网站截图、logo、标题、描述，使用方法和友链标签一模一样，唯一的区别是数据文件名称为 <code>sites.yml</code>，可以和友链数据混用，通过分组过滤实现不一样的效果。</p>
<p>样式：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% sites only:community<span class="emphasis">_team %}</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="dropmenu"><a href="#dropmenu" class="headerlink" title="dropmenu"></a>dropmenu</h3><h4 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h4><p>容器：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% menu 前缀（可省略）, 标题, 后缀（可省略） %}</span><br><span class="line">菜单内容</span><br><span class="line">{% endmenu %}</span><br></pre></td></tr></tbody></table></figure>
<p>菜单内容：</p>
<p>1.菜单项</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% menuitem 文本, 链接, 图标 %}</span><br></pre></td></tr></tbody></table></figure>
<p>2.分割线</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% menuitem hr %}</span><br></pre></td></tr></tbody></table></figure>
<p>3.子菜单</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% submenu 嵌套菜单, 图标 %}</span><br><span class="line">菜单内容</span><br><span class="line">{% endsubmenu %}</span><br></pre></td></tr></tbody></table></figure>
<h4 id="示例效果"><a href="#示例效果" class="headerlink" title="示例效果"></a>示例效果</h4><p>示例1</p>
<div class="dropmenu-wrapper">
              <div class="dropmenu">
                <a>下拉菜单</a>
                <ul class="list-v">
                  <li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/" title="主题源码">
                  <i class="fas fa-file-code fa-fw"></i>
                  主题源码
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/releases/" title="更新日志">
                  <i class="fas fa-clipboard-list fa-fw"></i>
                  更新日志
                </a>
              </li>
<hr>
<li>
              <a class="menuitem">
                <i class=" fas fa-question-circle fa-fw"></i>
                有疑问？
              </a>
              <ul class="list-v">
                <li>
                <a class="menuitem" href="/faqs/" title="看 FAQ">
                  看 FAQ
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/volantis-docs/" title="看 本站源码">
                  看 本站源码
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/issues/" title="提 Issue">
                  提 Issue
                </a>
              </li>
              </ul>
            </li>
                </ul>
              </div>
            </div>
<p>示例2</p>
<div class="dropmenu-wrapper">
              <span>这个是</span>
              <div class="dropmenu">
                <a>下拉菜单</a>
                <ul class="list-v">
                  <li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/" title="主题源码">
                  <i class="fas fa-file-code fa-fw"></i>
                  主题源码
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/releases/" title="更新日志">
                  <i class="fas fa-clipboard-list fa-fw"></i>
                  更新日志
                </a>
              </li>
<hr>
<li>
              <a class="menuitem">
                <i class=" fas fa-question-circle fa-fw"></i>
                有疑问？
              </a>
              <ul class="list-v">
                <li>
                <a class="menuitem" href="/faqs/" title="看 FAQ">
                  看 FAQ
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/volantis-docs/" title="看 本站源码">
                  看 本站源码
                </a>
              </li>

              </ul>
            </li>

                </ul>
              </div>
            </div>
<p>示例3</p>
<div class="dropmenu-wrapper">
              <span>这个是</span>
              <div class="dropmenu">
                <a>下拉菜单</a>
                <ul class="list-v">
                  <li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/" title="主题源码">
                  <i class="fas fa-file-code fa-fw"></i>
                  主题源码
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/releases/" title="更新日志">
                  <i class="fas fa-clipboard-list fa-fw"></i>
                  更新日志
                </a>
              </li>
<hr>
<li>
              <a class="menuitem">
                <i class=" fas fa-question-circle fa-fw"></i>
                有疑问？
              </a>
              <ul class="list-v">
                <li>
                <a class="menuitem" href="/faqs/" title="看 FAQ">
                  看 FAQ
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/volantis-docs/" title="看 本站源码">
                  看 本站源码
                </a>
              </li>

              </ul>
            </li>

                </ul>
              </div>
              <span>的示例效果。</span>
            </div>
<h4 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h4><figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">示例1</span><br><span class="line">{% menu 下拉菜单 %}</span><br><span class="line">{% menuitem 主题源码, https://github.com/volantis-x/hexo-theme-volantis/, fas fa-file-code %}</span><br><span class="line">{% menuitem 更新日志, https://github.com/volantis-x/hexo-theme-volantis/releases/, fas fa-clipboard-list %}</span><br><span class="line">{% menuitem hr %}</span><br><span class="line">{% submenu 有疑问？, fas fa-question-circle %}</span><br><span class="line">{% menuitem 看 FAQ, /faqs/ %}</span><br><span class="line">{% menuitem 看 本站源码, https://github.com/volantis-x/volantis-docs/ %}</span><br><span class="line">{% menuitem 提 Issue, https://github.com/volantis-x/hexo-theme-volantis/issues/ %}</span><br><span class="line">{% endsubmenu %}</span><br><span class="line">{% endmenu %}</span><br><span class="line">示例2</span><br><span class="line">{% menu 这个是, 下拉菜单 %}</span><br><span class="line">（同上）</span><br><span class="line">{% endmenu %}</span><br><span class="line">示例3</span><br><span class="line">{% menu 这个是, 下拉菜单, 的示例效果。 %}</span><br><span class="line">（同上）</span><br><span class="line">{% endmenu %}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="tab"><a href="#tab" class="headerlink" title="tab"></a>tab</h3><p>此插件移植自Next</p>
<p>语法：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% tabs 页面内不重复的ID %}</span><br><span class="line">&lt;!-- tab 栏目1 --&gt;</span><br><span class="line">内容</span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&lt;!-- tab 栏目2 --&gt;</span><br><span class="line">内容</span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">{% endtabs %}</span><br></pre></td></tr></tbody></table></figure>
<p>演示效果：</p>
<div class="tabs" id="tab-id"><ul class="nav-tabs"><li class="tab active"><a class="#tab-id-1">栏目1</a></li><li class="tab"><a class="#tab-id-2">栏目2</a></li></ul><div class="tab-content"><div class="tab-pane active" id="tab-id-1"><p>这是栏目1</p></div><div class="tab-pane" id="tab-id-2"><p>这是栏目2</p></div></div></div>
<p>源码：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line">{% tabs tab-id %}</span><br><span class="line"></span><br><span class="line">&lt;!-- tab 栏目1 --&gt;</span><br><span class="line"></span><br><span class="line">这是栏目1</span><br><span class="line"></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab 栏目2 --&gt;</span><br><span class="line"></span><br><span class="line">这是栏目2</span><br><span class="line"></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">{% endtabs %}</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>Hexo博客搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>volantis</tag>
      </tags>
  </entry>
  <entry>
    <title>echarts学习笔记</title>
    <url>/zh-CN/echarts/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://www.runoob.com/echarts/echarts-tutorial.html">ECharts 教程 | 菜鸟教程 (runoob.com)</a></p>
<p>ECharts 是一个使用 JavaScript 实现的开源可视化库，涵盖各行业图表，满足各种需求。</p>
<p>ECharts 遵循 Apache-2.0 开源协议，免费商用。</p>
<p>ECharts 兼容当前绝大部分浏览器（IE8/9/10/11，Chrome，Firefox，Safari等）及兼容多种设备，可随时随地任性展示。</p>
<h3 id="echarts安装"><a href="#echarts安装" class="headerlink" title="echarts安装"></a>echarts安装</h3><p>1.独立版本</p>
<p>我们可以在直接下载 <a href="https://cdn.staticfile.org/echarts/4.7.0/echarts.min.js">echarts.min.js </a>并用 <strong><script></strong> 标签引入。</p>
<p>2.使用CDN方法</p>
<p>以下推荐国外比较稳定的两个 CDN，国内还没发现哪一家比较好，目前还是建议下载到本地。</p>
<ul>
<li><strong>Staticfile CDN（国内）</strong> : <a href="https://cdn.staticfile.org/echarts/4.3.0/echarts.min.js">https://cdn.staticfile.org/echarts/4.3.0/echarts.min.js</a></li>
<li><strong>jsDelivr</strong>：<a href="https://cdn.jsdelivr.net/npm/echarts@4.3.0/dist/echarts.min.js。">https://cdn.jsdelivr.net/npm/echarts@4.3.0/dist/echarts.min.js。</a></li>
<li><strong>cdnjs</strong> : <a href="https://cdnjs.cloudflare.com/ajax/libs/echarts/4.3.0/echarts.min.js">https://cdnjs.cloudflare.com/ajax/libs/echarts/4.3.0/echarts.min.js</a></li>
</ul>
<p>3.npm方法</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install echarts --save</span><br></pre></td></tr></table></figure>
<h3 id="echarts配置语法"><a href="#echarts配置语法" class="headerlink" title="echarts配置语法"></a>echarts配置语法</h3><p>下面实例可以放在一个html文件里显示效果</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">// 创建html页面</span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;utf-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>第一个 ECharts 实例<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 引入 echarts.js --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://cdn.staticfile.org/echarts/4.3.0/echarts.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 为ECharts准备一个具备大小（宽高）的Dom --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;main&quot;</span> <span class="attr">style</span>=<span class="string">&quot;width: 600px;height:400px;&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">        <span class="comment">// 基于准备好的dom，初始化echarts实例</span></span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> myChart = echarts.<span class="title function_">init</span>(<span class="variable language_">document</span>.<span class="title function_">getElementById</span>(<span class="string">&#x27;main&#x27;</span>));</span></span><br><span class="line"><span class="language-javascript"> </span></span><br><span class="line"><span class="language-javascript">        <span class="comment">// 指定图表的配置项和数据</span></span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> option = &#123;</span></span><br><span class="line"><span class="language-javascript">            <span class="comment">// 为图表配置标题</span></span></span><br><span class="line"><span class="language-javascript">            <span class="attr">title</span>: &#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="attr">text</span>: <span class="string">&#x27;第一个 ECharts 实例&#x27;</span></span></span><br><span class="line"><span class="language-javascript">            &#125;,</span></span><br><span class="line"><span class="language-javascript">            <span class="comment">// 配置提示信息</span></span></span><br><span class="line"><span class="language-javascript">            <span class="attr">tooltip</span>: &#123;&#125;,</span></span><br><span class="line"><span class="language-javascript">            <span class="comment">// 图例组件展现了不同系列的标记(symbol)，颜色和名字。可以通过点击图例控制哪些系列不显示。</span></span></span><br><span class="line"><span class="language-javascript">            <span class="attr">legend</span>: &#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="attr">data</span>:[<span class="string">&#x27;销量&#x27;</span>]</span></span><br><span class="line"><span class="language-javascript">            &#125;,</span></span><br><span class="line"><span class="language-javascript">            <span class="comment">// 配置要在x轴显示的项</span></span></span><br><span class="line"><span class="language-javascript">            <span class="attr">xAxis</span>: &#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="attr">data</span>: [<span class="string">&quot;衬衫&quot;</span>,<span class="string">&quot;羊毛衫&quot;</span>,<span class="string">&quot;雪纺衫&quot;</span>,<span class="string">&quot;裤子&quot;</span>,<span class="string">&quot;高跟鞋&quot;</span>,<span class="string">&quot;袜子&quot;</span>]</span></span><br><span class="line"><span class="language-javascript">            &#125;,</span></span><br><span class="line"><span class="language-javascript">            <span class="comment">// 配置要在y轴显示的项</span></span></span><br><span class="line"><span class="language-javascript">            <span class="attr">yAxis</span>: &#123;&#125;,</span></span><br><span class="line"><span class="language-javascript">            <span class="comment">// 每个系列通过type决定自己的图表类型</span></span></span><br><span class="line"><span class="language-javascript">            <span class="attr">series</span>: [&#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="attr">name</span>: <span class="string">&#x27;销量&#x27;</span>, <span class="comment">// 系列名称</span></span></span><br><span class="line"><span class="language-javascript">                <span class="attr">type</span>: <span class="string">&#x27;bar&#x27;</span>, <span class="comment">// 系列图表类型</span></span></span><br><span class="line"><span class="language-javascript">                <span class="attr">data</span>: [<span class="number">5</span>, <span class="number">20</span>, <span class="number">36</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">20</span>] <span class="comment">// 系列中的数据内容</span></span></span><br><span class="line"><span class="language-javascript">            &#125;]</span></span><br><span class="line"><span class="language-javascript">        &#125;;</span></span><br><span class="line"><span class="language-javascript"> </span></span><br><span class="line"><span class="language-javascript">        <span class="comment">// 使用刚指定的配置项和数据显示图表。</span></span></span><br><span class="line"><span class="language-javascript">        myChart.<span class="title function_">setOption</span>(option);</span></span><br><span class="line"><span class="language-javascript">    </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>特别说明：</p>
<p>图例组件：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">legend: &#123;</span><br><span class="line">    data: [&#123;</span><br><span class="line">        name: &#x27;系列1&#x27;,</span><br><span class="line">        // 强制设置图形为圆。</span><br><span class="line">        icon: &#x27;circle&#x27;,</span><br><span class="line">        // 设置文本为红色</span><br><span class="line">        textStyle: &#123;</span><br><span class="line">            color: &#x27;red&#x27;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>系列列表：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">series: [&#123;</span><br><span class="line">    name: &#x27;销量&#x27;,  // 系列名称</span><br><span class="line">    type: &#x27;bar&#x27;,  // 系列图表类型</span><br><span class="line">    data: [5, 20, 36, 10, 10, 20]  // 系列中的数据内容</span><br><span class="line">&#125;]</span><br></pre></td></tr></table></figure>
<p>每个系列通过 type 决定自己的图表类型：</p>
<ul>
<li><strong>type: ‘bar’</strong>：柱状/条形图</li>
<li><strong>type: ‘line’</strong>：折线/面积图</li>
<li><strong>type: ‘pie’</strong>：饼图</li>
<li><strong>type: ‘scatter’</strong>：散点（气泡）图</li>
<li><strong>type: ‘effectScatter’</strong>：带有涟漪特效动画的散点（气泡）</li>
<li><strong>type: ‘radar’</strong>：雷达图</li>
<li><strong>type: ‘tree’</strong>：树型图</li>
<li><strong>type: ‘treemap’</strong>：树型图</li>
<li><strong>type: ‘sunburst’</strong>：旭日图</li>
<li><strong>type: ‘boxplot’</strong>：箱形图</li>
<li><strong>type: ‘candlestick’</strong>：K线图</li>
<li><strong>type: ‘heatmap’</strong>：热力图</li>
<li><strong>type: ‘map’</strong>：地图</li>
<li><strong>type: ‘parallel’</strong>：平行坐标系的系列</li>
<li><strong>type: ‘lines’</strong>：线图</li>
<li><strong>type: ‘graph’</strong>：关系图</li>
<li><strong>type: ‘sankey’</strong>：桑基图</li>
<li><strong>type: ‘funnel’</strong>：漏斗图</li>
<li><strong>type: ‘gauge’</strong>：仪表盘</li>
<li><strong>type: ‘pictorialBar’</strong>：象形柱图</li>
<li><strong>type: ‘themeRiver’</strong>：主题河流</li>
<li><strong>type: ‘custom’</strong>：自定义系列</li>
</ul>
<h3 id="echarts饼图"><a href="#echarts饼图" class="headerlink" title="echarts饼图"></a>echarts饼图</h3><p>饼图主要是通过扇形的弧度表现不同类目的数据在总和中的占比，它的数据格式比柱状图更简单，只有一维的数值，不需要给类目。因为不在直角坐标系上，所以也不需要 xAxis，yAxis。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">myChart.setOption(&#123;</span><br><span class="line">    series : [</span><br><span class="line">        &#123;</span><br><span class="line">            name: &#x27;访问来源&#x27;,</span><br><span class="line">            type: &#x27;pie&#x27;,    // 设置图表类型为饼图</span><br><span class="line">            radius: &#x27;55%&#x27;,  // 饼图的半径，外半径为可视区尺寸（容器高宽中较小一项）的 55% 长度。</span><br><span class="line">            data:[          // 数据数组，name 为数据项名称，value 为数据项值</span><br><span class="line">                &#123;value:235, name:&#x27;视频广告&#x27;&#125;,</span><br><span class="line">                &#123;value:274, name:&#x27;联盟广告&#x27;&#125;,</span><br><span class="line">                &#123;value:310, name:&#x27;邮件营销&#x27;&#125;,</span><br><span class="line">                &#123;value:335, name:&#x27;直接访问&#x27;&#125;,</span><br><span class="line">                &#123;value:400, name:&#x27;搜索引擎&#x27;&#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>完整源码：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;utf-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>第一个 ECharts 实例<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 引入 echarts.js --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://cdn.staticfile.org/echarts/4.3.0/echarts.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 为ECharts准备一个具备大小（宽高）的Dom --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;main&quot;</span> <span class="attr">style</span>=<span class="string">&quot;width: 600px;height:400px;&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">        <span class="comment">// 基于准备好的dom，初始化echarts实例</span></span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> myChart = echarts.<span class="title function_">init</span>(<span class="variable language_">document</span>.<span class="title function_">getElementById</span>(<span class="string">&#x27;main&#x27;</span>));</span></span><br><span class="line"><span class="language-javascript"> </span></span><br><span class="line"><span class="language-javascript">        myChart.<span class="title function_">setOption</span>(&#123;</span></span><br><span class="line"><span class="language-javascript">            series : [</span></span><br><span class="line"><span class="language-javascript">                &#123;</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">name</span>: <span class="string">&#x27;访问来源&#x27;</span>,</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">type</span>: <span class="string">&#x27;pie&#x27;</span>,    <span class="comment">// 设置图表类型为饼图</span></span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">radius</span>: <span class="string">&#x27;55%&#x27;</span>,  <span class="comment">// 饼图的半径，外半径为可视区尺寸（容器高宽中较小一项）的 55% 长度。</span></span></span><br><span class="line"><span class="language-javascript">                    <span class="comment">// roseType: &#x27;angle&#x27; // 饼图显示为南丁格尔图</span></span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">data</span>:[          <span class="comment">// 数据数组，name 为数据项名称，value 为数据项值</span></span></span><br><span class="line"><span class="language-javascript">                        &#123;<span class="attr">value</span>:<span class="number">235</span>, <span class="attr">name</span>:<span class="string">&#x27;视频广告&#x27;</span>&#125;,</span></span><br><span class="line"><span class="language-javascript">                        &#123;<span class="attr">value</span>:<span class="number">274</span>, <span class="attr">name</span>:<span class="string">&#x27;联盟广告&#x27;</span>&#125;,</span></span><br><span class="line"><span class="language-javascript">                        &#123;<span class="attr">value</span>:<span class="number">310</span>, <span class="attr">name</span>:<span class="string">&#x27;邮件营销&#x27;</span>&#125;,</span></span><br><span class="line"><span class="language-javascript">                        &#123;<span class="attr">value</span>:<span class="number">335</span>, <span class="attr">name</span>:<span class="string">&#x27;直接访问&#x27;</span>&#125;,</span></span><br><span class="line"><span class="language-javascript">                        &#123;<span class="attr">value</span>:<span class="number">400</span>, <span class="attr">name</span>:<span class="string">&#x27;搜索引擎&#x27;</span>&#125;</span></span><br><span class="line"><span class="language-javascript">                    ]</span></span><br><span class="line"><span class="language-javascript">                    <span class="comment">//itemStyle 参数可以设置诸如阴影、透明度、颜色、边框颜色、边框宽度等</span></span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">itemStyle</span>:&#123;</span></span><br><span class="line"><span class="language-javascript">                    	<span class="attr">normal</span>:&#123;</span></span><br><span class="line"><span class="language-javascript">							<span class="attr">shadowBlur</span>: <span class="number">200</span>,</span></span><br><span class="line"><span class="language-javascript">                			<span class="attr">shadowColor</span>: <span class="string">&#x27;rgba(0, 0, 0, 0.5)&#x27;</span></span></span><br><span class="line"><span class="language-javascript">                				&#125;</span></span><br><span class="line"><span class="language-javascript">                			  &#125;</span></span><br><span class="line"><span class="language-javascript">                &#125;</span></span><br><span class="line"><span class="language-javascript">            ]</span></span><br><span class="line"><span class="language-javascript">        &#125;)</span></span><br><span class="line"><span class="language-javascript">    </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>从上面两个来看，接下来我们只会更改变量<code>myChart</code>的配置，到时候进行替换就行了</p>
<h3 id="样式设置"><a href="#样式设置" class="headerlink" title="样式设置"></a>样式设置</h3><p>ECharts 可以通过样式设置来改变图形元素或者文字的颜色、明暗、大小等。</p>
<h4 id="颜色主题"><a href="#颜色主题" class="headerlink" title="颜色主题"></a>颜色主题</h4><p>ECharts4 开始，除了默认主题外，内置了两套主题，分别为 <strong>light</strong> 和 <strong>dark</strong>。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"># light theme</span><br><span class="line">var chart = echarts.init(dom, &#x27;light&#x27;);</span><br><span class="line"># dark theme</span><br><span class="line">var chart = echarts.init(dom, &#x27;dark&#x27;);</span><br></pre></td></tr></table></figure>
<p>另外，我们也可以在官方的 <a href="http://echarts.baidu.com/theme-builder/">主题编辑器</a> 选择自己喜欢的主题下载。</p>
<p><style>.hibepgewbeqv{zoom:50%;}</style><img src="/zh-CN/echarts/image-20221013174408235.png" class="lazyload" data-srcset="/zh-CN/echarts/image-20221013174408235.png" srcset="data:image/png;base64,666" class="hibepgewbeqv lazyload" alt="image-20221013174408235"></p>
<p>目前主题下载提供了 JS 版本和 JSON 版本。</p>
<p>如果你使用 JS 版本，可以将 JS 主题代码保存一个 <strong>主题名.js</strong> 文件，然后在 HTML 中引用该文件，最后在代码中使用该主题。</p>
<p>比如上图中我们选中了一个主题，将 JS 代码保存为 <strong>wonderland.js</strong>。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 引入主题 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://www.runoob.com/static/js/wonderland.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// HTML 引入 wonderland.js 文件后，在初始化的时候调用</span><br><span class="line">var myChart = echarts.init(dom, &#x27;wonderland&#x27;);</span><br><span class="line">// ...</span><br></pre></td></tr></table></figure>
<p>如果主题保存为 JSON 文件，那么可以自行加载和注册。</p>
<p>比如上图中我们选中了一个主题，将 JSON 代码保存为 <strong>wonderland.json</strong>。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">//主题名称是 wonderland</span><br><span class="line">$.getJSON(&#x27;wonderland.json&#x27;, function (themeJSON) &#123;</span><br><span class="line">    echarts.registerTheme(&#x27;wonderland&#x27;, themeJSON)</span><br><span class="line">    var myChart = echarts.init(dom, &#x27;wonderland&#x27;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>我们使用了 $.getJSON，所以需要引入 jQuery。</p>
<h4 id="调色盘"><a href="#调色盘" class="headerlink" title="调色盘"></a>调色盘</h4><p>调色盘可以在 option 中设置。</p>
<p>调色盘给定了一组颜色，图形、系列会自动从其中选择颜色。</p>
<p>可以设置全局的调色盘，也可以设置系列自己专属的调色盘。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">option = &#123;</span><br><span class="line">    // 全局调色盘。</span><br><span class="line">    color: [&#x27;#c23531&#x27;,&#x27;#2f4554&#x27;, &#x27;#61a0a8&#x27;, &#x27;#d48265&#x27;, &#x27;#91c7ae&#x27;,&#x27;#749f83&#x27;,  &#x27;#ca8622&#x27;, &#x27;#bda29a&#x27;,&#x27;#6e7074&#x27;, &#x27;#546570&#x27;, &#x27;#c4ccd3&#x27;],</span><br><span class="line"></span><br><span class="line">    series: [&#123;</span><br><span class="line">        type: &#x27;bar&#x27;,</span><br><span class="line">        // 此系列自己的调色盘。</span><br><span class="line">        color: [&#x27;#dd6b66&#x27;,&#x27;#759aa0&#x27;,&#x27;#e69d87&#x27;,&#x27;#8dc1a9&#x27;,&#x27;#ea7e53&#x27;,&#x27;#eedd78&#x27;,&#x27;#73a373&#x27;,&#x27;#73b9bc&#x27;,&#x27;#7289ab&#x27;, &#x27;#91ca8c&#x27;,&#x27;#f49f42&#x27;],</span><br><span class="line">        ...</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        type: &#x27;pie&#x27;,</span><br><span class="line">        // 此系列自己的调色盘。</span><br><span class="line">        color: [&#x27;#37A2DA&#x27;, &#x27;#32C5E9&#x27;, &#x27;#67E0E3&#x27;, &#x27;#9FE6B8&#x27;, &#x27;#FFDB5C&#x27;,&#x27;#ff9f7f&#x27;, &#x27;#fb7293&#x27;, &#x27;#E062AE&#x27;, &#x27;#E690D1&#x27;, &#x27;#e7bcf3&#x27;, &#x27;#9d96f5&#x27;, &#x27;#8378EA&#x27;, &#x27;#96BFFF&#x27;],</span><br><span class="line">        ...</span><br><span class="line">    &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="高亮样式emphasis"><a href="#高亮样式emphasis" class="headerlink" title="高亮样式emphasis"></a>高亮样式emphasis</h4><p>直接的样式设置是比较常用设置方式。纵观 ECharts 的 <a href="https://www.echartsjs.com/zh/option.html#title">option</a> 中，很多地方可以设置 <a href="https://www.echartsjs.com/zh/option.html#series.itemStyle">itemStyle</a>、<a href="https://www.echartsjs.com/zh/option.html#series-line.lineStyle">lineStyle</a>、<a href="https://www.echartsjs.com/zh/option.html#series-line.areaStyle">areaStyle</a>、<a href="https://www.echartsjs.com/zh/option.html#series.label">label</a> 等等。这些的地方可以直接设置图形元素的颜色、线宽、点的大小、标签的文字、标签的样式等等。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">// 高亮样式。</span><br><span class="line">emphasis: &#123;</span><br><span class="line">    itemStyle: &#123;</span><br><span class="line">        // 高亮时点的颜色</span><br><span class="line">        color: &#x27;red&#x27;</span><br><span class="line">    &#125;,</span><br><span class="line">    label: &#123;</span><br><span class="line">        show: true,</span><br><span class="line">        // 高亮时标签的文字</span><br><span class="line">        formatter: &#x27;高亮时显示的标签内容&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure>
<h3 id="异步加载数据"><a href="#异步加载数据" class="headerlink" title="异步加载数据"></a>异步加载数据</h3><p>ECharts 通常数据设置在 setOption 中，如果我们需要异步加载数据，可以配合 jQuery等工具，在异步获取数据后通过 setOption 填入数据和配置项就行。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">// echarts_test_data.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;data_pie&quot; : [</span><br><span class="line">    &#123;&quot;value&quot;:235, &quot;name&quot;:&quot;视频广告&quot;&#125;,</span><br><span class="line">    &#123;&quot;value&quot;:274, &quot;name&quot;:&quot;联盟广告&quot;&#125;,</span><br><span class="line">    &#123;&quot;value&quot;:310, &quot;name&quot;:&quot;邮件营销&quot;&#125;,</span><br><span class="line">    &#123;&quot;value&quot;:335, &quot;name&quot;:&quot;直接访问&quot;&#125;,</span><br><span class="line">    &#123;&quot;value&quot;:400, &quot;name&quot;:&quot;搜索引擎&quot;&#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">// 实例</span><br><span class="line">var myChart = echarts.init(document.getElementById(&#x27;main&#x27;));</span><br><span class="line">myChart.showLoading();  // 开启 loading 效果</span><br><span class="line">$.get(&#x27;https://www.runoob.com/static/js/echarts_test_data.json&#x27;, function (data) &#123;</span><br><span class="line">    myChart.hideLoading();  // 隐藏 loading 效果</span><br><span class="line">    myChart.setOption(&#123;</span><br><span class="line">        series : [</span><br><span class="line">            &#123;</span><br><span class="line">                name: &#x27;访问来源&#x27;,</span><br><span class="line">                type: &#x27;pie&#x27;,    // 设置图表类型为饼图</span><br><span class="line">                radius: &#x27;55%&#x27;,  // 饼图的半径，外半径为可视区尺寸（容器高宽中较小一项）的 55% 长度。</span><br><span class="line">                data:data.data_pie</span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;, &#x27;json&#x27;)</span><br></pre></td></tr></table></figure>
<h4 id="数据的动态更新"><a href="#数据的动态更新" class="headerlink" title="数据的动态更新"></a>数据的动态更新</h4><p>ECharts 由数据驱动，数据的改变驱动图表展现的改变，因此动态数据的实现也变得异常简单。</p>
<p>所有数据的更新都通过 setOption 实现，你只需要定时获取数据，setOption 填入数据，而不用考虑数据到底产生了那些变化，ECharts 会找到两组数据之间的差异然后通过合适的动画去表现数据的变化。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;utf-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>第一个 ECharts 实例<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://cdn.staticfile.org/jquery/2.2.4/jquery.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 引入 echarts.js --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://cdn.staticfile.org/echarts/4.3.0/echarts.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 为ECharts准备一个具备大小（宽高）的Dom --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;main&quot;</span> <span class="attr">style</span>=<span class="string">&quot;width: 600px;height:400px;&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> base = +<span class="keyword">new</span> <span class="title class_">Date</span>(<span class="number">2022</span>, <span class="number">10</span>, <span class="number">13</span>);</span></span><br><span class="line"><span class="language-javascript">		<span class="keyword">var</span> oneDay = <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000</span>;</span></span><br><span class="line"><span class="language-javascript">		<span class="keyword">var</span> date = [];</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">		<span class="keyword">var</span> data = [<span class="title class_">Math</span>.<span class="title function_">random</span>() * <span class="number">150</span>];</span></span><br><span class="line"><span class="language-javascript">		<span class="keyword">var</span> now = <span class="keyword">new</span> <span class="title class_">Date</span>(base);</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">		<span class="keyword">function</span> <span class="title function_">addData</span>(<span class="params">shift</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">		    now = [now.<span class="title function_">getFullYear</span>(), now.<span class="title function_">getMonth</span>() + <span class="number">1</span>, now.<span class="title function_">getDate</span>()].<span class="title function_">join</span>(<span class="string">&#x27;/&#x27;</span>);</span></span><br><span class="line"><span class="language-javascript">		    date.<span class="title function_">push</span>(now);</span></span><br><span class="line"><span class="language-javascript">		    data.<span class="title function_">push</span>((<span class="title class_">Math</span>.<span class="title function_">random</span>() - <span class="number">0.4</span>) * <span class="number">10</span> + data[data.<span class="property">length</span> - <span class="number">1</span>]);</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">		    <span class="keyword">if</span> (shift) &#123;</span></span><br><span class="line"><span class="language-javascript">		        date.<span class="title function_">shift</span>();</span></span><br><span class="line"><span class="language-javascript">		        data.<span class="title function_">shift</span>();</span></span><br><span class="line"><span class="language-javascript">		    &#125;</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">		    now = <span class="keyword">new</span> <span class="title class_">Date</span>(+<span class="keyword">new</span> <span class="title class_">Date</span>(now) + oneDay);</span></span><br><span class="line"><span class="language-javascript">		&#125;</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">		<span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">1</span>; i &lt; <span class="number">100</span>; i++) &#123;</span></span><br><span class="line"><span class="language-javascript">		    <span class="title function_">addData</span>();</span></span><br><span class="line"><span class="language-javascript">		&#125;</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">		option = &#123;</span></span><br><span class="line"><span class="language-javascript">		    <span class="attr">xAxis</span>: &#123;</span></span><br><span class="line"><span class="language-javascript">		        <span class="attr">type</span>: <span class="string">&#x27;category&#x27;</span>,</span></span><br><span class="line"><span class="language-javascript">		        <span class="attr">boundaryGap</span>: <span class="literal">false</span>,</span></span><br><span class="line"><span class="language-javascript">		        <span class="attr">data</span>: date</span></span><br><span class="line"><span class="language-javascript">		    &#125;,</span></span><br><span class="line"><span class="language-javascript">		    <span class="attr">yAxis</span>: &#123;</span></span><br><span class="line"><span class="language-javascript">		        <span class="attr">boundaryGap</span>: [<span class="number">0</span>, <span class="string">&#x27;50%&#x27;</span>],</span></span><br><span class="line"><span class="language-javascript">		        <span class="attr">type</span>: <span class="string">&#x27;value&#x27;</span></span></span><br><span class="line"><span class="language-javascript">		    &#125;,</span></span><br><span class="line"><span class="language-javascript">		    <span class="attr">series</span>: [</span></span><br><span class="line"><span class="language-javascript">		        &#123;</span></span><br><span class="line"><span class="language-javascript">		            <span class="attr">name</span>:<span class="string">&#x27;成交&#x27;</span>,</span></span><br><span class="line"><span class="language-javascript">		            <span class="attr">type</span>:<span class="string">&#x27;line&#x27;</span>,</span></span><br><span class="line"><span class="language-javascript">		            <span class="attr">smooth</span>:<span class="literal">true</span>,</span></span><br><span class="line"><span class="language-javascript">		            <span class="attr">symbol</span>: <span class="string">&#x27;none&#x27;</span>,</span></span><br><span class="line"><span class="language-javascript">		            <span class="attr">stack</span>: <span class="string">&#x27;a&#x27;</span>,</span></span><br><span class="line"><span class="language-javascript">		            <span class="attr">areaStyle</span>: &#123;</span></span><br><span class="line"><span class="language-javascript">		                <span class="attr">normal</span>: &#123;&#125;</span></span><br><span class="line"><span class="language-javascript">		            &#125;,</span></span><br><span class="line"><span class="language-javascript">		            <span class="attr">data</span>: data</span></span><br><span class="line"><span class="language-javascript">		        &#125;</span></span><br><span class="line"><span class="language-javascript">		    ]</span></span><br><span class="line"><span class="language-javascript">		&#125;;</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">		<span class="built_in">setInterval</span>(<span class="keyword">function</span> (<span class="params"></span>) &#123;</span></span><br><span class="line"><span class="language-javascript">		    <span class="title function_">addData</span>(<span class="literal">true</span>);</span></span><br><span class="line"><span class="language-javascript">		    myChart.<span class="title function_">setOption</span>(&#123;</span></span><br><span class="line"><span class="language-javascript">		        <span class="attr">xAxis</span>: &#123;</span></span><br><span class="line"><span class="language-javascript">		            <span class="attr">data</span>: date</span></span><br><span class="line"><span class="language-javascript">		        &#125;,</span></span><br><span class="line"><span class="language-javascript">		        <span class="attr">series</span>: [&#123;</span></span><br><span class="line"><span class="language-javascript">		            <span class="attr">name</span>:<span class="string">&#x27;成交&#x27;</span>,</span></span><br><span class="line"><span class="language-javascript">		            <span class="attr">data</span>: data</span></span><br><span class="line"><span class="language-javascript">		        &#125;]</span></span><br><span class="line"><span class="language-javascript">		    &#125;);</span></span><br><span class="line"><span class="language-javascript">		&#125;, <span class="number">500</span>);</span></span><br><span class="line"><span class="language-javascript">		</span></span><br><span class="line"><span class="language-javascript">        <span class="comment">// 基于准备好的dom，初始化echarts实例</span></span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> myChart = echarts.<span class="title function_">init</span>(<span class="variable language_">document</span>.<span class="title function_">getElementById</span>(<span class="string">&#x27;main&#x27;</span>));</span></span><br><span class="line"><span class="language-javascript">        myChart.<span class="title function_">setOption</span>(option)</span></span><br><span class="line"><span class="language-javascript">    </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>ECharts 使用 dataset 管理数据。</p>
<p>dataset 组件用于单独的数据集声明，从而数据可以单独管理，被多个组件复用，并且可以基于数据指定数据到视觉的映射。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">option = &#123;</span><br><span class="line">    legend: &#123;&#125;,</span><br><span class="line">    tooltip: &#123;&#125;,</span><br><span class="line">    dataset: &#123;</span><br><span class="line">        // 提供一份数据。</span><br><span class="line">        source: [</span><br><span class="line">            [&#x27;product&#x27;, &#x27;2015&#x27;, &#x27;2016&#x27;, &#x27;2017&#x27;],</span><br><span class="line">            [&#x27;Matcha Latte&#x27;, 43.3, 85.8, 93.7],</span><br><span class="line">            [&#x27;Milk Tea&#x27;, 83.1, 73.4, 55.1],</span><br><span class="line">            [&#x27;Cheese Cocoa&#x27;, 86.4, 65.2, 82.5],</span><br><span class="line">            [&#x27;Walnut Brownie&#x27;, 72.4, 53.9, 39.1]</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    // 声明一个 X 轴，类目轴（category）。默认情况下，类目轴对应到 dataset 第一列。</span><br><span class="line">    xAxis: &#123;type: &#x27;category&#x27;&#125;,</span><br><span class="line">    // 声明一个 Y 轴，数值轴。</span><br><span class="line">    yAxis: &#123;&#125;,</span><br><span class="line">    // 声明多个 bar 系列，默认情况下，每个系列会自动对应到 dataset 的每一列。</span><br><span class="line">    series: [</span><br><span class="line">        &#123;type: &#x27;bar&#x27;&#125;,</span><br><span class="line">        &#123;type: &#x27;bar&#x27;&#125;,</span><br><span class="line">        &#123;type: &#x27;bar&#x27;&#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>或者使用对象数组的格式</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">option = &#123;</span><br><span class="line">    legend: &#123;&#125;,</span><br><span class="line">    tooltip: &#123;&#125;,</span><br><span class="line">    dataset: &#123;</span><br><span class="line">        // 这里指定了维度名的顺序，从而可以利用默认的维度到坐标轴的映射。</span><br><span class="line">        // 如果不指定 dimensions，也可以通过指定 series.encode 完成映射，参见后文。</span><br><span class="line">        dimensions: [&#x27;product&#x27;, &#x27;2015&#x27;, &#x27;2016&#x27;, &#x27;2017&#x27;],</span><br><span class="line">        source: [</span><br><span class="line">            &#123;product: &#x27;Matcha Latte&#x27;, &#x27;2015&#x27;: 43.3, &#x27;2016&#x27;: 85.8, &#x27;2017&#x27;: 93.7&#125;,</span><br><span class="line">            &#123;product: &#x27;Milk Tea&#x27;, &#x27;2015&#x27;: 83.1, &#x27;2016&#x27;: 73.4, &#x27;2017&#x27;: 55.1&#125;,</span><br><span class="line">            &#123;product: &#x27;Cheese Cocoa&#x27;, &#x27;2015&#x27;: 86.4, &#x27;2016&#x27;: 65.2, &#x27;2017&#x27;: 82.5&#125;,</span><br><span class="line">            &#123;product: &#x27;Walnut Brownie&#x27;, &#x27;2015&#x27;: 72.4, &#x27;2016&#x27;: 53.9, &#x27;2017&#x27;: 39.1&#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    xAxis: &#123;type: &#x27;category&#x27;&#125;,</span><br><span class="line">    yAxis: &#123;&#125;,</span><br><span class="line">    series: [</span><br><span class="line">        &#123;type: &#x27;bar&#x27;&#125;,</span><br><span class="line">        &#123;type: &#x27;bar&#x27;&#125;,</span><br><span class="line">        &#123;type: &#x27;bar&#x27;&#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h4 id="数据到图形的映射"><a href="#数据到图形的映射" class="headerlink" title="数据到图形的映射"></a>数据到图形的映射</h4><p>我们可以在配置项中将数据映射到图形中。</p>
<p>我们可以使用 series.seriesLayoutBy 属性来配置 dataset 是列（column）还是行（row）映射为图形系列（series），默认是按照列（column）来映射。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">option = &#123;</span><br><span class="line">    legend: &#123;&#125;,</span><br><span class="line">    tooltip: &#123;&#125;,</span><br><span class="line">    dataset: &#123;</span><br><span class="line">        source: [</span><br><span class="line">            [&#x27;product&#x27;, &#x27;2012&#x27;, &#x27;2013&#x27;, &#x27;2014&#x27;, &#x27;2015&#x27;],</span><br><span class="line">            [&#x27;Matcha Latte&#x27;, 41.1, 30.4, 65.1, 53.3],</span><br><span class="line">            [&#x27;Milk Tea&#x27;, 86.5, 92.1, 85.7, 83.1],</span><br><span class="line">            [&#x27;Cheese Cocoa&#x27;, 24.1, 67.2, 79.5, 86.4]</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    xAxis: [</span><br><span class="line">        &#123;type: &#x27;category&#x27;, gridIndex: 0&#125;,</span><br><span class="line">        &#123;type: &#x27;category&#x27;, gridIndex: 1&#125;</span><br><span class="line">    ],</span><br><span class="line">    yAxis: [</span><br><span class="line">        &#123;gridIndex: 0&#125;,</span><br><span class="line">        &#123;gridIndex: 1&#125;</span><br><span class="line">    ],</span><br><span class="line">    grid: [</span><br><span class="line">        &#123;bottom: &#x27;55%&#x27;&#125;,</span><br><span class="line">        &#123;top: &#x27;55%&#x27;&#125;</span><br><span class="line">    ],</span><br><span class="line">    series: [</span><br><span class="line">        // 这几个系列会在第一个直角坐标系中，每个系列对应到 dataset 的每一行。</span><br><span class="line">        &#123;type: &#x27;bar&#x27;, seriesLayoutBy: &#x27;row&#x27;&#125;,</span><br><span class="line">        &#123;type: &#x27;bar&#x27;, seriesLayoutBy: &#x27;row&#x27;&#125;,</span><br><span class="line">        &#123;type: &#x27;bar&#x27;, seriesLayoutBy: &#x27;row&#x27;&#125;,</span><br><span class="line">        // 这几个系列会在第二个直角坐标系中，每个系列对应到 dataset 的每一列。</span><br><span class="line">        &#123;type: &#x27;bar&#x27;, xAxisIndex: 1, yAxisIndex: 1&#125;,</span><br><span class="line">        &#123;type: &#x27;bar&#x27;, xAxisIndex: 1, yAxisIndex: 1&#125;,</span><br><span class="line">        &#123;type: &#x27;bar&#x27;, xAxisIndex: 1, yAxisIndex: 1&#125;,</span><br><span class="line">        &#123;type: &#x27;bar&#x27;, xAxisIndex: 1, yAxisIndex: 1&#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><style>.orrmupgbrali{zoom:50%;}</style><img src="/zh-CN/echarts/image-20221013200332722.png" class="lazyload" data-srcset="/zh-CN/echarts/image-20221013200332722.png" srcset="data:image/png;base64,666" class="orrmupgbrali lazyload" alt="image-20221013200332722"></p>
<p>常用图表所描述的数据大部分是”二维表”结构，我们可以使用 series.encode 属性将对应的数据映射到坐标轴（如 X、Y 轴）</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">var option = &#123;</span><br><span class="line">    dataset: &#123;</span><br><span class="line">        source: [</span><br><span class="line">            [&#x27;score&#x27;, &#x27;amount&#x27;, &#x27;product&#x27;],</span><br><span class="line">            [89.3, 58212, &#x27;Matcha Latte&#x27;],</span><br><span class="line">            [57.1, 78254, &#x27;Milk Tea&#x27;],</span><br><span class="line">            [74.4, 41032, &#x27;Cheese Cocoa&#x27;],</span><br><span class="line">            [50.1, 12755, &#x27;Cheese Brownie&#x27;],</span><br><span class="line">            [89.7, 20145, &#x27;Matcha Cocoa&#x27;],</span><br><span class="line">            [68.1, 79146, &#x27;Tea&#x27;],</span><br><span class="line">            [19.6, 91852, &#x27;Orange Juice&#x27;],</span><br><span class="line">            [10.6, 101852, &#x27;Lemon Juice&#x27;],</span><br><span class="line">            [32.7, 20112, &#x27;Walnut Brownie&#x27;]</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    grid: &#123;containLabel: true&#125;,</span><br><span class="line">    xAxis: &#123;&#125;,</span><br><span class="line">    yAxis: &#123;type: &#x27;category&#x27;&#125;,</span><br><span class="line">    series: [</span><br><span class="line">        &#123;</span><br><span class="line">            type: &#x27;bar&#x27;,</span><br><span class="line">            encode: &#123;</span><br><span class="line">                // 将 &quot;amount&quot; 列映射到 X 轴。</span><br><span class="line">                x: &#x27;amount&#x27;,</span><br><span class="line">                // 将 &quot;product&quot; 列映射到 Y 轴。</span><br><span class="line">                y: &#x27;product&#x27;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>encode 声明的基本结构如下，其中冒号左边是坐标系、标签等特定名称，如 ‘x’, ‘y’, ‘tooltip’ 等，冒号右边是数据中的维度名（string 格式）或者维度的序号（number 格式，从 0 开始计数），可以指定一个或多个维度（使用数组）。通常情况下，下面各种信息不需要所有的都写，按需写即可。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">// 在任何坐标系和系列中，都支持：</span><br><span class="line">encode: &#123;</span><br><span class="line">    // 使用 “名为 product 的维度” 和 “名为 score 的维度” 的值在 tooltip 中显示</span><br><span class="line">    tooltip: [&#x27;product&#x27;, &#x27;score&#x27;]</span><br><span class="line">    // 使用 “维度 1” 和 “维度 3” 的维度名连起来作为系列名。（有时候名字比较长，这可以避免在 series.name 重复输入这些名字）</span><br><span class="line">    seriesName: [1, 3],</span><br><span class="line">    // 表示使用 “维度2” 中的值作为 id。这在使用 setOption 动态更新数据时有用处，可以使新老数据用 id 对应起来，从而能够产生合适的数据更新动画。</span><br><span class="line">    itemId: 2,</span><br><span class="line">    // 指定数据项的名称使用 “维度3” 在饼图等图表中有用，可以使这个名字显示在图例（legend）中。</span><br><span class="line">    itemName: 3</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 直角坐标系（grid/cartesian）特有的属性：</span><br><span class="line">encode: &#123;</span><br><span class="line">    // 把 “维度1”、“维度5”、“名为 score 的维度” 映射到 X 轴：</span><br><span class="line">    x: [1, 5, &#x27;score&#x27;],</span><br><span class="line">    // 把“维度0”映射到 Y 轴。</span><br><span class="line">    y: 0</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 单轴（singleAxis）特有的属性：</span><br><span class="line">encode: &#123;</span><br><span class="line">    single: 3</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 极坐标系（polar）特有的属性：</span><br><span class="line">encode: &#123;</span><br><span class="line">    radius: 3,</span><br><span class="line">    angle: 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 地理坐标系（geo）特有的属性：</span><br><span class="line">encode: &#123;</span><br><span class="line">    lng: 3,</span><br><span class="line">    lat: 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 对于一些没有坐标系的图表，例如饼图、漏斗图等，可以是：</span><br><span class="line">encode: &#123;</span><br><span class="line">    value: 3</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="视觉通道的映射"><a href="#视觉通道的映射" class="headerlink" title="视觉通道的映射"></a>视觉通道的映射</h4><p>我们可以使用 visualMap 组件进行视觉通道的映射。</p>
<p>视觉元素可以是：</p>
<ul>
<li>symbol: 图元的图形类别。</li>
<li>symbolSize: 图元的大小。</li>
<li>color: 图元的颜色。</li>
<li>colorAlpha: 图元的颜色的透明度。</li>
<li>opacity: 图元以及其附属物（如文字标签）的透明度。</li>
<li>colorLightness: 颜色的明暗度。</li>
<li>colorSaturation: 颜色的饱和度。</li>
<li>colorHue: 颜色的色调。</li>
</ul>
<p>visualMap 组件可以定义多个，从而可以同时对数据中的多个维度进行视觉映射。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">var option = &#123;</span><br><span class="line">    dataset: &#123;</span><br><span class="line">        source: [</span><br><span class="line">            [&#x27;score&#x27;, &#x27;amount&#x27;, &#x27;product&#x27;],</span><br><span class="line">            [89.3, 58212, &#x27;Matcha Latte&#x27;],</span><br><span class="line">            [57.1, 78254, &#x27;Milk Tea&#x27;],</span><br><span class="line">            [74.4, 41032, &#x27;Cheese Cocoa&#x27;],</span><br><span class="line">            [50.1, 12755, &#x27;Cheese Brownie&#x27;],</span><br><span class="line">            [89.7, 20145, &#x27;Matcha Cocoa&#x27;],</span><br><span class="line">            [68.1, 79146, &#x27;Tea&#x27;],</span><br><span class="line">            [19.6, 91852, &#x27;Orange Juice&#x27;],</span><br><span class="line">            [10.6, 101852, &#x27;Lemon Juice&#x27;],</span><br><span class="line">            [32.7, 20112, &#x27;Walnut Brownie&#x27;]</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    grid: &#123;containLabel: true&#125;,</span><br><span class="line">    xAxis: &#123;name: &#x27;amount&#x27;&#125;,</span><br><span class="line">    yAxis: &#123;type: &#x27;category&#x27;&#125;,</span><br><span class="line">    visualMap: &#123;</span><br><span class="line">        orient: &#x27;horizontal&#x27;,</span><br><span class="line">        left: &#x27;center&#x27;,</span><br><span class="line">        min: 10,</span><br><span class="line">        max: 100,</span><br><span class="line">        text: [&#x27;High Score&#x27;, &#x27;Low Score&#x27;],</span><br><span class="line">        // Map the score column to color</span><br><span class="line">        dimension: 0,</span><br><span class="line">        inRange: &#123;</span><br><span class="line">            color: [&#x27;#D7DA8B&#x27;, &#x27;#E15457&#x27;]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    series: [</span><br><span class="line">        &#123;</span><br><span class="line">            type: &#x27;bar&#x27;,</span><br><span class="line">            encode: &#123;</span><br><span class="line">                // Map the &quot;amount&quot; column to X axis.</span><br><span class="line">                x: &#x27;amount&#x27;,</span><br><span class="line">                // Map the &quot;product&quot; column to Y axis</span><br><span class="line">                y: &#x27;product&#x27;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="交互联动"><a href="#交互联动" class="headerlink" title="交互联动"></a>交互联动</h4><p>以下实例多个图表共享一个 dataset，并带有联动交互：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">setTimeout(function () &#123;</span><br><span class="line"></span><br><span class="line">    option = &#123;</span><br><span class="line">        legend: &#123;&#125;,</span><br><span class="line">        tooltip: &#123;</span><br><span class="line">            trigger: &#x27;axis&#x27;,</span><br><span class="line">            showContent: false</span><br><span class="line">        &#125;,</span><br><span class="line">        dataset: &#123;</span><br><span class="line">            source: [</span><br><span class="line">                [&#x27;product&#x27;, &#x27;2012&#x27;, &#x27;2013&#x27;, &#x27;2014&#x27;, &#x27;2015&#x27;, &#x27;2016&#x27;, &#x27;2017&#x27;],</span><br><span class="line">                [&#x27;Matcha Latte&#x27;, 41.1, 30.4, 65.1, 53.3, 83.8, 98.7],</span><br><span class="line">                [&#x27;Milk Tea&#x27;, 86.5, 92.1, 85.7, 83.1, 73.4, 55.1],</span><br><span class="line">                [&#x27;Cheese Cocoa&#x27;, 24.1, 67.2, 79.5, 86.4, 65.2, 82.5],</span><br><span class="line">                [&#x27;Walnut Brownie&#x27;, 55.2, 67.1, 69.2, 72.4, 53.9, 39.1]</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        xAxis: &#123;type: &#x27;category&#x27;&#125;,</span><br><span class="line">        yAxis: &#123;gridIndex: 0&#125;,</span><br><span class="line">        grid: &#123;top: &#x27;55%&#x27;&#125;,</span><br><span class="line">        series: [</span><br><span class="line">            &#123;type: &#x27;line&#x27;, smooth: true, seriesLayoutBy: &#x27;row&#x27;&#125;,</span><br><span class="line">            &#123;type: &#x27;line&#x27;, smooth: true, seriesLayoutBy: &#x27;row&#x27;&#125;,</span><br><span class="line">            &#123;type: &#x27;line&#x27;, smooth: true, seriesLayoutBy: &#x27;row&#x27;&#125;,</span><br><span class="line">            &#123;type: &#x27;line&#x27;, smooth: true, seriesLayoutBy: &#x27;row&#x27;&#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                type: &#x27;pie&#x27;,</span><br><span class="line">                id: &#x27;pie&#x27;,</span><br><span class="line">                radius: &#x27;30%&#x27;,</span><br><span class="line">                center: [&#x27;50%&#x27;, &#x27;25%&#x27;],</span><br><span class="line">                label: &#123;</span><br><span class="line">                    formatter: &#x27;&#123;b&#125;: &#123;@2012&#125; (&#123;d&#125;%)&#x27;</span><br><span class="line">                &#125;,</span><br><span class="line">                encode: &#123;</span><br><span class="line">                    itemName: &#x27;product&#x27;,</span><br><span class="line">                    value: &#x27;2012&#x27;,</span><br><span class="line">                    tooltip: &#x27;2012&#x27;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    myChart.on(&#x27;updateAxisPointer&#x27;, function (event) &#123;</span><br><span class="line">        var xAxisInfo = event.axesInfo[0];</span><br><span class="line">        if (xAxisInfo) &#123;</span><br><span class="line">            var dimension = xAxisInfo.value + 1;</span><br><span class="line">            myChart.setOption(&#123;</span><br><span class="line">                series: &#123;</span><br><span class="line">                    id: &#x27;pie&#x27;,</span><br><span class="line">                    label: &#123;</span><br><span class="line">                        formatter: &#x27;&#123;b&#125;: &#123;@[&#x27; + dimension + &#x27;]&#125; (&#123;d&#125;%)&#x27;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    encode: &#123;</span><br><span class="line">                        value: dimension,</span><br><span class="line">                        tooltip: dimension</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    myChart.setOption(option);</span><br><span class="line"></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p><style>.krkiwjynkxjc{zoom:50%;}</style><img src="/zh-CN/echarts/image-20221013201328866.png" class="lazyload" data-srcset="/zh-CN/echarts/image-20221013201328866.png" srcset="data:image/png;base64,666" class="krkiwjynkxjc lazyload" alt="image-20221013201328866"></p>
<h3 id="交互组件"><a href="#交互组件" class="headerlink" title="交互组件"></a>交互组件</h3><p>ECharts 提供了很多交互组件：例组件 legend、标题组件 title、视觉映射组件 visualMap、数据区域缩放组件 dataZoom、时间线组件 timeline。</p>
<h4 id="dataZoom"><a href="#dataZoom" class="headerlink" title="dataZoom"></a>dataZoom</h4><p>dataZoom 组件可以实现通过鼠标滚轮滚动，放大缩小图表的功能。</p>
<p>默认情况下 dataZoom 控制 x 轴，即对 x 轴进行<strong>数据窗口缩放</strong>和<strong>数据窗口平移</strong>操作。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">option = &#123;</span><br><span class="line">    xAxis: &#123;</span><br><span class="line">        type: &#x27;value&#x27;</span><br><span class="line">    &#125;,</span><br><span class="line">    yAxis: &#123;</span><br><span class="line">        type: &#x27;value&#x27;</span><br><span class="line">    &#125;,</span><br><span class="line">    dataZoom: [</span><br><span class="line">        &#123;   // 这个dataZoom组件，默认控制x轴。</span><br><span class="line">            type: &#x27;slider&#x27;, // 这个 dataZoom 组件是 slider 型 dataZoom 组件</span><br><span class="line">            start: 10,      // 左边在 10% 的位置。</span><br><span class="line">            end: 60         // 右边在 60% 的位置。</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    series: [</span><br><span class="line">        &#123;</span><br><span class="line">            type: &#x27;scatter&#x27;, // 这是个『散点图』</span><br><span class="line">            itemStyle: &#123;</span><br><span class="line">                opacity: 0.8</span><br><span class="line">            &#125;,</span><br><span class="line">            symbolSize: function (val) &#123;</span><br><span class="line">                return val[2] * 40;</span><br><span class="line">            &#125;,</span><br><span class="line">            data: [[&quot;14.616&quot;,&quot;7.241&quot;,&quot;0.896&quot;],[&quot;3.958&quot;,&quot;5.701&quot;,&quot;0.955&quot;],[&quot;2.768&quot;,&quot;8.971&quot;,&quot;0.669&quot;],[&quot;9.051&quot;,&quot;9.710&quot;,&quot;0.171&quot;],[&quot;14.046&quot;,&quot;4.182&quot;,&quot;0.536&quot;],[&quot;12.295&quot;,&quot;1.429&quot;,&quot;0.962&quot;],[&quot;4.417&quot;,&quot;8.167&quot;,&quot;0.113&quot;],[&quot;0.492&quot;,&quot;4.771&quot;,&quot;0.785&quot;],[&quot;7.632&quot;,&quot;2.605&quot;,&quot;0.645&quot;],[&quot;14.242&quot;,&quot;5.042&quot;,&quot;0.368&quot;]]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的实例只能拖动 dataZoom 组件来缩小或放大图表。如果想在坐标系内进行拖动，以及用鼠标滚轮（或移动触屏上的两指滑动）进行缩放，那么需要 再再加上一个 inside 型的 dataZoom 组件。</p>
<p>在以上实例基础上我们再增加 <strong>type: ‘inside’</strong> 的配置信息</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">option = &#123;</span><br><span class="line">    ...,</span><br><span class="line">    dataZoom: [</span><br><span class="line">        &#123;   // 这个dataZoom组件，默认控制x轴。</span><br><span class="line">            type: &#x27;slider&#x27;, // 这个 dataZoom 组件是 slider 型 dataZoom 组件</span><br><span class="line">            start: 10,      // 左边在 10% 的位置。</span><br><span class="line">            end: 60         // 右边在 60% 的位置。</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;   // 这个dataZoom组件，也控制x轴。</span><br><span class="line">            type: &#x27;inside&#x27;, // 这个 dataZoom 组件是 inside 型 dataZoom 组件</span><br><span class="line">            start: 10,      // 左边在 10% 的位置。</span><br><span class="line">            end: 60         // 右边在 60% 的位置。</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当然我们可以通过 dataZoom.xAxisIndex 或 dataZoom.yAxisIndex 来指定 dataZoom 控制哪个或哪些数轴。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">var data1 = [];</span><br><span class="line">var data2 = [];</span><br><span class="line">var data3 = [];</span><br><span class="line"></span><br><span class="line">var random = function (max) &#123;</span><br><span class="line">    return (Math.random() * max).toFixed(3);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">for (var i = 0; i &lt; 500; i++) &#123;</span><br><span class="line">    data1.push([random(15), random(10), random(1)]);</span><br><span class="line">    data2.push([random(10), random(10), random(1)]);</span><br><span class="line">    data3.push([random(15), random(10), random(1)]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">option = &#123;</span><br><span class="line">    animation: false,</span><br><span class="line">    legend: &#123;</span><br><span class="line">        data: [&#x27;scatter&#x27;, &#x27;scatter2&#x27;, &#x27;scatter3&#x27;]</span><br><span class="line">    &#125;,</span><br><span class="line">    tooltip: &#123;</span><br><span class="line">    &#125;,</span><br><span class="line">    xAxis: &#123;</span><br><span class="line">        type: &#x27;value&#x27;,</span><br><span class="line">        min: &#x27;dataMin&#x27;,</span><br><span class="line">        max: &#x27;dataMax&#x27;,</span><br><span class="line">        splitLine: &#123;</span><br><span class="line">            show: true</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    yAxis: &#123;</span><br><span class="line">        type: &#x27;value&#x27;,</span><br><span class="line">        min: &#x27;dataMin&#x27;,</span><br><span class="line">        max: &#x27;dataMax&#x27;,</span><br><span class="line">        splitLine: &#123;</span><br><span class="line">            show: true</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    dataZoom: [</span><br><span class="line">        &#123;</span><br><span class="line">            type: &#x27;slider&#x27;,</span><br><span class="line">            show: true,</span><br><span class="line">            xAxisIndex: [0],</span><br><span class="line">            start: 1,</span><br><span class="line">            end: 35</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            type: &#x27;slider&#x27;,</span><br><span class="line">            show: true,</span><br><span class="line">            yAxisIndex: [0],</span><br><span class="line">            left: &#x27;93%&#x27;,</span><br><span class="line">            start: 29,</span><br><span class="line">            end: 36</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            type: &#x27;inside&#x27;,</span><br><span class="line">            xAxisIndex: [0],</span><br><span class="line">            start: 1,</span><br><span class="line">            end: 35</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            type: &#x27;inside&#x27;,</span><br><span class="line">            yAxisIndex: [0],</span><br><span class="line">            start: 29,</span><br><span class="line">            end: 36</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    series: [</span><br><span class="line">        &#123;</span><br><span class="line">            name: &#x27;scatter&#x27;,</span><br><span class="line">            type: &#x27;scatter&#x27;,</span><br><span class="line">            itemStyle: &#123;</span><br><span class="line">                normal: &#123;</span><br><span class="line">                    opacity: 0.8</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            symbolSize: function (val) &#123;</span><br><span class="line">                return val[2] * 40;</span><br><span class="line">            &#125;,</span><br><span class="line">            data: data1</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            name: &#x27;scatter2&#x27;,</span><br><span class="line">            type: &#x27;scatter&#x27;,</span><br><span class="line">            itemStyle: &#123;</span><br><span class="line">                normal: &#123;</span><br><span class="line">                    opacity: 0.8</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            symbolSize: function (val) &#123;</span><br><span class="line">                return val[2] * 40;</span><br><span class="line">            &#125;,</span><br><span class="line">            data: data2</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            name: &#x27;scatter3&#x27;,</span><br><span class="line">            type: &#x27;scatter&#x27;,</span><br><span class="line">            itemStyle: &#123;</span><br><span class="line">                normal: &#123;</span><br><span class="line">                    opacity: 0.8,</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            symbolSize: function (val) &#123;</span><br><span class="line">                return val[2] * 40;</span><br><span class="line">            &#125;,</span><br><span class="line">            data: data3</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><style>.cllyeveasrxn{zoom:50%;}</style><img src="/zh-CN/echarts/image-20221013202500415.png" class="lazyload" data-srcset="/zh-CN/echarts/image-20221013202500415.png" srcset="data:image/png;base64,666" class="cllyeveasrxn lazyload" alt="image-20221013202500415"></p>
<h3 id="响应式"><a href="#响应式" class="headerlink" title="响应式"></a>响应式</h3><p>ECharts 图表显示在用户指定高宽的 DOM 节点（容器）中。</p>
<p>有时候我们希望在 PC 和 移动设备上都能够很好的展示图表的内容，实现响应式的设计，为了解决这个问题，ECharts 完善了组件的定位设置，并且实现了类似 <a href="https://www.runoob.com/css3/css3-mediaqueries.html">CSS Media Query</a> 的自适应能力。</p>
<h4 id="组件的定位和布局"><a href="#组件的定位和布局" class="headerlink" title="组件的定位和布局"></a>组件的定位和布局</h4><h5 id="left-right-top-bottom-width-height-定位方式"><a href="#left-right-top-bottom-width-height-定位方式" class="headerlink" title="left/right/top/bottom/width/height 定位方式"></a>left/right/top/bottom/width/height 定位方式</h5><p>这六个量中，每个量都可以是『绝对值』或者『百分比』或者『位置描述』。</p>
<ul>
<li><p>绝对值</p>
<p>单位是浏览器像素（px），用 <code>number</code> 形式书写（不写单位）。例如 <code>&#123;left: 23, height: 400&#125;</code>。</p>
</li>
<li><p>百分比</p>
<p>表示占 DOM 容器高宽的百分之多少，用 <code>string</code> 形式书写。例如 <code>&#123;right: &#39;30%&#39;, bottom: &#39;40%&#39;&#125;</code>。</p>
</li>
<li><p>位置描述</p>
<ul>
<li>可以设置 <code>left: &#39;center&#39;</code>，表示水平居中。</li>
<li>可以设置 <code>top: &#39;middle&#39;</code>，表示垂直居中。</li>
</ul>
</li>
</ul>
<p>这六个量的概念，和 CSS 中六个量的概念类似：</p>
<ul>
<li>left：距离 DOM 容器左边界的距离。</li>
<li>right：距离 DOM 容器右边界的距离。</li>
<li>top：距离 DOM 容器上边界的距离。</li>
<li>bottom：距离 DOM 容器下边界的距离。</li>
<li>width：宽度。</li>
<li>height：高度。</li>
</ul>
<p>在横向，left、right、width 三个量中，只需两个量有值即可，因为任两个量可以决定组件的位置和大小，例如 left 和 right 或者 right 和 width 都可以决定组件的位置和大小。 纵向，top、bottom、height 三个量，和横向类同不赘述。</p>
<h5 id="center-radius-定位方式"><a href="#center-radius-定位方式" class="headerlink" title="center / radius 定位方式"></a>center / radius 定位方式</h5><ul>
<li><p><code>center</code></p>
<p>是一个数组，表示 <code>[x, y]</code>，其中，<code>x</code>、<code>y</code>可以是『绝对值』或者『百分比』，含义和前述相同。</p>
</li>
<li><p><code>radius</code></p>
<p>是一个数组，表示 <code>[内半径, 外半径]</code>，其中，内外半径可以是『绝对值』或者『百分比』，含义和前述相同。</p>
<p>在自适应容器大小时，百分比设置是很有用的。</p>
</li>
</ul>
<h5 id="横向（horizontal）和纵向（vertical）"><a href="#横向（horizontal）和纵向（vertical）" class="headerlink" title="横向（horizontal）和纵向（vertical）"></a>横向（horizontal）和纵向（vertical）</h5><p>ECharts的『外观狭长』型的组件（如 legend、visualMap、dataZoom、timeline等），大多提供了『横向布局』『纵向布局』的选择。例如，在细长的移动端屏幕上，可能适合使用『纵向布局』；在PC宽屏上，可能适合使用『横向布局』。</p>
<p>横纵向布局的设置，一般在『组件』或者『系列』的 orient 或者 layout 配置项上，设置为 ‘horizontal’ 或者 ‘vertical’。</p>
<p>以下实例中我们可以可尝试拖动右下角的圆点，图表会随着屏幕尺寸变化，legend 和 系列会自动改变布局位置和方式。</p>
<p>实例中我们使用了 jQuery 来加载外部数据，使用时我们需要引入 jQuery 库。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">$.when(</span><br><span class="line">    $.getScript(&#x27;https://www.runoob.com/static/js/timelineGDP.js&#x27;),</span><br><span class="line">    $.getScript(&#x27;https://www.runoob.com/static/js/draggable.js&#x27;)</span><br><span class="line">).done(function () &#123;</span><br><span class="line"></span><br><span class="line">    draggable.init(</span><br><span class="line">        $(&#x27;div[_echarts_instance_]&#x27;)[0],</span><br><span class="line">        myChart,</span><br><span class="line">        &#123;</span><br><span class="line">            width: 700,</span><br><span class="line">            height: 400,</span><br><span class="line">            throttle: 70</span><br><span class="line">        &#125;</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    myChart.hideLoading();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    option = &#123;</span><br><span class="line">        baseOption: &#123;</span><br><span class="line">            title : &#123;</span><br><span class="line">                text: &#x27;南丁格尔玫瑰图&#x27;,</span><br><span class="line">                subtext: &#x27;纯属虚构&#x27;,</span><br><span class="line">                x:&#x27;center&#x27;</span><br><span class="line">            &#125;,</span><br><span class="line">            tooltip : &#123;</span><br><span class="line">                trigger: &#x27;item&#x27;,</span><br><span class="line">                formatter: &quot;&#123;a&#125; <span class="tag">&lt;<span class="name">br</span>/&gt;</span>&#123;b&#125; : &#123;c&#125; (&#123;d&#125;%)&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            legend: &#123;</span><br><span class="line">                data:[&#x27;rose1&#x27;,&#x27;rose2&#x27;,&#x27;rose3&#x27;,&#x27;rose4&#x27;,&#x27;rose5&#x27;,&#x27;rose6&#x27;,&#x27;rose7&#x27;,&#x27;rose8&#x27;]</span><br><span class="line">            &#125;,</span><br><span class="line">            toolbox: &#123;</span><br><span class="line">                show : true,</span><br><span class="line">                feature : &#123;</span><br><span class="line">                    mark : &#123;show: true&#125;,</span><br><span class="line">                    dataView : &#123;show: true, readOnly: false&#125;,</span><br><span class="line">                    magicType : &#123;</span><br><span class="line">                        show: true,</span><br><span class="line">                        type: [&#x27;pie&#x27;, &#x27;funnel&#x27;]</span><br><span class="line">                    &#125;,</span><br><span class="line">                    restore : &#123;show: true&#125;,</span><br><span class="line">                    saveAsImage : &#123;show: true&#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            calculable : true,</span><br><span class="line">            series : [</span><br><span class="line">                &#123;</span><br><span class="line">                    name:&#x27;半径模式&#x27;,</span><br><span class="line">                    type:&#x27;pie&#x27;,</span><br><span class="line">                    roseType : &#x27;radius&#x27;,</span><br><span class="line">                    label: &#123;</span><br><span class="line">                        normal: &#123;</span><br><span class="line">                            show: false</span><br><span class="line">                        &#125;,</span><br><span class="line">                        emphasis: &#123;</span><br><span class="line">                            show: true</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    lableLine: &#123;</span><br><span class="line">                        normal: &#123;</span><br><span class="line">                            show: false</span><br><span class="line">                        &#125;,</span><br><span class="line">                        emphasis: &#123;</span><br><span class="line">                            show: true</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    data:[</span><br><span class="line">                        &#123;value:10, name:&#x27;rose1&#x27;&#125;,</span><br><span class="line">                        &#123;value:5, name:&#x27;rose2&#x27;&#125;,</span><br><span class="line">                        &#123;value:15, name:&#x27;rose3&#x27;&#125;,</span><br><span class="line">                        &#123;value:25, name:&#x27;rose4&#x27;&#125;,</span><br><span class="line">                        &#123;value:20, name:&#x27;rose5&#x27;&#125;,</span><br><span class="line">                        &#123;value:35, name:&#x27;rose6&#x27;&#125;,</span><br><span class="line">                        &#123;value:30, name:&#x27;rose7&#x27;&#125;,</span><br><span class="line">                        &#123;value:40, name:&#x27;rose8&#x27;&#125;</span><br><span class="line">                    ]</span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                    name:&#x27;面积模式&#x27;,</span><br><span class="line">                    type:&#x27;pie&#x27;,</span><br><span class="line">                    roseType : &#x27;area&#x27;,</span><br><span class="line">                    data:[</span><br><span class="line">                        &#123;value:10, name:&#x27;rose1&#x27;&#125;,</span><br><span class="line">                        &#123;value:5, name:&#x27;rose2&#x27;&#125;,</span><br><span class="line">                        &#123;value:15, name:&#x27;rose3&#x27;&#125;,</span><br><span class="line">                        &#123;value:25, name:&#x27;rose4&#x27;&#125;,</span><br><span class="line">                        &#123;value:20, name:&#x27;rose5&#x27;&#125;,</span><br><span class="line">                        &#123;value:35, name:&#x27;rose6&#x27;&#125;,</span><br><span class="line">                        &#123;value:30, name:&#x27;rose7&#x27;&#125;,</span><br><span class="line">                        &#123;value:40, name:&#x27;rose8&#x27;&#125;</span><br><span class="line">                    ]</span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        media: [</span><br><span class="line">            &#123;</span><br><span class="line">                option: &#123;</span><br><span class="line">                    legend: &#123;</span><br><span class="line">                        right: &#x27;center&#x27;,</span><br><span class="line">                        bottom: 0,</span><br><span class="line">                        orient: &#x27;horizontal&#x27;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    series: [</span><br><span class="line">                        &#123;</span><br><span class="line">                            radius: [20, &#x27;50%&#x27;],</span><br><span class="line">                            center: [&#x27;25%&#x27;, &#x27;50%&#x27;]</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &#123;</span><br><span class="line">                            radius: [30, &#x27;50%&#x27;],</span><br><span class="line">                            center: [&#x27;75%&#x27;, &#x27;50%&#x27;]</span><br><span class="line">                        &#125;</span><br><span class="line">                    ]</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                query: &#123;</span><br><span class="line">                    minAspectRatio: 1</span><br><span class="line">                &#125;,</span><br><span class="line">                option: &#123;</span><br><span class="line">                    legend: &#123;</span><br><span class="line">                        right: &#x27;center&#x27;,</span><br><span class="line">                        bottom: 0,</span><br><span class="line">                        orient: &#x27;horizontal&#x27;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    series: [</span><br><span class="line">                        &#123;</span><br><span class="line">                            radius: [20, &#x27;50%&#x27;],</span><br><span class="line">                            center: [&#x27;25%&#x27;, &#x27;50%&#x27;]</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &#123;</span><br><span class="line">                            radius: [30, &#x27;50%&#x27;],</span><br><span class="line">                            center: [&#x27;75%&#x27;, &#x27;50%&#x27;]</span><br><span class="line">                        &#125;</span><br><span class="line">                    ]</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                query: &#123;</span><br><span class="line">                    maxAspectRatio: 1</span><br><span class="line">                &#125;,</span><br><span class="line">                option: &#123;</span><br><span class="line">                    legend: &#123;</span><br><span class="line">                        right: &#x27;center&#x27;,</span><br><span class="line">                        bottom: 0,</span><br><span class="line">                        orient: &#x27;horizontal&#x27;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    series: [</span><br><span class="line">                        &#123;</span><br><span class="line">                            radius: [20, &#x27;50%&#x27;],</span><br><span class="line">                            center: [&#x27;50%&#x27;, &#x27;30%&#x27;]</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &#123;</span><br><span class="line">                            radius: [30, &#x27;50%&#x27;],</span><br><span class="line">                            center: [&#x27;50%&#x27;, &#x27;70%&#x27;]</span><br><span class="line">                        &#125;</span><br><span class="line">                    ]</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                query: &#123;</span><br><span class="line">                    maxWidth: 500</span><br><span class="line">                &#125;,</span><br><span class="line">                option: &#123;</span><br><span class="line">                    legend: &#123;</span><br><span class="line">                        right: 10,</span><br><span class="line">                        top: &#x27;15%&#x27;,</span><br><span class="line">                        orient: &#x27;vertical&#x27;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    series: [</span><br><span class="line">                        &#123;</span><br><span class="line">                            radius: [20, &#x27;50%&#x27;],</span><br><span class="line">                            center: [&#x27;50%&#x27;, &#x27;30%&#x27;]</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &#123;</span><br><span class="line">                            radius: [30, &#x27;50%&#x27;],</span><br><span class="line">                            center: [&#x27;50%&#x27;, &#x27;75%&#x27;]</span><br><span class="line">                        &#125;</span><br><span class="line">                    ]</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    myChart.setOption(option);</span><br><span class="line"></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<h3 id="数据的视觉映射"><a href="#数据的视觉映射" class="headerlink" title="数据的视觉映射"></a>数据的视觉映射</h3><p>visualMap 组件中可以使用的视觉元素有：</p>
<ul>
<li>图形类别（symbol）</li>
<li>图形大小（symbolSize）</li>
<li>颜色（color）</li>
<li>透明度（opacity）</li>
<li>颜色透明度（colorAlpha）</li>
<li>颜色明暗度（colorLightness）</li>
<li>颜色饱和度（colorSaturation）</li>
<li>色调（colorHue）</li>
</ul>
<h4 id="数据和维度"><a href="#数据和维度" class="headerlink" title="数据和维度"></a>数据和维度</h4><p>ECharts 中的数据，一般存放于 <strong>series.data</strong> 中。</p>
<p>不同的图表类型，数据格式有所不一样，但是他们的共同特点就都是数据项（dataItem） 的集合。每个数据项含有 数据值（value） 和其他信息（可选）。每个数据值，可以是单一的数值（一维）或者一个数组（多维）。</p>
<p>series.data 最常见的形式 是线性表，即一个普通数组：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">series: &#123;</span><br><span class="line">    data: [</span><br><span class="line">        &#123;       // 这里每一个项就是数据项（dataItem）</span><br><span class="line">            value: 2323, // 这是数据项的数据值（value）</span><br><span class="line">            itemStyle: &#123;...&#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        1212,   // 也可以直接是 dataItem 的 value，这更常见。</span><br><span class="line">        2323,   // 每个 value 都是『一维』的。</span><br><span class="line">        4343,</span><br><span class="line">        3434</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">series: &#123;</span><br><span class="line">    data: [</span><br><span class="line">        &#123;                        // 这里每一个项就是数据项（dataItem）</span><br><span class="line">            value: [3434, 129,  &#x27;圣马力诺&#x27;], // 这是数据项的数据值（value）</span><br><span class="line">            itemStyle: &#123;...&#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        [1212, 5454, &#x27;梵蒂冈&#x27;],   // 也可以直接是 dataItem 的 value，这更常见。</span><br><span class="line">        [2323, 3223, &#x27;瑙鲁&#x27;],     // 每个 value 都是『三维』的，每列是一个维度。</span><br><span class="line">        [4343, 23,   &#x27;图瓦卢&#x27;]    // 假如是『气泡图』，常见第一维度映射到x轴，</span><br><span class="line">                                 // 第二维度映射到y轴，</span><br><span class="line">                                 // 第三维度映射到气泡半径（symbolSize）</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在图表中，往往默认把 value 的前一两个维度进行映射，比如取第一个维度映射到x轴，取第二个维度映射到y轴。如果想要把更多的维度展现出来，可以借助 visualMap 。</p>
<h4 id="visualMap组件"><a href="#visualMap组件" class="headerlink" title="visualMap组件"></a>visualMap组件</h4><p>visualMap 组件定义了把数据的指定维度映射到对应的视觉元素上。</p>
<p>visualMap 组件可以定义多个，从而可以同时对数据中的多个维度进行视觉映射。</p>
<p>visualMap 组件可以定义为 分段型（visualMapPiecewise） 或 连续型（visualMapContinuous），通过 type 来区分。例如</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">option = &#123;</span><br><span class="line">    visualMap: [</span><br><span class="line">        &#123; // 第一个 visualMap 组件</span><br><span class="line">            type: &#x27;continuous&#x27;, // 定义为连续型 visualMap</span><br><span class="line">            ...</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123; // 第二个 visualMap 组件</span><br><span class="line">            type: &#x27;piecewise&#x27;, // 定义为分段型 visualMap</span><br><span class="line">            ...</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    ...</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>分段型视觉映射组件，有三种模式：</p>
<ul>
<li>连续型数据平均分段: 依据 visualMap-piecewise.splitNumber 来自动平均分割成若干块。</li>
<li>连续型数据自定义分段: 依据 visualMap-piecewise.pieces 来定义每块范围。</li>
<li>离散数据根据类别分段: 类别定义在 visualMap-piecewise.categories 中。</li>
</ul>
<h4 id="映射方式配置"><a href="#映射方式配置" class="headerlink" title="映射方式配置"></a>映射方式配置</h4><p>visualMap 中可以指定数据的指定维度映射到对应的视觉元素上。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">option = &#123;</span><br><span class="line">    visualMap: [</span><br><span class="line">        &#123;</span><br><span class="line">            type: &#x27;piecewise&#x27;</span><br><span class="line">            min: 0,</span><br><span class="line">            max: 5000,</span><br><span class="line">            dimension: 3,       // series.data 的第四个维度（即 value[3]）被映射</span><br><span class="line">            seriesIndex: 4,     // 对第四个系列进行映射。</span><br><span class="line">            inRange: &#123;          // 选中范围中的视觉配置</span><br><span class="line">                color: [&#x27;blue&#x27;, &#x27;#121122&#x27;, &#x27;red&#x27;], // 定义了图形颜色映射的颜色列表，</span><br><span class="line">                                                    // 数据最小值映射到&#x27;blue&#x27;上，</span><br><span class="line">                                                    // 最大值映射到&#x27;red&#x27;上，</span><br><span class="line">                                                    // 其余自动线性计算。</span><br><span class="line">                symbolSize: [30, 100]               // 定义了图形尺寸的映射范围，</span><br><span class="line">                                                    // 数据最小值映射到30上，</span><br><span class="line">                                                    // 最大值映射到100上，</span><br><span class="line">                                                    // 其余自动线性计算。</span><br><span class="line">            &#125;,</span><br><span class="line">            outOfRange: &#123;       // 选中范围外的视觉配置</span><br><span class="line">                symbolSize: [30, 100]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        ...</span><br><span class="line">    ]</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">option = &#123;</span><br><span class="line">    visualMap: [</span><br><span class="line">        &#123;</span><br><span class="line">            ...,</span><br><span class="line">            inRange: &#123;          // 选中范围中的视觉配置</span><br><span class="line">                colorLightness: [0.2, 1], // 映射到明暗度上。也就是对本来的颜色进行明暗度处理。</span><br><span class="line">                                          // 本来的颜色可能是从全局色板中选取的颜色，visualMap组件并不关心。</span><br><span class="line">                symbolSize: [30, 100]</span><br><span class="line">            &#125;,</span><br><span class="line">            ...</span><br><span class="line">        &#125;,</span><br><span class="line">        ...</span><br><span class="line">    ]</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h3 id="事件处理"><a href="#事件处理" class="headerlink" title="事件处理"></a>事件处理</h3><p>ECharts 中我们可以通过监听用户的操作行为来回调对应的函数。</p>
<p>ECharts 通过 <strong>on</strong> 方法来监听用户的行为，例如监控用户的点击行为。</p>
<p>ECharts 中事件分为两种类型:</p>
<p>用户鼠标操作点击，如 <strong>‘click’、’dblclick’、’mousedown’、’mousemove’、’mouseup’、’mouseover’、’mouseout’、’globalout’、’contextmenu’</strong> 事件。</p>
<p>还有一种是用户在使用可以交互的组件后触发的行为事件，例如在切换图例开关时触发的 ‘legendselectchanged’ 事件），数据区域缩放时触发的 ‘datazoom’ 事件等等。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">myChart.on(&#x27;click&#x27;, function (params) &#123;</span><br><span class="line">    // 在用户点击后控制台打印数据的名称</span><br><span class="line">    console.log(params);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">myChart.on(&#x27;legendselectchanged&#x27;, function (params) &#123;</span><br><span class="line">    console.log(params);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">chart.on(&#x27;click&#x27;, &#x27;series.line&#x27;, function (params) &#123;</span><br><span class="line">    console.log(params);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">chart.on(&#x27;mouseover&#x27;, &#123;seriesIndex: 1, name: &#x27;xx&#x27;&#125;, function (params) &#123;</span><br><span class="line">    console.log(params);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<h4 id="鼠标事件"><a href="#鼠标事件" class="headerlink" title="鼠标事件"></a>鼠标事件</h4><p>ECharts 支持的鼠标事件类型，包括 ‘click’、’dblclick’、’mousedown’、’mousemove’、’mouseup’、’mouseover’、’mouseout’、’globalout’、’contextmenu’ 事件。</p>
<p>以下实例在点击柱形图时会弹出对话框：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 基于准备好的dom，初始化ECharts实例</span><br><span class="line">var myChart = echarts.init(document.getElementById(&#x27;main&#x27;));</span><br><span class="line"></span><br><span class="line">// 指定图表的配置项和数据</span><br><span class="line">var option = &#123;</span><br><span class="line">    xAxis: &#123;</span><br><span class="line">        data: [&quot;衬衫&quot;,&quot;羊毛衫&quot;,&quot;雪纺衫&quot;,&quot;裤子&quot;,&quot;高跟鞋&quot;,&quot;袜子&quot;]</span><br><span class="line">    &#125;,</span><br><span class="line">    yAxis: &#123;&#125;,</span><br><span class="line">    series: [&#123;</span><br><span class="line">        name: &#x27;销量&#x27;,</span><br><span class="line">        type: &#x27;bar&#x27;,</span><br><span class="line">        data: [5, 20, 36, 10, 10, 20]</span><br><span class="line">    &#125;]</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">// 使用刚指定的配置项和数据显示图表。</span><br><span class="line">myChart.setOption(option);</span><br><span class="line">// 处理点击事件并且弹出数据名称</span><br><span class="line">myChart.on(&#x27;click&#x27;, function (params) &#123;</span><br><span class="line">    alert(params.name);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>所有的鼠标事件包含参数 params，这是一个包含点击图形的数据信息的对象，格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    // 当前点击的图形元素所属的组件名称，</span><br><span class="line">    // 其值如 &#x27;series&#x27;、&#x27;markLine&#x27;、&#x27;markPoint&#x27;、&#x27;timeLine&#x27; 等。</span><br><span class="line">    componentType: string,</span><br><span class="line">    // 系列类型。值可能为：&#x27;line&#x27;、&#x27;bar&#x27;、&#x27;pie&#x27; 等。当 componentType 为 &#x27;series&#x27; 时有意义。</span><br><span class="line">    seriesType: string,</span><br><span class="line">    // 系列在传入的 option.series 中的 index。当 componentType 为 &#x27;series&#x27; 时有意义。</span><br><span class="line">    seriesIndex: number,</span><br><span class="line">    // 系列名称。当 componentType 为 &#x27;series&#x27; 时有意义。</span><br><span class="line">    seriesName: string,</span><br><span class="line">    // 数据名，类目名</span><br><span class="line">    name: string,</span><br><span class="line">    // 数据在传入的 data 数组中的 index</span><br><span class="line">    dataIndex: number,</span><br><span class="line">    // 传入的原始数据项</span><br><span class="line">    data: Object,</span><br><span class="line">    // sankey、graph 等图表同时含有 nodeData 和 edgeData 两种 data，</span><br><span class="line">    // dataType 的值会是 &#x27;node&#x27; 或者 &#x27;edge&#x27;，表示当前点击在 node 还是 edge 上。</span><br><span class="line">    // 其他大部分图表中只有一种 data，dataType 无意义。</span><br><span class="line">    dataType: string,</span><br><span class="line">    // 传入的数据值</span><br><span class="line">    value: number|Array</span><br><span class="line">    // 数据图形的颜色。当 componentType 为 &#x27;series&#x27; 时有意义。</span><br><span class="line">    color: string</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如何区分鼠标点击到了哪里：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">myChart.on(&#x27;click&#x27;, function (params) &#123;</span><br><span class="line">    if (params.componentType === &#x27;markPoint&#x27;) &#123;</span><br><span class="line">        // 点击到了 markPoint 上</span><br><span class="line">        if (params.seriesIndex === 5) &#123;</span><br><span class="line">            // 点击到了 index 为 5 的 series 的 markPoint 上。</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    else if (params.componentType === &#x27;series&#x27;) &#123;</span><br><span class="line">        if (params.seriesType === &#x27;graph&#x27;) &#123;</span><br><span class="line">            if (params.dataType === &#x27;edge&#x27;) &#123;</span><br><span class="line">                // 点击到了 graph 的 edge（边）上。</span><br><span class="line">            &#125;</span><br><span class="line">            else &#123;</span><br><span class="line">                // 点击到了 graph 的 node（节点）上。</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>使用 query 只对指定的组件的图形元素的触发回调：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chart.on(eventName, query, handler);</span><br></pre></td></tr></table></figure>
<p>query 可为 string 或者 Object。</p>
<p>如果为 string 表示组件类型。格式可以是 ‘mainType’ 或者 ‘mainType.subType’。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chart.on(&#x27;click&#x27;, &#x27;series&#x27;, function () &#123;...&#125;);</span><br><span class="line">chart.on(&#x27;click&#x27;, &#x27;series.line&#x27;, function () &#123;...&#125;);</span><br><span class="line">chart.on(&#x27;click&#x27;, &#x27;dataZoom&#x27;, function () &#123;...&#125;);</span><br><span class="line">chart.on(&#x27;click&#x27;, &#x27;xAxis.category&#x27;, function () &#123;...&#125;);</span><br></pre></td></tr></table></figure>
<p>如果为 Object，可以包含以下一个或多个属性，每个属性都是可选的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &lt;mainType&gt;Index: number // 组件 index</span><br><span class="line">    &lt;mainType&gt;Name: string // 组件 name</span><br><span class="line">    &lt;mainType&gt;Id: string // 组件 id</span><br><span class="line">    dataIndex: number // 数据项 index</span><br><span class="line">    name: string // 数据项 name</span><br><span class="line">    dataType: string // 数据项 type，如关系图中的 &#x27;node&#x27;, &#x27;edge&#x27;</span><br><span class="line">    element: string // 自定义系列中的 el 的 name</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chart.setOption(&#123;</span><br><span class="line">    // ...</span><br><span class="line">    series: [&#123;</span><br><span class="line">        name: &#x27;uuu&#x27;</span><br><span class="line">        // ...</span><br><span class="line">    &#125;]</span><br><span class="line">&#125;);</span><br><span class="line">chart.on(&#x27;mouseover&#x27;, &#123;seriesName: &#x27;uuu&#x27;&#125;, function () &#123;</span><br><span class="line">    // series name 为 &#x27;uuu&#x27; 的系列中的图形元素被 &#x27;mouseover&#x27; 时，此方法被回调。</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chart.setOption(&#123;</span><br><span class="line">    // ...</span><br><span class="line">    series: [&#123;</span><br><span class="line">        // ...</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        // ...</span><br><span class="line">        data: [</span><br><span class="line">            &#123;name: &#x27;xx&#x27;, value: 121&#125;,</span><br><span class="line">            &#123;name: &#x27;yy&#x27;, value: 33&#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;]</span><br><span class="line">&#125;);</span><br><span class="line">chart.on(&#x27;mouseover&#x27;, &#123;seriesIndex: 1, name: &#x27;xx&#x27;&#125;, function () &#123;</span><br><span class="line">    // series index 1 的系列中的 name 为 &#x27;xx&#x27; 的元素被 &#x27;mouseover&#x27; 时，此方法被回调。</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chart.setOption(&#123;</span><br><span class="line">    // ...</span><br><span class="line">    series: [&#123;</span><br><span class="line">        type: &#x27;graph&#x27;,</span><br><span class="line">        nodes: [&#123;name: &#x27;a&#x27;, value: 10&#125;, &#123;name: &#x27;b&#x27;, value: 20&#125;],</span><br><span class="line">        edges: [&#123;source: 0, target: 1&#125;]</span><br><span class="line">    &#125;]</span><br><span class="line">&#125;);</span><br><span class="line">chart.on(&#x27;click&#x27;, &#123;dataType: &#x27;node&#x27;&#125;, function () &#123;</span><br><span class="line">    // 关系图的节点被点击时此方法被回调。</span><br><span class="line">&#125;);</span><br><span class="line">chart.on(&#x27;click&#x27;, &#123;dataType: &#x27;edge&#x27;&#125;, function () &#123;</span><br><span class="line">    // 关系图的边被点击时此方法被回调。</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chart.setOption(&#123;</span><br><span class="line">    // ...</span><br><span class="line">    series: &#123;</span><br><span class="line">        // ...</span><br><span class="line">        type: &#x27;custom&#x27;,</span><br><span class="line">        renderItem: function (params, api) &#123;</span><br><span class="line">            return &#123;</span><br><span class="line">                type: &#x27;group&#x27;,</span><br><span class="line">                children: [&#123;</span><br><span class="line">                    type: &#x27;circle&#x27;,</span><br><span class="line">                    name: &#x27;my_el&#x27;,</span><br><span class="line">                    // ...</span><br><span class="line">                &#125;, &#123;</span><br><span class="line">                    // ...</span><br><span class="line">                &#125;]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        data: [[12, 33]]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">chart.on(&#x27;mouseup&#x27;, &#123;element: &#x27;my_el&#x27;&#125;, function () &#123;</span><br><span class="line">    // name 为 &#x27;my_el&#x27; 的元素被 &#x27;mouseup&#x27; 时，此方法被回调。</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>你可以在回调函数中获得这个对象中的数据名、系列名称后在自己的数据仓库中索引得到其它的信息候更新图表，显示浮层等等，如下示例代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">myChart.on(&#x27;click&#x27;, function (parmas) &#123;</span><br><span class="line">    $.get(&#x27;detail?q=&#x27; + params.name, function (detail) &#123;</span><br><span class="line">        myChart.setOption(&#123;</span><br><span class="line">            series: [&#123;</span><br><span class="line">                name: &#x27;pie&#x27;,</span><br><span class="line">                // 通过饼图表现单个柱子中的数据分布</span><br><span class="line">                data: [detail.data]</span><br><span class="line">            &#125;]</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<h4 id="行为事件"><a href="#行为事件" class="headerlink" title="行为事件"></a>行为事件</h4><p>在 ECharts 中基本上所有的组件交互行为都会触发相应的事件，常用的事件和事件对应参数在 events 文档中有列出。</p>
<p>下面是监听一个图例开关的示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 图例开关的行为只会触发 legendselectchanged 事件</span><br><span class="line">myChart.on(&#x27;legendselectchanged&#x27;, function (params) &#123;</span><br><span class="line">    // 获取点击图例的选中状态</span><br><span class="line">    var isSelected = params.selected[params.name];</span><br><span class="line">    // 在控制台中打印</span><br><span class="line">    console.log((isSelected ? &#x27;选中了&#x27; : &#x27;取消选中了&#x27;) + &#x27;图例&#x27; + params.name);</span><br><span class="line">    // 打印所有图例的状态</span><br><span class="line">    console.log(params.selected);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<h4 id="触发组件行为"><a href="#触发组件行为" class="headerlink" title="触发组件行为"></a>触发组件行为</h4><p>上面我们只说明了用户的交互操作，但有时候我们也会需要在程序里调用方法并触发图表的行为，比如显示 tooltip。</p>
<p>ECharts 通过 dispatchAction({ type: ‘’ }) 来触发图表行为，统一管理了所有动作，也可以根据需要去记录用户的行为路径。</p>
<p>以上实例用于轮播饼图中的 tooltip：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">setInterval(function () &#123;</span><br><span class="line">    var dataLen = option.series[0].data.length;</span><br><span class="line">    // 取消之前高亮的图形</span><br><span class="line">    myChart.dispatchAction(&#123;</span><br><span class="line">        type: &#x27;downplay&#x27;,</span><br><span class="line">        seriesIndex: 0,</span><br><span class="line">        dataIndex: app.currentIndex</span><br><span class="line">    &#125;);</span><br><span class="line">    app.currentIndex = (app.currentIndex + 1) % dataLen;</span><br><span class="line">    // 高亮当前图形</span><br><span class="line">    myChart.dispatchAction(&#123;</span><br><span class="line">        type: &#x27;highlight&#x27;,</span><br><span class="line">        seriesIndex: 0,</span><br><span class="line">        dataIndex: app.currentIndex</span><br><span class="line">    &#125;);</span><br><span class="line">    // 显示 tooltip</span><br><span class="line">    myChart.dispatchAction(&#123;</span><br><span class="line">        type: &#x27;showTip&#x27;,</span><br><span class="line">        seriesIndex: 0,</span><br><span class="line">        dataIndex: app.currentIndex</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;, 1000);</span><br></pre></td></tr></table></figure>
<h3 id="旭日图"><a href="#旭日图" class="headerlink" title="旭日图"></a>旭日图</h3><p>旭日图（Sunburst）由多层的环形图组成，在数据结构上，内圈是外圈的父节点。因此，它既能像饼图一样表现局部和整体的占比，又能像矩形树图一样表现层级关系。</p>
<p>ECharts 创建旭日图很简单，只需要在 series 配置项中声明类型为 <strong>sunburst</strong> 即可，data 数据结构以树形结构声明，看下一个简单的实例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">    &lt;meta charset=&quot;utf-8&quot;&gt;</span><br><span class="line">    &lt;title&gt;ECharts 实例&lt;/title&gt;</span><br><span class="line">    &lt;!-- 引入 echarts.js --&gt;</span><br><span class="line">    &lt;script src=&quot;https://cdn.staticfile.org/echarts/4.3.0/echarts.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">    &lt;!-- 为ECharts准备一个具备大小（宽高）的Dom --&gt;</span><br><span class="line">    &lt;div id=&quot;main&quot; style=&quot;width: 600px;height:400px;&quot;&gt;&lt;/div&gt;</span><br><span class="line">    &lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">        // 基于准备好的dom，初始化echarts实例</span><br><span class="line">        var myChart = echarts.init(document.getElementById(&#x27;main&#x27;));</span><br><span class="line"> </span><br><span class="line">        // 指定图表的配置项和数据</span><br><span class="line">        var option = &#123;</span><br><span class="line">            series: &#123;</span><br><span class="line">                type: &#x27;sunburst&#x27;,</span><br><span class="line">                data: [&#123;</span><br><span class="line">                    name: &#x27;A&#x27;,</span><br><span class="line">                    value: 10,</span><br><span class="line">                    children: [&#123;</span><br><span class="line">                        value: 3,</span><br><span class="line">                        name: &#x27;Aa&#x27;</span><br><span class="line">                    &#125;, &#123;</span><br><span class="line">                        value: 5,</span><br><span class="line">                        name: &#x27;Ab&#x27;</span><br><span class="line">                    &#125;]</span><br><span class="line">                &#125;, &#123;</span><br><span class="line">                    name: &#x27;B&#x27;,</span><br><span class="line">                    children: [&#123;</span><br><span class="line">                        name: &#x27;Ba&#x27;,</span><br><span class="line">                        value: 4</span><br><span class="line">                    &#125;, &#123;</span><br><span class="line">                        name: &#x27;Bb&#x27;,</span><br><span class="line">                        value: 2</span><br><span class="line">                    &#125;]</span><br><span class="line">                &#125;, &#123;</span><br><span class="line">                    name: &#x27;C&#x27;,</span><br><span class="line">                    value: 3</span><br><span class="line">                &#125;]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">        // 使用刚指定的配置项和数据显示图表。</span><br><span class="line">        myChart.setOption(option);</span><br><span class="line">    &lt;/script&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p><style>.umjcjdipcfgn{zoom:50%;}</style><img src="/zh-CN/echarts/image-20221013212851746.png" class="lazyload" data-srcset="/zh-CN/echarts/image-20221013212851746.png" srcset="data:image/png;base64,666" class="umjcjdipcfgn lazyload" alt="image-20221013212851746"></p>
<p>更多详情<a href="https://www.runoob.com/echarts/echarts-sunburst.html">ECharts 旭日图 | 菜鸟教程 (runoob.com)</a></p>
<p>累了，累了，学习笔记就写到这吧，不懂的去菜鸟教程自学</p>
</script></strong></p>]]></content>
      <categories>
        <category>echarts</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>echarts</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/zh-CN/hello-world/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>Hexo博客搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>博客搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>Grad-cam论文笔记</title>
    <url>/zh-CN/grad-cam/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf">Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization </a></p>
<p>2.<a href="https://github.com/jacobgil/pytorch-grad-cam">jacobgil/pytorch-grad-cam: Advanced AI Explainability for computer vision. Support for CNNs, Vision Transformers, Classification, Object detection, Segmentation, Image similarity and more. (github.com)</a></p>
<p>3.<a href="https://blog.csdn.net/Sylvia_Lan/article/details/123309476">一个库可视化类激活热力图Grad-CAM pytorch版本</a></p>
<p>4.<a href="https://blog.csdn.net/qq_37924224/article/details/119181028"> libpng warning: iCCP: known incorrect sRGB profile 警告</a></p>
<h3 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h3><p>从第二个参考博文里下载源码或者直接用pip安装，安装命令如下</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">pip install grad-cam</span><br></pre></td></tr></tbody></table></figure>
<h3 id="解读源码"><a href="#解读源码" class="headerlink" title="解读源码"></a>解读源码</h3><p>现解读的是源码根目录下的<code>cam.py</code>文件，即它的使用代码，采取注释的方式解读：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="comment"># 这里默认你了解它的各种变体，这里有11种变体，有时间我会各个解读一下，你们也可以自行了解</span></span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam <span class="keyword">import</span> GradCAM, \</span><br><span class="line">    HiResCAM, \</span><br><span class="line">    ScoreCAM, \</span><br><span class="line">    GradCAMPlusPlus, \</span><br><span class="line">    AblationCAM, \</span><br><span class="line">    XGradCAM, \</span><br><span class="line">    EigenCAM, \</span><br><span class="line">    EigenGradCAM, \</span><br><span class="line">    LayerCAM, \</span><br><span class="line">    FullGrad, \</span><br><span class="line">    GradCAMElementWise</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam <span class="keyword">import</span> GuidedBackpropReLUModel</span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam.utils.image <span class="keyword">import</span> show_cam_on_image, \</span><br><span class="line">    deprocess_image, \</span><br><span class="line">    preprocess_image</span><br><span class="line"><span class="keyword">from</span> pytorch_grad_cam.utils.model_targets <span class="keyword">import</span> ClassifierOutputTarget</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里是命令行参数，等下给出运行命令你就一下子明白了，最重要的就是选择method,必须得跟choices里的一样</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_args</span>():</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--use-cuda'</span>, action=<span class="string">'store_true'</span>, default=<span class="literal">False</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">'Use NVIDIA GPU acceleration'</span>)</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">'--image-path'</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        default=<span class="string">'./examples/both.png'</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">'Input image path'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--aug_smooth'</span>, action=<span class="string">'store_true'</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">'Apply test time augmentation to smooth the CAM'</span>)</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">'--eigen_smooth'</span>,</span><br><span class="line">        action=<span class="string">'store_true'</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">'Reduce noise by taking the first principle componenet'</span></span><br><span class="line">        <span class="string">'of cam_weights*activations'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--method'</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">'gradcam'</span>,</span><br><span class="line">                        choices=[<span class="string">'gradcam'</span>, <span class="string">'hirescam'</span>, <span class="string">'gradcam++'</span>,</span><br><span class="line">                                 <span class="string">'scorecam'</span>, <span class="string">'xgradcam'</span>,</span><br><span class="line">                                 <span class="string">'ablationcam'</span>, <span class="string">'eigencam'</span>,</span><br><span class="line">                                 <span class="string">'eigengradcam'</span>, <span class="string">'layercam'</span>, <span class="string">'fullgrad'</span>],</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">'Can be gradcam/gradcam++/scorecam/xgradcam'</span></span><br><span class="line">                             <span class="string">'/ablationcam/eigencam/eigengradcam/layercam'</span>)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    args.use_cuda = args.use_cuda <span class="keyword">and</span> torch.cuda.is_available()</span><br><span class="line">    <span class="keyword">if</span> args.use_cuda:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'Using GPU for acceleration'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'Using CPU for computation'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> args</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="string">""" python cam.py -image-path &lt;path_to_image&gt;</span></span><br><span class="line"><span class="string">    Example usage of loading an image, and computing:</span></span><br><span class="line"><span class="string">        1. CAM</span></span><br><span class="line"><span class="string">        2. Guided Back Propagation</span></span><br><span class="line"><span class="string">        3. Combining both</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    args = get_args()</span><br><span class="line">    methods = \</span><br><span class="line">        {<span class="string">"gradcam"</span>: GradCAM,</span><br><span class="line">         <span class="string">"hirescam"</span>:HiResCAM,</span><br><span class="line">         <span class="string">"scorecam"</span>: ScoreCAM,</span><br><span class="line">         <span class="string">"gradcam++"</span>: GradCAMPlusPlus,</span><br><span class="line">         <span class="string">"ablationcam"</span>: AblationCAM,</span><br><span class="line">         <span class="string">"xgradcam"</span>: XGradCAM,</span><br><span class="line">         <span class="string">"eigencam"</span>: EigenCAM,</span><br><span class="line">         <span class="string">"eigengradcam"</span>: EigenGradCAM,</span><br><span class="line">         <span class="string">"layercam"</span>: LayerCAM,</span><br><span class="line">         <span class="string">"fullgrad"</span>: FullGrad,</span><br><span class="line">         <span class="string">"gradcamelementwise"</span>: GradCAMElementWise}</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 这里就可以替换你的模型了，用之前记得引入自己的模型</span></span><br><span class="line">    model = models.resnet50(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Choose the target layer you want to compute the visualization for.</span></span><br><span class="line">    <span class="comment"># Usually this will be the last convolutional layer in the model.</span></span><br><span class="line">    <span class="comment"># Some common choices can be:</span></span><br><span class="line">    <span class="comment"># Resnet18 and 50: model.layer4</span></span><br><span class="line">    <span class="comment"># VGG, densenet161: model.features[-1]</span></span><br><span class="line">    <span class="comment"># mnasnet1_0: model.layers[-1]</span></span><br><span class="line">    <span class="comment"># You can print the model to help chose the layer</span></span><br><span class="line">    <span class="comment"># You can pass a list with several target layers,</span></span><br><span class="line">    <span class="comment"># in that case the CAMs will be computed per layer and then aggregated.</span></span><br><span class="line">    <span class="comment"># You can also try selecting all layers of a certain type, with e.g:</span></span><br><span class="line">    <span class="comment"># from pytorch_grad_cam.utils.find_layers import find_layer_types_recursive</span></span><br><span class="line">    <span class="comment"># find_layer_types_recursive(model, [torch.nn.ReLU])</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 选择你想要可视化的特征层</span></span><br><span class="line">    target_layers = [model.layer4]</span><br><span class="line">    <span class="comment"># 读取图片</span></span><br><span class="line">    rgb_img = cv2.imread(args.image_path, <span class="number">1</span>)[:, :, ::-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    rgb_img = np.float32(rgb_img) / <span class="number">255</span></span><br><span class="line">    <span class="comment"># 预处理图片</span></span><br><span class="line">    input_tensor = preprocess_image(rgb_img,</span><br><span class="line">                                    mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                                    std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># We have to specify the target we want to generate</span></span><br><span class="line">    <span class="comment"># the Class Activation Maps for.</span></span><br><span class="line">    <span class="comment"># If targets is None, the highest scoring category (for every member in the batch) will be used.</span></span><br><span class="line">    <span class="comment"># You can target specific categories by</span></span><br><span class="line">    <span class="comment"># targets = [e.g ClassifierOutputTarget(281)]</span></span><br><span class="line">    <span class="comment"># 如果target设置为None,就会选择得分最高的类别</span></span><br><span class="line">    targets = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Using the with statement ensures the context is freed, and you can</span></span><br><span class="line">    <span class="comment"># recreate different CAM objects in a loop.</span></span><br><span class="line">    cam_algorithm = methods[args.method]</span><br><span class="line">    <span class="keyword">with</span> cam_algorithm(model=model,</span><br><span class="line">                       target_layers=target_layers,</span><br><span class="line">                       use_cuda=args.use_cuda) <span class="keyword">as</span> cam:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># AblationCAM and ScoreCAM have batched implementations.</span></span><br><span class="line">        <span class="comment"># You can override the internal batch size for faster computation.</span></span><br><span class="line">        cam.batch_size = <span class="number">32</span></span><br><span class="line">        grayscale_cam = cam(input_tensor=input_tensor,</span><br><span class="line">                            targets=targets,</span><br><span class="line">                            aug_smooth=args.aug_smooth,</span><br><span class="line">                            eigen_smooth=args.eigen_smooth)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Here grayscale_cam has only one image in the batch</span></span><br><span class="line">        grayscale_cam = grayscale_cam[<span class="number">0</span>, :]</span><br><span class="line">        <span class="comment"># 展示图片</span></span><br><span class="line">        cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cam_image is RGB encoded whereas "cv2.imwrite" requires BGR encoding.</span></span><br><span class="line">        cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)</span><br><span class="line"></span><br><span class="line">    gb_model = GuidedBackpropReLUModel(model=model, use_cuda=args.use_cuda)</span><br><span class="line">    gb = gb_model(input_tensor, target_category=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    cam_mask = cv2.merge([grayscale_cam, grayscale_cam, grayscale_cam])</span><br><span class="line">    cam_gb = deprocess_image(cam_mask * gb)</span><br><span class="line">    gb = deprocess_image(gb)</span><br><span class="line"></span><br><span class="line">    cv2.imwrite(<span class="string">f'<span class="subst">{args.method}</span>_cam.jpg'</span>, cam_image)</span><br><span class="line">    cv2.imwrite(<span class="string">f'<span class="subst">{args.method}</span>_gb.jpg'</span>, gb)</span><br><span class="line">    cv2.imwrite(<span class="string">f'<span class="subst">{args.method}</span>_cam_gb.jpg'</span>, cam_gb)</span><br></pre></td></tr></tbody></table></figure>
<p>这里有很多类都是源码里写了的，可以看源码了解一下，将该代码放在你的模型下，另起一个文件，导入自己的模型即可</p>
<h3 id="Grad-cam应用"><a href="#Grad-cam应用" class="headerlink" title="Grad-cam应用"></a>Grad-cam应用</h3><p>上述代码放在<code>test.py</code>文件里了</p>
<p>终端命令：<code>python test.py --image-path zophie.png --method gradcam --use-cuda</code></p>
<p><style>.ulkdbmlmpwnw{}</style><img src="/zh-CN/grad-cam/grad-cam/image-20220922205142809.png" class="lazyload" data-srcset="/zh-CN/grad-cam/grad-cam/image-20220922205142809.png" srcset="data:image/png;base64,666" class="ulkdbmlmpwnw lazyload"></p>
<p>issue:</p>
<p>1.可能有一两个包没用，比如<code>HiResCAM</code>,<code>GradCAMElementWise</code></p>
<p>这很简单，就是这个代码人家一直在更新，你只要使用<code>pip uninstall grad-cam</code>卸载,然后重新使用<code>pip install grad-cam</code>命令安装一遍就行了</p>
<p>2.显示<code>libpng warning: iCCP: known incorrect sRGB profile</code></p>
<p>放心，这只是一个警告啦，具体参考博文4</p>
<p>结果显示：</p>
<p>1.gradcam_cam.jpg</p>
<p><style>.yhyppberysgh{zoom:25%;}</style><img src="/zh-CN/grad-cam/grad-cam/image-20220922205934818.png" class="lazyload" data-srcset="/zh-CN/grad-cam/grad-cam/image-20220922205934818.png" srcset="data:image/png;base64,666" class="yhyppberysgh lazyload" alt="image-20220922205934818"></p>
<p>2.gradcam_cam_gb.jpg</p>
<p><style>.eppcpduwwliv{zoom:25%;}</style><img src="/zh-CN/grad-cam/grad-cam/image-20220922210017803.png" class="lazyload" data-srcset="/zh-CN/grad-cam/grad-cam/image-20220922210017803.png" srcset="data:image/png;base64,666" class="eppcpduwwliv lazyload" alt="image-20220922210017803"></p>
<p>3.gradcam_gb.jpg</p>
<p><style>.hecqidmhsyxj{zoom:25%;}</style><img src="/zh-CN/grad-cam/grad-cam/image-20220922210044972.png" class="lazyload" data-srcset="/zh-CN/grad-cam/grad-cam/image-20220922210044972.png" srcset="data:image/png;base64,666" class="hecqidmhsyxj lazyload" alt="image-20220922210044972"></p>
<p>可以看出文件命名方式为类激活方法+预处理方法，有兴趣的可以去看看预处理方法，代码里有写</p>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>grad-cam</tag>
      </tags>
  </entry>
  <entry>
    <title>Remove Element</title>
    <url>/zh-CN/leetcode01/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>原文链接：<a href="https://siddontang.gitbooks.io/leetcode-solution/content/array/remove_element.html">Remove Element · LeetCode题解 </a></p>
<p>题目：Given an array and a value, remove all instances of that &gt; value in place and return the new length.</p>
<p>The order of elements can be changed. It doesn’t matter what you leave beyond the new length.</p>
<p>作为开胃菜，我当然选取了最容易的一道题目，在一个数组里面移除指定value，并且返回新的数组长度。这题唯一需要注意的地方在于<code>in place</code>，不能新建另一个数组。</p>
<p>方法很简单，使用两个游标i，j，遍历数组，如果碰到了value，使用j记录位置，同时递增i，直到下一个非value出现，将此时i对应的值复制到j的位置上，增加j，重复上述过程直到遍历结束。这时候j就是新的数组长度。</p>
<p>题解：</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>{</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	<span class="function"><span class="type">int</span> <span class="title">removeElement</span><span class="params">(<span class="type">int</span> A[], <span class="type">int</span> n, <span class="type">int</span> elem)</span></span>{</span><br><span class="line">		<span class="type">int</span> i=<span class="number">0</span>;</span><br><span class="line">		<span class="type">int</span> j=<span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++){</span><br><span class="line">			<span class="keyword">if</span>(A[i]==elem){</span><br><span class="line">				<span class="keyword">continue</span>;</span><br><span class="line">			}</span><br><span class="line">			A[j]=A[i];</span><br><span class="line">			j++;</span><br><span class="line">		}</span><br><span class="line">		<span class="keyword">return</span> j;</span><br><span class="line">	}</span><br><span class="line">	</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure>
<p>举一个最简单的例子，譬如数组为1，2，2，3，2，4，我们需要删除2，首先初始化i和j为0，指向第一个位置，因为第一个元素为1，所以A[0] = A[0]，i和j都加1，而第二个元素为2，我们递增i，直到碰到3，此时A[1] = A[3]，也就是3，递增i和j，这时候下一个元素又是2，递增i，直到碰到4，此时A[2] = A[5]，也就是4，再次递增i和j，这时候数组已经遍历完毕，结束。这时候j的值为3，刚好就是新的数组的长度。</p>
]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>力扣题解</tag>
      </tags>
  </entry>
  <entry>
    <title>highcharts学习笔记</title>
    <url>/zh-CN/highcharts%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>建议使用cdn</p>
<p>安装jQuery</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 使用 Staticfile CDN 静态资源库的jQuery资源</span></span><br><span class="line">http://cdn.staticfile.org/jquery/2.1.4/jquery.min.js</span><br><span class="line"><span class="comment"># 使用百度静态资源库的jQuery资源</span></span><br><span class="line">http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js</span><br></pre></td></tr></tbody></table></figure>
<p>安装Highcharts</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 官方CDN地址</span></span><br><span class="line">http://code.highcharts.com/highcharts.js</span><br></pre></td></tr></tbody></table></figure>
<h3 id="配置语法"><a href="#配置语法" class="headerlink" title="配置语法"></a>配置语法</h3><p>可以将下面文件保存在<code>highcharts.html</code>文件中，用浏览器打开</p>
<figure class="highlight html"><table><tbody><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>Highcharts<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"http://code.highcharts.com/highcharts.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"container"</span> <span class="attr">style</span>=<span class="string">"width: 550px; height: 400px; margin: 0 auto"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">language</span>=<span class="string">"JavaScript"</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">$(<span class="variable language_">document</span>).<span class="title function_">ready</span>(<span class="keyword">function</span>(<span class="params"></span>) {</span></span><br><span class="line"><span class="language-javascript">   <span class="comment">// 为图表配置标题</span></span></span><br><span class="line"><span class="language-javascript">   <span class="keyword">var</span> title = {</span></span><br><span class="line"><span class="language-javascript">       <span class="attr">text</span>: <span class="string">'月平均气温'</span>   </span></span><br><span class="line"><span class="language-javascript">   };</span></span><br><span class="line"><span class="language-javascript">   <span class="comment">// 为图表配置副标题</span></span></span><br><span class="line"><span class="language-javascript">   <span class="keyword">var</span> subtitle = {</span></span><br><span class="line"><span class="language-javascript">        <span class="attr">text</span>: <span class="string">'Source: runoob.com'</span></span></span><br><span class="line"><span class="language-javascript">   };</span></span><br><span class="line"><span class="language-javascript">   <span class="comment">// 配置要在x轴显示的项</span></span></span><br><span class="line"><span class="language-javascript">   <span class="keyword">var</span> xAxis = {</span></span><br><span class="line"><span class="language-javascript">       <span class="attr">categories</span>: [<span class="string">'一月'</span>, <span class="string">'二月'</span>, <span class="string">'三月'</span>, <span class="string">'四月'</span>, <span class="string">'五月'</span>, <span class="string">'六月'</span></span></span><br><span class="line"><span class="language-javascript">              ,<span class="string">'七月'</span>, <span class="string">'八月'</span>, <span class="string">'九月'</span>, <span class="string">'十月'</span>, <span class="string">'十一月'</span>, <span class="string">'十二月'</span>]</span></span><br><span class="line"><span class="language-javascript">   };</span></span><br><span class="line"><span class="language-javascript">   <span class="comment">// 配置要在y轴显示的项</span></span></span><br><span class="line"><span class="language-javascript">   <span class="keyword">var</span> yAxis = {</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">title</span>: {</span></span><br><span class="line"><span class="language-javascript">         <span class="attr">text</span>: <span class="string">'Temperature (\xB0C)'</span></span></span><br><span class="line"><span class="language-javascript">      },</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">plotLines</span>: [{</span></span><br><span class="line"><span class="language-javascript">         <span class="attr">value</span>: <span class="number">0</span>,</span></span><br><span class="line"><span class="language-javascript">         <span class="attr">width</span>: <span class="number">1</span>,</span></span><br><span class="line"><span class="language-javascript">         <span class="attr">color</span>: <span class="string">'#808080'</span></span></span><br><span class="line"><span class="language-javascript">      }]</span></span><br><span class="line"><span class="language-javascript">   };   </span></span><br><span class="line"><span class="language-javascript">   <span class="comment">// 配置提示信息</span></span></span><br><span class="line"><span class="language-javascript">   <span class="keyword">var</span> tooltip = {</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">valueSuffix</span>: <span class="string">'\xB0C'</span></span></span><br><span class="line"><span class="language-javascript">   }</span></span><br><span class="line"><span class="language-javascript">   <span class="comment">// 配置图表向右对齐</span></span></span><br><span class="line"><span class="language-javascript">   <span class="keyword">var</span> legend = {</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">layout</span>: <span class="string">'vertical'</span>,</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">align</span>: <span class="string">'right'</span>,</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">verticalAlign</span>: <span class="string">'middle'</span>,</span></span><br><span class="line"><span class="language-javascript">      <span class="attr">borderWidth</span>: <span class="number">0</span></span></span><br><span class="line"><span class="language-javascript">   };</span></span><br><span class="line"><span class="language-javascript">   <span class="comment">// 配置图表要展示的数据。每个系列是个数组，每一项在图片中都会生成一条曲线</span></span></span><br><span class="line"><span class="language-javascript">   <span class="keyword">var</span> series =  [</span></span><br><span class="line"><span class="language-javascript">      {</span></span><br><span class="line"><span class="language-javascript">         <span class="attr">name</span>: <span class="string">'Tokyo'</span>,</span></span><br><span class="line"><span class="language-javascript">         <span class="attr">data</span>: [<span class="number">7.0</span>, <span class="number">6.9</span>, <span class="number">9.5</span>, <span class="number">14.5</span>, <span class="number">18.2</span>, <span class="number">21.5</span>, <span class="number">25.2</span>,</span></span><br><span class="line"><span class="language-javascript">            <span class="number">26.5</span>, <span class="number">23.3</span>, <span class="number">18.3</span>, <span class="number">13.9</span>, <span class="number">9.6</span>]</span></span><br><span class="line"><span class="language-javascript">      }, </span></span><br><span class="line"><span class="language-javascript">      {</span></span><br><span class="line"><span class="language-javascript">         <span class="attr">name</span>: <span class="string">'New York'</span>,</span></span><br><span class="line"><span class="language-javascript">         <span class="attr">data</span>: [-<span class="number">0.2</span>, <span class="number">0.8</span>, <span class="number">5.7</span>, <span class="number">11.3</span>, <span class="number">17.0</span>, <span class="number">22.0</span>, <span class="number">24.8</span>,</span></span><br><span class="line"><span class="language-javascript">            <span class="number">24.1</span>, <span class="number">20.1</span>, <span class="number">14.1</span>, <span class="number">8.6</span>, <span class="number">2.5</span>]</span></span><br><span class="line"><span class="language-javascript">      }, </span></span><br><span class="line"><span class="language-javascript">      {</span></span><br><span class="line"><span class="language-javascript">         <span class="attr">name</span>: <span class="string">'Berlin'</span>,</span></span><br><span class="line"><span class="language-javascript">         <span class="attr">data</span>: [-<span class="number">0.9</span>, <span class="number">0.6</span>, <span class="number">3.5</span>, <span class="number">8.4</span>, <span class="number">13.5</span>, <span class="number">17.0</span>, <span class="number">18.6</span>,</span></span><br><span class="line"><span class="language-javascript">            <span class="number">17.9</span>, <span class="number">14.3</span>, <span class="number">9.0</span>, <span class="number">3.9</span>, <span class="number">1.0</span>]</span></span><br><span class="line"><span class="language-javascript">      }, </span></span><br><span class="line"><span class="language-javascript">      {</span></span><br><span class="line"><span class="language-javascript">         <span class="attr">name</span>: <span class="string">'London'</span>,</span></span><br><span class="line"><span class="language-javascript">         <span class="attr">data</span>: [<span class="number">3.9</span>, <span class="number">4.2</span>, <span class="number">5.7</span>, <span class="number">8.5</span>, <span class="number">11.9</span>, <span class="number">15.2</span>, <span class="number">17.0</span>, </span></span><br><span class="line"><span class="language-javascript">            <span class="number">16.6</span>, <span class="number">14.2</span>, <span class="number">10.3</span>, <span class="number">6.6</span>, <span class="number">4.8</span>]</span></span><br><span class="line"><span class="language-javascript">      }</span></span><br><span class="line"><span class="language-javascript">   ];</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">   <span class="keyword">var</span> json = {};</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">   json.<span class="property">title</span> = title;</span></span><br><span class="line"><span class="language-javascript">   json.<span class="property">subtitle</span> = subtitle;</span></span><br><span class="line"><span class="language-javascript">   json.<span class="property">xAxis</span> = xAxis;</span></span><br><span class="line"><span class="language-javascript">   json.<span class="property">yAxis</span> = yAxis;</span></span><br><span class="line"><span class="language-javascript">   json.<span class="property">tooltip</span> = tooltip;</span></span><br><span class="line"><span class="language-javascript">   json.<span class="property">legend</span> = legend;</span></span><br><span class="line"><span class="language-javascript">   json.<span class="property">series</span> = series;</span></span><br><span class="line"><span class="language-javascript">   &lt;!-- highcharts库使用json格式配置--&gt;</span></span><br><span class="line"><span class="language-javascript">   $(<span class="string">'#container'</span>).<span class="title function_">highcharts</span>(json);</span></span><br><span class="line"><span class="language-javascript">});</span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<p><style>.wnlnysyijjjw{zoom:50%;}</style><img src="/zh-CN/highcharts%E6%95%99%E7%A8%8B/highcharts%E6%95%99%E7%A8%8B/image-20221016200021330.png" class="lazyload" data-srcset="/zh-CN/highcharts%E6%95%99%E7%A8%8B/highcharts%E6%95%99%E7%A8%8B/image-20221016200021330.png" srcset="data:image/png;base64,666" class="wnlnysyijjjw lazyload" alt="image-20221016200021330"></p>
]]></content>
  </entry>
  <entry>
    <title>nn.relu和F.relu的区别</title>
    <url>/zh-CN/nn_relu%E5%92%8CF_relu%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/u011501388/article/details/86602275">PyTorch之nn.ReLU与F.ReLU的区别</a></p>
<p>示例代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet_1</span>(nn.Module):</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=n</span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">         )</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.features(x)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet_2</span>(nn.Module):</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=n</span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">         )</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = F.ReLU(x)</span><br></pre></td></tr></tbody></table></figure>
<p>在如上网络中，AlexNet_1与AlexNet_2实现的结果是一致的，但是可以看到将ReLU层添加到网络有两种不同的实现，即nn.ReLU和F.ReLU两种实现方法。<br>其中nn.ReLU作为一个层结构，必须添加到nn.Module容器中才能使用，而F.ReLU则作为一个函数调用，看上去作为一个函数调用更方便更简洁。具体使用哪种方式，取决于编程风格。在PyTorch中,nn.X都有对应的函数版本F.X，但是并不是所有的F.X均可以用于forward或其它代码段中，因为当网络模型训练完毕时，在存储model时，在forward中的F.X函数中的参数是无法保存的。也就是说，在forward中，使用的F.X函数一般均没有状态参数，比如F.ReLU，F.avg_pool2d等，均没有参数，它们可以用在任何代码片段中。</p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>函数库</tag>
        <tag>relu</tag>
      </tags>
  </entry>
  <entry>
    <title>np.cumsum()函数</title>
    <url>/zh-CN/np_cumsum/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://www.cnblogs.com/mrlayfolk/p/12251704.html">numpy cumsum()函数简介 - zhengcixi - 博客园 (cnblogs.com)</a></p>
<p>2.<a href="https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.cumsum.html">numpy.cumsum — NumPy v1.10 Manual (scipy.org)</a></p>
<p>函数原型：<code>numpy.``cumsum</code>(<em>a</em>, <em>axis=None</em>, <em>dtype=None</em>, <em>out=None</em>)</p>
<p>可参考链接：<a href="https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.cumsum.html查看各个参数的含义。[">https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.cumsum.html查看各个参数的含义。[</a><br>](<a href="http://github.com/numpy/numpy/blob/v1.10.1/numpy/core/fromnumeric.py#L2038-L2106">http://github.com/numpy/numpy/blob/v1.10.1/numpy/core/fromnumeric.py#L2038-L2106</a>)</p>
<p>函数作用：求数组的所有元素的累计和，可通过参数axis指定求某个轴向的统计值。这里所说的轴可按照下图的含义理解：</p>
<p><style>.dyrjfroglqvx{}</style><img src="/zh-CN/np_cumsum/np_cumsum/1078885-20220224122651707-981078391.png" class="lazyload" data-srcset="/zh-CN/np_cumsum/np_cumsum/1078885-20220224122651707-981078391.png" srcset="data:image/png;base64,666" class="dyrjfroglqvx lazyload" alt="img"></p>
<p> 下面举例进行说明：</p>
<p>（1）不指定axis参数</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">1 &gt;&gt;&gt; a = np.array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">2 &gt;&gt;&gt; a</span><br><span class="line">3 array([[1, 2, 3],</span><br><span class="line">4        [4, 5, 6]])</span><br><span class="line">5 &gt;&gt;&gt; a.cumsum()</span><br><span class="line">6 array([ 1,  3,  6, 10, 15, 21], dtype=int32)</span><br></pre></td></tr></tbody></table></figure>
<p>可以看出，不指定axis参数时，把二维数组当作了一维数组处理，进行累计求和运算。</p>
<p>（2）指定参数axis=0</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">1 &gt;&gt;&gt; arr</span><br><span class="line">2 array([[0, 1, 2],</span><br><span class="line">3        [3, 4, 5],</span><br><span class="line">4        [6, 7, 8]])</span><br><span class="line">5 &gt;&gt;&gt; np.cumsum(arr, axis=0) </span><br><span class="line">6 array([[ 0,  1,  2],</span><br><span class="line">7        [ 3,  5,  7],</span><br><span class="line">8        [ 9, 12, 15]], dtype=int32)</span><br><span class="line">9 &gt;&gt;&gt;</span><br></pre></td></tr></tbody></table></figure>
<p>np.cumsum(arr, axis=0)和arr.cumsum(axis=0)是一样的。可以看出，上述代码是按照轴0进行累计求和的。</p>
<p>（3）指定参数axis=1</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">1 &gt;&gt;&gt; arr</span><br><span class="line">2 array([[0, 1, 2],</span><br><span class="line">3        [3, 4, 5],</span><br><span class="line">4        [6, 7, 8]])</span><br><span class="line">5 &gt;&gt;&gt; arr.cumsum(axis=1)</span><br><span class="line">6 array([[ 0,  1,  3],</span><br><span class="line">7        [ 3,  7, 12],</span><br><span class="line">8        [ 6, 13, 21]], dtype=int32)</span><br></pre></td></tr></tbody></table></figure>
<p>可以看出，上述代码是按照轴1进行累计求和的。</p>
<p>关于更高维的数组的运算就不测试了，暂时也用不上。 </p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>cumsum</tag>
      </tags>
  </entry>
  <entry>
    <title>np.newaxis详解</title>
    <url>/zh-CN/np_newaxis/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>原文链接：<a href="https://blog.csdn.net/THMAIL/article/details/121762644">np.newaxis作用详解—-超简单理解方式，通透</a></p>
<p>np.newaxis的作用就是在这一位置增加一个一维，这一位置指的是np.newaxis所在的位置，举个例子如下。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x1 = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="comment"># the shape of x1 is (5,)</span></span><br><span class="line">x1_new = x1[:, np.newaxis]</span><br><span class="line"><span class="comment"># now, the shape of x1_new is (5, 1)</span></span><br><span class="line"><span class="comment"># array([[1],</span></span><br><span class="line"><span class="comment">#        [2],</span></span><br><span class="line"><span class="comment">#        [3],</span></span><br><span class="line"><span class="comment">#        [4],</span></span><br><span class="line"><span class="comment">#        [5]])</span></span><br><span class="line">x1_new = x1[np.newaxis,:]</span><br><span class="line"><span class="comment"># now, the shape of x1_new is (1, 5)</span></span><br><span class="line"><span class="comment"># array([[1, 2, 3, 4, 5]])</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>函数库</tag>
        <tag>newaxis</tag>
      </tags>
  </entry>
  <entry>
    <title>pyperclip模块</title>
    <url>/zh-CN/pyperclip/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>相关链接：<a href="https://blog.csdn.net/qq_36830101/article/details/116205724">python之Pyperclip</a></p>
<p>pyperclip模块有copy()和paste()两个模块，你需要安装该模块</p>
<p>使用代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pyperclip</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pyperclip.copy(<span class="string">'Hello world!'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pyperclip.paste()</span><br><span class="line"><span class="string">'Hello world!'</span></span><br></pre></td></tr></tbody></table></figure>
<p>当然，如果你的程序之外的某个程序改变了剪贴板的内容，paste()函数就会返回它。</p>
<p>总结：这个模块在写程序时可以用来读取外面剪贴板的内容，或者读取你需要读取已经写入的某个内容，方便快捷</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>函数库</tag>
      </tags>
  </entry>
  <entry>
    <title>pip换源</title>
    <url>/zh-CN/pip%E6%8D%A2%E6%BA%90/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>由于使用pip或pip3安装python第三方包时，经常出现read timed out问题，所以需要将pip的官方软件源服务器换成国内的镜像服务器，从而提升python软件包安装效率和成功率，pip 国内的一些镜像：</p>
<ul>
<li>阿里云 <a href="http://mirrors.aliyun.com/pypi/simple/">http://mirrors.aliyun.com/pypi/simple/</a></li>
<li>中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/">https://pypi.mirrors.ustc.edu.cn/simple/</a></li>
<li>豆瓣(douban) <a href="http://pypi.douban.com/simple/">http://pypi.douban.com/simple/</a></li>
<li>清华大学 <a href="https://pypi.tuna.tsinghua.edu.cn/simple/">https://pypi.tuna.tsinghua.edu.cn/simple/</a></li>
<li>中国科学技术大学 <a href="http://pypi.mirrors.ustc.edu.cn/simple/">http://pypi.mirrors.ustc.edu.cn/simple/</a></li>
</ul>
<h2 id="更换源"><a href="#更换源" class="headerlink" title="更换源"></a>更换源</h2><h3 id="临时使用"><a href="#临时使用" class="headerlink" title="临时使用"></a>临时使用</h3><p>可以在使用 pip 的时候在后面加上-i 参数，指定 pip 源</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">eg: pip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></tbody></table></figure>
<h3 id="永久修改"><a href="#永久修改" class="headerlink" title="永久修改"></a>永久修改</h3><h4 id="linux"><a href="#linux" class="headerlink" title="linux"></a>linux</h4><p>修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件，文件夹要加“.”，表示是隐藏文件夹)， 内容如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">[<span class="keyword">global</span>]</span><br><span class="line">index-url = https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host = https://pypi.tuna.tsinghua.edu.cn</span><br></pre></td></tr></tbody></table></figure>
<h4 id="windows"><a href="#windows" class="headerlink" title="windows"></a>windows</h4><p>1.pip永久换源</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">pip config <span class="built_in">set</span> <span class="keyword">global</span>.index-url https://mirrors.aliyun.com/pypi/simple/</span><br></pre></td></tr></tbody></table></figure>
<p>在cmd命令行中输入上述命令即可。</p>
<p>最后，升级 pip 到最新的版本</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">pip install pip -U</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">python -m pip install --user --upgrade pip</span><br></pre></td></tr></tbody></table></figure>
<p>2.直接在 user 目录中创建一个 pip 目录，如：C:\Users\xx\pip，在 pip 目录下新建文件 pip.ini，即 %HOMEPATH%\pip\pip.ini，内容如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host = pypi.tuna.tsinghua.edu.cn</span><br></pre></td></tr></tbody></table></figure>
<p>可以在开始运行里面输入三个点 <code>...</code>，敲回车即可打开用户目录。</p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pip换源</tag>
      </tags>
  </entry>
  <entry>
    <title>pth权重文件转换为onnx权重文件</title>
    <url>/zh-CN/pth_to_onnx/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/weixin_50980847/article/details/126409707">pth文件转为onnx格式<em>业精于勤。荒于嬉。的博客-CSDN博客</em>.pth转onnx</a></p>
<p>2.<a href="https://blog.csdn.net/weixin_45709671/article/details/118545500">Pytorch——初探onnx（1）解决upsample<em>bilinear2d转换问题</em>零壹博弈的博客-CSDN博客</a></p>
<p>3.<a href="https://blog.csdn.net/magic_show_time/article/details/122476306">https://blog.csdn.net/magic_show_time/article/details/122476306</a></p>
<p>由于想利用zetane这个软件可视化目标检测模型里面的特征变化过程，zetane只支持<code>onnx</code>、<code>h5</code>、<code>ZTN</code>文件,而我的目标检测模型得到的权重文件是<code>pth</code>文件,因此需要用到权重转换</p>
<p>经过一系列的摸索，得出以下代码（亲测可用）：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.onnx</span><br><span class="line"><span class="keyword">import</span> onnxruntime <span class="keyword">as</span> ort</span><br><span class="line"><span class="keyword">from</span> nets <span class="keyword">import</span> ssd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建.pth模型</span></span><br><span class="line"></span><br><span class="line">model = ssd.SSD300(<span class="number">21</span>,<span class="string">'vgg'</span>)</span><br><span class="line"><span class="comment"># 加载权重</span></span><br><span class="line">model_path = <span class="string">'./model_data/ep250-loss1.474-val_loss2.826.pth'</span></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">model_statedict = torch.load(model_path, map_location=device)</span><br><span class="line">model.load_state_dict(model_statedict)</span><br><span class="line"></span><br><span class="line">model.to(device)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">input_data = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">300</span>, <span class="number">300</span>, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转化为onnx模型</span></span><br><span class="line">input_names = [<span class="string">'input'</span>]</span><br><span class="line">output_names = [<span class="string">'output'</span>]</span><br><span class="line"></span><br><span class="line">torch.onnx.export(model, input_data, <span class="string">'ssdv7.onnx'</span>, opset_version=<span class="number">11</span>, verbose=<span class="literal">True</span>, input_names=input_names,</span><br><span class="line">                  output_names=output_names)</span><br></pre></td></tr></tbody></table></figure>
<p>遇到的问题：</p>
<p>如果你是按照我给的两篇文章写代码，你会发现很简单</p>
<p>1.模型无法加载，<code>load_state_dict</code>出现问题</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 记住，模型参数一定要写全，根据自己的网络模型写即可</span></span><br><span class="line"><span class="comment"># 错误代码</span></span><br><span class="line">model = ssd.SSD300()</span><br><span class="line"><span class="comment"># 正确代码</span></span><br><span class="line">model = ssd.SSD300(21, <span class="string">'vgg'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>2.权重文件因为模型中上采样导出失败问题</p>
<p><style>.snkiwxxlhwov{zoom:80%;}</style><img src="/zh-CN/pth_to_onnx/pth_to_onnx/image-20221031111722457.png" class="lazyload" data-srcset="/zh-CN/pth_to_onnx/pth_to_onnx/image-20221031111722457.png" srcset="data:image/png;base64,666" class="snkiwxxlhwov lazyload" alt="image-20221031111722457"></p>
<p>按方法2解决即可，pytorch版本问题</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 错误代码</span></span><br><span class="line">torch.onnx.export(model, input_data, <span class="string">'ssdv7.onnx'</span>, opset_version=9, verbose=True, input_names=input_names, output_names=output_names)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正确代码</span></span><br><span class="line">torch.onnx.export(model, input_data, <span class="string">'ssdv7.onnx'</span>, opset_version=11, verbose=True, input_names=input_names,</span><br><span class="line">                  output_names=output_names)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>ok,接下来就是zetane加载模型了，看看最终的模型吧。</p>
<p><style>.qaevqunoowfy{zoom:80%;}</style><img src="/zh-CN/pth_to_onnx/pth_to_onnx/image-20221031112309197.png" class="lazyload" data-srcset="/zh-CN/pth_to_onnx/pth_to_onnx/image-20221031112309197.png" srcset="data:image/png;base64,666" class="qaevqunoowfy lazyload" alt="image-20221031112309197"></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>文件转换</tag>
      </tags>
  </entry>
  <entry>
    <title>python知识补充</title>
    <url>/zh-CN/python%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>1.为源文件指定不同的字符编码</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"><span class="section"># 语法，python源文件默认使用UTF-8编码,特殊的编码注释必须在文件的第一行或者第二行定义</span></span><br><span class="line"><span class="section"># -<span class="emphasis">*- coding: encoding -*</span>-</span></span><br><span class="line">eg:</span><br><span class="line"><span class="section"># 使用windows-1252编码</span></span><br><span class="line"><span class="section"># -<span class="emphasis">*- coding: cp-1252 -*</span>-</span></span><br></pre></td></tr></tbody></table></figure>
<p>2.交互模式中，最近一个表达式的值赋给变量<strong>_</strong>。我们可以把它当做一个桌面计算器，方便的用于连续计算。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tax = <span class="number">12.5</span> /<span class="number">100</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>price = <span class="number">100.5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>price * tax</span><br><span class="line"><span class="number">12.5625</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>price + _</span><br><span class="line"><span class="number">113.0625</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">round</span>(_, <span class="number">2</span>)</span><br><span class="line"><span class="number">113.06</span></span><br></pre></td></tr></tbody></table></figure>
<p>3.在交互式解释器中，输出的字符串会用引号引起来，特殊字符会用反斜杠转义，如果前面带有\的字符被当做特殊字符，可以使用原始字符串，方法是在第一个引号前面加上一个r</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="string">'c:\user\test'</span>)</span><br><span class="line">c:\user</span><br><span class="line">test</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="string">r'c:\user\test'</span>)</span><br><span class="line">c:\user\test</span><br></pre></td></tr></tbody></table></figure>
<p>4.python能够优雅的处理那些没有意义的切片：一个过大的索引值（即下标值大于字符串实际长度）将被字符串实际长度所代替，当上边界比下边界大时（即切片左值大于右值）就返回空字符串</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># word = 'superman'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>word[<span class="number">4</span>:<span class="number">42</span>]</span><br><span class="line"><span class="string">'rman'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>word[<span class="number">42</span>:]</span><br><span class="line"><span class="string">''</span></span><br></pre></td></tr></tbody></table></figure>
<p>5.序列的切片，一定要左边的数字小于右边的数字，否则返回空</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>lang</span><br><span class="line"><span class="string">'python'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lst</span><br><span class="line">[<span class="string">'python'</span>, <span class="string">'java'</span>, <span class="string">'c++'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lang[-<span class="number">1</span>:-<span class="number">3</span>]</span><br><span class="line"><span class="string">''</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lang[-<span class="number">3</span>:-<span class="number">1</span>]</span><br><span class="line"><span class="string">'ho'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lst[-<span class="number">3</span>:-<span class="number">1</span>]</span><br><span class="line">[<span class="string">'python'</span>, <span class="string">'java'</span>]</span><br></pre></td></tr></tbody></table></figure>
<p>6.反转</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>alst = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>alst[::-<span class="number">1</span>]</span><br><span class="line">[<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(<span class="built_in">reversed</span>(alst))</span><br><span class="line">[<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure>
<p>7.append()和extend()是原地修改，没有返回值</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>one = [<span class="string">'good'</span>, <span class="string">'good'</span>, <span class="string">'study'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>another = one.extend([<span class="string">'day'</span>, <span class="string">'day'</span>, <span class="string">'up'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>another <span class="comment">#没有返回值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>one</span><br><span class="line">[<span class="string">'good'</span>, <span class="string">'good'</span>, <span class="string">'study'</span>, <span class="string">'day'</span>, <span class="string">'day'</span>, <span class="string">'up'</span>]</span><br></pre></td></tr></tbody></table></figure>
<p>8.extend()和append()区别</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>lst = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lst.append([<span class="string">'quit'</span>, <span class="string">'final'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lst</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, [<span class="string">'quit'</span>, <span class="string">'final'</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lst.extend([<span class="string">'test'</span>, <span class="string">'different'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lst</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, [<span class="string">'quit'</span>, <span class="string">'final'</span>], <span class="string">'test'</span>, <span class="string">'different'</span>]</span><br></pre></td></tr></tbody></table></figure>
<p>9.如果元组只有一个元素时，应该在该元素后面加一个半角的英文逗号</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = (<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">type</span>(a)</span><br><span class="line">&lt;<span class="built_in">type</span> <span class="string">'int'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = (<span class="number">3</span>, )</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">type</span>(b)</span><br><span class="line">&lt;<span class="built_in">type</span> <span class="string">'tuple'</span>&gt;</span><br></pre></td></tr></tbody></table></figure>
<p>10.</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch官方文档中文版</title>
    <url>/zh-CN/pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h1><h2 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h2><h3 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h3><p>CLASS    <code>torch.nn.Module</code>    <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">SOURCE</a></p>
<p>它是所有神经网络模型的基类，你的模块应该继承于该类。</p>
<p>模块还能包含其他模块，允许把它们嵌套在一个树结构中。你可以分配子模块作为常规属性：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))</span><br></pre></td></tr></tbody></table></figure>
<p>用这种方式分配的模块将会显示，并且当你调用<code>to( )</code>等等方法，它们的参数也将被转换。</p>
<p>注：</p>
<p>正如上面的例子一样，一个<code>__init__()</code>调用父类必须在子类赋值之前完成。</p>
<hr>
<p>变量</p>
<p><code>training(bool)</code>-布尔值代表这个模块是训练模式还是评估模式</p>
<p><code>add_module(name,    module)</code>    <a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.add_module">SOURCE</a></p>
<p>​                添加一个子模块到当前模块</p>
<p>​                这个模块可以用给定的名称作为属性访问模块</p>
<p>​                参数：</p>
<p>​                        ·name(string)-子模块的名字，这个子模块可以用给定的名称作为属性访问。</p>
<p>​                        ·module(Module)-子模块添加到模块上</p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>typora数学公式教程</title>
    <url>/zh-CN/typora%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://www.cnblogs.com/Xuxiaokang/p/15654336.html">Typora数学公式输入指导手册 </a></p>
<p>上面这个链接教程非常详细，有时间我会一个一个测试，有啥补充的可以私信我</p>
]]></content>
      <categories>
        <category>typora</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu软件安装的问题总结</title>
    <url>/zh-CN/ubuntu%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考链接：</p>
<p>1.<a href="https://blog.csdn.net/zeye5731/article/details/124478766">ubuntu安装软件时，status-code=409报错解决方案</a></p>
<p>2.<a href="https://blog.csdn.net/qq_37006829/article/details/112428179?spm=1001.2101.3001.6661.1&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-112428179-blog-124478766.pc_relevant_multi_platform_whitelistv4&amp;utm_relevant_index=1"> 解决ubuntu软件商店无法安装软件提示snap问题</a></p>
<p>今天一大早上想去虚拟机上（Linux）看看昨天晚上没安装的软件，给重新安装一下，打开软件商店，结果遇到两个问题，昨天晚上我是用软件更新器安装了一会就退出关闭了</p>
<p>issue1:</p>
<p>安装火狐浏览器时，显示<code>无法安装更新，status-code=409</code></p>
<p>解决方案：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">(base) z@E580:~$ snap changes</span><br><span class="line">ID   Status  Spawn                     Ready                     Summary</span><br><span class="line">6    Done    6 days ago, at 16:03 CST  6 days ago, at 16:04 CST  自动刷新 snap "snapd"</span><br><span class="line">7    Doing   today at 16:40 CST        -                         Install "xmind" snap from "latest/stable" channel</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>找到正在安装的软件，将其强行关闭</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">(base) z@E580:~$ snap abort 7</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>重新安装</p>
<p>issue2:</p>
<p><style>.pxkljpcumuwj{}</style><img src="/zh-CN/ubuntu%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E9%97%AE%E9%A2%98/ubuntu%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E9%97%AE%E9%A2%98/c89ffdc107df9a4e9185e7d51d5a8a21.png" class="lazyload" data-srcset="/zh-CN/ubuntu%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E9%97%AE%E9%A2%98/ubuntu%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E9%97%AE%E9%A2%98/c89ffdc107df9a4e9185e7d51d5a8a21.png" srcset="data:image/png;base64,666" class="pxkljpcumuwj lazyload"></p>
<p>原因是我们之前安装软件的时候没有安装完成就推出软件商店了，他已经安装了，只是没有安装完成而以，</p>
<p>解决办法：</p>
<p>1.查看安装详情：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">snap changes</span><br></pre></td></tr></tbody></table></figure>
<p><style>.otflybalgaye{}</style><img src="/zh-CN/ubuntu%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E9%97%AE%E9%A2%98/ubuntu%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E9%97%AE%E9%A2%98/d9085116a96c5c63859c5259fa0230bb.png" class="lazyload" data-srcset="/zh-CN/ubuntu%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E9%97%AE%E9%A2%98/ubuntu%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E9%97%AE%E9%A2%98/d9085116a96c5c63859c5259fa0230bb.png" srcset="data:image/png;base64,666" class="otflybalgaye lazyload"></p>
<p>ID7的进程是我之前安装失败的</p>
<p>2.清除当前安装，然后再重新安装，命令如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">snap abort 7</span><br></pre></td></tr></tbody></table></figure>
<p>3.重新去软件商店安装</p>
<p>其实，还有另外一种办法，就是重新去软件更新器安装，继续完成更新，没有必要去软件商店更新，希望对你有所启发</p>
]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>软件安装</tag>
      </tags>
  </entry>
  <entry>
    <title>matplotlib.use(&#39;agg&#39;)的作用机理</title>
    <url>/zh-CN/use_agg/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>博文链接：</p>
<p><a href="https://blog.csdn.net/yyywxk/article/details/117294208">matplotlib.use(‘agg‘)“语句的作用机理</a></p>
<p><a href="https://www.cnblogs.com/suntp/p/6519386.html">matplotlib中什么是后端 </a></p>
<p>问题：在检查SSD代码时，在<code>callback.py</code>文件中发现出现了<code>matplotlib.use('agg')</code>的语句。<code>PyCharm</code>中不显示绘图。</p>
<p>相关示例：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">'agg'</span>) </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># matplotlib.use('agg')必须在本句执行前运行</span></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">2</span>*np.pi, <span class="number">0.001</span>) </span><br><span class="line">y = np.sin(<span class="number">2</span> * np.pi * x) </span><br><span class="line">plt.clf() </span><br><span class="line">plt.plot(x,y) </span><br><span class="line">l = plt.axhline(linewidth=<span class="number">1</span>, color=<span class="string">'black'</span>) </span><br><span class="line">l = plt.axvline(linewidth=<span class="number">1</span>, color=<span class="string">'black'</span>) </span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>原理分析：</p>
<p>实际上，这样的理解是表面的，这个语句确实会使得在<code>Pycharm</code>运行时无法显示图，但是必须注意，这是其原理导致的，而这个语句并不是设置<code>Pycharm</code>不显示图的语句，其实前述的代码中，删掉 <code>plt.show()</code>，也不会显示图片。</p>
<p><code>matplotlib</code>的<code>use()</code>命令其实是用来配置<code>matplotlib</code>的<code>backend</code>（后端）的命令。所谓后端，就是一个渲染器，用于将前端代码渲染成我们想要的图像。后端详细的解释可参考博客：<a href="https://www.cnblogs.com/suntp/p/6519386.html">matplotlib中什么是后端 </a></p>
<p>对于用户接口，典型的渲染器是<code>Agg</code>，它是使用<code>Anti-Grain Geometry C++</code>库来产生光栅(像素)图。</p>
<p>那么为什么这样设置<code>Pycharm</code>会导致其不显示图片呢？</p>
<p>可以查看一下目前的后端设置。方法是执行下面代码。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>matplotlib.get_backend()</span><br><span class="line">Qt5Agg</span><br></pre></td></tr></tbody></table></figure>
<p>也就是说，Pycharm运行的时候，默认的后端是<code>Qt5Agg</code>。</p>
<p>实际上，<code>Agg</code> 渲染器是非交互式的后端，没有GUI界面，所以不显示图片，它是用来生成图像文件。<code>Qt5Agg</code> 是意思是Agg渲染器输出到Qt5绘图面板，它是交互式的后端，拥有在屏幕上展示的能力</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title>wandb</title>
    <url>/zh-CN/wandb/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>未完待续，后面补充</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title>vue3学习笔记</title>
    <url>/zh-CN/vue3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script>]]></content>
      <categories>
        <category>vue</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title>visdom使用教程</title>
    <url>/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/fengdu78/article/details/106596308?spm=1001.2101.3001.6650.1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-106596308-blog-122678583.pc_relevant_multi_platform_whitelistv3&amp;utm_relevant_index=2">PyTorch深度学习训练可视化工具visdom</a></p>
<p>在进行深度学习实验时，能够可视化地对训练过程和结果进行展示是非常有必要的。除了Torch版本的TensorBoard工具TensorBoardX之外，Torch官方也提供了一款非常好用的可视化神器——visdom。visdom是一款用于创建、组织和共享实时大量训练数据可视化的灵活工具。</p>
<p><style>.cgtavictvrih{}</style><img src="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy80bE4xWE9ac2hmZEpydjhOVllHNzZBRlM3NDJpYnhXMEo4dlk3NnVXU05TSzZHSVBqazRpYTVvS2VUY05WaWNHQWVzSm5Qc1dWNkR6MWczZ3dnQlRBYUpRdy82NDA" class="lazyload" data-srcset="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy80bE4xWE9ac2hmZEpydjhOVllHNzZBRlM3NDJpYnhXMEo4dlk3NnVXU05TSzZHSVBqazRpYTVvS2VUY05WaWNHQWVzSm5Qc1dWNkR6MWczZ3dnQlRBYUpRdy82NDA" srcset="data:image/png;base64,666" class="cgtavictvrih lazyload"></p>
<p>深度学习模型训练通常放在远程的服务器上，服务器上训练的一个问题就在于不能方便地对训练进行可视化，相较于TensorFlow的可视化工具TensorBoard，visdom则是对应于PyTorch的可视化工具。</p>
<h3 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h3><p> 直接通过pip install visdom即可完成安装，之后在终端输入如下命令即可启动visdom服务：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">python -m visdom.server </span><br></pre></td></tr></tbody></table></figure>
<p>启动服务后输入本地或者远程地址，端口号8097，即可打开visdom主页。</p>
<h3 id="主要元素"><a href="#主要元素" class="headerlink" title="主要元素"></a>主要元素</h3><p>visdom界面简单，主要构成元素包括窗口(Windows)、环境(Environments)、状态(State)、过滤(Filter)、视图(Views)等。</p>
<p>   环境：用于对可视化空间进行分区，比如在对训练进行可视化的时候我们可以在一个环境里对loss进行可视化，在另一个环境下对训练的输入输出进行可视化。</p>
<p><style>.oijmcrkimrem{}</style><img src="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy80bE4xWE9ac2hmZEpydjhOVllHNzZBRlM3NDJpYnhXMEpWdHFhNXhUZWxpYWF1N2JKN3VYVjBDS2p6OVNuQ3pHQ3puRExocVlSZFBHRU1EbUFhZjZqYWh3LzY0MA" class="lazyload" data-srcset="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy80bE4xWE9ac2hmZEpydjhOVllHNzZBRlM3NDJpYnhXMEpWdHFhNXhUZWxpYWF1N2JKN3VYVjBDS2p6OVNuQ3pHQ3puRExocVlSZFBHRU1EbUFhZjZqYWh3LzY0MA" srcset="data:image/png;base64,666" class="oijmcrkimrem lazyload"></p>
<p>状态：visdom会自动缓存你创建的可视化内容，当页面关闭之后，重新加载便可恢复这些内容。</p>
<p>过滤：可用于筛选可视化窗口，快速查找。</p>
<p><style>.ajtlxydempap{}</style><img src="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy80bE4xWE9ac2hmZEpydjhOVllHNzZBRlM3NDJpYnhXMEpkQ3dJcHZEZWo3TENQdGliV0cwOTJ1WEt2U0IxWXZFSmRNNHBOS1hDbDhqUHp2ZXhtTXVYZlJnLzY0MA" class="lazyload" data-srcset="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy80bE4xWE9ac2hmZEpydjhOVllHNzZBRlM3NDJpYnhXMEpkQ3dJcHZEZWo3TENQdGliV0cwOTJ1WEt2U0IxWXZFSmRNNHBOS1hDbDhqUHp2ZXhtTXVYZlJnLzY0MA" srcset="data:image/png;base64,666" class="ajtlxydempap lazyload"></p>
<p>视图：可以快速地对可视化窗口进行排列和管理。</p>
<h3 id="应用示例"><a href="#应用示例" class="headerlink" title="应用示例"></a>应用示例</h3><p>   visdom将可以进行可视化的对象都放在基础模块中，包括单/多张图像、文本、语音、视频、svg矢量图、属性网格、matplotlib绘图对象、序列化状态对象等。基础图形由plotly提供，主要包括散点图、折线图、热图、茎叶图、柱形图、箱线图、表面图、等高线图、网格图等。</p>
<p>   以matplotlib绘图对象为例进行展示。</p>
<p><style>.zmdamjugfism{}</style><img src="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy80bE4xWE9ac2hmZEpydjhOVllHNzZBRlM3NDJpYnhXMEpId3lweWtTSHdjTVRMaWNGdkJyWEplMGNqWGg0UjBhSWs0UUgxd1FHWlVLV0lIOGtadXNVZDRBLzY0MA" class="lazyload" data-srcset="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy80bE4xWE9ac2hmZEpydjhOVllHNzZBRlM3NDJpYnhXMEpId3lweWtTSHdjTVRMaWNGdkJyWEplMGNqWGg0UjBhSWs0UUgxd1FHWlVLV0lIOGtadXNVZDRBLzY0MA" srcset="data:image/png;base64,666" class="zmdamjugfism lazyload"></p>
<p> 具体到深度学习训练时，我们可以在<a href="https://so.csdn.net/so/search?q=torch&amp;spm=1001.2101.3001.7020">torch</a>训练代码下插入visdom的可视化模块：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> args.steps_plot &gt; <span class="number">0</span> <span class="keyword">and</span> step % args.steps_plot == <span class="number">0</span>:</span><br><span class="line">    image = inputs[<span class="number">0</span>].cpu().data</span><br><span class="line">    vis.image(image,<span class="string">f'input (epoch: <span class="subst">{epoch}</span>, step: <span class="subst">{step}</span>)'</span>)</span><br><span class="line">    vis.image(outputs[<span class="number">0</span>].cpu().<span class="built_in">max</span>(<span class="number">0</span>)[<span class="number">1</span>].data, <span class="string">f'output (epoch: <span class="subst">{epoch}</span>, step: <span class="subst">{step}</span>)'</span>)</span><br><span class="line">    vis.image(targets[<span class="number">0</span>].cpu().data, <span class="string">f'target (epoch: <span class="subst">{epoch}</span>, step: <span class="subst">{step}</span>)'</span>)</span><br><span class="line">    vis.image(loss, <span class="string">f'loss (epoch: <span class="subst">{epoch}</span>, step: <span class="subst">{step}</span>)'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>  将上述模块插入到VOC 2012语义分割训练中，效果如下：</p>
<p><style>.ujefwqvhlxmc{}</style><img src="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy80bE4xWE9ac2hmZEpydjhOVllHNzZBRlM3NDJpYnhXMEpTM29aSDBKQjh1SkUzRlNYRGlhejRKaERRS2lhcDFRM01naldiY0RyZk1kRXB5Sno4b1Rvb0hNdy82NDA" class="lazyload" data-srcset="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy80bE4xWE9ac2hmZEpydjhOVllHNzZBRlM3NDJpYnhXMEpTM29aSDBKQjh1SkUzRlNYRGlhejRKaERRS2lhcDFRM01naldiY0RyZk1kRXB5Sno4b1Rvb0hNdy82NDA" srcset="data:image/png;base64,666" class="ujefwqvhlxmc lazyload"></p>
<p> 也可以监控训练过程中的loss变化：</p>
<p><style>.qlmfhjfbnwib{}</style><img src="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi80bE4xWE9ac2hmZEpydjhOVllHNzZBRlM3NDJpYnhXMEppYmJCdXo5TUdnZlQ5VnpiaFgyN1JhcmtKWUw2RzR3OEQ1Mk41ZGtpY05rUmpJc1o0Q2poaGQ5dy82NDA" class="lazyload" data-srcset="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi80bE4xWE9ac2hmZEpydjhOVllHNzZBRlM3NDJpYnhXMEppYmJCdXo5TUdnZlQ5VnpiaFgyN1JhcmtKWUw2RzR3OEQ1Mk41ZGtpY05rUmpJc1o0Q2poaGQ5dy82NDA" srcset="data:image/png;base64,666" class="qlmfhjfbnwib lazyload"></p>
<h3 id="问题集合"><a href="#问题集合" class="headerlink" title="问题集合"></a>问题集合</h3><p>1.第一次安装后想运行试试看，结果出现了以下问题：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"E:/code_set/ssd-v5/frs.py"</span>, line <span class="number">3</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    vis = visdom.Vidom(env=<span class="string">'model_1'</span>)</span><br><span class="line">AttributeError: module <span class="string">'visdom'</span> has no attribute <span class="string">'Vidom'</span></span><br></pre></td></tr></tbody></table></figure>
<p>通过查阅相关资料，发现是自己导包导错了，写的时候并没有报错，很不容易发现</p>
<p>错误代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> visdom</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">vis = visdom.Vidom(env=<span class="string">'model_1'</span>)</span><br><span class="line">vis.text(<span class="string">'Hello World'</span>, win=<span class="string">'text1'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>正确代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> visdom</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">vis = visdom.Visdom(env=<span class="string">'model_1'</span>)</span><br><span class="line">vis.text(<span class="string">'Hello World'</span>, win=<span class="string">'text1'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>还可能是因为将文件命名成了visdom.py,导致代码内调包时引用了同级目录内的文件，具体参考：<a href="https://blog.csdn.net/llp858759099/article/details/104743482">使用visdom时遇到问题 AttributeError: module ‘visdom’ has no attribute ‘Visdom’</a></p>
<p>第一次成功应用visdom模块跑线性图</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> visdom</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">vis = visdom.Vidom(env=<span class="string">'model_1'</span>)</span><br><span class="line">vis.text(<span class="string">'Hello World'</span>, win=<span class="string">'text1'</span>)</span><br><span class="line">vis2 = visdom.Visdom(env=<span class="string">'test2'</span>)</span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">10</span>):</span><br><span class="line">    x = torch.Tensor([ii])</span><br><span class="line">    y = x</span><br><span class="line"></span><br><span class="line">    vis2.line(X=x,Y=y,win=<span class="string">'polynomial'</span>,update=<span class="string">'append'</span> <span class="keyword">if</span> ii&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">x = torch.arange(<span class="number">0</span>,<span class="number">9</span>,<span class="number">0.1</span>)</span><br><span class="line">y = (x ** <span class="number">2</span>)/<span class="number">9</span></span><br><span class="line">vis2.line(X=x,Y=y,win=<span class="string">'polynomial'</span>,name = <span class="string">'this is new trace'</span>,update=<span class="string">'append'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>结果：</p>
<p><style>.fbvojpnxcyhr{}</style><img src="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/image-20220920214849080.png" class="lazyload" data-srcset="/zh-CN/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/visdom%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/image-20220920214849080.png" srcset="data:image/png;base64,666" class="fbvojpnxcyhr lazyload"></p>
<p>记得选择环境test2</p>
<p>未完待续。。。。。。。。</p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title>zetane的使用说明</title>
    <url>/zh-CN/zetane%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>未完待续。。。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>zetane</tag>
      </tags>
  </entry>
  <entry>
    <title>Warnings_unsample</title>
    <url>/zh-CN/warning_unsample/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/WJ_MeiMei/article/details/94037960">warnings.warn(“nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.”</a></p>
<p>刚刚想测试一下grad-cam的代码，结果模型碰到一个问题，就是nn.functional.unsample被抛弃了</p>
<p>问题：</p>
<p><code>UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.</code></p>
<p><style>.thdzeasrrzgu{zoom:50%;}</style><img src="/zh-CN/warning_unsample/warning_unsample/image-20220922212820063.png" class="lazyload" data-srcset="/zh-CN/warning_unsample/warning_unsample/image-20220922212820063.png" srcset="data:image/png;base64,666" class="thdzeasrrzgu lazyload" alt="image-20220922212820063"></p>
<p>代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">feat1 = F.upsample_bilinear(self.fusion_layers[<span class="number">1</span>](feat1), size=(<span class="number">38</span>, <span class="number">38</span>))</span><br></pre></td></tr></tbody></table></figure>
<p>错误原因：</p>
<p>python版本问题，python3.5 支持 upsample 函数，python3.6 不支持 upsample 函数</p>
<p><strong>假如我们忽略这个警告，会导致实验效果降低，简单来说，这个警告一定要改</strong></p>
<p>解决办法：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">feat1 = F.interpolate(self.fusion_layers[<span class="number">1</span>](feat1), size=(<span class="number">38</span>, <span class="number">38</span>), mode=<span class="string">'bilinear'</span>, align_corners=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>warnings</tag>
      </tags>
  </entry>
  <entry>
    <title>jsDelivr+Github 使用方法</title>
    <url>/zh-CN/%E5%85%8D%E8%B4%B9cdn/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://zhuanlan.zhihu.com/p/76951130">免费CDN：jsDelivr+Github 使用方法</a></p>
<p>由于本人搭建博客需要换字体，又怕别人的链接失效，因此，想要把资源把握在自己的手里，通过查阅相关资料，找到上面的方法，尝试了一下，这里做下记录。</p>
<p>放在Github的资源在国内加载速度比较慢，因此需要使用CDN加速来优化网站打开速度，jsDelivr + Github便是免费且好用的CDN，非常适合博客网站使用。</p>
<h3 id="新建github仓库"><a href="#新建github仓库" class="headerlink" title="新建github仓库"></a>新建github仓库</h3><p><style>.rrbdrlcpwbfb{zoom: 50%;}</style><img src="/zh-CN/%E5%85%8D%E8%B4%B9cdn/%E5%85%8D%E8%B4%B9cdn/image-20221007231742803.png" class="lazyload" data-srcset="/zh-CN/%E5%85%8D%E8%B4%B9cdn/%E5%85%8D%E8%B4%B9cdn/image-20221007231742803.png" srcset="data:image/png;base64,666" class="rrbdrlcpwbfb lazyload" alt="image-20221007231742803"></p>
<h3 id="克隆仓库到本地"><a href="#克隆仓库到本地" class="headerlink" title="克隆仓库到本地"></a>克隆仓库到本地</h3><p><style>.gfhiyxoqsfwm{zoom:50%;}</style><img src="/zh-CN/%E5%85%8D%E8%B4%B9cdn/%E5%85%8D%E8%B4%B9cdn/image-20221007231924762.png" class="lazyload" data-srcset="/zh-CN/%E5%85%8D%E8%B4%B9cdn/%E5%85%8D%E8%B4%B9cdn/image-20221007231924762.png" srcset="data:image/png;base64,666" class="gfhiyxoqsfwm lazyload" alt="image-20221007231924762"></p>
<p>在本地目录右键 Git Bash Here，执行以下命令：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git@github.com:pistachio0812/CDN.git</span><br></pre></td></tr></tbody></table></figure>
<h3 id="上传资源"><a href="#上传资源" class="headerlink" title="上传资源"></a>上传资源</h3><p>复制需要上传的资源到本地git仓库（注：jsDelivr不支持加载超过20M的资源），在本地git仓库目录下右键 Git Bash Here，执行以下命令：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git status                    //查看状态</span><br><span class="line">git add .                     //添加所有文件到暂存区</span><br><span class="line">git commit -m <span class="string">'第一次提交'</span>      //把文件提交到仓库</span><br><span class="line">git push                      //推送至远程仓库</span><br></pre></td></tr></tbody></table></figure>
<p><style>.kjdwntgqsvcg{zoom:50%;}</style><img src="/zh-CN/%E5%85%8D%E8%B4%B9cdn/%E5%85%8D%E8%B4%B9cdn/image-20221007232608806.png" class="lazyload" data-srcset="/zh-CN/%E5%85%8D%E8%B4%B9cdn/%E5%85%8D%E8%B4%B9cdn/image-20221007232608806.png" srcset="data:image/png;base64,666" class="kjdwntgqsvcg lazyload" alt="image-20221007232608806"></p>
<h3 id="发布仓库"><a href="#发布仓库" class="headerlink" title="发布仓库"></a>发布仓库</h3><p>点击release发布</p>
<p><style>.ghvwjqgxnieq{zoom:50%;}</style><img src="/zh-CN/%E5%85%8D%E8%B4%B9cdn/%E5%85%8D%E8%B4%B9cdn/image-20221007232855759.png" class="lazyload" data-srcset="/zh-CN/%E5%85%8D%E8%B4%B9cdn/%E5%85%8D%E8%B4%B9cdn/image-20221007232855759.png" srcset="data:image/png;base64,666" class="ghvwjqgxnieq lazyload" alt="image-20221007232855759"></p>
<p>自定义发布版号</p>
<p><style>.hoevmikclubg{zoom:50%;}</style><img src="/zh-CN/%E5%85%8D%E8%B4%B9cdn/%E5%85%8D%E8%B4%B9cdn/image-20221007233355564.png" class="lazyload" data-srcset="/zh-CN/%E5%85%8D%E8%B4%B9cdn/%E5%85%8D%E8%B4%B9cdn/image-20221007233355564.png" srcset="data:image/png;base64,666" class="hoevmikclubg lazyload" alt="image-20221007233355564"></p>
<h3 id="通过jsDelivr引用资源"><a href="#通过jsDelivr引用资源" class="headerlink" title="通过jsDelivr引用资源"></a>通过jsDelivr引用资源</h3><p>使用方法：<a href="https://link.zhihu.com/?target=https%3A//cdn.jsdelivr.net/gh/">https://cdn.jsdelivr.net/gh/</a>你的用户名/你的仓库名@发布的版本号/文件路径</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">https://cdn.jsdelivr.net/gh/pistachio0812/CDN@0.1.0/cdn-fonts/MaShanZheng.woff2</span><br></pre></td></tr></tbody></table></figure>
<p>注意：版本号不是必需的，是为了区分新旧资源，如果不使用版本号，将会直接引用最新资源，除此之外还可以使用某个范围内的版本，查看所有资源等，具体使用方法如下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">// 加载任何Github发布、提交或分支</span><br><span class="line">https://cdn.jsdelivr.net/gh/user/repo@version/file</span><br><span class="line"></span><br><span class="line">// 加载 jQuery v3.2.1</span><br><span class="line">https://cdn.jsdelivr.net/gh/jquery/jquery@3.2.1/dist/jquery.min.js</span><br><span class="line"></span><br><span class="line">// 使用版本范围而不是特定版本</span><br><span class="line">https://cdn.jsdelivr.net/gh/jquery/jquery@3.2/dist/jquery.min.js   https://cdn.jsdelivr.net/gh/jquery/jquery@3/dist/jquery.min.js</span><br><span class="line"> </span><br><span class="line">// 完全省略该版本以获取最新版本</span><br><span class="line">https://cdn.jsdelivr.net/gh/jquery/jquery/dist/jquery.min.js</span><br><span class="line"> </span><br><span class="line">// 将“.min”添加到任何JS/CSS文件中以获取缩小版本，如果不存在，将为会自动生成</span><br><span class="line">https://cdn.jsdelivr.net/gh/jquery/jquery@3.2.1/src/core.min.js</span><br><span class="line"> </span><br><span class="line">// 在末尾添加 / 以获取资源目录列表</span><br><span class="line">https://cdn.jsdelivr.net/gh/jquery/jquery/</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>github</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>CDN</tag>
      </tags>
  </entry>
  <entry>
    <title>可视化技术</title>
    <url>/zh-CN/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8A%80%E6%9C%AF/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://www.toutiao.com/article/6725276071358366215">CNN模型的可视化</a></p>
<h2 id="CNNVis"><a href="#CNNVis" class="headerlink" title="CNNVis"></a><a href="http://shixialiu.com/publications/cnnvis/demo/">CNNVis</a></h2><p>清华大学视觉分析组做的一个网站，目的是为了更好地分析深度卷积神经网络。大家可以在训练的时候采取不同的卷积核尺寸和个数对照来看训练的中间过程。</p>
<h2 id="PlotNeuralNet"><a href="#PlotNeuralNet" class="headerlink" title="PlotNeuralNet"></a><a href="https://github.com/HarisIqbal88/PlotNeuralNet">PlotNeuralNet</a></h2><p>1.安装texlive</p>
<p>ubuntu</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">(1)下载texlive镜像 </span><br><span class="line">https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/</span><br><span class="line">(2)使用图形化安装界面，需要安装perl的tk组件 </span><br><span class="line">sudo apt-get install perl-tk</span><br><span class="line">(3)加载镜像文件安装</span><br><span class="line">sudo mount -o loop texlive.iso /mnt</span><br><span class="line">cd /mnt </span><br><span class="line">sudo ./install-tl -gui </span><br><span class="line">(4)安装texlive-latex-extra</span><br><span class="line">sudo apt-get install texlive-latex-extra</span><br></pre></td></tr></tbody></table></figure>
<p>windows</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">(1)下载并安装[MikTex](https://miktex.org/download)</span><br><span class="line">(2)下载并安装bash, 详情见 Git bash(https://git-scm.com/download/win)or Cygwin(https://www.cygwin.com/)</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorboard</title>
    <url>/zh-CN/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/LeungSr/article/details/120800763">✔✔✔ TensorBoard 的正确打开方法（含错误解决方法，超详细</a></p>
<h3 id="安装tensorboard"><a href="#安装tensorboard" class="headerlink" title="安装tensorboard"></a>安装tensorboard</h3><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">pip install tensorboard</span><br></pre></td></tr></tbody></table></figure>
<h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>略</p>
<h3 id="打开文件"><a href="#打开文件" class="headerlink" title="打开文件"></a>打开文件</h3><p>tf_logs文件下存放的就是训练模型时的日志文件，如下图所示：</p>
<p><style>.xniwkmtkxwnw{zoom:80%;}</style><img src="/zh-CN/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/image-20221127103837594.png" class="lazyload" data-srcset="/zh-CN/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/image-20221127103837594.png" srcset="data:image/png;base64,666" class="xniwkmtkxwnw lazyload" alt="image-20221127103837594"></p>
<p>cmd下用如下命令打开:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">tensorboard --logdir "absolute_path"</span><br></pre></td></tr></tbody></table></figure>
<p><style>.euxgzbbetkxi{zoom:80%;}</style><img src="/zh-CN/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/image-20221127103736045.png" class="lazyload" data-srcset="/zh-CN/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/image-20221127103736045.png" srcset="data:image/png;base64,666" class="euxgzbbetkxi lazyload" alt="image-20221127103736045"></p>
<h3 id="打开网页"><a href="#打开网页" class="headerlink" title="打开网页"></a>打开网页</h3><p>打开<a href="http://localhost:6006/">http://localhost:6006/</a></p>
<p><style>.kvqilkkzuwvt{zoom:80%;}</style><img src="/zh-CN/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/image-20221127104257117.png" class="lazyload" data-srcset="/zh-CN/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorboard/image-20221127104257117.png" srcset="data:image/png;base64,666" class="kvqilkkzuwvt lazyload" alt="image-20221127104257117"></p>
<p>如果不懂，参考博文</p>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>tensorboard</tag>
      </tags>
  </entry>
  <entry>
    <title>学习网站集合</title>
    <url>/zh-CN/%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99%E9%9B%86%E5%90%88/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="万门好课"><a href="#万门好课" class="headerlink" title="万门好课"></a>万门好课</h2><p><a href="https://www.wanmen.org/" title="加油吧少年">万门好课</a>是一家提供多品类原创精品课程的在线教育平台 ，课程覆盖IT与互联网类、职业成长类、经济金融类、本科学习类等领域 。 整体课程定位侧重于“用户刚需”类课程，如语言版块的出国英语考试类课程、小语种培训课程，本科学习版块的各学科基础大课，以及特色的万门通识课程包括PS、化妆等。</p>
<h2 id="网易云课堂"><a href="#网易云课堂" class="headerlink" title="网易云课堂"></a>网易云课堂</h2><p><a href="https://study.163.com/">网易云课堂</a>立足于实用性的要求，网易云课堂与多家教育、培训机构建立合作，课程数量已达4100+，课时总数超50000,涵盖实用软件、IT与互联网、外语学习、生活家居、兴趣爱好、职场技能、金融管理、考试认证、中小学、亲子教育等十余大门类。</p>
<h2 id="网易公开课"><a href="#网易公开课" class="headerlink" title="网易公开课"></a>网易公开课</h2><p><a href="https://open.163.com">网易公开课</a>首批1200集课程上线，其中有200多集配有中文字幕。用户可以在线免费观看来自于哈佛大学等世界级名校的公开课课程，可汗学院，TED等教育性组织的精彩视频，内容涵盖人文、社会、艺术、科学、金融等领域。 力求为爱学习的网友创造一个公开的免费课程平台，借此向外界公开招聘兼职字幕翻译。</p>
<h2 id="爱课程网"><a href="#爱课程网" class="headerlink" title="爱课程网"></a>爱课程网</h2><p><a href="https://www.icourses.cn/home">爱课程网</a>利用现代信息技术和网络技术， 面向高校师生和社会大众。提供优质教育资源共享和个性化教学资源服务，具有资源浏览、搜索、重组、评价、课程包的导入导出、发布、互动参与和“教”“学”兼备等功能。</p>
<h2 id="粉笔网"><a href="#粉笔网" class="headerlink" title="粉笔网"></a>粉笔网</h2><p><a href="https://www.fenbi.com">粉笔网</a>是一个互联网教育平台，业务包含：公务员考试，考研、教师资格、事业单位、英语、建造、财会等技能培训；利用技术手段实现智能批改功能，并提供免费题库，供用户查阅学习，利用网络直播，进行线上授课，同时提供实物图书、试卷以及客户服务。</p>
<h2 id="魔方英语"><a href="#魔方英语" class="headerlink" title="魔方英语"></a><a href="http://www.mofunenglish.com/">魔方英语</a></h2><h2 id="声同小语种论坛"><a href="#声同小语种论坛" class="headerlink" title="声同小语种论坛"></a><a href="http://www.somdom.com/">声同小语种论坛</a></h2>]]></content>
      <categories>
        <category>科研推荐</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>学术搜索网站集合</title>
    <url>/zh-CN/%E5%AD%A6%E6%9C%AF%E6%90%9C%E7%B4%A2%E7%BD%91%E7%AB%99%E5%AF%BC%E8%88%AA/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>搞学术研究的必不可少的就是论文，而且是大量的论文。因此，在这里我特意把我平时用到的积累到的论文搜索网站集中到了这里，后续遇到了其他的还会继续补充。</p>
<h2 id="谷歌学术"><a href="#谷歌学术" class="headerlink" title="谷歌学术"></a>谷歌学术</h2><p><a href="https://xs.scqylaw.com/">谷歌学术</a>是一个可以免费搜索学术文章的Google网络应用。2004年11月，Google第一次发布了Google学术搜索的试用版。该项索引包括了世界上绝大部分出版的学术期刊， 可广泛搜索学术文献的简便方法。您可以从一个位置搜索众多学科和资料来源：来自学术著作出版商、专业性社团、预印本、各大学及其他学术组织的经同行评论的文章、论文、图书、摘要和文章。Google 学术搜索可帮助您在整个学术领域中确定相关性最强的研究。</p>
<p>相关页面如下：</p>
<p><img src="https://img.99lb.net/images1/logo.png" class="lazyload" data-srcset="https://img.99lb.net/images1/logo.png" srcset="data:image/png;base64,666" alt="谷歌学术搜索"></p>
<h2 id="github"><a href="#github" class="headerlink" title="github"></a>github</h2><p><a href="https://github.com">github</a>于2008年4月10日正式上线，除了代码仓库托管及基本的Web管理界面以外，还提供了订阅、讨论组、文本渲染、在线文件编辑器、协作图谱（报表）、代码片段分享（Gist）等功能。目前，其注册用户已经超过350万，托管版本数量也是非常之多，其中不乏知名开源项目Ruby on Rails、jQuery、python等。</p>
<p>相关页面如下：</p>
<p><img src="https://pic2.zhimg.com/v2-3224a8e2da3f76575baaa77e5768fcb2_1440w.jpg?source=172ae18b" class="lazyload" data-srcset="https://pic2.zhimg.com/v2-3224a8e2da3f76575baaa77e5768fcb2_1440w.jpg?source=172ae18b" srcset="data:image/png;base64,666" alt="github"></p>
<h2 id="CVPR"><a href="#CVPR" class="headerlink" title="CVPR"></a>CVPR</h2><p>国际计算机视觉与模式识别会议（<a href="http://www.cvpapers.com/">CVPR</a>）是IEEE一年一度的学术性会议，会议的主要内容是计算机视觉与模式识别技术。CVPR是世界顶级的计算机视觉会议（三大顶会之一，另外两个是ICCV和ECCV，近年来每年有约1500名参加者，收录的论文数量一般300篇左右。本会议每年都会有固定的研讨主题，而每一年都会有公司赞助该会议并获得在会场展示的机会。</p>
<p>相关页面如下：</p>
<p><img src="http://www.cvpapers.com/img/cvpapers.png" class="lazyload" data-srcset="http://www.cvpapers.com/img/cvpapers.png" srcset="data:image/png;base64,666" alt="CVPR"></p>
<h2 id="CVF"><a href="#CVF" class="headerlink" title="CVF"></a>CVF</h2><p><a href="https://openaccess.thecvf.com/">CVF</a>研究论文是由计算机视觉基金会提供的开放获取版本。除水印外，它们与接受的版本相同;最后发表的论文集可以在IEEE Xplore上找到。本材料的提出，以确保及时传播学术和技术工作。版权和其中的所有权利由作者或其他版权持有人保留。所有复制此信息的人都应遵守每个作者的版权所援引的条款和约束。</p>
<h2 id="arXiv"><a href="#arXiv" class="headerlink" title="arXiv"></a>arXiv</h2><p><a href="https://arxiv.org/">arXiv</a>是一个免费分发服务和开放获取的档案，涵盖物理、数学、计算机科学、定量生物学、定量金融学、统计学、电气工程和系统科学以及经济学领域的2,040,232篇学术文章。</p>
<h2 id="paperswithcode"><a href="#paperswithcode" class="headerlink" title="paperswithcode"></a>paperswithcode</h2><p><a href="https://paperswithcode.com/">Papers With Code</a>代码论文的任务是创建一个免费和开放的资源与机器学习论文，代码，数据集，方法和评估表。我们相信，在NLP和ML的支持下，与社区合作是最好的。这个网站上的所有内容都是公开许可的CC-BY-SA(与维基百科一样)，每个人都可以贡献——寻找“编辑”按钮!我们还运营专门的门户网站，提供天文学、物理学、计算机科学、数学和统计学的论文代码。</p>
]]></content>
      <categories>
        <category>科研推荐</category>
      </categories>
      <tags>
        <tag>文献工具</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow学习笔记</title>
    <url>/zh-CN/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>tf中矩阵量存在形式常用有三种，具体如下：<br>1.tf.Variable()<br>表示神经网络中可变化的量（可以通过trainable=False设置成不可变),可在运行中赋值，可以通过constant或者其他方式进行初始化。<br>2.tf.constant()<br>可以通过numpy中的array或者list,还有给定的shape和数值进行赋值<br>3.tf.placeholder()<br>相当于占位符，也是有shape的量，因为训练过程中需要不断赋值和替换值，而整体计算的结构是不变的。<br>代码：<br>//导包</p>
<p><code>import tensorflow as tf</code>    </p>
<p>//定义变量<br><code>A = tf.Variable(tf.ones([4,4]))</code></p>
<p>//变量初始化<br><code>import numpy as np      
cst = tf.constant(np.ones([4,4]),dtype=tf.float32)</code></p>
<h1 id="需要指定类型dtype-tf-float32-tf中不能隐式转换浮点和整型"><a href="#需要指定类型dtype-tf-float32-tf中不能隐式转换浮点和整型" class="headerlink" title="需要指定类型dtype=tf.float32,tf中不能隐式转换浮点和整型"></a>需要指定类型dtype=tf.float32,tf中不能隐式转换浮点和整型</h1><h1 id="cst-tf-constant-1-0-shape-4-4-dtype-tf-float32-也是可以的"><a href="#cst-tf-constant-1-0-shape-4-4-dtype-tf-float32-也是可以的" class="headerlink" title="cst = tf.constant(1.0,shape=[4,4],dtype=tf.float32)也是可以的"></a>cst = tf.constant(1.0,shape=[4,4],dtype=tf.float32)也是可以的</h1><p><code>A = tf.Variable(cst)</code></p>
<p>//定义placeholder<br><code>X = tf.placeholder(dtype=tf.float32,shape=[4,1])</code></p>
<p>//矩阵相乘<br><code>C = tf.matmul(A,X)</code></p>
<p>//定义Session<br><code>sess = tf.Session()</code></p>
<p>//初始化变量<br><code>init = tf.global_variables_initializer()</code></p>
<p>//执行初始化<br><code>sess.run(init)</code></p>
<p>//运行矩阵相乘<br><code>sess.run(C,feed_dict={X:[[1],[1],[1],[1]]})</code></p>
<p>//获取变量值<br><code>A_val = A.value()  
Avalue = sess.run(A_val)</code></p>
<p>//完整矩形相乘代码<br><code>import tensorflow as tf  
import numpy as np</code></p>
<p><code>X = tf.placeholder(dtype=tf.float32,shape=[4,1])  
A = tf.Variable(tf.zeros([4,4]))  
C = tf.matmul(A,X)  
sess = tf.session()  
init = tf.global_variables_initializer()  
sess.run(init)  
print(sess.run(A))</code></p>
<p>//为了使计算图更加清晰，可以使用variable_scope()<br>//定义变量名称<br><code>with  tf.variable_scope("first-nn-layer"):
    W = tf.Variable(tf.zeros([784,10]),name="W")
    b = tf.Variable(tf.zeros([10]),name="b")
    y = tf.matmul(x,W)+b
    variable_summaries(W)</code></p>
<p>//标识不同的变量<br>//不同作用域下的同名变量<br><code>with tf.variable_scope("first-nn-layer"):
    W = tf.Variable(tf.zeros([784,10]),name="W")
    b = tf.Variable(tf.zeros([10]),name="b")
    W1 = tf.Variable(tf.zeros([784,10]),name="W")
print(W.name)
print(W1.name)</code></p>
<h1 id="w、w1虽然name一样，但是计算中依然当成不同的变量，让同一个scope的同一变量可以通过get-variable-函数"><a href="#w、w1虽然name一样，但是计算中依然当成不同的变量，让同一个scope的同一变量可以通过get-variable-函数" class="headerlink" title="w、w1虽然name一样，但是计算中依然当成不同的变量，让同一个scope的同一变量可以通过get_variable()函数"></a>w、w1虽然name一样，但是计算中依然当成不同的变量，让同一个scope的同一变量可以通过get_variable()函数</h1><p>//获取变量<br><code>with tf.variable_scope("first-nn-layer") as scope:
    W = tf.get_variable("W",[784, 10])
    b = tf.get_variable("b",[10])
    scope.reuse_variables()#缺少则会报错
    W1 = tf.get_variables("W",shape=[784,10])
print(W.name)
print(W1.name)</code></p>
<h1 id="w、w1属于同一个变量"><a href="#w、w1属于同一个变量" class="headerlink" title="w、w1属于同一个变量"></a>w、w1属于同一个变量</h1><p>注：若此时缺少了scope.reuse_variables()则会报错，因为同时引用了同一个变量，对于不同层的变量，可以利用variable_scope进行区分，在再次引入相关变量时，需要加上reuse=True,否则依然会报错。如果变量不存在时加上reuse=True,依然会报错，因为该变量不存在</p>
<p><code>with tf.variable_scope("first-nn-layer") as scope:
    W = tf.get_variable("W",[784, 10])
    b = tf.get_variable("b",[10])
with tf.variable_scope("second-nn-layer") as scope:
    W = tf.get_variable("W",[784, 10])
    b = tf.get_variable("b",[10])
with tf.variable_scope("second-nn-layer", reuse=True) as scope:
    W3 = tf.get_variable("W",[784, 10])
    b3 = tf.get_variable("b",[10])
print(W.name)
print(W3.name)</code></p>
<p>//保存模型<br>//定义saver<br><code>saver = tf.train.Saver()</code></p>
<p>//在训练过程中进行保存，保存为训练过程中的变量<br>//变量保存<br><code>for itr in range(1000):
    ...
    saver.save(sess,"model/al",global_step=itr)</code></p>
<p>//加载计算<br>//变量载入<br><code>saver.restore(sess,"model/v2-200")</code></p>
<p>3.4构建计算图<br>//前面在描述计算图，这里观察所建立的计算图<br>//定义summary<br><code>train_writer = tf.summary.FileWriter("logdir",sess.graph)</code></p>
<p>注：sess.graph就是描绘的计算图，”logdir”是log的存储文件夹。在Shell中运行Tensorboard,在浏览器中输入localhost:6006,然后点击graph就可以看到设计的网络模型了。</p>
<p>//问题：描绘的计算图非常杂乱无章，变量命名的可读性很差，需要进行整理。<br>//变量命名<br><code>x = tf.placeholder(tf.float32,[None,784],name="input_x")
label = tf.placeholder(tf.float32,[None,10],name="input_label")
W = tf.Variable(tf.zeros([874,10]),name="W")
b = tf.Variable(tf.zeros([10]),name="b")</code></p>
<p>//问题：依然不够清楚，可以将输入层的x和label归为一类<br>//定义作用域<br><code>with tf.variable_scope("input"):
    x = tf.placeholder(tf.float32,[None,784],name="input_x")
    label = tf.placeholder(tf.float32,[None,10],name="input_label")
with tf.variable_scope("first-nn-layer"):
    W = tf.Variable(tf.zeros([784,10]), name="W")
    b = tf.Variable(tf.zeros([10]),name="b")
    y = tf.matmul(x,W)+b
with tf.variable_scope("loss"):
    loss = tf.reduce_mean(tf.square(y-label))</code></p>
<p>//同一作用域下的同名变量是相同的，涉及到变量复用的问题，以及后续变量的获取，为了观察变量的变化，需要观察的变量加入summary函数<br>//定义summary函数<br><code>def variable_summaries(var):
    with tf.name_scope('summaries'):
        mean = tf.reduce_mean(var)
        tf.summary.scalar('mean',mean)
        with tf.name_scope('stddev'):
            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))
        tf.summary.scalar('stddev',stddev)
        tf.summary.scalar('max',tf.reduce_max(var))
        tf.summary.scalar('min',tf.reduce_min(var))
        tf.summary.histogram('histogram',var)</code></p>
<p>//若要观测W的相关情况，调用summary函数<br>//调用summary函数<br><code>variable_summaries(W)</code></p>
<p>//再用merge_all函数收集summary信息<br>//获取summary信息<br><code>merged = tf.summary.merge_all()</code></p>
<p>//summary保存<br><code>summary = sess.run(merged, feed_dict={x:batch_xs,label:batch_ys})
train_writer.add_summary(summary,itr)</code></p>
<p>注：此时可以在网页中访问，观察变量随迭代变化的情况，可以通过不同的方式对变量进行观测，比如时序统计、histogram,这些统计信息对于分析训练过程是非常重要的</p>
<p>3.5全连接网络构建<br>//tf官方手写识别版本的简化版本<br>//单层全连接网络</p>
<h1 id="引入库"><a href="#引入库" class="headerlink" title="引入库"></a>引入库</h1><p><code>from tensorflow.examples.tutorials.mnist import input_data#产生数据，手写识别的图片和标签
import tensorflow as tf</code></p>
<h1 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h1><p><code>mnist = input_data.read_data_sets("MNIST_data/",one_hot=True)</code></p>
<h1 id="构建网络模型"><a href="#构建网络模型" class="headerlink" title="构建网络模型"></a>构建网络模型</h1><h1 id="x-label分别为图形数据和标签数据"><a href="#x-label分别为图形数据和标签数据" class="headerlink" title="x,label分别为图形数据和标签数据"></a>x,label分别为图形数据和标签数据</h1><p><code>x = tf.placeholder(tf.float32,[None,784])
label = tf.placeholder(tf.float32,[None,10])</code></p>
<h1 id="构建单层网络中的权值和偏置"><a href="#构建单层网络中的权值和偏置" class="headerlink" title="构建单层网络中的权值和偏置"></a>构建单层网络中的权值和偏置</h1><p><code>W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10])</code></p>
<h1 id="本例中无非线性激活函数"><a href="#本例中无非线性激活函数" class="headerlink" title="本例中无非线性激活函数"></a>本例中无非线性激活函数</h1><p><code>y = tf.matmul(x,W)+b</code></p>
<h1 id="定义损失函数为欧氏距离，但这并不是最好的，多分类问题通常使用交叉熵"><a href="#定义损失函数为欧氏距离，但这并不是最好的，多分类问题通常使用交叉熵" class="headerlink" title="定义损失函数为欧氏距离，但这并不是最好的，多分类问题通常使用交叉熵"></a>定义损失函数为欧氏距离，但这并不是最好的，多分类问题通常使用交叉熵</h1><p><code>loss = tf.reduce_mean(tf.square(y-label))</code></p>
<h1 id="若使用交叉熵损失函数"><a href="#若使用交叉熵损失函数" class="headerlink" title="若使用交叉熵损失函数"></a>若使用交叉熵损失函数</h1><p><code>soft_max = tf.nn.softmax(logit, axis=1)
loss = tf.reduce_mean(-label*tf.log(soft_max))</code></p>
<h1 id="用梯度迭代算法"><a href="#用梯度迭代算法" class="headerlink" title="用梯度迭代算法"></a>用梯度迭代算法</h1><p><code>train_step = tf.train.GradientDescentOptimizer(0.005).minimize(loss)</code></p>
<h1 id="用于验证"><a href="#用于验证" class="headerlink" title="用于验证"></a>用于验证</h1><p><code>correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(label,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float.32))</code></p>
<h1 id="定义会话"><a href="#定义会话" class="headerlink" title="定义会话"></a>定义会话</h1><p><code>sess = tf.Session()</code></p>
<h1 id="初始化所有变量"><a href="#初始化所有变量" class="headerlink" title="初始化所有变量"></a>初始化所有变量</h1><p><code>sess.run(tf.global_variable_initializer())</code></p>
<h1 id="迭代过程"><a href="#迭代过程" class="headerlink" title="迭代过程"></a>迭代过程</h1><p><code>for itr in range(3000):
    batch_xs,batch_ys = mnist.train.next_batch(100)
    sess.run(train_step, feed_dict={x:batch_xs,label:batch_ys})
    if itr%10==0:
        print("step:%6d accuracy:"%iter, sess.run(accuracy, feed_dict={x:mnist.test.images, label:mnist.test.labels}))</code></p>
<h1 id="获取W取值"><a href="#获取W取值" class="headerlink" title="获取W取值"></a>获取W取值</h1><p><code>W_value = sess.run(W.value())</code></p>
<p>//定义一个单层全连接函数<br><code>def full_layer(input_tensor, out_dim, name="full"):
    with tf.variable_scope(name):
        shape = input_tensor.get_shape()as_list()
        W = tf.get_variable('W',(shape[1],out_dim),dtype=tf.float32, initalizer=tf.truncated_normal_initializer(stddev=0.1))
        b = tf.get_variable('b',[out_dim], dtype=tf.float32, initializer=tf.constant_initializer(0))
        out = tf.matmul(input_tensor, W)+b
    return tf.nn.sigmoid(nn)</code></p>
<p>3.6CNN构建<br>//CNN手写识别</p>
<h1 id="预读取MNIST手写字库"><a href="#预读取MNIST手写字库" class="headerlink" title="预读取MNIST手写字库"></a>预读取MNIST手写字库</h1><p><code>from tensorflow.examples.tutorials.mnist import input_data
  mnist = input_data.read_data_sets("MNIST_data",one_hot=True)</code></p>
<p><code>import tensorflow as tf</code></p>
<h1 id="用正态分布随机数初始化变量，本例中仅作为权值"><a href="#用正态分布随机数初始化变量，本例中仅作为权值" class="headerlink" title="用正态分布随机数初始化变量，本例中仅作为权值"></a>用正态分布随机数初始化变量，本例中仅作为权值</h1><p><code>def weight_variable(shape):
    initial=tf.truncated_normal(shape,stddev=0.1)</code></p>
<pre><code>#正态分布
`return tf.Variable(initial)`
</code></pre><h1 id="用常量方式初始化偏置"><a href="#用常量方式初始化偏置" class="headerlink" title="用常量方式初始化偏置"></a>用常量方式初始化偏置</h1><p><code>def bias_variable(shape):
    initial=tf.constant(0.1,shape=shape)</code></p>
<pre><code>#常数分布
   `return tf.Variable(initial)`
</code></pre><h1 id="定义二维卷积的过程"><a href="#定义二维卷积的过程" class="headerlink" title="定义二维卷积的过程"></a>定义二维卷积的过程</h1><p>def conv2d(x,W):<br>    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=’SAME’)</p>
<h1 id="定义池化层，简单地说就是选个最大的数，进一步降低自由参数的个数"><a href="#定义池化层，简单地说就是选个最大的数，进一步降低自由参数的个数" class="headerlink" title="定义池化层，简单地说就是选个最大的数，进一步降低自由参数的个数"></a>定义池化层，简单地说就是选个最大的数，进一步降低自由参数的个数</h1><p><code>def max_pool_2x2(x):
    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')</code></p>
<p><code>x = tf.placeholder(tf.float32,shape=[100,784])
y = tf.placeholder(tf.float32,shape=[100,10])</code></p>
<p><code>W_conv1 = weight_variable([5,5,1,32])
b_conv1 = bias_variable([32])
x_image = tf.reshape(x,[-1,28,28,1])
y_conv1 = tf.nn.relu(conv2d(x_iamge,W_conv1)+b_conv1)
y_pool1 = max_pool_2x2(y_conv1)</code></p>
<p><code>W_conv2 = weight_variable([5,5,32,64])
b_conv2 = weight_variable([64])
y_conv2 = tf.nn.relu(conv2d(y_pool1,W_conv2)+b_conv2)
y_pool2 = max_pool_2x2(y_conv2)</code></p>
<p><code>y_fc_flat = tf.reshape(y_pool2,[-1,7*7*64])
W_fc1 = weight_variable([7*7*64,10])
b_fc1 = bias_variable([10])
y_fc1 = tf.nn.relu(tf.matmul(y_fc_flat,W_fc1)+b_fc1)</code></p>
<p><code>cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_fc1))
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</code></p>
<p><code>sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)</code></p>
<p><code>for i in range(1000):
    bx,by = mnist.train.next_batch(100)
    sess.run(train_step,feed_dict={x:bx,y:by})</code></p>
<p><code>import numpy as np
import matplotlib.pyplot as plt</code></p>
<h1 id="设置输出风格，为画图美观"><a href="#设置输出风格，为画图美观" class="headerlink" title="设置输出风格，为画图美观"></a>设置输出风格，为画图美观</h1><p><code>import matplotlib  as mpl
mpl.style.use('seaborn-darkgrid')</code></p>
<p><code>val = W_conv1.value()
convVal = np.array(sess.run(val))
convVal = np.reshape(convVal,[5,5,32])
plt.imshow(convVal[:,:,6])
plt.show()</code></p>
<p>3.8多架构运行<br>//GPU使用<br>GPU可以加速深度学习作业的训练速度，如果服务器有多个GPU,那么tensorflow默认使用全部<br>使用部分GPU:</p>
<p>python程序启动时调用：<br><code>CUDA_VISIBLE_DEVICES=0.2.3 python script.py</code></p>
<p>python代码内进行调用：<br><code>import os
os.environ["CUDA_VISIBLE_DEVICES"]="1"</code></p>
<p>//配置GPU显存<br>某些情况下，多作业或者共享GPU的场景中，可以控制tf使用GPU显存大小<br><code>gpuOptions = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpuOptions))</code></p>
<p>//GPU运行代码</p>
<h1 id="将变量的定义和分配定义到GPU上进行"><a href="#将变量的定义和分配定义到GPU上进行" class="headerlink" title="将变量的定义和分配定义到GPU上进行"></a>将变量的定义和分配定义到GPU上进行</h1><p><code>with tf.device('/gpu:0'):
    W = tf.get_variable('W',(in_dim,out_dim),dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.1))
    b = tf.get_variable('b'),[out_dim],dtype=tf.float32,initializer=tf.constant_initializer(0))
    net = tf.matmul(input_tensor,W)+b</code></p>
<h1 id="在CPU上计算激活函数"><a href="#在CPU上计算激活函数" class="headerlink" title="在CPU上计算激活函数"></a>在CPU上计算激活函数</h1><p><code>with tf.device('/cpu:0'):
    net = tf.nn.sigmoid(net)</code></p>
<p>//多CPU使用，多设备计算<br>//利用标号简单的区分并运行</p>
<h1 id="在CPU0上计算"><a href="#在CPU0上计算" class="headerlink" title="在CPU0上计算"></a>在CPU0上计算</h1><p><code>with tf.device('/cpu:0')
    ...
    net = tf.nn.sigmoid(net)</code></p>
<h1 id="在CPU1上计算"><a href="#在CPU1上计算" class="headerlink" title="在CPU1上计算"></a>在CPU1上计算</h1><p><code>with tf.device('/cpu:1')
    ...
    net = tf.nn.sigmoid(net)</code></p>
<p>//在集群上运行，需要在多个主机上准备多份代码，代码前面部分相同，后续有所不同<br>//定义多主机运行参数</p>
<h1 id="这里的地址形式为IP-Port"><a href="#这里的地址形式为IP-Port" class="headerlink" title="这里的地址形式为IP:Port"></a>这里的地址形式为IP:Port</h1><p><code>cluster = tf.train.ClusterSpectf.train.ClusterSpec({
    "worker":[
        "xx.xx.xx.xx:2222",    #/job:worker/task:0
        "xx.xx.xx.xx:2222",    #这里job的名称为自定义
        "xx.xx.xx.xx:2222"    #task编号同样需在Server中定义
        ],
    "ps":[
        "xx.xx.xx.xx:2222",
        "xx.xx.xx.xx:2222"
        ]})
server = tf.train.Server(cluster, job_name="worker", task_index=0)</code></p>
<p>//定义第二个主机参数</p>
<h1 id="这里的地址形式为IP-Port-1"><a href="#这里的地址形式为IP-Port-1" class="headerlink" title="这里的地址形式为IP:Port"></a>这里的地址形式为IP:Port</h1><p><code>cluster = tf.train.ClusterSpectf.train.ClusterSpec({
    "worker":[
        "xx.xx.xx.xx:2222",    #/job:worker/task:0
        "xx.xx.xx.xx:2222",    #这里job的名称为自定义
        "xx.xx.xx.xx:2222"    #task编号同样需在Server中定义
        ],
    "ps":[
        "xx.xx.xx.xx:2222",
        "xx.xx.xx.xx:2222"
        ]})
server = tf.train.Server(cluster, job_name="worker", task_index=1)</code></p>
<p>//不同设备的运行代码<br><code>with tf.device('/job:worker/task:0/cpu:0'):
    ...</code></p>
<p>//将不同的任务分配到不同的计算节点上<br>//分配计算任务<br><code>with tf.device(tf.train.replica_device_setter(
    worker_device="/job:worker/task:%d" %task_index,cluster=cluster)</code></p>
<p>//函数replica_device_setter会将变量参数的定义部分自动定义到ps服务中，后续需要定义Session,用于执行这个过程<br>//多主机运行</p>
<h1 id="定义句柄，迭代多少步后停止迭代"><a href="#定义句柄，迭代多少步后停止迭代" class="headerlink" title="定义句柄，迭代多少步后停止迭代"></a>定义句柄，迭代多少步后停止迭代</h1><p><code>hooks = [tf.train.StopAtStepHook(last_step=1000000)]</code></p>
<h1 id="MonitoredTrainingSession函数会完成会话初始化工作"><a href="#MonitoredTrainingSession函数会完成会话初始化工作" class="headerlink" title="MonitoredTrainingSession函数会完成会话初始化工作"></a>MonitoredTrainingSession函数会完成会话初始化工作</h1><h1 id="保存checkpoint-恢复checkpoint-异常判断等"><a href="#保存checkpoint-恢复checkpoint-异常判断等" class="headerlink" title="保存checkpoint,恢复checkpoint,异常判断等"></a>保存checkpoint,恢复checkpoint,异常判断等</h1><h1 id="这里需要定义master主机，定义保存、控制操作的master"><a href="#这里需要定义master主机，定义保存、控制操作的master" class="headerlink" title="这里需要定义master主机，定义保存、控制操作的master"></a>这里需要定义master主机，定义保存、控制操作的master</h1><p><code>with tf.train.MonitroedTrainingSession(
    master=server.target,
    is_chief=(task_index==0),
    checkpoint_dir="dir/to/cp",
    hooks=hooks) as mon_sess:
    ...</code></p>
<p>注：在程序运行过程中，需要认为将程序分配到各个主机上，依次运行各个主机</p>
<p>//队列用于数据读取和处理，队列可以是先进先出队列，也可以是随机队列，用于随机化输出<br>//tf中队列的操作是对于训练前的过程而言的，有以下作用<br>1.多线程数据预处理并将其推入队列<br>2.在执行过程中，队列不断提供训练数据<br>//简单实例说明队列使用<br><code>def simple_shuffle_batch(source,capacity,batch_size=10):</code></p>
<pre><code>#定义随机序列
`queue = tf.RandomShuffleQueue(
    capacity=capacity,
    min_after_dequeue=int(0.9*capacity),
    shapes=source.shape,
    dtypes=source.dtype)`
#定义enqueue过程
`enqueue = queue.enqueue(source)`
#定义执行进程个数
`num_threads = 4
qr = tf.train.QueueRunner(queue,[enqueue]*num_threads)`
#声明Queue runner,使得其可以被执行
`tf.train.add_queue_runner(qr)`
#获取数据
`return queue.dequeue_many(batch_size)`
</code></pre><h1 id="产生测试数据"><a href="#产生测试数据" class="headerlink" title="产生测试数据"></a>产生测试数据</h1><p><code>input = tf.constant(list(range(100)))
input = tf.data.Dataset.from_tensor_slices(input)
input = input.make_one_shot_iterator().get_next()</code></p>
<h1 id="定义函数"><a href="#定义函数" class="headerlink" title="定义函数"></a>定义函数</h1><p><code>get_batch = simple_shuffle_batch(input,capacity=20)</code></p>
<h1 id="定义session"><a href="#定义session" class="headerlink" title="定义session"></a>定义session</h1><p><code>with tf.train.MonitoredSession() as sess:
    while not sess.should_stop():
        print(sess.run(get_batch))</code></p>
<p>注：队列操作可以使得数据读取过程得到并行的优化，这对于提升程序的运行速度是很有利的。</p>
<p>//tf相关扩展<br>4.2.1 tf Layers<br>//全连接网络</p>
<h1 id="layers定义全连接网络"><a href="#layers定义全连接网络" class="headerlink" title="layers定义全连接网络"></a>layers定义全连接网络</h1><p><code>net = tf.layers.dense(inputs=net, units=units, activation=tf.nn.relu)</code></p>
<h1 id="卷积网络"><a href="#卷积网络" class="headerlink" title="卷积网络"></a>卷积网络</h1><p><code>net = tf.layers.conv2d(
    inputs=net, #输入
    filters=n_features, #输出特征数
    kernel-size=[5, 5], #卷积核心大小
    padding="same", #边界
    activation=tf.nn.relu #激活函数
    )</code></p>
<p>//前馈神经网络函数</p>
<h1 id="二维最大池化"><a href="#二维最大池化" class="headerlink" title="二维最大池化"></a>二维最大池化</h1><p><code>net = tf.layers.max_pooling2d(...)</code></p>
<h1 id="二维平均池化"><a href="#二维平均池化" class="headerlink" title="二维平均池化"></a>二维平均池化</h1><p><code>net = tf.layers.average_pooling2d(...)</code></p>
<h1 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h1><p><code>net = tf.layers.conv2d(...)</code></p>
<h1 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h1><p><code>net = tf.layers.dropout(...)</code></p>
<h1 id="展开"><a href="#展开" class="headerlink" title="展开"></a>展开</h1><p><code>net = tf.layers.flatten(...)</code></p>
<h1 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h1><p><code>net = tf.layers.batch_normalization(...)</code></p>
<p>4.2.2 tf Slim</p>
<h1 id="卷积函数"><a href="#卷积函数" class="headerlink" title="卷积函数"></a>卷积函数</h1><p><code>def conv2d_layer(input_tensor, size=1, feature=128, name='conv1d'):
    with tf.variable_scope(name):
        shape = input_tensor.get_shape.as_list()
        kernel = tf.get_variable('kernel', (size, size, shape[-1], feature), dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1))
        b = tf.get_variable('b', [feature], dtype=tf.float32, initializer=tf.constant_initializer(0))
        out = tf.nn.conv2d(input_tensor, kernel, strides=[1,2,2,1],padding='SAME')+b
    return tf.nn.relu(out)</code></p>
<h1 id="全连接函数"><a href="#全连接函数" class="headerlink" title="全连接函数"></a>全连接函数</h1><p><code>def full_layer(input_tensor, out_dim, name='full'):
    with tf.variable_scope(name):
        shape = input_tensor.get_shape.as_list()
        W = tf.get_variable('W', (shape[1], out_dim), dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1))
        b = tf.get_variabel('b', [out_dim], dtype=tf.float32, initializer=tf.constant_initializer(0))
        out = tf.matmul(input_tensor, W)+b
    return out</code></p>
<p>//slim实现卷积，tfv2取消该库</p>
<h1 id="引入slim库"><a href="#引入slim库" class="headerlink" title="引入slim库"></a>引入slim库</h1><p><code>import tensorflow.contrib.slim as slim</code></p>
<h1 id="定义卷积层"><a href="#定义卷积层" class="headerlink" title="定义卷积层"></a>定义卷积层</h1><p><code>net = slim.conv2d(inputs, 16, 4, strides=2, activation_fn=tf.nn.relu, scope='conv1')</code></p>
<h1 id="加入池化层"><a href="#加入池化层" class="headerlink" title="加入池化层"></a>加入池化层</h1><p><code>net = tf.nn.max_pool(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')</code></p>
<p><code>net = slim.conv2d(net, 32, 4, strides=2, activation_fn=tf.nn.relu, scope='conv2')
net = tf.nn.max_pool(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')</code></p>
<h1 id="flatten层，用于将三维的图形数据展开成一维数据，用于全连接层"><a href="#flatten层，用于将三维的图形数据展开成一维数据，用于全连接层" class="headerlink" title="flatten层，用于将三维的图形数据展开成一维数据，用于全连接层"></a>flatten层，用于将三维的图形数据展开成一维数据，用于全连接层</h1><p><code>net = slim.flatten(net)</code></p>
<h1 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h1><p><code>y = slim.fully_connected(net, 10, activation_fn=line, scope='full', reuse=False)</code></p>
<p>4.2.3 tfLearn<br>//tflearn抽象层次更高，代码可读性更好,其是一个完整的生态<br>//基础网络架构</p>
<h1 id="全连接"><a href="#全连接" class="headerlink" title="全连接"></a>全连接</h1><p><code>net = tflearn.fully_connected(...)</code></p>
<h1 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h1><p><code>net = tflearn.conv_2d(...)</code></p>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p><code>net = tflearn.lstm(...)</code></p>
<h1 id="dropout-1"><a href="#dropout-1" class="headerlink" title="dropout"></a>dropout</h1><p><code>net = tflearn.dropout(...)</code></p>
<p>//输入函数<br><code>network = tflearn.input_data(shape=[None, 28, 28, 1], name='input')</code></p>
<p>//优化部分</p>
<h1 id="定义优化过程"><a href="#定义优化过程" class="headerlink" title="定义优化过程"></a>定义优化过程</h1><p><code>network = tflearn.layers.estimator.regression(
    network,
    optimizer='adam', #优化方法
    learning_rate=0.01, #学习率
    loss='categorical_crossentropy', #损失函数
    name='target')</code></p>
<p>//利用tflearn完成手写数字的识别任务<br><code>import tflearn
from tflearn.layers.core import input_data,dropout, fully_connected
from tflearn.layers.conv import conv_2d, , max_pool_2d
from tflearn.layers.normalization import local_response_normalization
from tflearn.layers.estimator import regression</code></p>
<h1 id="载入并处理数据"><a href="#载入并处理数据" class="headerlink" title="载入并处理数据"></a>载入并处理数据</h1><p><code>import tflearn.datasets.mnist as mnist
X, Y, testX, testY = mnist.load_data(one_hot=True)</code></p>
<h1 id="转换为二维图形"><a href="#转换为二维图形" class="headerlink" title="转换为二维图形"></a>转换为二维图形</h1><p><code>X = X.reshape([-1, 28, 28, 1])
testX = testX.reshape([-1, 28, 28, 1])</code></p>
<h1 id="建立神经网络"><a href="#建立神经网络" class="headerlink" title="建立神经网络"></a>建立神经网络</h1><p><code>network = tflearn.input_data(shape=[None, 28, 28, 1], name='input')
network = conv_2d(network, 32, 3, activation='relu', regularizer='L2')
network = max_pool_2d(network)
network = local_response_normalization(network)
network = fully_connected(network, 128, activation='tanh')
network = dropout(network, 0.8)
network = fully_connected(network, 256, activation='tanh')
network = dropout(network, 0.8)
network = fully_connected(network, 10, activation='softmax')</code></p>
<h1 id="定义优化过程-1"><a href="#定义优化过程-1" class="headerlink" title="定义优化过程"></a>定义优化过程</h1><p><code>network = regression(
    network,
    optimizer='adam',
    learning_rate=0.01,
    loss='categorical_crossentropy',
    name='target')</code></p>
<h1 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h1><p><code>model = tflearn.DNN(network, tensorboard_verbose=0)
model.fit({'input':X}, {'target':Y}, n_epoch=20,
    validation_set=({'input':testX}, {'target':testY}),
    snapshot_step=100, show_metric=True, run_id='convnet_mnist')</code></p>
<p>//Keras代码可读性好，并且横跨多个机器学习框架，但其扩展性较差<br>//引入库<br><code>from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D</code></p>
<p>//Keras直接顺序加入模型，无需通过数据方式进行传递<br>//基础网络层<br><code>from keras.models import Sequential
model = Sequential()</code></p>
<h1 id="加入卷积层"><a href="#加入卷积层" class="headerlink" title="加入卷积层"></a>加入卷积层</h1><p><code>model.add(Conv2D(...))</code></p>
<h1 id="加入池化层-1"><a href="#加入池化层-1" class="headerlink" title="加入池化层"></a>加入池化层</h1><p><code>model.add(MaxPooling2D(...))</code></p>
<h1 id="加入全连接层"><a href="#加入全连接层" class="headerlink" title="加入全连接层"></a>加入全连接层</h1><p><code>model.add(Dense(...))</code></p>
<h1 id="dropout-2"><a href="#dropout-2" class="headerlink" title="dropout"></a>dropout</h1><p><code>model.add(Dropout(0.25))</code></p>
<p>//定义model后可直接加入多种层进行操作，同样其需要定义训练函数<br>//定义优化过程<br><code>from keras.optimizers import SGD</code></p>
<h1 id="定义迭代算法"><a href="#定义迭代算法" class="headerlink" title="定义迭代算法"></a>定义迭代算法</h1><p><code>sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd)</code></p>
<h1 id="训练过程-1"><a href="#训练过程-1" class="headerlink" title="训练过程"></a>训练过程</h1><p><code>model.fit(x_train, y_train, batch_szie=32, epochs=10)</code></p>
<h1 id="评估训练效果"><a href="#评估训练效果" class="headerlink" title="评估训练效果"></a>评估训练效果</h1><p><code>score = model.evaluate(x_test, y_test, batch_size=32)</code></p>
<p>//完整代码<br><code>import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.optimizers import SGD</code></p>
<h1 id="这里utils为自己定义的库函数，用于载入数据"><a href="#这里utils为自己定义的库函数，用于载入数据" class="headerlink" title="这里utils为自己定义的库函数，用于载入数据"></a>这里utils为自己定义的库函数，用于载入数据</h1><p><code>import utils
X, Y, testX, testY = utils.load_data(one_hot=True)
model = Sequential()</code></p>
<h1 id="定义神经网络过程"><a href="#定义神经网络过程" class="headerlink" title="定义神经网络过程"></a>定义神经网络过程</h1><p><code>model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))</code></p>
<p><code>model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Conv2D(64, (3, 3), activatin='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))</code></p>
<h1 id="展开为一维数据用于全连接层"><a href="#展开为一维数据用于全连接层" class="headerlink" title="展开为一维数据用于全连接层"></a>展开为一维数据用于全连接层</h1><p><code>model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))</code></p>
<h1 id="梯度迭代算法"><a href="#梯度迭代算法" class="headerlink" title="梯度迭代算法"></a>梯度迭代算法</h1><p><code>sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd)</code></p>
<h1 id="训练过程-2"><a href="#训练过程-2" class="headerlink" title="训练过程"></a>训练过程</h1><p><code>model.fit(x_train, y_train, batch_size=32, epochs=10)</code></p>
<h1 id="效果评估"><a href="#效果评估" class="headerlink" title="效果评估"></a>效果评估</h1><p><code>score = model.evaluate(x_test, y_test, batch_size=32)</code></p>
<p>4.3 Tensorboard与问题监控<br>//tensorboard最重要的作用就在于观察训练过程中的各种问题并改善，包括梯度消失、过拟合等问题<br>//获取所有可训练的参数<br><code>var_list_w = [var for var in tf.trainable_variables() if 'w' in var.name]
var_list_b = [var for var in tf.trainable_variables() if 'b' in var.name]</code></p>
<p>//利用定义的梯度算法来计算梯度<br><code>gradient_w = optimizer.compute_gradients(loss, var_list=var_list_w)
gradient_b = optimizer.compute_gradients(loss, var_list=var_list_b)</code></p>
<p>//返回的梯度是一个列表，可对其进行各种列表操作<br>//加入summary操作<br><code>for idx, itr_g in enumerate(gradient_w):
    variable_summaries(itr_g[0], 'layer%d-w-grad'%idx)
for idx, itr_g in enumerate(gradient_b):
    variable_summaries(itr_g[0], 'layer%d-b-grad'%idx</code></p>
<p><code>for idx, itr_g in enumerate(var_list_w):
    variable_summaries(itr_g, 'layer%d-w-grad'%idx)
for idx, itr_g in enumerate(var_list_b):
    variable_summaries(itr_g, 'layer%d-b-grad'%idx)</code></p>
<p>4.4改善深度神经网络<br>//出现梯度消失一种最有效的方式就是进行BN操作<br>//batchnorm层<br><code>net = tf.contrib.layers.batch_norm(net)</code></p>
<p>//加入BN层的神经网络</p>
<h1 id="对于sigmoid激活函数来讲，BN操作效果可能不理想"><a href="#对于sigmoid激活函数来讲，BN操作效果可能不理想" class="headerlink" title="对于sigmoid激活函数来讲，BN操作效果可能不理想"></a>对于sigmoid激活函数来讲，BN操作效果可能不理想</h1><p><code>net = slim.fully_connected(x, 4, activation_fn=tf.nn.sigmoid, scope='full1', reuse=False)
net = tf.contrib.layers.batch_norm(net)</code></p>
<p><code>net = slim.fully_connected(net, 8, activation_fn=tf.nn.sigmoid, scope='full2', reuse=False)
net = tf.contrib.layers.batch_norm(net)</code></p>
<p><code>net = slim.fully_connected(net, 8, activation_fn=tf.nn.sigmoid, scope='full3', reuse=False)
net = tf.contrib.layers.batch_norm(net)</code></p>
<p><code>net = slim.fully_connected(net, 4, activation_fn=tf.nn.sigmoid, scope='full4', reuse=False)
net = tf.contrib.layers.batch_norm(net)</code></p>
<p><code>net = slim.fully_connected(net, 3, activation_fn=tf.nn.sigmoid, scope='full5', reuse=False)</code></p>
<p><code>loss = tf.reduce_mean(tf.square(y-label))</code></p>
<p>4.5性能优化建议<br>//训练前的优化技巧<br>1.网络结构优化<br>Relu和BN能够有效加快神经网络训练速度<br>卷积核心的选取可以从大的卷积核心修改为多个小的卷积核心<br>将nxn修改为nx1+1xn，减少参数量，不同的输出内容之间可以进行concat<br>引入跨层支路解决梯度问题（ResNet)<br>2.初始值的选取<br>不好的初始值对训练的影响非常大，有效的初始化方法包括xavier初始化方法和He初始化方法<br>3.数据预处理<br>包括去均值和方差均衡<br>//训练过程中的优化技巧<br>1）优化算法的选择<br>Adam<br>2）学习率的选取<br>从大的步长开始进行迭代，逐步减少学习率<br>3）Batchsize选择<br>4）model ensembles<br>使用不同初始值同时训练多个模型，预测过程中将多个模型输出结果做平均，有效提升结果精度<br>5)dropout选择<br>从0.5附近进行调整，调整步长为0.05左右</p>
<p>//物体检测<br>1.传统检测方法<br>2001年，基于Haar特征和Adaboost检测方法引起轰动<br>2012年之前，三方面不断创新与优化：特征的设计更新、检测窗口的选择、分类器的设计更新</p>
<p>2.深度学习的物体检测<br>1）基于分类的物体检测<br>处理过程：图像被分解成多个小区域，每个小区域将运行一个分类算法以确定区域是否包含待检测物体，之后再在这个小区域的周围确认物体的边界框。代表算法：R-CNN、Fast-RCNN、Faster-RCNN<br>2) 基于回归的物体检测<br>将问题建模为回归问题，通过深度神经网络直接预测出边界框和所属类别的置信度。代表算法：SSD、YOLO模型</p>
<p>//YOLO模型<br>官网：<a href="https://pjreddie.com/darknet/yolo/">https://pjreddie.com/darknet/yolo/</a><br>//选讲tiny YOLO v1模型，由9个卷积层和3个全连接层组成，每个卷积层都由卷积层、LeakyRelu和Max Pooling操作组成，前9个卷积层可被理解为特征提取器，最后三个全连接层可被理解为预测边界框的回归器。<br>参考论文：You Only Look Once:Unified, Real-Time Object Detection<br>参考实例：<a href="https://github.com/xslittlegrass/CarND-Vehicle-Detection">https://github.com/xslittlegrass/CarND-Vehicle-Detection</a><br>模型参数：45089374<br>深度学习框架：Keras 1.2.2</p>
<p>//构建YOLO模型网络结构<br><code>import keras
from keras.models import Sequential
from keras.layers.convolutional import Convlution2D, MaxPooling2D
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.core import Flatten, Dense, Activation, Reshape
from utils import load_weights, Box, yolo_net_out_to_car_boxes, draw_box
def construct_yolo_model():
    keras.backend.set_image_dim_ordering('th')
    model = Sequential()
    model.add(Convolution2D(16, 3, 3, input_shape=(3, 448, 448), border_mode='same', subsample=(1, 1)))
    model.add(LeakyReLU(alpha=0.1))
    model.add(MaxPooling2D(pool_size=(2, 2)))</code></p>
<pre><code>model.add(Convolution2D(32, 3, 3, border_mode='same'))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid'))

model.add(Convolution2D(32, 3, 3, border_mode='same'))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid'))

model.add(Convolution2D(64, 3, 3, border_mode='same'))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid'))

model.add(Convolution2D(128, 3, 3, border_mode='same'))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid'))

model.add(Convolution2D(256, 3, 3, border_mode='same'))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid'))

model.add(Convolution2D(512, 3, 3, border_mode='same'))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid'))

model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(LeakyReLU(alpha=0.1))

model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(LeakyReLU(alpha=0.1))

model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(LeakyReLU(alpha=0.1

model.add(Flatten())
model.add(Dense(256))
model.add(Dense(4096))
model.add(LeakyReLU(alpha=0.1))
model.add(Dense(1470))
model.summary()
return model
</code></pre><p>注：网络的输入是形状为（3,448,448)的图像，其输出是一个1470维度的向量，它包含预测边界框、物体类别信息。1470矢量被分成三个部分，分别给出了所属类别概率、置信度和边框坐标。这三个部分进一步划分为49个小区域，与每个单元的预测相对应。<br>输出向量信息组织方式：<br><code>probability:49*20=980</code><br>判断类别，20个类别<br><code>confidence:49*2=98</code><br>是否包含物体（0,1）<br><code>box coordinates:49*8=392
(x_min,y_min,x_max,y_max),(c_x,c_y,w,h)</code></p>
<p>8.4.3车辆图像数据探索<br>1.车辆视频数据预处理<br>//预处理及可视化图像<br><code>def visualize_images():
    imagePath = './test_images/test1.jpg'
    image = plt.imread(imagePath)</code></p>
<pre><code>#去除顶部和底部图片
`image_crop = image[300:650,500:,:]`
#将图片转换为模型所需要的输入格式
`resized = cv2.resize(image_crop, (448, 448))
f1,(ax11,ax22,ax33) = plt.subplot(1, 3, figsize=(16, 6))
ax11.imshow(image)
ax22.imshow(image_crop)
ax33.imshow(resized)
pylab.show()
return resized`
</code></pre><p>8.4.5迁移学习<br>通过迁移学习加载使用Pre-trained YOLO模型进行行车检测。具体做法是将pre-trained模型中的权重加载进之前构造的模型结构中，官网提供的权重，可以通过脚本解析成Keras能够加载的格式。<br>//加载YOLO模型权重<br>`def load_model_weights(model):</p>
<pre><code>#预训练权重网址：https://pjreddie.com/darknet/yolo/
load_weights(model, './yolo-tiny.weights')`
</code></pre><p>//加载模型权重的具体逻辑<br><code>def load_weights(model, yolo_weight_file):
    data = np.fromfile(yolo_weight_file, np.float32)
    data = data[4:]
    index = 0
    for layer in model.layers:
        shape = [w.shape for w in layer.get_weights()]
        if shape !=[]:
            kshape, bshape = shape
            bia = data[index:index+np.prod(bshape)].reshape(bshape)
            index += np.prod(bshape)
            ker = data[index:index:index+np.prod(kshape)].reshape(kshape)
            index += np.prod(kshape)
            layer.set_weights([ker, bia])</code></p>
<p>//模型推断<br>//使用模型进行在线推断，预测出车辆区域<br>`def inference_image(model, resized):</p>
<pre><code>#转置
batch = np.transpose(resized, (2, 0, 1))
#将像素值变换到-1~1
batch = 2*(batch/255.) - 1
#将一张图片转为数组
batch = np.expand_dims(batch, axis=0）
out = model.predict(batch)
return out`
</code></pre><p>//绘制检测结果<br>//将上述的预测结果转换为边框坐标，同时基于阈值进行预测<br><code>th = 0.17
boxes = yolo_net_to_out_to_car_boxes(out[0], threshold=th)</code></p>
<p>//定义box边框对象，判断是否保留预测的边框结果通过c,在图像上绘制车辆位置通过对象中的坐标信息</p>
<h1 id="定义box类，存储边框信息和物体检测类别等信息"><a href="#定义box类，存储边框信息和物体检测类别等信息" class="headerlink" title="定义box类，存储边框信息和物体检测类别等信息"></a>定义box类，存储边框信息和物体检测类别等信息</h1><p>`class Box:<br>def <strong>init</strong>(self):</p>
<pre><code>#x, y轴坐标
self.x, self.y = float(), float()
#边框宽度和长度
self.w, self.h = float(), float()
#置信度
self.c = float()
#所属类别概率
self.prob = float()`
</code></pre><p>//通过yolo_net_to_out_to_car_boxes方法，将预测出的Vector转换为Box对象信息。其核心逻辑是解析模型预测输出向量中的坐标、类别和置信度信息<br>//置信度大于阈值边界框则进行保留<br><code>class_num = 6 #yolo模型可以预测多种类别，6为车辆所属类别
p = probs[grid, :] *bx.c
if p[class_num]&gt;=threshold:
    bx.prob = p[class_num]
    boxes.append(bx)</code></p>
<p>//将结果绘制在图像上<br><code>def visualize_image_car_detection(boxes):
    imagePath = './test_images/test1.jpg'
    image = plt.imread(imagePath)
    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    ax1.imshow(image)
    ax2.imshow(draw_box(boxes, plt.imread(imagePath), [[500, 1280],[300,650]]))
    pylab.show()</code></p>
<p>//将边框绘制在图像上<br><code>def draw_box(boxes, im, crop_dim):
    imgcv = im
    [xmin, xmax] = crop_dim[0]
    [ymin, ymax] = crop_dim[1]
    for b in boxes:
        h, w, _ = imgcv.shape
        left = int((b.x-b.w/2.)*w)
        right = int((b.x+b.w/2.)*w)
        top = int((b.y-b.h/2.)*h)
        bot = int((b.y+b.h/2.)*h)
        left = int(left*(xmax-xmin)/w+xmin)
        right = int(right*(xmax-xmin)/w+xmin)
        top = int(top*(ymax-ymin)/h+ymin)
        bot = int(bot*(ymax-ymin)/h+ymin)
        if left&lt;0 : left=0
        if right&gt;w-1 : right=w-1
        if top&lt;0 : top=0
        if bot&gt;h-1 : bot=h-1
        thick = int((h+w)//150)
        cv2.rectangle(imgcv, (left, top), (right, bot), (255,0,0), thick)
    return imgcv</code></p>
<p>8.5.1英伟达End to End模型<br>End to End的好处：通过缩减人工预处理和后续处理，尽可能使模型从原始输入到输出，使得其根据网络模型能够有足够多的空间进行自动调节，从而减少基于规则的复杂变化。<br>缺点：可解释性较差，准确度和精度不容易受控制。<br>//构建英伟达模型<br><code>import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten, Lambda
from keras.layers import Conv2D, Dropout
from keras import losses
def nvida_model():
    model = Sequential()
    model.add(Lambda(lambda x: x/127.5-1., input_shape=(img_height, img_width, img_channels)))
    model.add(Conv2D(24, kernel_size=(5, 5), strides=(2, 2), padding='valid', kernel_initializer='he_normal', activation='elu'))
    model.add(Conv2D(36, kernel_size=(5, 5), strides=(2, 2), padding='valid', kernel_initializer='he_normal', activation='elu'))
    model.add(Conv2D(48, kernel_size=(5, 5), strides=(2, 2), padding='valid', kernel_initializer='he_normal', activation='elu'))
    model.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='valid', kernel_initializer='he_normal', activation='elu'))
    model.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='valid', kernel_initializer='he_normal', activation='elu
    model.add(Flatten())
    model.add(Dense(1164, kernel_initializer='he_normal', activation='elu'))
    model.add(Dense(100, kernel_initializer='he_normal', activation='elu'))
    model.add(Dense(50, kernel_initializer='he_normal', activation='elu'))
    model.add(Dense(10, kernel_initializer='he_normal', activation='elu'))
    model.add(Dense(1, kernel_initializer='he_normal'))
    model.compile(loss='mse', optimizer='Adadelta')
    return model</code></p>
<p>//8.5.3数据分析<br>1）转向控制数据分布</p>
<h1 id="绘制转向分布"><a href="#绘制转向分布" class="headerlink" title="绘制转向分布"></a>绘制转向分布</h1><p><code>def steering_distribution():
    wheel_sig = pd.read_csv(params.data_dir+'/epoch01_steering.csv')
    wheel_sig.head()
wheel_sig.wheel.hist(bins=50)</code></p>
<p>2)数据变化幅度</p>
<h1 id="绘制转向变化幅度"><a href="#绘制转向变化幅度" class="headerlink" title="绘制转向变化幅度"></a>绘制转向变化幅度</h1><p><code>def angel_visualize():
    wheel_sig = pd.read_csv(params.data_dir+'/epoch01_steering.csv')
    wheel_sig.plot(x='frame', y='wheel')
    plt.show()</code></p>
<p>8.5.4读入视频，并处理图像<br>//使用OpenCV从视频中提取图像，以及与其对应的转向角度并返回</p>
<h1 id="提取图像并处理"><a href="#提取图像并处理" class="headerlink" title="提取图像并处理"></a>提取图像并处理</h1><p>`imgs = []<br>wheels = []<br>epochs = [10]<br>for epoch in epochs:<br>    vid_path = utils.join_dir(params.data_dir, ‘epoch{:0&gt;2}_front.mp4’.format(epoch))<br>    assert os.path.isfile(vid_path)<br>    frame_count = frame_count_func(vid_path)<br>    cap = cv2.VideoCapture(vid_path)<br>    for frame_id in range(frame_count):<br>        while True:</p>
<pre><code>    #通过OpenCV中的VideoCapture进行视频中图像的提取
        ret, img = cap.read()
        if not ret:
            break
        #用户可以自定义对图像的处理、扩展和增强操作
        img = a_image_convert.img_preprocess(img, color_mode, flip=False)
        imgs.append(img)
    csv.path = os.path.join(data_dir, 'epoch{:0&gt;2}_steering.csv'.format(epoch))
    rows = pd.read_csv(csv.path)
    yy = rows['wheel'].values
    wheels.extend(yy)
    cap.release()
imgs = np.array(imgs)
wheels = np.array(wheels)
wheels = np.reshape(wheels, (len(wheels), 1)
return imgs, wheels`
</code></pre><p>8.5.5深度学习模型构建与训练<br>//训练模型<br>`def training(model, X_train_RGB, y_train_RGB):<br>    RGB_model = model<br>    time_start = time.time()</p>
<pre><code>#fit the model
RGB_history = RGB_model.fit(X_train_RGB, y_train_RGB, epochs=30, batch_size=batch_size)
return RGB_model, RGB_history`
</code></pre><p>//可视化结果</p>
<h1 id="将训练过程中的loss误差进行可视化"><a href="#将训练过程中的loss误差进行可视化" class="headerlink" title="将训练过程中的loss误差进行可视化"></a>将训练过程中的loss误差进行可视化</h1><p><code>def visualize_label(RGB_history):
    print(RGB_history.history['loss']
    plt.figure(figsize=(9, 6))
    plt.plot(RGB_history.history['loss'])
    plt.title('model loss')
    plt.ylabel('Loss', fontsize=12)
    plt.xlabel('Epoch', fontsize=12)
    plt.legend(['train RGB'], loc='upper right')
    plt.grid()
    plt.show()</code></p>
<p>//可视化<br>//数据的绘图过程就是将前面所得到的一系列数据，通过静态、动态的二维、三维图形进行展示<br>1.Matplotlib<br>//绘制y=sinx图像<br><code>import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0, 4*np.pi, 1000)
y = np.sin(x)
plt.plot(x,y)</code></p>
<p>//利用API,绘制更加审美要求的图像<br>`import numpy as np<br>import matplotlib.pyplot as plt<br>import matplotlib as mpl</p>
<h1 id="设置图片风格"><a href="#设置图片风格" class="headerlink" title="设置图片风格"></a>设置图片风格</h1><p>mpl.style.use(‘seaborn-darkgrid’)</p>
<h1 id="定义曲线"><a href="#定义曲线" class="headerlink" title="定义曲线"></a>定义曲线</h1><p>x = np.linspace(0, 4*np.pi, 100)<br>y1 = np.sin(x)<br>y2 = np.sin(x+1)<br>y3 = np.sin(x+2)</p>
<h1 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a>绘图</h1><p>plt.plot(x, y1, color=’#009900’, lw=6, alpha=0.6)<br>plt.plot(x, y2, color=’#990000’, lw=6, alpha=0.6)<br>plt.plot(x, y3, color=’#000099’, lw=6, alpha=0.6)</p>
<h1 id="展示"><a href="#展示" class="headerlink" title="展示"></a>展示</h1><p>plt.show()`</p>
<p>9.4ECharts<br>//ECharts提供了常规的折线图、柱状图、散点图、饼图、K线图等等，功能强大。<br>//ECharts图形绘制<br>略</p>
<p>//文本向量化<br>//文本向量化函数</p>
<h1 id="文本TfIdf向量化"><a href="#文本TfIdf向量化" class="headerlink" title="文本TfIdf向量化"></a>文本TfIdf向量化</h1><p><code>from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidVectorizer()
vectors = vectorizer.fit_transform(datas)</code></p>
<p>//文本向量化的数据进行降维<br>//LDA降维<br><code>from sklearn.decomposition import LatentDirichletAllocation
lda = LatenDirichletAllocation(n_components=n_topic, max_iter=5,
    learning_method = 'online',
    learning_offset = 50.,
    radom_state = 0)</code></p>
<h1 id="用LDA方法降维数据"><a href="#用LDA方法降维数据" class="headerlink" title="用LDA方法降维数据"></a>用LDA方法降维数据</h1><p><code>dr_vectors = lad.fit_transform(vectors)</code></p>
<p>9.6三维可视化<br>//ECharts地图柱状图<br><code>myChart.setOption({
    visualMap: {
        show: flase,
        calculable: true,
        realtime: false,
        inRange: {
        color: ['#313695', '#4575b4', '#74add1', '#abd9e9',
                '#e0f3f8', '#ffffbf', '#fee090', '#fdae61',
                '#f46d43', '#d73027', '#d73027', 'a50026']
                },
        outOfRange: {
            colorAlpha: 0
            },
        max: linedata[1]
        },
        ...
        series: [{
            type: 'bar3D',
            shading: 'realistic',
            coordinateSystem: 'mapbox',
            barSize: 0.2,
            silent: true,
            data: linedata[0]
            }]
        });</code></p>
<p>//利用Matplotlib完成对三维数据的可视化任务<br><code>from mpl_toolkits.mplot3d import axes3d
import matplotlib.pyplot as plt
from matplotlib import cm
import matplotlib.style as style
style.use('seaborn-darkgrid')</code></p>
<h1 id="定义三维画布"><a href="#定义三维画布" class="headerlink" title="定义三维画布"></a>定义三维画布</h1><p><code>fig = plt.figure()
ax = fig.gca(projection='3d')</code></p>
<h1 id="获取数据-1"><a href="#获取数据-1" class="headerlink" title="获取数据"></a>获取数据</h1><p><code>X, Y, Z = axes3d.get_test_data(0.05)</code></p>
<h1 id="绘制surface"><a href="#绘制surface" class="headerlink" title="绘制surface"></a>绘制surface</h1><p><code>ax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.3)</code></p>
<h1 id="绘制等值线"><a href="#绘制等值线" class="headerlink" title="绘制等值线"></a>绘制等值线</h1><p><code>cst = ax.contourf(X, Y, Z, zdir='z', offset=-100, cmap=cm.coolwarm)
cst = ax.contourf(X, Y, Z, zdir='x', offset=-40, cmap=cm.coolwarm)
cst = ax.contourf(X, Y, Z, zdir='y', offset=40, cmap=cm.coolwarm)</code></p>
<p><code>plt.show()</code></p>
<p>9.7动态可视化<br>//Matplotlib中用于数据动态演示的方法为animation,其可以通过函数进行简单的调用，以进行动态图形的演示工作<br>//动画展示<br><code>import matplotlib.animation as animation
animation.FuncAniamtion(
    ...
    )</code></p>
<p>//动态可视化的展示方式是在普通的图表之上通过不断地修改数据并进行展示，这种修改可以通过setOption而得到的，在实现上可以通过函数递归的方式实现动态数据的可视化工作<br><code>function update(){
    myChart.setOption(...);
        setTimeout(update, UPDATE_DURATION);
        }
update();</code></p>
<p>//优化实践<br>10.1通用深度神经网络训练优化建议</p>
<p>1）通用的较为优化的训练过程<br>1.将问题转换为相似的经典问题场景，参照paper中的配置和调优技巧进行最初的实验与优化<br>2.优化算法：选用随机梯度下降（SGD)算法，虽然批量梯度下降（BGD)相比SGD有一些优势，但是在处理大规模数据时，SGD及其优化变种更加简单和快速。<br>3.随机Shuffle样本：应尽量避免连续处理的样本属于同一类别的情况。尽量选择当前样本能让模型产生较大的误差，而不是较小的误差<br>4.规范化数据：输入的每个变量均值最好趋近于0.变换输入变量，使其协方差相近，变量间尽量不要相关<br>5.激活函数的选取：相比Sigmoid函数，tanh和Relu有更好的收敛速度。<br>6.权重初始化：可以随机通过一种分布，均值为0.<br>7.选择学习率：每个权重都可以选取属于自己的学习率。处于低层的权重学习率最好大于高层的权重学习率。学习率最好正比于每个单元的输入数量。</p>
<p>2）CNN训练过程中通常关注的优化点和参数<br>一般比较关注：Learning Rate,Weight Decay,Momentum,Batchsize,Init Weights,数据增强<br>eg:在Resnet中，使用SGD优化算法优化方法训练，mini-batch的大小设置为256，学习率初始化为0.1.随着训练进行，当Loss不再下降，会每次自适应以10倍进行缩减学习率。模型训练用了60x10^4轮迭代。Weight Decay设置为0.0001，同时设置momentum为0.9</p>
<p>3)RNN训练过程中通常关注的优化点和参数<br>一般比较关注：SGD,正则化，规范化梯度，Pad Sentence,Init Weight, Batch Size, Embedding输入，输出控制，Vacabulary Size, Sampled Softmax<br>eg:Google发布的TTS模型TACOTRON为例</p>
<p>10.1.1 过拟合和欠拟合<br>欠拟合：若训练集和测试集的误差有收敛但很高时，则为高偏差<br>过拟合：若训练集和测试集的误差较大时，则为高方差</p>
<p>解决过拟合的方法：<br>正则化，数据增强，Early Stop, Dropout, Batch Normalization</p>
<p>解决欠拟合的方法：<br>1.使用更加复杂的深度学习网络架构<br>2.添加其他特征项，有时候模型出现欠拟合的情况是因为特征项不够导致的，可以添加其他特征项来很好的解决这个问题<br>3.减少正则化参数和组件，正则化的目的是用来防止过拟合。</p>
<p>10.1.2数据增强<br>//数据增强的根本原因在于机器在学习的过程中会在模型中遇到大量的参数，同时为了防止过拟合<br>1）对于图像数据，可采取：<br>1.图像平移：使得网络学习到平移不变的特性<br>2.图像旋转：使得网络学习到旋转不变的特性<br>3.图像亮度变化<br>4.裁剪<br>5.缩放<br>6.图像模糊:用不同的卷积模板产生模糊图像<br>2）语音识别中对输入数据添加随机噪声等方式<br>3）NLP中最常用的方式就是进行近义词替换等方式<br>4）噪声注入，可以对输入添加噪声，也可以对隐藏层或者输出层添加噪声</p>
<p>10.1.3梯度消失<br>//实验数据显示了深度神经网络在训练过程中，随着epoch的增加各隐藏层的学习率变化。前面隐藏层的学习速度要低于后面的隐藏层<br>//梯度消失的原因：根据链式法则，如果每一层神经元对上一层输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层的传播后，误差对输入层的偏导也会趋近于0<br>解决梯度消失的策略：<br>1.BN<br>2.RNN中使用LSTM:适用于RNN,门控制和长时记忆可缓解和解决梯度消失问题<br>3.激活函数Relu:新的激活函数解析性质更好，其在一定程度上克服了sigmoid函数和tanh函数的梯度消失问题。<br>4.在RNN反向传播过程中减少时间步长度。</p>
<p>10.1.4初始化权重<br>//在参数解空间内，好的权重初始化方式，意味着离全局最小值更近。<br>1.高斯初始化，为权重初始化较小的值，权重按照高斯分布随机进行初始化，固定均值和方差<br>2.Xaiver更新方法，使用tanh为激活函数，效果较好。进行梯度更新时，收敛速度较快，然而没有考虑Relu<br>3.MSRA方法，适用于从头训练深层深度神经网络的网络结构。权重以高斯分布随机进行初始化，方差需要考虑空间过滤器的大小和过滤器数量的影响。</p>
<p>10.1.5优化算法<br>近些年最常用的是采用Adam优化算法，也可以采用自适应学习率的方法实现快速收敛。</p>
<p>10.1.6超参数选择<br>一些实践经验：<br>1.在验证集上进行调参<br>2.优先调Learning Rate<br>3.通过初期设计卷积层尽量深、卷积核尽量多的模型，强行让模型拟合训练集，这时会出现过拟合，之后通过Dropout、正则化和Data Augument等等方式去改善模型结果<br>4.调整模型的层数和卷积核数量</p>
<p>//通过Scikit-learn的网格搜索库进行参数调优实例<br>1.常见搜索参数<br>学习率、Dropout、Epochs和神经元数量<br>2.数据集下载<br>数据集为Pima Indians Onset of Diabetes分类数据集<br>下载地址：<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/">https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/</a><br>3.搜索最优batchsize和epochs<br>//以20的步长，从10到100逐步评估不同的微型批尺寸，epochs分别设置为10、50、100<br><code>import numpy
from sklearn.grida_search import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier</code></p>
<h1 id="Function-to-create-model-required-for-KerasClassifier"><a href="#Function-to-create-model-required-for-KerasClassifier" class="headerlink" title="Function to create model, required for KerasClassifier"></a>Function to create model, required for KerasClassifier</h1><p>`def create_model():</p>
<pre><code>#create model
model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))`


#compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
return model
</code></pre><p>`#fix random seed for reproducibility<br>seed = 7<br>numpy.random.seed(seed)</p>
<h1 id="load-dataset"><a href="#load-dataset" class="headerlink" title="load dataset"></a>load dataset</h1><p>dataset = numpy.loadtxt(“pima-indians-diabetes.csv”, delimiter=’,’)</p>
<h1 id="split-into-input-x-and-output-Y-variables"><a href="#split-into-input-x-and-output-Y-variables" class="headerlink" title="split into input (x) and output (Y) variables"></a>split into input (x) and output (Y) variables</h1><p>X = [:, 0:8]<br>Y = [:, 8]</p>
<h1 id="create-model"><a href="#create-model" class="headerlink" title="create model"></a>create model</h1><p>model = KerasClassifier(build_fn=create_model, verbose=0)</p>
<h1 id="define-the-grid-search-parameters"><a href="#define-the-grid-search-parameters" class="headerlink" title="define the grid search parameters"></a>define the grid search parameters</h1><p>batch_size = [10, 20, 40, 60, 80, 100]<br>epochs = [10, 50, 100]<br>param_grid = dict(batch_size=batch_size, nb_epoch=epochs)<br>grid = GridSearchCV(estimator=model, param_grid=parm_grid, n_jobs=-1)<br>grid_result = grid_fit(X,Y)</p>
<h1 id="summarize-results"><a href="#summarize-results" class="headerlink" title="summarize results"></a>summarize results</h1><p>print(“Best: %f using %s” % (grid<em>result.best_score</em>, grid<em>result.best_params))<br>for params, mean_score, scores in grid_result.grid_scores</em>:<br>    print(“%f (%f) with: %r” % (scores.mean(), scores.std(), params))`</p>
<p>10.2深度学习系统性能优化建议<br>10.2.1输入及预处理流水线优化<br>输入流水线：从磁盘读取图像，将JPEG预处理为张量，进行数据预处理裁剪、翻转等，然后进行批处理操作<br>1.在CPU端进行预处理<br>//在CPU端上放置输入预处理操作可以显著提高性能，GPU专注训练<br>//控制代码在CPU端执行<br>`with tf.device(“/cpu:0”):</p>
<pre><code># function to get and process data.
</code></pre><p>​    distored_inputs = load_and_preprocess_images()`</p>
<p>2.使用大文件<br>读取大量的小文件会显著影响I/O性能<br>1）转换为TFRecord格式<br>2）小数据集加载到内存</p>
<p>10.2.2数据格式<br>NHWC的方存局部性更好（每三个输入像素即可得到一个输出像素），NCHW则必须等所有通道输入都准备好后才能得到最终的输出结果，需要占用较大的临时空间。<br>tf默认NHWC格式，Nvidia cuDNN默认NCHW格式<br>注：设计网络时充分考虑这两种格式，最好能够灵活切换，在GPU上训练时使用NCHW格式，在CPU上做预测时使用NHWC格式</p>
<p>10.2.3编译优化<br>//通过bazel命令对特定平台对tf进行编译<br><code>bazel build -c opt --copt=-march="brodewell" --config=cuda
//tensorflow/tools/pip_package:build_pip_package</code></p>
<p>10.2.4GPU性能瓶颈诊断<br>//参考如下分析步骤对作业进行优化<br>1）对代码进行性能分析<br>2）找到运行慢的阶段<br>3）分析慢的原因<br>4）修改成更快的实现<br>5）再次对代码进行性能分析</p>
<p>//处理器有两个关键的性能瓶颈：浮点计算量和内存吞吐量。<br>//可通过以下工具进行深度学习作业的性能分析<br>1.Tensorflow性能分析工具Timeline(获取执行图中每个节点的执行时间）<br>1）创建metadata运行时对象<br>2）获取运行时信息创建Timeline对象<br>3）将Timeline对象写入json文件<br>4）Chrome加载trace的json文件</p>
<p>//tensorflow使用Timeline进行性能分析<br><code>import tensorflow as tf
from tensorflow.python.client import timeline</code></p>
<p><code>x = tf.random_normal([1000, 1000])
y = tf.random_normal([1000, 1000])
res = tf.matmul(x, y)</code></p>
<p><code>#run the graph with full trace option
with tf.Session() as sess:
    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
    run_metadata = tf.RunMetadata()
    sess.run(res, options=run_options, run_metadata=run_metadata)</code></p>
<pre><code>#create Timeline variable，then write it into json file
t1 = timeline.Timeline(run_metadata.step_stats)
ctf = t1.generate_chrome_trace_format()
with open('timeline.json', 'w') as f:
    f.write(ctf)
</code></pre><p>可以打开谷歌chrome浏览器，转到chrome://tracing页并加载timeline.json文件，接下来，可以进行程序的profiling</p>
<p>2.常用的GPU分析工具<br>1）nvprof是英伟达性能分析工具<br>2）nvvp则是带GUI的英伟达可视化性能分析工具</p>
<p>10.2.5CPU瓶颈优化<br>1）多线程方式优化<br>以下两个针对tensorflow的配置可以通过适配线程池进行CPU的性能优化<br>intra_op_parallelism_threads：对tf操作符内部的任务进行并行化<br>inter_op_parallelism_threads: 控制多个运算符之间的并行化运算<br>//多线程优化<br><code>config = tf.ConfigProto()
config.intra_op_parallelism_threads = 22
config.inter_op_parallelism_threads = 22
tf.session(config=config)</code></p>
<p>2)使用SIMD高级指令集<br>参考tf官方文档的”Performance Guide”章节</p>
<p>10.2.6模型压缩<br>模型小型化：从模型权重的角度进行压缩和从网络架构的角度进行压缩<br>网络架构角度：提出新的网络结构或卷积方法进行压缩优化，如SqueezeNet, MobileNets等<br>模型权重角度：一般是在已经训练好的模型上进行裁剪，然后fine-tuning到原有模型的准确率，一般的优化方式包括剪枝、权值共享、神经网络二值化等。</p>
<p>10.3工程实践建议<br>10.3.1Model格式转换<br>框架间的模型转换<br>参考链接：<br>1.<a href="https://github.com/ysh329/deep-learning-model-convertor">https://github.com/ysh329/deep-learning-model-convertor</a><br>2.<a href="https://github.com/Microsoft/MMdnn">https://github.com/Microsoft/MMdnn</a></p>
<p>10.3.2迁移学习（Transfer Learning)<br>其思想是将训练好的模型参数迁移到新的模型来帮助新模型的训练和预测。</p>
<p>//通过MNIST数据集0~4的数字训练一个模型，然后将模型迁移到5~9数据集上进行迁移学习<br>1）在MNIST数据集上训练一个简单的卷积神经网络，只预测0~4的数字<br>2）将训练好的预测0~4数据集的模型，应用到5~9数据集上。对模型冻结卷积层参数，Fine-Tuning全连接层。<br>//keras迁移学习实例<br><code>from __future__ import print_function
import datetime
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K</code></p>
<p><code>now = datetime.datetime.now
batch_size = 128
num_classes = 5
epochs = 5</code></p>
<p>`#input images dimensions<br>img_rows, img_cols = 28, 28</p>
<h1 id="number-of-convolutional-filters-to-use"><a href="#number-of-convolutional-filters-to-use" class="headerlink" title="number of convolutional filters  to use"></a>number of convolutional filters  to use</h1><p>filters = 32</p>
<h1 id="size-of-pooling-area-for-max-pooling"><a href="#size-of-pooling-area-for-max-pooling" class="headerlink" title="size of pooling area for max pooling"></a>size of pooling area for max pooling</h1><p>pool_size = 2</p>
<h1 id="convolution-kernel-size"><a href="#convolution-kernel-size" class="headerlink" title="convolution kernel size"></a>convolution kernel size</h1><p>kernel_size = 3`</p>
<p><code>if K.image_data_format()=='channels_first':
    input_shape = (1, img_rows, img_cols)
else:
    input_shape = (img_rows, img_cols, 1)</code></p>
<p><code>def train_model(model, train, test, num_classes):
    x_train = train[0].reshape((train[0].shape[0],)+input_shape)
    x_test = test[0].reshape((test[0].shape[0],)+input_shape)
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train /= 255
    x_test /= 255
    print('x_train shape:', x_train.shape)
    print(x_train.shape[0], 'train samples')
    print(x_test.shape[0], 'test samples')</code></p>
<pre><code>#convert class vectors to binary class matrics
y_train = keras.utils.to_categorical(train[1], num_classes)
y_test = keras.utils.to_categorical(test[1], num_classes)

model.compile(
    loss = 'categorical_crossentropy',
    optimizer = 'adadelta',
    metrics = ['accuracy']
    )

t = now()
model.fit(x_train, y_train,
    batch_size = batch_size,
    epochs = epochs,
    verbose = 1,
    validation_data = (x_test, y_test))
print('Training time: %s' %(now() -t))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test score:', score[0])
</code></pre><p>`#the data,shuffled and spilt between train and test sets<br>(x_train, y_train), (x_test, y_test) = mnist.load-data()</p>
<h1 id="create-two-datasets-one-with-digits-below-5-and-one-with-5-and-above"><a href="#create-two-datasets-one-with-digits-below-5-and-one-with-5-and-above" class="headerlink" title="create two datasets one with digits below 5 and one with 5 and above"></a>create two datasets one with digits below 5 and one with 5 and above</h1><p>x_train_lt5 = x_train[y_train&lt;5]<br>y_train_lt5 = x_train[y_train&lt;5]<br>x_test_lt5 = x_test[y_test&lt;5]<br>y_test_lt5 = y_test[y_test&lt;5]`</p>
<p><code>x_train_get5 = x_train[y_train&gt;=5]
y_train_get5 = y_train[y_train&gt;=5]-5
x_test_get5 = x_test[y_test&gt;=5]
y_test_get5 = y_test[y_test&gt;=5]-5</code></p>
<p><code>#define two groups of layers:feature(convolutions) and classification(dense)
feature_layers = [
        Conv2D(filters, kernel_size,
            padding='valid',
            input_shape=input_shape),
        Activation('relu'),
        Conv2D(filters, kernel_size),
        Activation('relu'),
        MaxPooling2D(pool_size=pool_size),
        Dropout(0.5),
        Flatten()]
classification_layers=[
    Dense(128),
    Activation('relu'),
    Dropout(0.5),
    Dense(num_classes),
    Activation('softmax')]</code></p>
<p><code>#create complete model
model = Sequential(feature_layers+classification_layers)</code></p>
<p><code>#train model for 5-digit classification(0~4)
train_model(model,
    (x_train_lt5, y_train_lt5),
    (x_test_lt5, y_test_lt5), num_classes)</code></p>
<p><code>#freeze feature layers and rebuild model
for l in feature_layers:
    l.trainable = False</code></p>
<p><code>#transfer: train dense layers for new classification task(5~9)
train_model(model,
    (x_train_gte5, y_train_get5),
    (x_test_get5, y_test_get5), num_classes)</code></p>
<p>​<br>​<br>​    </p>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>开放图谱协议</title>
    <url>/zh-CN/%E5%BC%80%E6%94%BE%E5%9B%BE%E8%B0%B1%E5%8D%8F%E8%AE%AE/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://zhuanlan.zhihu.com/p/36072541">什么是 Open Graph 标签？不懂你还做什么社交营销优化？</a></p>
<p>2.<a href="https://smo.knowem.com/">https://smo.knowem.com/</a></p>
<p>3.<a href="https://hexo.io/zh-cn/docs/helpers#open-graph">辅助函数（Helpers） | Hexo</a></p>
<p>今天在继续配置我的博客时，发现有一个open_graph不懂，结果一查才知道这是facebook推的一个开放图谱协议，用于分享网站的时候用，于是抱着学习的目的，继续深挖下去</p>
<p>按照第一个链接的方法，我查到了我网站的开放协议配置情况</p>
<p><style>.dtlfuvdolllj{}</style><img src="/zh-CN/%E5%BC%80%E6%94%BE%E5%9B%BE%E8%B0%B1%E5%8D%8F%E8%AE%AE/%E5%BC%80%E6%94%BE%E5%9B%BE%E8%B0%B1%E5%8D%8F%E8%AE%AE/image-20220926204719850.png" class="lazyload" data-srcset="/zh-CN/%E5%BC%80%E6%94%BE%E5%9B%BE%E8%B0%B1%E5%8D%8F%E8%AE%AE/%E5%BC%80%E6%94%BE%E5%9B%BE%E8%B0%B1%E5%8D%8F%E8%AE%AE/image-20220926204719850.png" srcset="data:image/png;base64,666" class="dtlfuvdolllj lazyload"></p>
<p>和配置情况对比一下，是一样的</p>
<p>open graph protocol和_config.yml是一致的，twitter card info和主题volantis配置文件是一致的</p>
<figure class="highlight xml"><table><tbody><tr><td class="code"><pre><span class="line"># _config.volantis.yml文件</span><br><span class="line">open_graph:</span><br><span class="line">  image: volantis-static/media/org.volantis/blog/favicon/android-chrome-192x192.png # https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/favicon/android-chrome-192x192.png</span><br><span class="line">  twitter_card: summary # summary_large_image , summary</span><br></pre></td></tr></tbody></table></figure>
<p>如果想配置里面的内容，打开第三个链接</p>
<p><style>.gfmcwjhjvlqy{zoom:50%;}</style><img src="/zh-CN/%E5%BC%80%E6%94%BE%E5%9B%BE%E8%B0%B1%E5%8D%8F%E8%AE%AE/%E5%BC%80%E6%94%BE%E5%9B%BE%E8%B0%B1%E5%8D%8F%E8%AE%AE/image-20220926205301882.png" class="lazyload" data-srcset="/zh-CN/%E5%BC%80%E6%94%BE%E5%9B%BE%E8%B0%B1%E5%8D%8F%E8%AE%AE/%E5%BC%80%E6%94%BE%E5%9B%BE%E8%B0%B1%E5%8D%8F%E8%AE%AE/image-20220926205301882.png" srcset="data:image/png;base64,666" class="gfmcwjhjvlqy lazyload" alt="image-20220926205301882"></p>
<p>原来的配置实在太简陋了，分享了网站估计也没人看，那就开始配置吧</p>
<figure class="highlight xml"><table><tbody><tr><td class="code"><pre><span class="line">open_graph:</span><br><span class="line">  title: page.title</span><br><span class="line">  url: url</span><br><span class="line">  author: config.author</span><br><span class="line">  image:  # https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/favicon/android-chrome-192x192.png</span><br><span class="line">  twitter_card: summary # summary_large_image , summary</span><br></pre></td></tr></tbody></table></figure>
<p>结束了，以后如果还有改进再来补充</p>
]]></content>
      <categories>
        <category>Hexo博客搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>open_graph</tag>
      </tags>
  </entry>
  <entry>
    <title>微信小程序开发</title>
    <url>/zh-CN/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://www.w3cschool.cn/weixinapp/">微信小程序开发文档_w3cschool</a></p>
<p>2.<a href="https://mp.weixin.qq.com/">https://mp.weixin.qq.com/</a></p>
<p>3.<a href="https://developers.weixin.qq.com/miniprogram/dev/framework/">微信开放文档 (qq.com)</a></p>
<p>4.<a href="https://developers.weixin.qq.com/miniprogram/dev/devtools/download.html">微信开发者工具下载地址与更新日志 | 微信开放文档 (qq.com)</a></p>
]]></content>
      <categories>
        <category>开发日记</category>
      </categories>
      <tags>
        <tag>微信小程序</tag>
      </tags>
  </entry>
  <entry>
    <title>扁平时代的写作·节选</title>
    <url>/zh-CN/%E6%89%81%E5%B9%B3%E6%97%B6%E4%BB%A3%E7%9A%84%E5%86%99%E4%BD%9C/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>​        一个「扁平」的世界里众声喧沸。从原则上说，由编辑、审查、批准一类关卡所组成的文化权力体系几近瓦解，每一个IP地址自由发声，都可能成为强大的文化媒体。英才惨遭埋没的可能，伪学与赝品一手遮天的可能，在传统意义上都会减少。全民批评权的运用，也是一种有益的破坏性检验。不过问题的另一面，是胡说比深思容易，粗品比精品多产，优秀者至少没有数量上的优势。一旦优劣平权成了优劣俱放，文化产量中庸质与恶质的占比肯定大大攀升，低端文化产能不仅无法淘汰，还可能日益滚大和坐大。一些优秀作品即使生产出来，也可能在过量的文化淹没中，在受众们暴饮暴食式的阅读之后，在食欲不振的这些快餐者们那里，出现影响力的严重折扣。一旦肠胃已经吃坏了，再多的良药也都无济于事。</p>
<p>​        一个「扁平」的世界里多数为王。在一般的情况下，有些潮流可以修复民众良知，是真理的脱颖而出；有些潮流泯灭民众良知，是泡沫和垃圾的霸道横行。但不管是哪种情况，多数人的理解力构成潮流的边界，那么大众型和通俗化的真理尚有机会，而冷门的、偏僻的、艰险的、高难的——又常常是重要的文化探索，则可能缺氧。进一步说，市场总是嗅觉灵敏地跟踪多数，跟踪购买力的所在，以实现利润最大化。它们必然就低不就高，随众不随寡，视高深、高难、高雅为营销毒药，并有足够的本领使舆论、奖项、教育、权力等资源向低端集中，打造出泡沫霸权和垃圾霸权。一种品质趋下的文化诱导机制，在这种情况下几乎难以避免。</p>
<p>​        一个「扁平」的世界还有易破难立的特点。特别是自18世纪启蒙运动以来，敬畏感随着上帝一同消失。叛逆比服从更流行，权利比责任更动心，无论左右翼都造反成癖，在获得解构主义一番学术装备后更是见立必破打倒一切。这一过程削弱了上帝与王权，清算了教条与伪善，其功绩不可低估；但无政府式的激进狂飚若无解药，其结局必是相对性等同虚无性，民主化等同民粹化，任何共识难以成，真理永远缺位。真理也许还是有的，但在很多时候只剩下每个人那里「我」的真理，即自恋、自闭、自利的各种强辞，甚至是专职扒粪的哄客四起——这不过是社会沦入一片「原子化」散沙的文化表征。圣人、先知、导师一类从此不再，文化成了一地碎片和自由落体。一个个公权政府在这样的逐利时代也更像个总务处，无心也无力充当精神旗帜，无心也无力实施有效的社会调控。避骂自保的公关活动已够他们忙的了，讨好票源和收买民意已够他们累的了，他们哪还有建构民族与人类精神的远大抱负和坚定行动？</p>
<p>​        越来越多的迹象表明，一旦失去文化的约束和引导机制，一个扁平的世界就是没有方向的世界，是无深度和无高度的世界。即使有成打的托翁和莎翁再世，他们通常也形同刺猬而不是狮子，是暗燃而不是火炬——在生态、经济、政治等重大危机逼近之前，在民众的真理渴求大增之前，情况大体如此。</p>
<p>​        这个时代当然还有文化，有文化运动与文化冲突，也不乏轮番登台的文化偶像。不过，与传统意义上的圣人、先知、导师不同，很多现代文化偶像形式大于内容，迎合多于独行，公关造势优于埋头苦干，成功获利重于大道担当。这些人不过是营构一种虚假的方向，在无方向时代满足一种偶像消费，其中既包括对偶像的适时狂拜，也包括对偶像的适时狂毁。在这里，狂拜或狂毁只在一念，不需深思熟虑和身体力行，因此所需偶像不必经久耐用，隔数月或隔几天就更换一个，实为摊档上的寻常。正因为如此，很多偶像不得不焦灼难安，不得不到处奔走，拼命保持公众能见度成了他们的殊死搏斗，也成了他们与以往大师的明显区别之一。一个个豪华大片就这样火了，又冷了；一个个惊世的主义就这样火了，又冷了；一个个让人开心的狂生或浪女就这样火了，又冷了——到后来，很多人参与围观纯粹是为了有权开骂，争相点击只是赢来讥嘲和自秀高明的资格，于是火就是为了冷，或者说火本身就是冷。</p>
<p>​        中国互联网络信息中心2008年的统计报告显示，高达47％左右的公众已经不信任或不太信任网络。美国佩尤研究中心2004年的调查统计显示，媒体公信力一直下滑，比如对CNN信任值已跌至32％，即大多数人持怀疑态度。有意思的是，这一类文化产业不正是公众用高点击率、高收视率、高票房额等热心喂养起来的么？不都是文化市场上的成功典范么？时值二十一世纪，人类有了前所未有的文化自由选择权，但为什么从这时起人类倒变得如此犹疑不定、六神无主、手足无措、茫然无计，竟找不到自己真正信赖和需要的东西？如果人类长期处于这样一种文化消费中的自我分裂和自我对抗，那么这种所好即所疑、所乐即所耻、所爱即所憎的左右两难，是不是一种文化狂欢之下的精神死机状态？</p>
<p>​        也许需要重新启动，重新确定一个方向。</p>
<p>​        一个重建精神价值的方向。</p>
<p>​        这需要很多人的共同努力，重建一种非权力化和非利益化的文化核心、级差以及组织，即文明教化的正常体系。是的，在这里我愿意重新使用「教化」这样一个词，在人类几百年来钟情于「自由」一词以后，在有效教化与宽幅自由互为条件的奇诡历史之中。</p>
]]></content>
      <categories>
        <category>文学黑洞</category>
      </categories>
      <tags>
        <tag>今日故事</tag>
      </tags>
  </entry>
  <entry>
    <title>感受野的计算</title>
    <url>/zh-CN/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://www.likecs.com/show-204875434.html">感受野的计算</a></p>
<p>2.<a href="https://www.jiqizhixin.com/graph/technologies/4821b1eb-34c3-4532-9dca-a97411441f23">感受野 | 机器之心 </a></p>
<p>计算机视觉中常常出现感受野的概念，我是在修改SSD网络中的过程中，发现增强感受野是非常有必要的。SSD的主干网络是VGG16,其中有个结论：低层特征图的感受野较小，高层特征图的感受野较大。</p>
<h2 id="感受野定义"><a href="#感受野定义" class="headerlink" title="感受野定义"></a>感受野定义</h2><p>神经网络中每一层输出特征图上的像素点在输入图片上的映射的区域大小，也就是特征图上的每一个点对应的输入图片的区域。</p>
<h2 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h2><p><style>.ptfezspbfmjf{zoom:50%;}</style><img src="/zh-CN/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/image-20220427214037594.png" class="lazyload" data-srcset="/zh-CN/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/image-20220427214037594.png" srcset="data:image/png;base64,666" class="ptfezspbfmjf lazyload" alt="image-20220427214037594"></p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>在VGG16中：pool5中<br>pool5:RF=2<br>conv5_3 ：RF=(2-1)<em>1+3=4<br>conv5_2 : RF=(4-1)</em>1+3=6<br>conv5_2: RF=(6-1)<em>1+3=8<br>conv5_1: RF=(8-1)</em>2+2=16<br>pool4 : RF=(8-1)<em>2+2=16<br>conv4_3: RF=(16-1)</em>1+3=18<br>conv4_2: RF=(18-1)<em>1+3=20<br>conv4_1 : RF=(20-1)</em>1+3=22<br>pool3: RF=(22-1)<em>2+2=44<br>conv3_3: RF=(44-1)</em>1+3=46<br>conv3_2: RF=(46-1)<em>1+3=48<br>conv3_1: RF=(48-1)</em>1+3=50<br>pool2: RF=(50-1)<em>2+2=100<br>conv2_2: RF=(150-1)</em>1+3=152<br>conv2_1: RF=（152-1)<em>1+3=154<br>pool1: RF=(154-1)</em>2+2=208<br>conv1_2: RF=(208-1)<em>1+3=210<br>conv1_1: RF=(210-1)</em>1+3=212<br>计算结果为：pool5输出的特征图在输入图片上的感受野为212*</p>
<p>结果如下图所示：</p>
<p><style>.bdtlagfnqfvp{}</style><img src="/zh-CN/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/image-20220427214256808.png" class="lazyload" data-srcset="/zh-CN/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/image-20220427214256808.png" srcset="data:image/png;base64,666" class="bdtlagfnqfvp lazyload" alt="image-20220427214256808"></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>感受野</tag>
      </tags>
  </entry>
  <entry>
    <title>提醒幸福</title>
    <url>/zh-CN/%E6%8F%90%E9%86%92%E5%B9%B8%E7%A6%8F/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>我们从小就习惯了在提醒中过日子。天气刚有一丝风吹草动，妈妈就说，别忘了多穿衣服。才相识了一个朋友，爸爸就说，小心<br>他是个骗子。你取得了一点成功，还没容得乐出声来，所有关切着你的人一起说，别骄傲！你沉浸在欢快中的时候，自己不停地<br>对自己说：「千万不可太高兴，苦难也许马上就要降临……」我们已经习惯了在提醒中过日子。看得见的恐惧和看不见的恐惧始<br>终像乌鸦盘旋在头顶。</p>
<p>在皓月当空的良宵，提醒会走出来对你说：注意风暴。于是我们忽略了皎洁的月光，急急忙忙做好风暴来临前的一切准备。当我们大睁着眼睛枕戈待旦之时，风暴却像迟归的羊群，不知在哪里徘徊。当我们实在忍受不了等待灾难的煎熬时，我们甚至会恶意<br>地祈盼风暴早些到来。</p>
<p>风暴终于姗姗地来了。我们怅然发现，所做的准备多半是没有用的。事先能够抵御的风险毕竟有限，世上无法预计的灾难却是无<br>限的。战胜灾难靠的更多的是临门一脚，先前的惴惴不安帮不上忙。</p>
<p>当风暴的尾巴终于远去，我们守住零乱的家园。气还没有喘匀，新的提醒又智慧地响起来，我们又开始对未来充满恐惧的期待。</p>
<p>人生总是有灾难。其实大多数人早已练就了对灾难的从容，我们只是还没有学会灾难间隙的快活。我们太多注重了自己警觉苦难<br>，我们太忽视提醒幸福。请从此注意幸福！幸福也需要提醒吗？</p>
<p>提醒注意跌倒……提醒注意路滑……提醒受骗上当……提醒荣辱不惊……先哲们提醒了我们一万零一次，却不提醒我们幸福。</p>
<p>也许他们认为幸福不提醒也跑不了的。也许他们以为好的东西你自会珍惜，犯不上谆谆告诫。也许他们太崇尚血与火，觉得幸福<br>无足挂齿。他们总是站在危崖上，指点我们逃离未来的苦难。但避去苦难之后的时间是什么？</p>
<p>那就是幸福啊！</p>
<p>享受幸福是需要学习的，当幸福即将来临的时刻需要提醒。人可以自然而然地学会感官的享乐，人却无法天生地掌握幸福的韵律。灵魂的快意同器官的舒适像一对孪生兄弟，时而相傍相依，时而南辕北辙。幸福是一种心灵的振颤。它像会倾听音乐的耳朵一样，需要不断地训练。</p>
<p>简言之，幸福就是没有痛苦的时刻。它出现的频率并不像我们想象的那样少。</p>
<p>人们常常只是在幸福的金马车已经驶过去很远，捡起地上的金鬃毛说，原来我见过它。</p>
<p>人们喜爱回味幸福的标本，却忽略幸福披着露水散发清香的时刻。那时候我们往往步履匆匆，瞻前顾后不知在忙着什么。</p>
<p>世上有预报台风的，有预报蝗虫的，有预报瘟疫的，有预报地震的。没有人预报幸福。其实幸福和世界万物一样，有它的征兆。</p>
<p>幸福常常是朦胧的，很有节制地向我们喷洒甘霖。你不要总希冀轰轰烈烈的幸福，它多半只是悄悄地扑面而来。你也不要企图把水龙头拧得更大，使幸福很快地流失。而需静静地以平和之心，体验幸福的真谛。幸福绝大多数是朴素的。它不会像信号弹似的，在很高的天际闪烁红色的光芒。它披着本色外衣，亲切温暖地包裹起我们。</p>
<p>幸福不喜欢喧嚣浮华，常常在暗淡中降临。贫困中相濡以沫的一块糕饼，患难中心心相印的一个眼神，父亲一次粗糙的抚摸，女<br>友一个温馨的字条……这都是千金难买的幸福啊。像一粒粒缀在旧绸子上的红宝石，在凄凉中愈发熠熠夺目。</p>
<p>幸福有时会同我们开一个玩笑，乔装打扮而来。机遇、友情、成功、团圆……</p>
<p>它们都酷似幸福，但它们并不等同于幸福。幸福会借了它们的衣裙，袅袅婷婷而来，走得近了，揭去帏幔，才发觉它有钢铁般的<br>内核。幸福有时会很短暂，不像苦难似的笼罩天空。如果把人生的苦难和幸福分置天平两端，苦难体积庞大，幸福可能只是一块小小的矿石。但指针一定要向幸福这一侧倾斜，因为它有生命的黄金。</p>
<p>幸福有梯形的切面，它可以扩大也可以缩小，就看你是否珍惜。</p>
<p>我们要提高对于幸福的警惕，当它到来的时刻，激情地享受每一分钟。据科学家研究，有意注意的结果比无意要好得多。</p>
<p>当春天来临的时候，我们要对自己说，这是春天啦！心里就会泛起茸茸的绿意。</p>
<p>幸福的时候，我们要对自己说，请记住这一刻！幸福就会长久地伴随我们。那我们岂不是拥有了更多的幸福！</p>
<p>所以，丰收的季节，先不要去想可能的灾年，我们还有漫长的冬季来得及考虑这件事。我们要和朋友们跳舞唱歌，渲染喜悦。既<br>然种子已经回报了汗水，我们就有权沉浸幸福。不要管以后的风霜雨雪，让我们先把麦子磨成面，烘一个香喷喷的面包。</p>
<p>所以，当我们从天涯海角相聚在一起的时候，请不要踌躇片刻后的别离。在今后漫长的岁月里，有无数孤寂的夜晚可以独自品尝<br>愁绪。现在的每一分钟，都让它像纯净的酒精，燃烧成幸福的淡蓝色火焰，不留一丝渣滓。让我们一起举杯，说：我们幸福。</p>
<p>所以，当我们守候在年迈的父母膝下时，哪怕他们鬓发苍苍，哪怕他们垂垂老矣，你都要有勇气对自己说：我很幸福。因为天地<br>无常，总有一天你会失去他们，会无限追悔此刻的时光。</p>
<p>幸福并不与财富地位声望婚姻同步，这只是你心灵的感觉。</p>
<p>所以，当我们一无所有的时候，我们也能够说：我很幸福。因为我们还有健康的身体。当我们不再享有健康的时候，那些最勇敢<br>的人可以依然微笑着说：我很幸福。因为我还有一颗健康的心。甚至当我们连心也不再存在的时候，那些人类最优秀的分子仍旧可以对宇宙大声说：我很幸福。因为我曾经生活过。</p>
<p>常常提醒自己注意幸福，就像在寒冷的日子里经常看看太阳，心就不知不觉暖洋洋亮光光。</p>
]]></content>
      <categories>
        <category>文学黑洞</category>
      </categories>
      <tags>
        <tag>今日故事</tag>
      </tags>
  </entry>
  <entry>
    <title>色彩搭配</title>
    <url>/zh-CN/%E6%95%99%E4%BD%A0%E5%AD%A6%E4%BC%9A%E8%89%B2%E5%BD%A9%E6%90%AD%E9%85%8D/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="中国色"><a href="#中国色" class="headerlink" title="中国色"></a>中国色</h3><p><a href="http://zhongguose.com/">中国色</a></p>
<h3 id="COULEUR"><a href="#COULEUR" class="headerlink" title="COULEUR"></a>COULEUR</h3><p><a href="https://www.code-couleur.com/signification/">couleur</a></p>
<h3 id="color-space"><a href="#color-space" class="headerlink" title="color space"></a>color space</h3><p><a href="https://mycolor.space/">color space</a></p>
<h3 id="hyper-color"><a href="#hyper-color" class="headerlink" title="hyper color"></a>hyper color</h3><p><a href="https://hypercolor.dev/">Hypercolor</a></p>
<h3 id="colorable"><a href="#colorable" class="headerlink" title="colorable"></a>colorable</h3><p><a href="https://colorable.jxnblk.com/">Colorable</a></p>
<h3 id="brandcolors"><a href="#brandcolors" class="headerlink" title="brandcolors"></a>brandcolors</h3><p><a href="https://brandcolors.net/">BrandColors</a></p>
<h3 id="九月ppt"><a href="#九月ppt" class="headerlink" title="九月ppt"></a>九月ppt</h3><p><a href="https://jiuyueppt.com/#/">九月PPT</a></p>
<h3 id="huemint"><a href="#huemint" class="headerlink" title="huemint"></a>huemint</h3><p><a href="https://huemint.com/">Huemint - AI color palette generator</a></p>
<h3 id="Adobe-Color"><a href="#Adobe-Color" class="headerlink" title="Adobe Color"></a>Adobe Color</h3><p><a href="https://color.adobe.com/zh/create/color-wheel">色輪、調色盤產生器 | Adobe Color</a></p>
]]></content>
      <categories>
        <category>色彩搭配</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>数据处理</title>
    <url>/zh-CN/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="单进程"><a href="#单进程" class="headerlink" title="单进程"></a>单进程</h2><p>​        在单进程模式下，DataLoader 初始化的进程和取数据的进程是一样的 。因此，数据加载可能会阻止计算。但是，当用于在进程之间共享数据的资源（例如共享内存，文件描述符）有限时，或者当整个数据集很小并且可以完全加载到内存中时，此模式可能是我们首选。此外，单进程加载通常可以显示更多可读的错误跟踪，这<strong>对于我们调试代码很有用</strong>。</p>
<h2 id="多进程"><a href="#多进程" class="headerlink" title="多进程"></a>多进程</h2><p>·多进程处理</p>
<p>​        为了避免在加载数据时阻塞计算，PyTorch 提供了一个简单的开关，只需将参数设置 num_workers 为正整数即可执行多进程数据加载，而设置为 0 时执行单线程数据加载。</p>
<p>​        在设置多进程模式时，每次 DataLoader 创建 iterator 时（例如，当调用 enumerate(dataloader) 时），都会创建 num_workers 个工作进程。此时dataset, collate_fn, worker_init_fn 都会被传到每个worker中，而每个 worker 都用独立的进程。</p>
<p>​        对于 map-style 数据，主线程会用 Sampler 产生 indices，并将它们送到 worker 里。因此，shuffle 是在主线程做的。</p>
<p>​        而对于 iterable-style 数据，因为每个 worker 都有相同的 data 复制样本，并在各个进程里进行不同的操作，以防止每个进程输出的数据是重复的，所以一般会使用 torch.utils.data.get_worker_info() 来进行辅助处理。这里，torch.utils.data.get_worker_info() 会返回 worker 进程的一些信息(如id, dataset, num_workers, seed)，如果在主线程的话返回 None。</p>
<p>​        <strong>注意</strong>，通常不建议在多进程加载中返回 CUDA 张量，因为在使用 CUDA 和在多处理中共享 CUDA 张量时存在许多微妙之处（文档中提出：只要接收过程保留张量的副本，就需要发送过程来保留原始张量）。建议采用 pin_memory=True ，以将数据快速传输到支持 CUDA 的 GPU。简而言之，<strong>不建议在使用多线程的情况下返回 CUDA 的 Tensor</strong>。</p>
<h2 id="锁页内存"><a href="#锁页内存" class="headerlink" title="锁页内存"></a>锁页内存</h2><p>​        首先我们先了解一下锁页内存的概念。</p>
<p>​        主机中的内存，有两种存在方式，一是锁页，二是不锁页。锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。主机到 GPU 副本源自固定（页面锁定）内存时，速度要快得多。CPU 张量和存储暴露了一种 pin_memory() 方法，该方法返回对象的副本，并将数据放在固定的区域中。</p>
<p>​        <strong>而显卡中的显存全部是锁页内存！当计算机的内存充足的时候，可以设置 pin_memory=True</strong>。设置 pin_memory=True，则意味着生成的 Tensor 数据最开始是属于内存中的锁页内存，这样将内存的 Tensor 转义到 GPU 的显存就会更快一些。同时，由于 pin_memory 的作用是将张量返回之前将其复制到 CUDA 固定的内存中，所以只有在 CUDA 环境支持下才有用。</p>
<p>​        PyTorch 原生的 pin_memory 方法如下，其支持大部分 python 数据类型的处理：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pin_memory</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, torch.Tensor):</span><br><span class="line">        <span class="keyword">return</span> data.pin_memory()</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, string_classes):</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, container_abcs.Mapping):</span><br><span class="line">        <span class="keyword">return</span> {k: pin_memory(sample) <span class="keyword">for</span> k, sample <span class="keyword">in</span> data.items()}</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, <span class="built_in">tuple</span>) <span class="keyword">and</span> <span class="built_in">hasattr</span>(data, <span class="string">'_fields'</span>):  <span class="comment"># namedtuple</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">type</span>(data)(*(pin_memory(sample) <span class="keyword">for</span> sample <span class="keyword">in</span> data))</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, container_abcs.<span class="type">Sequence</span>):</span><br><span class="line">        <span class="keyword">return</span> [pin_memory(sample) <span class="keyword">for</span> sample <span class="keyword">in</span> data]</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">hasattr</span>(data, <span class="string">"pin_memory"</span>):</span><br><span class="line">        <span class="keyword">return</span> data.pin_memory()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></tbody></table></figure>
<p>​        默认情况下，如果固定逻辑对于一个属于自定义类型（custom type）的 batch（如果有一个 collate_fn 返回自定义批处理类型的批处理，则会发生），或者如果该批处理的每个元素都是 custom type，则该固定逻辑将无法识别它们，它会返回该批处理（或那些元素）而无需固定内存。而要为自定义批处理或数据类型启用内存固定，我们需使用 pin_memory() 在自定义类型上自定义一个方法。如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCustomBatch</span>:</span><br><span class="line">    <span class="comment"># 自定义一个类，该类不能被PyTorch原生的pin_memory方法所支持</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        transposed_data = <span class="built_in">list</span>(<span class="built_in">zip</span>(*data))</span><br><span class="line">        self.inp = torch.stack(transposed_data[<span class="number">0</span>], <span class="number">0</span>)</span><br><span class="line">        self.tgt = torch.stack(transposed_data[<span class="number">1</span>], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># custom memory pinning method on custom type</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pin_memory</span>(<span class="params">self</span>):</span><br><span class="line">        self.inp = self.inp.pin_memory()</span><br><span class="line">        self.tgt = self.tgt.pin_memory()</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_wrapper</span>(<span class="params">batch</span>):</span><br><span class="line">    <span class="keyword">return</span> SimpleCustomBatch(batch)</span><br><span class="line"></span><br><span class="line">inps = torch.arange(<span class="number">10</span> * <span class="number">5</span>, dtype=torch.float32).view(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">tgts = torch.arange(<span class="number">10</span> * <span class="number">5</span>, dtype=torch.float32).view(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">dataset = TensorDataset(inps, tgts)</span><br><span class="line"></span><br><span class="line">loader = DataLoader(dataset, batch_size=<span class="number">2</span>, collate_fn=collate_wrapper,</span><br><span class="line">                    pin_memory=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_ndx, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">    <span class="built_in">print</span>(sample.inp.is_pinned())  <span class="comment"># True</span></span><br><span class="line">    <span class="built_in">print</span>(sample.tgt.is_pinned())  <span class="comment"># True</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="预取"><a href="#预取" class="headerlink" title="预取"></a>预取</h2><p>DataLoader 通过指定 prefetch_factor （默认为 2）来进行数据的预取。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_MultiProcessingDataLoaderIter</span>(<span class="title class_ inherited__">_BaseDataLoaderIter</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, loader</span>):</span><br><span class="line">        ...</span><br><span class="line">        self._reset(loader, first_iter=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_reset</span>(<span class="params">self, loader, first_iter=<span class="literal">False</span></span>):</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># prime the prefetch loop</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self._prefetch_factor * self._num_workers):</span><br><span class="line">            self._try_put_index()</span><br></pre></td></tr></tbody></table></figure>
<p>通过源码可以看到，prefetch 功能仅适用于多进程加载中（下面也会有多进程 dataloader 的部分代码分析）。</p>
<h2 id="代码详解"><a href="#代码详解" class="headerlink" title="代码详解"></a>代码详解</h2><p>那么现在让我们来看看具体的代码调用流程：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> train_loader:</span><br><span class="line">    ......</span><br></pre></td></tr></tbody></table></figure>
<p>for 循环会调用 dataloader 的 <strong><strong>iter</strong>(self)</strong> 方法，以此获得迭代器来遍历 dataset。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoader</span>(<span class="type">Generic</span>[T_co]):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>) -&gt; <span class="string">'_BaseDataLoaderIter'</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.persistent_workers <span class="keyword">and</span> self.num_workers &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> self._iterator <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                self._iterator = self._get_iterator()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self._iterator._reset(self)</span><br><span class="line">            <span class="keyword">return</span> self._iterator</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self._get_iterator()</span><br></pre></td></tr></tbody></table></figure>
<p>在 <strong><strong>iter</strong>(self)</strong> 方法中，dataloader 调用了 self._get_iterator() 方法，根据 num_workers 获得迭代器，并指示是进行单进程还是多进程处理。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoader</span>(<span class="type">Generic</span>[T_co]):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get_iterator</span>(<span class="params">self</span>) -&gt; <span class="string">'_BaseDataLoaderIter'</span>:</span><br><span class="line">        <span class="keyword">if</span> self.num_workers == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> _SingleProcessDataLoaderIter(self)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.check_worker_number_rationality()</span><br><span class="line">            <span class="keyword">return</span> _MultiProcessingDataLoaderIter(self)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>为了描述更加清晰，我们只考虑单进程的代码。下面是 class _SingleProcessDataLoaderIter(_BaseDataLoaderIter) ，以及其父类 class _BaseDataLoaderIter(object): 的重点代码片段：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_BaseDataLoaderIter</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, loader: DataLoader</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 初始化赋值一些 DataLoader 参数，</span></span><br><span class="line">        <span class="comment"># 以及用户输入合法性进行校验</span></span><br><span class="line">        self._dataset = loader.dataset</span><br><span class="line">        self._dataset_kind = loader._dataset_kind</span><br><span class="line">        self._index_sampler = loader._index_sampler</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>) -&gt; <span class="string">'_BaseDataLoaderIter'</span>:</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_reset</span>(<span class="params">self, loader, first_iter=<span class="literal">False</span></span>):</span><br><span class="line">        self._sampler_iter = <span class="built_in">iter</span>(self._index_sampler)</span><br><span class="line">        self._num_yielded = <span class="number">0</span></span><br><span class="line">        self._IterableDataset_len_called = loader._IterableDataset_len_called</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_next_index</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">next</span>(self._sampler_iter)  <span class="comment"># may raise StopIteration</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_next_data</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__next__</span>(<span class="params">self</span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">        <span class="keyword">with</span> torch.autograd.profiler.record_function(self._profile_name):</span><br><span class="line">            <span class="keyword">if</span> self._sampler_iter <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                self._reset()</span><br><span class="line">            data = self._next_data() <span class="comment"># 重点代码行，通过此获取数据</span></span><br><span class="line">            self._num_yielded += <span class="number">1</span></span><br><span class="line">            ...</span><br><span class="line">            <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="built_in">next</span> = __next__  <span class="comment"># Python 2 compatibility</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self._index_sampler) <span class="comment"># len(_BaseDataLoaderIter) == len(self._index_sampler)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getstate__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">"{} cannot be pickled"</span>, self.__class__.__name__)</span><br></pre></td></tr></tbody></table></figure>
<p><em>BaseDataLoaderIter 是所有 DataLoaderIter 的父类。dataloader获得了迭代器之后，for 循环需要调用 <strong><strong>next</strong>()</strong> 来获得下一个对象，从而实现遍历。通过 **<em>_next</em></em>()** 方法调用 _next_data() 获取数据。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_SingleProcessDataLoaderIter</span>(<span class="title class_ inherited__">_BaseDataLoaderIter</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, loader</span>):</span><br><span class="line">        <span class="built_in">super</span>(_SingleProcessDataLoaderIter, self).__init__(loader)</span><br><span class="line">        <span class="keyword">assert</span> self._timeout == <span class="number">0</span></span><br><span class="line">        <span class="keyword">assert</span> self._num_workers == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self._dataset_fetcher = _DatasetKind.create_fetcher(</span><br><span class="line">            self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_next_data</span>(<span class="params">self</span>):</span><br><span class="line">        index = self._next_index()  <span class="comment"># may raise StopIteration</span></span><br><span class="line">        data = self._dataset_fetcher.fetch(index)  <span class="comment"># may raise StopIteration</span></span><br><span class="line">        <span class="keyword">if</span> self._pin_memory:</span><br><span class="line">            data = _utils.pin_memory.pin_memory(data)</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></tbody></table></figure>
<p>从 _SingleProcessDataLoaderIter 的初始化参数可以看到，其在父类 _BaseDataLoaderIter 的基础上定义了 _dataset_fetcher，并传入 _dataset，_auto_collation，_collate_fn 等参数，用于定义获取数据的方式。其具体实现会在稍后解释。</p>
<p>在 _next_data() 被调用后，其需要 _next_index() 获取 index，并通过获得的 index 传入 _dataset_fetcher 中获取对应样本。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoader</span>(<span class="type">Generic</span>[T_co]):</span><br><span class="line">    ...</span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_auto_collation</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.batch_sampler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_index_sampler</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self._auto_collation:</span><br><span class="line">            <span class="keyword">return</span> self.batch_sampler</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.sampler</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_BaseDataLoaderIter</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_reset</span>(<span class="params">self, loader, first_iter=<span class="literal">False</span></span>):</span><br><span class="line">        self._sampler_iter = <span class="built_in">iter</span>(self._index_sampler)</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_next_index</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># sampler_iter 来自于 index_sampler</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">next</span>(self._sampler_iter)  <span class="comment"># may raise StopIteration</span></span><br></pre></td></tr></tbody></table></figure>
<p>从这里看出，dataloader 提供了 sampler（可以是batch_sampler 或者是其他 sampler 子类），然后 _SingleProcessDataLoaderIter 迭代 sampler 获得索引。</p>
<p>下面我们来看看 fetcher，fetcher 需要 index 来获取元素，并同时支持 Map-style dataset（对应 _MapDatasetFetcher）和 Iterable-style dataset（对应 _IterableDatasetFetcher），使其在 Dataloader 内能使用相同的接口 fetch，代码更加简洁。</p>
<p>· 对于 Map-style：直接输入索引 index，作为 map 的 key，获得对应的样本（即 value）。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_MapDatasetFetcher</span>(<span class="title class_ inherited__">_BaseDatasetFetcher</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, auto_collation, collate_fn, drop_last</span>):</span><br><span class="line">        <span class="built_in">super</span>(_MapDatasetFetcher, self).__init__(dataset, auto_collation, collate_fn, drop_last)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fetch</span>(<span class="params">self, possibly_batched_index</span>):</span><br><span class="line">        <span class="keyword">if</span> self.auto_collation:</span><br><span class="line">            <span class="comment"># 有batch_sampler，_auto_collation就为True，</span></span><br><span class="line">            <span class="comment"># 就优先使用batch_sampler，对应在fetcher中传入的就是一个batch的索引</span></span><br><span class="line">            data = [self.dataset[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> possibly_batched_index]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data = self.dataset[possibly_batched_index]</span><br><span class="line">        <span class="keyword">return</span> self.collate_fn(data)</span><br></pre></td></tr></tbody></table></figure>
<p>· 对于 Iterable-style: <strong><strong>init</strong></strong> 方法内设置了 dataset 初始的迭代器，fetch 方法内获取元素，此时 index 其实已经没有多大作用了。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_IterableDatasetFetcher</span>(<span class="title class_ inherited__">_BaseDatasetFetcher</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, auto_collation, collate_fn, drop_last</span>):</span><br><span class="line">        <span class="built_in">super</span>(_IterableDatasetFetcher, self).__init__(dataset, auto_collation, collate_fn, drop_last)</span><br><span class="line">        self.dataset_iter = <span class="built_in">iter</span>(dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fetch</span>(<span class="params">self, possibly_batched_index</span>):</span><br><span class="line">        <span class="keyword">if</span> self.auto_collation:</span><br><span class="line">            <span class="comment"># 对于batch_sampler（即auto_collation==True）</span></span><br><span class="line">            <span class="comment"># 直接使用往后遍历并提取len(possibly_batched_index)个样本（即1个batch的样本）</span></span><br><span class="line">            data = []</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> possibly_batched_index:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    data.append(<span class="built_in">next</span>(self.dataset_iter))</span><br><span class="line">                <span class="keyword">except</span> StopIteration:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(data) == <span class="number">0</span> <span class="keyword">or</span> (self.drop_last <span class="keyword">and</span> <span class="built_in">len</span>(data) &lt; <span class="built_in">len</span>(possibly_batched_index)):</span><br><span class="line">                <span class="keyword">raise</span> StopIteration</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 对于sampler，直接往后遍历并提取1个样本</span></span><br><span class="line">            data = <span class="built_in">next</span>(self.dataset_iter)</span><br><span class="line">        <span class="keyword">return</span> self.collate_fn(data)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>最后，我们通过索引传入 fetcher，fetch 得到想要的样本。因此，整个过程调用关系总结如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">loader.iter --&gt; self._get_iterator() --&gt; class _SingleProcessDataLoaderIter --&gt; class _BaseDataLoaderIter --&gt; __next__() --&gt; self._next_data() --&gt; self._next_index() --&gt;next(self._sampler_iter) 即 next(iter(self._index_sampler)) --&gt; 获得 index --&gt; self._dataset_fetcher.fetch(index) --&gt; 获得 data</span><br></pre></td></tr></tbody></table></figure>
<p>而对于多进程而言，借用 PyTorch 内源码的注释，其运行流程解释如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"># Our data model looks like this (queues are indicated with curly brackets):</span><br><span class="line">#</span><br><span class="line">#                main process                              ||</span><br><span class="line">#                     |                                    ||</span><br><span class="line">#               {index_queue}                              ||</span><br><span class="line">#                     |                                    ||</span><br><span class="line">#              worker processes                            ||     DATA</span><br><span class="line">#                     |                                    ||</span><br><span class="line">#            {worker_result_queue}                         ||     FLOW</span><br><span class="line">#                     |                                    ||</span><br><span class="line">#      pin_memory_thread of main process                   ||   DIRECTION</span><br><span class="line">#                     |                                    ||</span><br><span class="line">#               {data_queue}                               ||</span><br><span class="line">#                     |                                    ||</span><br><span class="line">#                data output                               \/</span><br><span class="line">#</span><br><span class="line"># P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if</span><br><span class="line">#      `pin_memory=False`.</span><br></pre></td></tr></tbody></table></figure>
<p>首先 dataloader 基于 multiprocessing 产生多进程，每个子进程的输入输出通过两个主要的队列（multiprocessing.Queue() 类）产生，分别为：</p>
<p>· index_queue：每个子进程的队列中需要处理的任务的下标</p>
<p>· _worker_result_queue：返回时处理完任务的下标</p>
<p>· data_queue：表明经过 pin_memory 处理后的数据队列</p>
<p>并且有以下这些比较重要的 flag 参数来协调各个 worker 之间的工作：</p>
<p>· _send_idx: 发送索引，用来记录这次要放 index_queue 中 batch 的 idx</p>
<p>· _rcvd_idx: 接受索引，记录要从 data_queue 中取出的 batch 的 idx</p>
<p>· _task_info: 存储将要产生的 data 信息的 dict，key为 task idx（由 0 开始的整形索引），value 为 (worker_id,) 或 (worker_id, data)，分别对应数据未取和已取的情况</p>
<p>· _tasks_outstanding: 整形，代表已经准备好的 task/batch 的数量（可能有些正在准备中）</p>
<p>每个 worker 一次产生一个 batch 的数据，返回 batch 数据前放入下一个批次要处理的数据下标，对应构造函数子进程初始化如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_MultiProcessingDataLoaderIter</span>(<span class="title class_ inherited__">_BaseDataLoaderIter</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, loader</span>):</span><br><span class="line">        <span class="built_in">super</span>(_MultiProcessingDataLoaderIter, self).__init__(loader)</span><br><span class="line">        ...</span><br><span class="line">        self._worker_result_queue = multiprocessing_context.Queue()  <span class="comment"># 把该worker取出的数放入该队列，用于进程间通信</span></span><br><span class="line">        ...</span><br><span class="line">        self._workers_done_event = multiprocessing_context.Event()</span><br><span class="line">        self._index_queues = []</span><br><span class="line">        self._workers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self._num_workers):</span><br><span class="line">            index_queue = multiprocessing_context.Queue()  <span class="comment"># 索引队列，每个子进程一个队列放要处理的下标</span></span><br><span class="line">            index_queue.cancel_join_thread()</span><br><span class="line">            <span class="comment"># _worker_loop 的作用是：从index_queue中取索引，然后通过collate_fn处理数据，</span></span><br><span class="line">            <span class="comment"># 然后再将处理好的 batch 数据放到 data_queue 中。（发送到队列中的idx是self.send_idx）</span></span><br><span class="line">            w = multiprocessing_context.Process(</span><br><span class="line">                target=_utils.worker._worker_loop,  <span class="comment"># 每个worker子进程循环执行的函数，主要将数据以(idx, data)的方式传入_worker_result_queue中</span></span><br><span class="line">                args=(self._dataset_kind, self._dataset, index_queue, </span><br><span class="line">                      self._worker_result_queue, self._workers_done_event,</span><br><span class="line">                      self._auto_collation, self._collate_fn, self._drop_last,</span><br><span class="line">                      self._base_seed + i, self._worker_init_fn, i, self._num_workers,</span><br><span class="line">                      self._persistent_workers))</span><br><span class="line">            w.daemon = <span class="literal">True</span></span><br><span class="line">            w.start()</span><br><span class="line">            self._index_queues.append(index_queue)</span><br><span class="line">            self._workers.append(w)</span><br><span class="line">        <span class="keyword">if</span> self._pin_memory:</span><br><span class="line">            self._pin_memory_thread_done_event = threading.Event()</span><br><span class="line">            self._data_queue = queue.Queue()  <span class="comment"># 用于存取出的数据进行 pin_memory 操作后的结果</span></span><br><span class="line">            pin_memory_thread = threading.Thread(</span><br><span class="line">                target=_utils.pin_memory._pin_memory_loop,</span><br><span class="line">                args=(self._worker_result_queue, self._data_queue,</span><br><span class="line">                      torch.cuda.current_device(),</span><br><span class="line">                      self._pin_memory_thread_done_event))</span><br><span class="line">            pin_memory_thread.daemon = <span class="literal">True</span></span><br><span class="line">            pin_memory_thread.start()</span><br><span class="line">            <span class="comment"># Similar to workers (see comment above), we only register</span></span><br><span class="line">            <span class="comment"># pin_memory_thread once it is started.</span></span><br><span class="line">            self._pin_memory_thread = pin_memory_thread</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._data_queue = self._worker_result_queue</span><br><span class="line">        ...</span><br><span class="line">        self._reset(loader, first_iter=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_reset</span>(<span class="params">self, loader, first_iter=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>()._reset(loader, first_iter)</span><br><span class="line">        self._send_idx = <span class="number">0</span>  <span class="comment"># idx of the next task to be sent to workers，发送索引，用来记录这次要放 index_queue 中 batch 的 idx</span></span><br><span class="line">        self._rcvd_idx = <span class="number">0</span>  <span class="comment"># idx of the next task to be returned in __next__，接受索引，记录要从 data_queue 中取出的 batch 的 idx</span></span><br><span class="line">        <span class="comment"># information about data not yet yielded, i.e., tasks w/ indices in range [rcvd_idx, send_idx).</span></span><br><span class="line">        <span class="comment"># map: task idx =&gt; - (worker_id,)        if data isn't fetched (outstanding)</span></span><br><span class="line">        <span class="comment">#                  \ (worker_id, data)   if data is already fetched (out-of-order)</span></span><br><span class="line">        self._task_info = {}</span><br><span class="line">        <span class="comment"># _tasks_outstanding 指示当前已经准备好的 task/batch 的数量（可能有些正在准备中）</span></span><br><span class="line">        <span class="comment"># 初始值为 0, 在 self._try_put_index() 中 +1,在 self._next_data 中-1</span></span><br><span class="line">        self._tasks_outstanding = <span class="number">0</span>  <span class="comment"># always equal to count(v for v in task_info.values() if len(v) == 1)</span></span><br><span class="line">        <span class="comment"># this indicates status that a worker still has work to do *for this epoch*.</span></span><br><span class="line">        self._workers_status = [<span class="literal">True</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self._num_workers)] </span><br><span class="line">        <span class="comment"># We resume the prefetching in case it was enabled</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> first_iter:</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(self._num_workers):</span><br><span class="line">                self._index_queues[idx].put(_utils.worker._ResumeIteration())</span><br><span class="line">            resume_iteration_cnt = self._num_workers</span><br><span class="line">            <span class="keyword">while</span> resume_iteration_cnt &gt; <span class="number">0</span>:</span><br><span class="line">                data = self._get_data()</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, _utils.worker._ResumeIteration):</span><br><span class="line">                    resume_iteration_cnt -= <span class="number">1</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># 初始化的时候，就将 2*num_workers 个 (batch_idx, sampler_indices) 放到 index_queue 中</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self._prefetch_factor * self._num_workers):</span><br><span class="line">            self._try_put_index() <span class="comment"># 进行预取</span></span><br></pre></td></tr></tbody></table></figure>
<p>dataloader 初始化的时候，每个 worker 的 index_queue 默认会放入<strong>两个</strong> batch 的 index，从 index_queue 中取出要处理的下标。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_try_put_index</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># self._prefetch_factor 默认为 2</span></span><br><span class="line">        <span class="keyword">assert</span> self._tasks_outstanding &lt; self._prefetch_factor * self._num_workers</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            index = self._next_index()</span><br><span class="line">        <span class="keyword">except</span> StopIteration:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self._num_workers):  <span class="comment"># find the next active worker, if any</span></span><br><span class="line">            worker_queue_idx = <span class="built_in">next</span>(self._worker_queue_idx_cycle)</span><br><span class="line">            <span class="keyword">if</span> self._workers_status[worker_queue_idx]:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># not found (i.e., didn't break)</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        self._index_queues[worker_queue_idx].put((self._send_idx, index)) <span class="comment"># 放入 任务下标 和 数据下标</span></span><br><span class="line">        self._task_info[self._send_idx] = (worker_queue_idx,)</span><br><span class="line">        <span class="comment"># _tasks_outstanding + 1，表明预备好的batch个数+1</span></span><br><span class="line">        self._tasks_outstanding += <span class="number">1</span></span><br><span class="line">        <span class="comment"># send_idx 发送索引, 记录从sample_iter中发送索引到index_queue的次数</span></span><br><span class="line">        self._send_idx += <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure>
<p>调用 _next_data(self) 方法进行数据读取，其中 _process_data(self, data) 用于返回数据。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_next_data</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> self._rcvd_idx &lt; self._send_idx: <span class="comment"># 确保待处理的任务(待取的batch)下标 &gt; 处理完毕要返回的任务(已经取完的batch)下标</span></span><br><span class="line">                info = self._task_info[self._rcvd_idx]</span><br><span class="line">                worker_id = info[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(info) == <span class="number">2</span> <span class="keyword">or</span> self._workers_status[worker_id]:  <span class="comment"># has data or is still active</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">del</span> self._task_info[self._rcvd_idx]</span><br><span class="line">                self._rcvd_idx += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># no valid `self._rcvd_idx` is found (i.e., didn't break)</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> self._persistent_workers:</span><br><span class="line">                    self._shutdown_workers()</span><br><span class="line">                <span class="keyword">raise</span> StopIteration</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Now `self._rcvd_idx` is the batch index we want to fetch</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check if the next sample has already been generated</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(self._task_info[self._rcvd_idx]) == <span class="number">2</span>:</span><br><span class="line">                data = self._task_info.pop(self._rcvd_idx)[<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">return</span> self._process_data(data)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">assert</span> <span class="keyword">not</span> self._shutdown <span class="keyword">and</span> self._tasks_outstanding &gt; <span class="number">0</span></span><br><span class="line">            idx, data = self._get_data() <span class="comment"># 调用 self._try_get_data() 从 self._data_queue 中取数</span></span><br><span class="line">            self._tasks_outstanding -= <span class="number">1</span>  <span class="comment"># 表明预备好的batch个数需要减1</span></span><br><span class="line">            <span class="keyword">if</span> self._dataset_kind == _DatasetKind.Iterable:</span><br><span class="line">                <span class="comment"># Check for _IterableDatasetStopIteration</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, _utils.worker._IterableDatasetStopIteration):</span><br><span class="line">                    <span class="keyword">if</span> self._persistent_workers:</span><br><span class="line">                        self._workers_status[data.worker_id] = <span class="literal">False</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        self._mark_worker_as_unavailable(data.worker_id)</span><br><span class="line">                    self._try_put_index()</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> idx != self._rcvd_idx:</span><br><span class="line">                <span class="comment"># store out-of-order samples</span></span><br><span class="line">                self._task_info[idx] += (data,)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">del</span> self._task_info[idx]</span><br><span class="line">                <span class="keyword">return</span> self._process_data(data) <span class="comment"># 返回数据</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_process_data</span>(<span class="params">self, data</span>):</span><br><span class="line">        self._rcvd_idx += <span class="number">1</span></span><br><span class="line">        self._try_put_index() <span class="comment"># 同上，主要放入队列索引 以及 更新flag</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, ExceptionWrapper):</span><br><span class="line">            data.reraise()</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></tbody></table></figure>
<p>这样，多进程模式的 dataloader 就能通过多个 worker 的协作来共同完成数据的加载。</p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
      </tags>
  </entry>
  <entry>
    <title>CV必备文献</title>
    <url>/zh-CN/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E5%90%88%E9%9B%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这里是我平时会看的一些文献，有些没得时间看，但这些确实很值得一看，放在这里，做个提醒，如果后续我记起来了，后续还会有补充，大家可以留言推荐，仅限计算机视觉方向</p>
<p>1.RFBNet</p>
<p>论文标题：Receptive Field Block Net for Accurate and Fast Object Detection</p>
<p>论文地址：<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper.pdf">Receptive Field Block Net </a></p>
<p>源码地址：<a href="https://github.com/ruinmessi/RFBNet">https://github.com/ruinmessi/RFBNet</a></p>
<p>2.Fast R-CNN</p>
<p>3.Faster R-CNN</p>
<p>4.ResNet</p>
<p>5.Inception</p>
<p>6.Mask R-CNN</p>
<p>7.YOLOv1</p>
<p>8.SSD</p>
<p>9.DSSD</p>
<p>10.ASDD</p>
<p>11.FSSD</p>
<p>12.FASSD</p>
<p>论文标题：FASSD: A Feature Fusion and Spatial Attention-Based Single Shot Detector for Small Object Detection</p>
<p>论文地址：<a href="https://www.mdpi.com/2079-9292/9/9/1536">https://www.mdpi.com/2079-9292/9/9/1536</a></p>
<p>源码地址：</p>
<p>13.AlexNet</p>
<p>14.SIFT</p>
<p>15.HOG</p>
<p>16.FPN</p>
<p>论文标题：Feature Pyramid Networks for Object Detection</p>
<p>论文地址：<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf">Feature Pyramid Networks for Object Detection (thecvf.com)</a></p>
<p>源码地址：<a href="https://github.com/unsky/FPN">FPN: Feature Pyramid Networks for Object Detection </a></p>
<p>17.RefineDet</p>
<p>18.M2Det</p>
<p>19.RSSD</p>
<p>论文标题：Enhancement of SSD by concatenating feature maps for object detection</p>
<p>论文地址：</p>
<p>源码地址：</p>
<p>20.MDSSD</p>
<p>论文标题：Mdssd: Multi-scale deconvolutional single shot detector for small objects</p>
<p>论文地址：</p>
<p>论文源码：</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>文献推荐</tag>
      </tags>
  </entry>
  <entry>
    <title>数据集介绍</title>
    <url>/zh-CN/%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="参考博文"><a href="#参考博文" class="headerlink" title="参考博文"></a>参考博文</h3><p>1.<a href="https://blog.csdn.net/qq_45616304/article/details/117912347">针对 VOC2007和VOC2012 的具体用法</a></p>
<p>2.<a href="https://blog.csdn.net/a_piece_of_ppx/article/details/118701340">Pascal Voc（07+12）联合训练并在07上测试</a></p>
<h3 id="VOC2007和VOC2012用法"><a href="#VOC2007和VOC2012用法" class="headerlink" title="VOC2007和VOC2012用法"></a>VOC2007和VOC2012用法</h3><p>目前广大研究者们普遍使用的是 VOC2007和VOC2012数据集，因为二者是互斥的，不相容的。</p>
<p>论文中针对 VOC2007和VOC2012 的具体用法有以下几种：</p>
<p>1.只用VOC2007的trainval 训练，使用VOC2007的test测试。<br>2.只用VOC2012的trainval 训练，使用VOC2012的test测试，这种用法很少使用，因为大家都会结合VOC2007使用。<br>3.（推荐）使用 VOC2007 的 train+val 和 VOC2012的 train+val 训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12 ，研究者可以自己测试在VOC2007上的结果，因为VOC2007的test是公开的。<br>4.使用 VOC2007 的 train+val+test 和 VOC2012的 train+val训练，然后使用 VOC2012的test测试，这个用法是论文中经常看到的 07++12 ，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。<br>5.先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val、 VOC2012的 train+val 微调训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12+COCO 。<br>6.先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val+test 、 VOC2012的 train+val 微调训练，然后使用 VOC2012的test测试 ，这个用法是论文中经常看到的 07++12+COCO，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。</p>
<h3 id="VOC07-12联合训练并在07上测试"><a href="#VOC07-12联合训练并在07上测试" class="headerlink" title="VOC07+12联合训练并在07上测试"></a>VOC07+12联合训练并在07上测试</h3><h4 id="数据分布"><a href="#数据分布" class="headerlink" title="数据分布"></a>数据分布</h4><p>对于分类/检测任务而言，完成07 + 12数据集合并后，共得到如下数据：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"># 5011+11540=16551, 12608+27450=40058</span><br><span class="line">训练数据：16551张图像，共40058个目标</span><br><span class="line"># 全部来自voc07_test</span><br><span class="line">测试数据：4952张图像，共12032个目标</span><br></pre></td></tr></tbody></table></figure>
<p>组成如下所示：</p>
<p>训练数据：</p>
<p>1.VOC2007的训练集提供了：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">训练数据：2501张图像，共6301个目标</span><br><span class="line">验证数据：2510张图像，共6307个目标</span><br><span class="line">训练+验证数据：5011张图像，共12608个目标</span><br></pre></td></tr></tbody></table></figure>
<p>2.VOC2012的训练集提供了：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">训练数据：5717张图像，共13609个目标</span><br><span class="line">验证数据：5823张图像，共13841个目标</span><br><span class="line">训练+验证数据：11540张图像，共27450个目标</span><br></pre></td></tr></tbody></table></figure>
<p>测试数据：</p>
<p>1.VOC2007的测试集提供了：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">测试数据：4952张图像，共12032个目标</span><br></pre></td></tr></tbody></table></figure>
<h3 id="各种数据集介绍"><a href="#各种数据集介绍" class="headerlink" title="各种数据集介绍"></a>各种数据集介绍</h3><h4 id="Pascal-VOC"><a href="#Pascal-VOC" class="headerlink" title="Pascal VOC"></a>Pascal VOC</h4><p>官网地址：<a href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/">Pascal VOC Dataset Mirror (pjreddie.com)</a></p>
<h4 id="MS-COCO"><a href="#MS-COCO" class="headerlink" title="MS COCO"></a>MS COCO</h4><h4 id="ILSVRC"><a href="#ILSVRC" class="headerlink" title="ILSVRC"></a>ILSVRC</h4>]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title>未婚妻</title>
    <url>/zh-CN/%E6%9C%AA%E5%A9%9A%E5%A6%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>度过了几天假期之后，我要回巴黎了。</p>
<p>当我走进车站，火车已挤满了旅客。大多数的车门前，都站着一个男人或一个妇女，好像是在拦阻后来的旅客。</p>
<p>尽管如此，我还是踮起脚尖向每一个车厢内部观看，希望能找到一个座位。我发现靠近车门坐着的旅客旁边，有一个空座位，但<br>上面放着两个大篮子，里面的鸡子和鸭子把头伸在篮子外面。</p>
<p>我犹豫了好一会之后，决定走进车厢。我说很对不起了，让我来把篮子移开。可是一位穿着工作服的农民对我说：「小姐，请等一等，我就来把它们从这里拿开。」</p>
<p>当我把放在农民膝上的水果篮子提在手中时，他轻轻地把两篮家禽塞在凳子下面。</p>
<p>我们都听到鸭子叫喊，表示不高兴。母鸡却低下头，像是受委屈的样子。农民的妻子一面喊着鸭子鸡子的名字，一面对它们说着<br>话。</p>
<p>我坐下以后，鸭子也安静下来了。这时，坐在我对面的旅客问农民是否他把家禽带到市场上去卖。</p>
<p>农民回答说：「先生，不是送上市场的。后天，我的儿子就要结婚，我把鸡鸭带来送给儿子。」</p>
<p>他脸上显出幸福愉快的神情。他看了看周围的人，仿佛要向所有的人们都表达他自己的快乐。另外的旅客都留心倾听，他们似乎<br>听了之后感到很高兴。只有一个老媪是例外，她占了两人的座位，枕着三个枕头，正在叱骂拥塞在车厢中的农民。</p>
<p>火车开动了。刚才说话的旅客开始阅读报纸，这时农民对他说：「我的儿子在巴黎，他是一家商店的职员，将和一位小姐结婚，<br>她也是一家商店的职员。」</p>
<p>这个旅客把已经打开的报纸放在他膝上，同时移动身子坐在凳的边沿。他问道：「未婚妻美丽吗？」</p>
<p>农民说：「我不知道，我还没有见过她。」</p>
<p>这个旅客有些惊讶，又说：「真的吗？假如她长得丑，使你不喜欢，将怎样办？」</p>
<p>农民回答：「这种事情可能发生，但我相信，她会使我们喜欢，因为我们的儿子很爱我们，他不会娶一个难看的妻子。」</p>
<p>农民的妻子又补上一句：「再说，既然她使我们的儿子腓利普喜欢，她也会使我们喜欢的。」</p>
<p>农民的妻子转过身来向着我，我看到她一双柔和的眼睛肿充满着微笑。她的面容娇小玲珑，非常可爱，我不能相信她就是一个已<br>达结婚年龄的儿子的母亲。</p>
<p>她想知道我是否也去巴黎，当我回答说也是去巴黎时，这个旅客就开玩笑了。他说：「我打赌：这位小姐就是未婚妻，她是来迎<br>接她的公公婆婆而没有介绍自己使他们认识。」</p>
<p>所有的眼睛都向我注视，我羞得面红耳赤，这时农民夫妇同声说：「嗳！真是这样的话，我们将非常高兴。」</p>
<p>我向他们说明这完全是误会。可是这个旅客提醒他们，说我曾沿着火车走过两次，好像我是尽力找认什么人；又说我在登上车厢<br>前是多么犹豫迟疑。</p>
<p>所有的人都笑起来，我在困窘中解释说，这个座位是我能够找到的惟一的座位。</p>
<p>农民的妻子说：「这没有什么关系，你非常使我喜欢。假使我们的媳妇能像你那样，我将多么高兴。」</p>
<p>农民接着说：「是啊，我们的媳妇最好能像你。」</p>
<p>这个旅客对于他自己的这番笑话感到很得意，他带着开玩笑的样子看了我一眼后，对农民夫妇说：「你们相信我没有弄错。当你们到达巴黎时，你们的儿子会对你们说：『这位就是我的未婚妻。』」</p>
<p>他说完后，放声大笑一阵，便往凳子里边一坐，开始专心读他的报纸了。</p>
<p>一会儿以后，农民的妻子完全转身向着我；她在她带来的篮子底层找寻一会儿，便拿出一块煎饼。她一面把煎饼请我吃，一面对<br>我说，这煎饼是她今天早晨亲手做的。</p>
<p>我不知怎样辞谢才好，只得采用夸大的方法，把伤风说成发烧，她才把这块煎饼放在篮子的底层。</p>
<p>接着，她又请我吃一串葡萄，我不得不接受了。</p>
<p>当火车在一个站停下时，我很难阻止农民下车为我购买一杯热的饮料。</p>
<p>我看到这一对好心人一心只想爱他们儿子选中的未婚妻时，自己因不是他们的媳妇而感到遗憾。他们的爱情使自己觉得多么温暖<br>。我是孤女，从未见过父母的慈容；而和我一起生活的人，谁都对我漠不关心。</p>
<p>我惊异地看到他们的眼光时时注视在我身上，好像他们是在爱抚我那样。</p>
<p>到达巴黎时，我帮助他们把篮子从车上搬下来，并领他们向出口处走去。</p>
<p>当我看到一个身材高高的青年向他们扑过来，用双臂抱着他们时，我就稍稍离开他们远一些。他热情地吻着他父亲，又吻着他母<br>亲。</p>
<p>父母只管笑眯眯地接受儿子的亲吻，连服务员推着的行李车快要撞他们时所发出的警铃声，他们都听不到；急于赶路的旅客把臂肘撞在他们身上，他们好像也没有感觉到。</p>
<p>他们在前面走着，我在后面跟着。儿子的一只手臂挽着鸭篮子，另一只手臂抱着他妈妈的肩膀，他微微弯着身躯靠向妈妈，笑嘻<br>嘻地倾听母亲说话。</p>
<p>他像他父亲，一双眼睛鲜明快乐，笑声爽快而响亮。</p>
<p>外面，天几乎全黑了。我撑起大衣的领子。我落在他们后面，稍离开他们几步路。这时，他们的儿子去雇一辆车子。</p>
<p>农民爱抚着一只染有各种颜色、很美丽的花母鸡的头时，对他妻<br>子说：「假如我早知道她不是我们媳妇，那么我早就把这只花母鸡送给她了。」</p>
<p>「是啊！假如我早知道……」</p>
<p>农民的妻子向着已经走出车站的长长人群做手势，眼睛望着远处说：「她已随着人群走了。」</p>
<p>正在这时，他们的儿子已雇到一辆车子回来了。他尽可能好地把他的父母安顿在车上，他自己却在赶车人的旁边坐下，而且侧转身子，以免遮了他父母的视线。</p>
<p>他看起来长得身强力壮，性情温和，我想他的未婚妻一定是很幸福的。</p>
<p>他们的车子消失在黑暗中了，于是我沿着每一条街道慢吞吞地走去。孤零零的我不由自主地回到了自己的房间。</p>
<p>我已二十岁了，还没有一个人来向我谈过爱情。</p>
]]></content>
      <categories>
        <category>文学黑洞</category>
      </categories>
      <tags>
        <tag>今日故事</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习</title>
    <url>/zh-CN/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="常见概念"><a href="#常见概念" class="headerlink" title="常见概念"></a>常见概念</h3><h4 id="TP、TN、FP和FN概念"><a href="#TP、TN、FP和FN概念" class="headerlink" title="TP、TN、FP和FN概念"></a>TP、TN、FP和FN概念</h4><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>P(分类器认为是正样本)</th>
<th>N(分类器认为是负样本)</th>
</tr>
</thead>
<tbody>
<tr>
<td>T(正确分类)</td>
<td>TP</td>
<td>TN</td>
</tr>
<tr>
<td>F(错误分类)</td>
<td>FP</td>
<td>FN</td>
</tr>
</tbody>
</table>
</div>
<p>注：</p>
<p>TP（True Positives）意思就是被分为了正样本，而且分对了。（样本是正样本）<br>TN（True Negatives）意思就是被分为了负样本，而且分对了。(样本是负样本)<br>FP（False Positives）意思就是被分为了正样本，但是分错了。（样本是负样本）<br>FN（False Negatives）意思就是被分为了负样本，但是分错了。（样本是正样本）</p>
<h4 id="正确率"><a href="#正确率" class="headerlink" title="正确率"></a>正确率</h4><p>正确率是我们最常见的评价指标，通常来说，正确率越高，分类器越好。TP是分类器认为是正样本而且确实是正样本的样本数，FP是分类器认为是正样本但实际上不是正样本的样本数，Precision就是“分类器认为是正类并且确实是正类的部分占所有分类器认为是正类的比例”。</p>
<p><style>.pmrtndxpmoju{zoom:50%;}</style><img src="/zh-CN/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20220425220526167.png" class="lazyload" data-srcset="/zh-CN/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20220425220526167.png" srcset="data:image/png;base64,666" class="pmrtndxpmoju lazyload" alt="image-20220425220526167"></p>
<h4 id="召回率"><a href="#召回率" class="headerlink" title="召回率"></a>召回率</h4><p>TP是分类器认为是正样本而且确实是正样本的样本数，FN是分类器认为是负样本但实际上不是负样本的样本数，Recall就是“分类器认为是正类并且确实是正类的部分占所有确实是正类的比例”。</p>
<p><style>.bcgpkvtjhxvv{zoom:50%;}</style><img src="/zh-CN/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20220425221025699.png" class="lazyload" data-srcset="/zh-CN/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20220425221025699.png" srcset="data:image/png;base64,666" class="bcgpkvtjhxvv lazyload" alt="image-20220425221025699"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>详解注意力机制</title>
    <url>/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p>注意力机制就是让网络关注到它更需要关注的地方，是一种网络自适应注意的方式。注意力机制可以分为通道注意力，空间注意力以及二者的结合。</p>
<h3 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h3><h4 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">SENet</a></h4><p>2017年提出的SENet是最后一届ImageNet竞赛的冠军，其实现示意图如下所示，对于输入进来的特征层，我们关注其每一个通道的权重，对于SENet而言，其重点是获得输入进来的特征层，每一个通道的权值。利用SENet，我们可以让网络关注它最需要关注的通道。</p>
<p>其具体实现方式就是：<br>1、对输入进来的特征层进行全局平均池化。<br>2、然后进行两次全连接，第一次全连接神经元个数较少，第二次全连接神经元个数和输入特征层相同。<br>3、在完成两次全连接后，我们再取一次Sigmoid将值固定到0-1之间，此时我们获得了输入特征层每一个通道的权值（0-1之间）。<br>4、在获得这个权值后，我们将这个权值乘上原输入特征层即可。</p>
<p><style>.eoxgcvcbpbwo{}</style><img src="/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331191026838.png" class="lazyload" data-srcset="/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331191026838.png" srcset="data:image/png;base64,666" class="eoxgcvcbpbwo lazyload"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">se_block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel, ratio=<span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(se_block, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">                nn.Linear(channel, channel // ratio, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                nn.Linear(channel // ratio, channel, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, c, _, _ = x.size()</span><br><span class="line">        y = self.avg_pool(x).view(b, c)</span><br><span class="line">        y = self.fc(y).view(b, c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x * y</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="CBAM"><a href="#CBAM" class="headerlink" title="CBAM"></a><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf">CBAM</a></h4><p>CBAM将<strong>通道注意力机制和空间注意力机制</strong>进行一个结合，相比于<strong>SENet只关注通道的注意力机制</strong>可以取得更好的效果。其实现示意图如下所示，CBAM会对输入进来的特征层，分别进行<strong>通道注意力机制的处理和空间注意力机制的处理</strong>。</p>
<p><style>.jfaioyttfpvm{}</style><img src="/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331155344988.png" class="lazyload" data-srcset="/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331155344988.png" srcset="data:image/png;base64,666" class="jfaioyttfpvm lazyload"></p>
<p>下图是通道注意力机制和空间注意力机制的具体实现方式：<br>图像的上半部分为通道注意力机制，通道注意力机制的实现可以分为两个部分，我们会对输入进来的单个特征层，分别进行全局平均池化和全局最大池化。之后对平均池化和最大池化的结果，利用共享的全连接层进行处理，我们会对处理后的两个结果进行相加，然后取一个sigmoid，此时我们获得了输入特征层每一个通道的权值（0-1之间）。在获得这个权值后，我们将这个权值乘上原输入特征层即可。</p>
<p>图像的下半部分为空间注意力机制，我们会对输入进来的特征层，在每一个特征点的通道上取最大值和平均值。之后将这两个结果进行一个堆叠，利用一次通道数为1的卷积调整通道数，然后取一个sigmoid，此时我们获得了输入特征层每一个特征点的权值（0-1之间）。在获得这个权值后，我们将这个权值乘上原输入特征层即可。</p>
<p><style>.khktefpnlrbn{}</style><img src="/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331155451951.png" class="lazyload" data-srcset="/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331155451951.png" srcset="data:image/png;base64,666" class="khktefpnlrbn lazyload"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ChannelAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_planes, ratio=<span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ChannelAttention, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.max_pool = nn.AdaptiveMaxPool2d(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用1x1卷积代替全连接</span></span><br><span class="line">        self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))</span><br><span class="line">        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))</span><br><span class="line">        out = avg_out + max_out</span><br><span class="line">        <span class="keyword">return</span> self.sigmoid(out)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpatialAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size=<span class="number">7</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SpatialAttention, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> kernel_size <span class="keyword">in</span> (<span class="number">3</span>, <span class="number">7</span>), <span class="string">'kernel size must be 3 or 7'</span></span><br><span class="line">        padding = <span class="number">3</span> <span class="keyword">if</span> kernel_size == <span class="number">7</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">2</span>, <span class="number">1</span>, kernel_size, padding=padding, bias=<span class="literal">False</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        avg_out = torch.mean(x, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        max_out, _ = torch.<span class="built_in">max</span>(x, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        x = torch.cat([avg_out, max_out], dim=<span class="number">1</span>)</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> self.sigmoid(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">cbam_block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel, ratio=<span class="number">8</span>, kernel_size=<span class="number">7</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(cbam_block, self).__init__()</span><br><span class="line">        self.channelattention = ChannelAttention(channel, ratio=ratio)</span><br><span class="line">        self.spatialattention = SpatialAttention(kernel_size=kernel_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x * self.channelattention(x)</span><br><span class="line">        x = x * self.spatialattention(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="ECANet"><a href="#ECANet" class="headerlink" title="ECANet"></a><a href="https://sci-hub.mksa.top/10.1109/cvpr42600.2020.01155">ECANet</a></h4><p>ECANet是也是通道注意力机制的一种实现形式。ECANet可以看作是SENet的改进版。<br>ECANet的作者认为SENet对通道注意力机制的预测带来了副作用，捕获所有通道的依赖关系是低效并且是不必要的。<br>在ECANet的论文中，作者认为卷积具有良好的跨通道信息获取能力。</p>
<p>ECA模块的思想是非常简单的，它去除了原来SE模块中的全连接层，直接在全局平均池化之后的特征上通过一个1D卷积进行学习。</p>
<p>既然使用到了1D卷积，那么1D卷积的卷积核大小的选择就变得非常重要了，了解过卷积原理的同学很快就可以明白，1D卷积的卷积核大小会影响注意力机制每个权重的计算要考虑的通道数量。用更专业的名词就是跨通道交互的覆盖率。</p>
<p>如下图所示，左图是常规的SE模块，右图是ECA模块。ECA模块用1D卷积替换两次全连接。</p>
<p><style>.aprvblydpwgb{}</style><img src="/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331160018390.png" class="lazyload" data-srcset="/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331160018390.png" srcset="data:image/png;base64,666" class="aprvblydpwgb lazyload"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">eca_block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel, b=<span class="number">1</span>, gamma=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(eca_block, self).__init__()</span><br><span class="line">        kernel_size = <span class="built_in">int</span>(<span class="built_in">abs</span>((math.log(channel, <span class="number">2</span>) + b) / gamma))</span><br><span class="line">        kernel_size = kernel_size <span class="keyword">if</span> kernel_size % <span class="number">2</span> <span class="keyword">else</span> kernel_size + <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Conv1d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=kernel_size, padding=(kernel_size - <span class="number">1</span>) // <span class="number">2</span>, bias=<span class="literal">False</span>) </span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = self.avg_pool(x)</span><br><span class="line">        y = self.conv(y.squeeze(-<span class="number">1</span>).transpose(-<span class="number">1</span>, -<span class="number">2</span>)).transpose(-<span class="number">1</span>, -<span class="number">2</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">        y = self.sigmoid(y)</span><br><span class="line">        <span class="keyword">return</span> x * y.expand_as(x)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="CA"><a href="#CA" class="headerlink" title="CA"></a><a href="https://arxiv.org/pdf/2103.02907.pdf">CA</a></h4><p>Mobile Network设计的最新研究成果表明，通道注意力（例如，SE注意力）对于提升模型性能具有显著效果，但它们通常会忽略位置信息，而位置信息对于生成空间选择性attention maps是非常重要。</p>
<p>coordinate注意力将通道注意力分解为两个1维特征编码过程，分别沿2个空间方向聚合特征。这样，可以沿一个空间方向捕获远程依赖关系，同时可以沿另一空间方向保留精确的位置信息。然后将生成的特征图分别编码为一对方向感知和位置敏感的attention map，可以将其互补地应用于输入特征图，以增强关注对象的表示。</p>
<p>如下图所示，Coordinate Attention通过精确的位置信息对通道关系和长期依赖性进行编码，具体操作分为Coordinate信息嵌入和Coordinate Attention生成2个步骤。</p>
<p><style>.bbbehhwfkswl{}</style><img src="/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220427215849678.png" class="lazyload" data-srcset="/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220427215849678.png" srcset="data:image/png;base64,666" class="bbbehhwfkswl lazyload" alt="image-20220427215849678"></p>
<p>实现代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CA_Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel, h, w, reduction=<span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(CA_Block, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.h = h</span><br><span class="line">        self.w = w</span><br><span class="line"></span><br><span class="line">        self.avg_pool_x = nn.AdaptiveAvgPool2d((h, <span class="number">1</span>))</span><br><span class="line">        self.avg_pool_y = nn.AdaptiveAvgPool2d((<span class="number">1</span>, w))</span><br><span class="line"></span><br><span class="line">        self.conv_1x1 = nn.Conv2d(in_channels=channel, out_channels=channel//reduction, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.bn = nn.BatchNorm2d(channel//reduction)</span><br><span class="line"></span><br><span class="line">        self.F_h = nn.Conv2d(in_channels=channel//reduction, out_channels=channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.F_w = nn.Conv2d(in_channels=channel//reduction, out_channels=channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.sigmoid_h = nn.Sigmoid()</span><br><span class="line">        self.sigmoid_w = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line"></span><br><span class="line">        x_h = self.avg_pool_x(x).permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        x_w = self.avg_pool_y(x)</span><br><span class="line"></span><br><span class="line">        x_cat_conv_relu = self.relu(self.conv_1x1(torch.cat((x_h, x_w), <span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line">        x_cat_conv_split_h, x_cat_conv_split_w = x_cat_conv_relu.split([self.h, self.w], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        s_h = self.sigmoid_h(self.F_h(x_cat_conv_split_h.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)))</span><br><span class="line">        s_w = self.sigmoid_w(self.F_w(x_cat_conv_split_w))</span><br><span class="line"></span><br><span class="line">        out = x * s_h.expand_as(x) * s_w.expand_as(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    x = torch.randn(<span class="number">1</span>, <span class="number">16</span>, <span class="number">128</span>, <span class="number">64</span>)    <span class="comment"># b, c, h, w</span></span><br><span class="line">    ca_model = CA_Block(channel=<span class="number">16</span>, h=<span class="number">128</span>, w=<span class="number">64</span>)</span><br><span class="line">    y = ca_model(x)</span><br><span class="line">    <span class="built_in">print</span>(y.shape)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="注意力机制的应用"><a href="#注意力机制的应用" class="headerlink" title="注意力机制的应用"></a>注意力机制的应用</h3><p>注意力机制是一个即插即用的模块，理论上可以放在任何一个特征层后面，可以放在主干网络，也可以放在加强特征提取网络。</p>
<p>由于放置在主干会导致网络的预训练权重无法使用，本文以YoloV4-tiny为例，将注意力机制应用加强特征提取网络上。</p>
<p>如下图所示，我们在主干网络提取出来的两个有效特征层上增加了注意力机制，同时对上采样后的结果增加了注意力机制。</p>
<p><style>.fkiwccqkwefb{}</style><img src="/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331160207149.png" class="lazyload" data-srcset="/zh-CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331160207149.png" srcset="data:image/png;base64,666" class="fkiwccqkwefb lazyload"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">attention_block = [se_block, cbam_block, eca_block]</span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------------------------------------#</span></span><br><span class="line"><span class="comment">#   特征层-&gt;最后的输出</span></span><br><span class="line"><span class="comment">#---------------------------------------------------#</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YoloBody</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, anchors_mask, num_classes, phi=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(YoloBody, self).__init__()</span><br><span class="line">        self.phi            = phi</span><br><span class="line">        self.backbone       = darknet53_tiny(<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        self.conv_for_P5    = BasicConv(<span class="number">512</span>,<span class="number">256</span>,<span class="number">1</span>)</span><br><span class="line">        self.yolo_headP5    = yolo_head([<span class="number">512</span>, <span class="built_in">len</span>(anchors_mask[<span class="number">0</span>]) * (<span class="number">5</span> + num_classes)],<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">        self.upsample       = Upsample(<span class="number">256</span>,<span class="number">128</span>)</span><br><span class="line">        self.yolo_headP4    = yolo_head([<span class="number">256</span>, <span class="built_in">len</span>(anchors_mask[<span class="number">1</span>]) * (<span class="number">5</span> + num_classes)],<span class="number">384</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="number">1</span> &lt;= self.phi <span class="keyword">and</span> self.phi &lt;= <span class="number">3</span>:</span><br><span class="line">            self.feat1_att      = attention_block[self.phi - <span class="number">1</span>](<span class="number">256</span>)</span><br><span class="line">            self.feat2_att      = attention_block[self.phi - <span class="number">1</span>](<span class="number">512</span>)</span><br><span class="line">            self.upsample_att   = attention_block[self.phi - <span class="number">1</span>](<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   生成CSPdarknet53_tiny的主干模型</span></span><br><span class="line">        <span class="comment">#   feat1的shape为26,26,256</span></span><br><span class="line">        <span class="comment">#   feat2的shape为13,13,512</span></span><br><span class="line">        <span class="comment">#---------------------------------------------------#</span></span><br><span class="line">        feat1, feat2 = self.backbone(x)</span><br><span class="line">        <span class="keyword">if</span> <span class="number">1</span> &lt;= self.phi <span class="keyword">and</span> self.phi &lt;= <span class="number">3</span>:</span><br><span class="line">            feat1 = self.feat1_att(feat1)</span><br><span class="line">            feat2 = self.feat2_att(feat2)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 13,13,512 -&gt; 13,13,256</span></span><br><span class="line">        P5 = self.conv_for_P5(feat2)</span><br><span class="line">        <span class="comment"># 13,13,256 -&gt; 13,13,512 -&gt; 13,13,255</span></span><br><span class="line">        out0 = self.yolo_headP5(P5) </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 13,13,256 -&gt; 13,13,128 -&gt; 26,26,128</span></span><br><span class="line">        P5_Upsample = self.upsample(P5)</span><br><span class="line">        <span class="comment"># 26,26,256 + 26,26,128 -&gt; 26,26,384</span></span><br><span class="line">        <span class="keyword">if</span> <span class="number">1</span> &lt;= self.phi <span class="keyword">and</span> self.phi &lt;= <span class="number">3</span>:</span><br><span class="line">            P5_Upsample = self.upsample_att(P5_Upsample)</span><br><span class="line">        P4 = torch.cat([P5_Upsample,feat1],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 26,26,384 -&gt; 26,26,256 -&gt; 26,26,255</span></span><br><span class="line">        out1 = self.yolo_headP4(P4)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out0, out1</span><br><span class="line"><span class="comment"># 研究方向为CV的可以关注Bubbliiiing,也可以顺道关注一下博主心系五道口，谢谢！！！</span></span><br><span class="line">————————————————</span><br><span class="line">版权声明：本文为CSDN博主「Bubbliiiing」的原创文章，遵循CC <span class="number">4.0</span> BY-SA版权协议，转载请附上原文出处链接及本声明。</span><br><span class="line">原文链接：https://blog.csdn.net/weixin_44791964/article/details/<span class="number">121371986</span></span><br></pre></td></tr></tbody></table></figure>]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>用两个栈实现队列</title>
    <url>/zh-CN/%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>1.用两个栈实现队列</p>
<p>题目：<a href="https://leetcode.cn/problems/yong-liang-ge-zhan-shi-xian-dui-lie-lcof/">剑指 Offer 09. 用两个栈实现队列 - 力扣（LeetCode）</a></p>
<p>题解：<a href="https://leetcode.cn/problems/yong-liang-ge-zhan-shi-xian-dui-lie-lcof/solution/mian-shi-ti-09-yong-liang-ge-zhan-shi-xian-dui-l-2/">面试题09. 用两个栈实现队列（清晰图解） - 用两个栈实现队列 - 力扣（LeetCode）</a></p>
<p>2</p>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>力扣题解</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数</title>
    <url>/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/weixin_44791964/article/details/117338865">激活函数介绍</a></p>
<p>2.<a href="[torch.nn.functional — PyTorch 1.11.0 documentation](https://pytorch.org/docs/stable/nn.functional.html">pytorch激活函数官方文档</a>)</p>
<h2 id="什么是激活函数"><a href="#什么是激活函数" class="headerlink" title="什么是激活函数"></a>什么是激活函数</h2><p>活函数（Activation functions）对于神经网络模型学习与理解复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。</p>
<p>如果网络中不使用激活函数，网络每一层的输出都是上层输入的线性组合，无论神经网络有多少层，输出都是输入的线性组合。</p>
<p>如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，此时神经网络就可以应用到各类非线性场景当中了。</p>
<p>常见的激活函数如sigmoid、tanh、relu等，它们的输入输出映射均为非线性，这样才可以给网络赋予非线性逼近能力。</p>
<h2 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h2><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><p>Sigmoid函数是一个在生物学中常见的S型函数，它能够把输入的连续实值变换为0和1之间的输出，如果输入是特别小的负数，则输出为0，如果输入是特别大的正数，则输出为1。即将输入量映射到0到1之间。</p>
<p><style>.yozxxpyjbnpu{zoom:50%;}</style><img src="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102716672.png" class="lazyload" data-srcset="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102716672.png" srcset="data:image/png;base64,666" class="yozxxpyjbnpu lazyload"></p>
<p>Sigmoid可以作为非线性激活函数赋予网络非线性区分能力，也可以用来做二分类。其计算公式为：</p>
<p><style>.bdasugkjhast{}</style><img src="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20190719210645402.png" class="lazyload" data-srcset="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20190719210645402.png" srcset="data:image/png;base64,666" class="bdasugkjhast lazyload"></p>
<p>优点：</p>
<p>1.曲线过渡平滑，处处可导；<br>缺点：</p>
<p>1.幂函数运算较慢，激活函数计算量大；<br>2.求取反向梯度时，Sigmoid的梯度在饱和区域非常平缓，很容易造称梯度消失的问题，减缓收敛速度。</p>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>Tanh是一个奇函数，它能够把输入的连续实值变换为-1和1之间的输出<strong>，</strong>如果输入是特别小的负数，则输出为-1，如果输入是特别大的正数，则输出为1<strong>；</strong>解决了Sigmoid函数的不是0均值的问题。</p>
<p><style>.yzvueuemnobe{zoom:50%;}</style><img src="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102737126.png" class="lazyload" data-srcset="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102737126.png" srcset="data:image/png;base64,666" class="yzvueuemnobe lazyload" alt="在这里插入图片描述"></p>
<p>Tanh可以作为非线性激活函数赋予网络非线性区分能力。其计算公式为：</p>
<p><style>.hqzylundvhfl{}</style><img src="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20190719210849124.png" class="lazyload" data-srcset="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20190719210849124.png" srcset="data:image/png;base64,666" class="hqzylundvhfl lazyload"></p>
<p>优点：</p>
<p>1.曲线过渡平滑，处处可导；<br>2.具有良好的对称性，网络是0均值的。<br>缺点：</p>
<p>1.与Sigmoid类似，幂函数运算较慢，激活函数计算量大；<br>2.与Sigmoid类似，求取反向梯度时，Tanh的梯度在饱和区域非常平缓，很容易造称梯度消失的问题，减缓收敛速度。</p>
<h3 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h3><p>线性整流函数（Rectified Linear Unit, ReLU），是一种深度神经网络中常用的激活函数，整个函数可以分为两部分，<strong>在小于0的部分，激活函数的输出为0；在大于0的部分，激活函数的输出为输入</strong>。</p>
<p><style>.durwiypqgdmo{zoom:50%;}</style><img src="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102747835.png" class="lazyload" data-srcset="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102747835.png" srcset="data:image/png;base64,666" class="durwiypqgdmo lazyload" alt="在这里插入图片描述"></p>
<p>计算公式为：</p>
<p><style>.ngwkpvgotbki{}</style><img src="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20190719211201399.png" class="lazyload" data-srcset="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20190719211201399.png" srcset="data:image/png;base64,666" class="ngwkpvgotbki lazyload"></p>
<p>优点：</p>
<p>1.收敛速度快，不存在饱和区间，在大于0的部分梯度固定为1，有效解决了Sigmoid中存在的梯度消失的问题；<br>2.计算速度快，ReLU只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的指数运算，具有类生物性质。<br>缺点：</p>
<p>1.它在训练时可能会“死掉”。如果一个非常大的梯度经过一个ReLU神经元，更新过参数之后，这个神经元的的值都小于0，此时ReLU再也不会对任何数据有激活现象了。如果这种情况发生，那么从此所有流过这个神经元的梯度将都变成 0。合理设置学习率，会降低这种情况的发生概率。</p>
<h3 id="Swish"><a href="#Swish" class="headerlink" title="Swish"></a>Swish</h3><p>Swish是Sigmoid和ReLU的改进版，类似于ReLU和Sigmoid的结合，β是个常数或可训练的参数。Swish 具备无上界有下界、平滑、非单调的特性。Swish 在深层模型上的效果优于 ReLU。</p>
<p><style>.rncxgbcropop{zoom:50%;}</style><img src="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102949435.png" class="lazyload" data-srcset="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102949435.png" srcset="data:image/png;base64,666" class="rncxgbcropop lazyload" alt="在这里插入图片描述"></p>
<p>计算公式为：</p>
<p><style>.vrkiyjabniqn{}</style><img src="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/image-20220421221448860.png" class="lazyload" data-srcset="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/image-20220421221448860.png" srcset="data:image/png;base64,666" class="vrkiyjabniqn lazyload" alt="image-20220421221448860"></p>
<p>优点：</p>
<ul>
<li>Swish具有一定ReLU函数的优点；</li>
<li>Swish具有一定Sigmoid函数的优点；</li>
<li>Swish函数可以看做是介于线性函数与ReLU函数之间的平滑函数。</li>
</ul>
<p>缺点：</p>
<ul>
<li>运算复杂，速度较慢。</li>
</ul>
<h3 id="Mish"><a href="#Mish" class="headerlink" title="Mish"></a>Mish</h3><p>Mish与Swish激活函数类似，Mish具备无上界有下界、平滑、非单调的特性。Mish在深层模型上的效果优于 ReLU。无上边界可以避免由于激活值过大而导致的函数饱和。</p>
<p><style>.idpnlxnrmcyy{zoom:50%;}</style><img src="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528103004303.png" class="lazyload" data-srcset="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528103004303.png" srcset="data:image/png;base64,666" class="idpnlxnrmcyy lazyload" alt="在这里插入图片描述"></p>
<p>计算公式：</p>
<p><style>.pjrybgvqequj{}</style><img src="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/image-20220421222005130.png" class="lazyload" data-srcset="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/image-20220421222005130.png" srcset="data:image/png;base64,666" class="pjrybgvqequj lazyload" alt="image-20220421222005130"></p>
<p>优点：</p>
<ul>
<li><strong>Mish具有一定ReLU函数的优点，收敛快速</strong>；</li>
<li><strong>Mish具有一定Sigmoid函数的优点，函数平滑</strong>；</li>
<li><strong>Mish函数可以看做是介于线性函数与ReLU函数之间的平滑函数</strong>。</li>
</ul>
<p>缺点：</p>
<ul>
<li><strong>运算复杂，速度较慢</strong>。</li>
</ul>
<h3 id="Swish和Mish的梯度对比"><a href="#Swish和Mish的梯度对比" class="headerlink" title="Swish和Mish的梯度对比"></a>Swish和Mish的梯度对比</h3><p><style>.iaddewmqhurd{zoom:50%;}</style><img src="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528105245598.png" class="lazyload" data-srcset="/zh-CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528105245598.png" srcset="data:image/png;base64,666" class="iaddewmqhurd lazyload" alt="在这里插入图片描述"></p>
<h2 id="绘制代码"><a href="#绘制代码" class="headerlink" title="绘制代码"></a>绘制代码</h2><p>pytorch官方文档中的激活函数基本都在这里</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    y = np.exp(x) / (np.exp(x) + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Tanh</span>(<span class="params">x</span>):</span><br><span class="line">    y = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))</span><br><span class="line">    <span class="comment"># y = np.tanh(x)</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ReLU</span>(<span class="params">x</span>):</span><br><span class="line">    y = np.where(x &lt; <span class="number">0</span>, <span class="number">0</span>, x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">LeakyReLU</span>(<span class="params">x, a</span>):  </span><br><span class="line">    <span class="comment"># LeakyReLU的a参数不可训练，人为指定。</span></span><br><span class="line">    y = np.where(x &lt; <span class="number">0</span>, a * x, x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">PReLU</span>(<span class="params">x, a</span>):  </span><br><span class="line">    <span class="comment"># PReLU的a参数可训练</span></span><br><span class="line">    y = np.where(x &lt; <span class="number">0</span>, a * x, x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ReLU6</span>(<span class="params">x</span>):</span><br><span class="line">    y = np.minimum(np.maximum(x, <span class="number">0</span>), <span class="number">6</span>)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Swish</span>(<span class="params">x, b</span>):</span><br><span class="line">    y = x * (np.exp(b*x) / (np.exp(b*x) + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Mish</span>(<span class="params">x</span>):</span><br><span class="line">	<span class="comment"># 这里的Mish已经经过e和ln的约运算</span></span><br><span class="line">    temp = <span class="number">1</span> + np.exp(x)</span><br><span class="line">    y = x * ((temp*temp-<span class="number">1</span>) / (temp*temp+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Grad_Swish</span>(<span class="params">x, b</span>):</span><br><span class="line">    y_grad = np.exp(b*x)/(<span class="number">1</span>+np.exp(b*x)) + x * (b*np.exp(b*x) / ((<span class="number">1</span>+np.exp(b*x))*(<span class="number">1</span>+np.exp(b*x))))</span><br><span class="line">    <span class="keyword">return</span> y_grad</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Grad_Mish</span>(<span class="params">x</span>):</span><br><span class="line">    temp = <span class="number">1</span> + np.exp(x)</span><br><span class="line">    y_grad = (temp*temp-<span class="number">1</span>) / (temp*temp+<span class="number">1</span>) + x*(<span class="number">4</span>*temp*(temp-<span class="number">1</span>)) / ((temp*temp+<span class="number">1</span>)*(temp*temp+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> y_grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    x = np.arange(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    plt.plot(x, Sigmoid(x))</span><br><span class="line">    plt.title(<span class="string">"Sigmoid"</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    plt.plot(x, Tanh(x))</span><br><span class="line">    plt.title(<span class="string">"Tanh"</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    plt.plot(x, ReLU(x))</span><br><span class="line">    plt.title(<span class="string">"ReLU"</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    plt.plot(x, LeakyReLU(x, <span class="number">0.1</span>))</span><br><span class="line">    plt.title(<span class="string">"LeakyReLU"</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    plt.plot(x, PReLU(x, <span class="number">0.25</span>))</span><br><span class="line">    plt.title(<span class="string">"PReLU"</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    plt.plot(x, ReLU6(x))</span><br><span class="line">    plt.title(<span class="string">"ReLU6"</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    plt.plot(x, Swish(x, <span class="number">1</span>))</span><br><span class="line">    plt.title(<span class="string">"Swish"</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    plt.plot(x, Mish(x))</span><br><span class="line">    plt.title(<span class="string">"Mish"</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    plt.plot(x, Grad_Mish(x))</span><br><span class="line">    plt.plot(x, Grad_Swish(x, <span class="number">1</span>))</span><br><span class="line">    plt.title(<span class="string">"Gradient of Mish and Swish"</span>)</span><br><span class="line">    plt.legend([<span class="string">'Mish'</span>, <span class="string">'Swish'</span>])</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>函数库</tag>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉单词表</title>
    <url>/zh-CN/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%8D%95%E8%AF%8D%E8%A1%A8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>artificial neural network,ANN        人工神经网络</p>
<p>perceptron        感知机，人工神经元</p>
<p>activation function        激活函数</p>
<p>rectified linear unit,RELU        修正线性单元</p>
<p>bias        偏置</p>
<p>loss function        损失函数</p>
<p>universal approximation theorem        万能逼近定理</p>
<p>one-hot encoding        独热编码</p>
<p>cross-entropy        交叉熵</p>
<p>dropout        丢弃</p>
<p>bagging        装袋</p>
<p>model averaging        模型平均</p>
<p>batch normalization        批归一化</p>
<p>backpropagation        反向传播</p>
<p>stochastic gradient descent,SGD        随机梯度下降</p>
<p>acquisition         学习，获得</p>
<p>integrate       整合，集成，合并</p>
<p>diverse         多样化的，不同的</p>
<p>tune          调优</p>
<p>curation        内容管理</p>
<p>projection      投影，预测</p>
<p>coherent         有条理的，连贯的</p>
<p>redundant        冗余的</p>
<p>entity          实体</p>
<p>synthetic     合成的，虚假的，不诚恳的</p>
<p>spammy     垃圾邮件式的，无聊的</p>
<p>crowdsourcing          众包</p>
<p>continuity        连续性，连贯性</p>
<p>manifold       多种多样的</p>
<p>inherent        固有的，内在的</p>
<p>pseudo     假的，仿冒的</p>
<p>ensemble         套</p>
<p>heuristic        启发式的；启发式教育法</p>
<p>erroneous        错误的，不正确的</p>
<p>resilient        有弹性的，可迅速恢复的</p>
<p>degraded        堕落的，退化的</p>
<p>converge        收敛，集中</p>
<p>outlier        离群值，异常值</p>
<p>violate        违反，违背</p>
<p>syntactic        语法的</p>
<p>cartesian        笛卡尔的</p>
<p>categorical        分类，绝对的</p>
<p>prune        修剪</p>
<p>param        停止</p>
<p>translation  invariance        平移不变性</p>
<p>suppress        抑制，镇压，阻止</p>
<p>bidirectional        双向</p>
<p>tabular        扁平的，列成表格的</p>
<p>revenue        收入，税收</p>
<p>latency        延迟</p>
<p>harmonic        和声的，谐和的，音乐般的</p>
<p>harmonic mean        调和平均数</p>
<p>harmonic series        调和级数</p>
<p>rote        死记硬背，生搬硬套</p>
<p>bid        出价，投标</p>
<p>leaderboard        排行榜，通栏广告</p>
<p>minor        较小的，次要的，轻微的</p>
<p>contaminated        受污染的，弄脏的</p>
<p>tradeoff        权衡，折中</p>
<p>ensemble learning        集成学习</p>
<p>decompose        分解，使腐烂</p>
<p>intrinsic        内在的，固有的</p>
<p>notable         显要的，值得注意的；非常成功的，令人尊敬的</p>
<p>camouflaged        伪装的</p>
<p>facilitate        促进，使便利</p>
<p>overlap    与……重叠，部分地相同；重叠的部分，互搭量</p>
<p>threshold    入口，门槛，开始，极限，临界值</p>
<p>conjecture    猜测，推测</p>
<p>within    在……之内</p>
<p>oversample:过采样</p>
<p>trade off:权衡，卖掉，折中方案</p>
<p>ultimately:最后，根本，基本上</p>
<p>robotics：机器人学</p>
<p>areial:空中的，航空的，空气的</p>
<p>underperform:表现不佳，工作不如预期</p>
<p>crucial:重要的，决定性的</p>
<p>high-resolution:高分辨率的</p>
<p>deploy:配置，展开，部署</p>
<p>barely:仅仅，勉强，几乎不</p>
<p>tumor:肿瘤，肿块</p>
<p>diagnosis:诊断</p>
<p>inspection:检查，视察</p>
<p>defect:缺陷，缺点，不足之处</p>
<p>annotate:注释，作注解</p>
<p>address:地址，编址</p>
<p>potentially:可能地，潜在地</p>
<p>imply:意味，暗示，隐含</p>
<p>diversity:多样性，差异</p>
<p>generalize:概括，推广，使……一般化</p>
<p>portion:部分</p>
<p>crop:裁剪</p>
<p>merge:合并</p>
<p>align:匹配，排列，对齐，对准</p>
<p>mask:掩码，掩膜</p>
<p>cascade:小瀑布，串联，级联</p>
<p>fuse:融合，熔接，熔化</p>
<p>computational:计算的</p>
<p>overhead:经常性费用，运营费用</p>
<p>fraction:分数，部分，小部分，稍微</p>
<p>schematic illustration:示意图</p>
<p>respect to:关于，考虑</p>
<p>validate:验证，确认，使生效</p>
<p>stochastic:随机的，猜测的</p>
<p>decay    衰退，衰减</p>
<p>coefficient    系数，率</p>
<p>explicitly    明确地，明白地</p>
<p>outline    大纲，概要</p>
<p>distillation:蒸馏</p>
<p>curvature:曲率</p>
<p>stochastic    随机</p>
<p>variance    差异，方差</p>
<p>spectrum    光谱，频谱；范围</p>
<p>neat    灵巧的，整洁的；优雅的，平滑的</p>
<p>heterogenerous    由很多种类组成的</p>
<p>intricate    复杂的，错综的</p>
<p>arbitrary    任意的，武断的</p>
<p>vanilla    香草，比较原始的</p>
<p>sketch    示意图</p>
<p>incarnation    化身，典型</p>
<p>waive    放弃，搁置</p>
<p>shrinkage    收缩，皱缩，缩水; 跌价; 抽缩</p>
<p>alleviate    缓解，减轻</p>
<p>de-facto    事实上</p>
<p>corpus    文集，语料库</p>
<p>unprecedented    前所未有的</p>
<p>inductive    归纳的</p>
<p>empirical    经验主义的</p>
<p>allergic    过敏的，反感的</p>
<p>pollen     花粉</p>
<p>badminton    羽毛球运动</p>
<p>pharmacy    药房</p>
<p>jasmine    茉莉</p>
<p>latent    潜在的，潜伏的，潜意识的</p>
<p>prepend    预先考虑</p>
<p>embedding    编码</p>
<p>alternating    交互的</p>
<p>interpolation    插入，篡改，添写</p>
<p>de-duplicate    删除重复数据</p>
<p>suite    一套，套件</p>
<p>geometric    几何图形的，几何的</p>
<p>intermediate    中间的</p>
<p>fine-tuning    微调</p>
<p>appendix    附录</p>
<p>warmup    预热</p>
<p>least-squares regression    最小二乘回归</p>
<p>on-the-fly    匆匆忙忙地；在空中；（计）运行中</p>
<p>literature    文献</p>
<p>outperform    胜过，做的比……好</p>
<p>substantially    实质上；大体上；充分地</p>
<p>standard deviation    标准差</p>
<p>co-training    协同训练</p>
<p>boost    促进，增加</p>
<p>overtake    赶上，压倒，突然来袭</p>
<p>plateau    趋于平稳，进入停滞期</p>
<p>vanish    消失</p>
<p>versus     与</p>
<p>saturate    饱和的</p>
<p>principal    最主要的</p>
<p>plausible    貌似可信的，花言巧语的；貌似真实的，貌似有理的</p>
<p>sinusoidal    正弦曲线的</p>
<p>degree    程度</p>
<p>analogous    类似的</p>
<p>preliminary    初步的</p>
<p>manual    手动的，手工的</p>
<p>insight    洞察力，领悟</p>
<p>exponentially    以指数方式的</p>
<p>holistically    整体论地</p>
<p>unidirectional    单向性的</p>
<p>incorporate    包含，吸收，体现；把……合并</p>
<p>alleviate    减轻</p>
<p>shallow    浅的，肤浅的</p>
<p>discriminate    区分，辨别</p>
<p>coarser    粗糙的</p>
<p>granularity    间隔尺寸，粒度</p>
<p>derived    导出的，衍生的，派生的</p>
<p>predecessor    前任，前辈;(被取代的)原有事物，前身</p>
<p>cloze    adj. 完形的；填充测验法的</p>
<p>cloze task    完形填空</p>
<p>recipe    秘诀，处方</p>
<p>distinctive    有特色的，与众不同的</p>
<p>unambiguously    不含糊地，明白地</p>
<p>intuitively    直观地；直觉地</p>
<p>trivially    琐细地，平凡地，无能地</p>
<p>mitigate    使缓和，使减轻</p>
<p>monolingual    单语的；仅用一种语言的；仅懂一种语言的</p>
<p>procedure    程序，手续，步骤</p>
<p>degenerate    使退化，恶化</p>
<p>de-facto    (法)实际上的</p>
<p>explicitly    显式地</p>
<p>reformulate    重新构造</p>
<p>ensemble    全体，总效果</p>
<p>nontrivial    重要的，显著的</p>
<p>obstacle    阻碍，障碍</p>
<p>notorious    臭名昭著的，声名狼藉的</p>
<p>vanishing/exploding gradients    梯度消失/梯度爆炸</p>
<p>hamper    妨碍，束缚</p>
<p>degradation    退化，降级，堕落</p>
<p>thoroughly    完全地，彻底地</p>
<p>counterpart    副本，配对物</p>
<p>feasible    可行的，可能的</p>
<p>akin to     类似于</p>
<p>generic    类的，属性的; 一般的; 不受商标保护的; [生]属的，类的</p>
<p>retrieval    检索</p>
<p>quantization    量化</p>
<p>partial differential equations    偏微分方程</p>
<p>auxiliary    辅助的，备用的</p>
<p>Concurrent    并发的，同时发生的</p>
<p>asymptotically    渐近地</p>
<p>counterintuitive    违反直觉的</p>
<p>perturbations    [流]扰动，不安</p>
<p>trial    测试</p>
<p>curse    咒骂，诅咒</p>
<p>estimation    评估，评价，判断</p>
<p>surrogate    代理的</p>
<p>prominent    突出的，显著的，卓越的，杰出的</p>
<p>thes    命题，论文</p>
<p>recalibrate    重新校准</p>
<p>pruning    剪枝</p>
<p>proxy    代理人，代表权</p>
<p>compound    加重; 使复杂化; 混合;混合的</p>
<p>criteria    标准，条件</p>
<p>panoptic    全景的</p>
<p>controversial    有争议的</p>
<p>problematic    有疑问的，有问题的</p>
<p>contrastive    对比的</p>
<p>intuitive    直觉的; 凭直觉获知的; 直观的</p>
<p>preserve    保存；保护；维持；腌；禁猎</p>
<p>intractable    棘手的；难治的；倔强的；不听话的</p>
<p>pretext    借口，托辞; 假象，掩饰</p>
<p>permutation    排列，置换</p>
<p>discrimination    区别对待; 鉴别力; 区别</p>
<p>shuffle    洗牌; 曳脚而行; 搬移; 搁置，随手放</p>
<p>neatness    整洁，干净</p>
<p>blur    模糊</p>
<p>permutation    排列，置换</p>
<p>infrared    红外线的</p>
<p>attenuation    衰减，衰变</p>
<p>tricky    棘手的，难对付的</p>
<p>plethora    过多，过剩</p>
<p>deluge    泛滥，淹没</p>
<p>elaborate    精心制作的，详尽的</p>
<p>repurpose    改换意图，重新</p>
<p>assistive    辅助性的</p>
<p>eliminate    消除</p>
<p>duplicate    重复的</p>
<p>coordinate    坐标</p>
<p>refreshingly    清爽地，有精神地，令人耳目一新地</p>
<p>millisecond     毫秒</p>
<p>implicitly    隐式地</p>
<p>delimiter    分隔符</p>
<p>diverge    分歧，相异</p>
<p>remedy    解决方法，纠正方法</p>
<p>deviation    偏差</p>
<p>coarse    粗糙的</p>
<p>begnign    无有害的，认为无关紧要的</p>
<p>malicious    恶意的，怀恨的</p>
<p>rigorous    严格的</p>
<p>outlier    离群值</p>
<p>deliberate    故意的；深思熟虑的；从容的</p>
<p>susceptible    易受影响的；易感动的；容许…的</p>
<p>leverage    利用</p>
<p>kinda    有点，有几分</p>
<p>centroid    形心，重心</p>
<p>exclusively    专门地，唯一地</p>
<p>collision    碰撞，警告</p>
<p>hint    暗示，示意</p>
<p>stand-alone    （计算机）独立运行的；（公司、组织）独立的</p>
<p>photometric distortion    光度失真</p>
<p>geometric    几何失真</p>
<p>hue    色调</p>
<p>saturation    饱和度</p>
<p>superimpose    叠加</p>
<p>adjacent    相邻的</p>
<p>incorporate    包含，吸收；体现；把……合并</p>
<p>mimic    模仿</p>
<p>tentative    初步的</p>
<p>tackle    处理</p>
<p>cortex    皮层</p>
<p>factorization    因子分解，因式分解</p>
<p>compatible    兼容的</p>
<p>substantially    实质上；大体上；充分地</p>
<p>explicitly    明确地；明白地</p>
<p>reformulate    v. 再制订；换种方式说（或表达）</p>
<p>nontrival    重要的</p>
<p>notorious    声名狼藉的，臭名昭著的</p>
<p>aggregated    聚合的，合计的</p>
<p>cardinality    基数</p>
<p>acquisition    获得物</p>
<p>obscured    遮挡</p>
<p>out-of-view    看不见的</p>
<p>precedent    先前的</p>
<p>intuitive    直观的</p>
<p>overhead    开销</p>
<p>efficacy    功效，效力</p>
<p>hierarchical    分层的</p>
<p>foeval    视网膜中心的</p>
<p>iteratively    迭代地</p>
<p>salient    重点的</p>
<p>modality    形式，形态</p>
<p>in a conditional fashion    有条件的方式</p>
<p>squeeze    挤压</p>
<p>excitation    激励</p>
<p>self-contained    独立的，设备齐全的，沉默寡言的</p>
<p>aggregate    集合，聚集</p>
<p>superscript    上标</p>
<p>an apples to apples comparison    比较两个相近的事物</p>
<p>thoroughly    彻底地，完全地</p>
<p>conjecture    推测，猜测</p>
<p>auxiliary    辅助的</p>
<p>parentheses    圆括号，插入成分</p>
<p>nest    嵌套</p>
<p>the best of both worlds    两全其美</p>
<p>incur    带来（成本、花费）等；招致，遭受</p>
<p>decimal    十进位的，小数的</p>
<p>timestamp    时间戳</p>
<p>clause    从句，分句；（法律文件的）条款</p>
<p>overlap    重叠</p>
<p>coalesce    合并，联合</p>
<p>rollup    归纳，卷曲，袅袅上升</p>
<p>coarse-to-fine    由粗到细，由繁到简</p>
<p>tic-tac-toe    井字棋，圈叉游戏</p>
<p>overlay    覆在……上面，覆盖</p>
<p>cached    贮藏起来，高速缓存</p>
<p>eigen    特征，固有的</p>
<p>tremendous    巨大的，极好的</p>
<p>versus    （比赛或诉讼中）以……为对手，与……竞争；与……相对，与……相比</p>
<p>delineating    描述，描绘</p>
<p>primitive    原始的</p>
<p>recalibrating    重新调整</p>
<p>magenta    洋红色</p>
<p>cyan    青绿色</p>
<p>pentagon    五边形</p>
<p>hexagon    六边形</p>
<p>diamond    菱形</p>
<p>line chart    折线图</p>
<p>flip    翻转</p>
<p>alias    别名</p>
<p>incurring    招致，遭受</p>
<p>ellipse    椭圆</p>
<p>tile    平铺，瓷砖</p>
<p>diversity    多样性</p>
<p>discard    丢弃</p>
<p>adequately    充分地，足够地</p>
<p>flaw    缺陷，缺点</p>
<p>susceptible    易得病的，易受影响的；（人）易受感动的，易动感情的</p>
<p>caret    脱字符号，插入符号</p>
<p>teardown    拆卸</p>
<p>untangle    理清，整顿，解开……纠结</p>
<p>duration    持续时间</p>
<p>optical    光学的；（装置）光电的</p>
<p>decent    像样的，尚好的，得体的</p>
<p>inevitable    必然发生的，不可避免的</p>
<p>leverage    n.影响力，杠杆作用adj.充分利用</p>
<p>overwhelm    压倒，压垮</p>
<p>degenerate    恶化，堕落，退化</p>
<p>elaborate    详细说明，复杂的</p>
<p>mitigate    减轻</p>
<p>severe    严重的，艰巨的</p>
<p>hamper    阻碍</p>
<p>induce    诱导，引诱</p>
<p>deteriorate    恶化，变坏</p>
<p>atomic    原子的，核能的</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>专业名词</tag>
      </tags>
  </entry>
  <entry>
    <title>烟之外</title>
    <url>/zh-CN/%E7%83%9F%E4%B9%8B%E5%A4%96/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在涛声中唤你的名字，而你的名字<br>已在千帆之外<br>潮来潮去<br>左边的鞋印才下午<br>右边的鞋印已黄昏了<br>六月原是一本很感伤的书<br>结局如此之凄美<br>——落日西沉</p>
<p>你依然凝视<br>那人眼中展示的一片纯白<br>他跪向你，向昨日那朵美了整个<br>下午的云<br>海哟，为何在众灯之中<br>独点亮那一盏茫然<br>还能抓住什么呢？<br>你那曾被称为云的眸子<br>现有人叫作<br>烟</p>
]]></content>
      <categories>
        <category>文学黑洞</category>
      </categories>
      <tags>
        <tag>今日故事</tag>
      </tags>
  </entry>
  <entry>
    <title>Q &amp; A</title>
    <url>/zh-CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="就地操作"><a href="#就地操作" class="headerlink" title="就地操作"></a>就地操作</h3><p>问题来源：看代码发现<code>self.relu=nn.ReLU(inplace=True)</code>不明白<code>inplace=True</code>什么意思。</p>
<p>解答：查看pytorch官网关于ReLU定义，<a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU">ReLU</a>其中inplace参数表示可以选择就地执行操作，默认为False,就地执行操作是指图像处理函数的输入图像和输出图像是同一对象，即同一张图像，常规的图像处理函数是不支持输入图像和输出图像是同一图像的。</p>
<p>eg:中值滤波函数</p>
<p><code>medianBlur(src, dst, 7);  //常规操作</code></p>
<p><code>medianBlur(src,  src, 7); //就地操作</code></p>
<p>就地操作直接更改张量的内容，而无需复制它。由于它不创建输入的副本，因此在处理高维数据时减少了内存使用，就地操作有助于使用更少的GPU内存，详情请看该博客<a href="https://www.ksgszhuce.com/tetl/37.html">如何在Pytorch中执行就地操作</a></p>
<h3 id="torch-max中keepdim的作用"><a href="#torch-max中keepdim的作用" class="headerlink" title="torch.max中keepdim的作用"></a>torch.max中keepdim的作用</h3><p>torch.max的用法：</p>
<p>(max, max_indices) = torch.max(input, dim, keepdim=False)</p>
<ul>
<li>输入：</li>
</ul>
<ol>
<li>input 是输入的tensor。</li>
<li>dim 是索引的维度，dim=0寻找每一列的最大值，dim=1寻找每一行的最大值。</li>
<li>keepdim 表示是否需要保持输出的维度与输入一样，keepdim=True表示输出和输入的维度一样，keepdim=False表示输出的维度被压缩了，也就是输出会比输入低一个维度。</li>
</ol>
<ul>
<li>输出：</li>
</ul>
<ol>
<li><p>max 表示取最大值后的结果。</p>
</li>
<li><p>2max_indices 表示最大值的索引</p>
</li>
</ol>
<p><code>import torch</code></p>
<p><code>import numpy as np</code></p>
<p><code>x = torch.randint(0,9,(2,4))</code></p>
<p><code>print(x)
tensor([[7, 8, 7, 2],
        [6, 0, 3, 0]])</code></p>
<h1 id="取每一行的最大值，torch-max的输出结果"><a href="#取每一行的最大值，torch-max的输出结果" class="headerlink" title="取每一行的最大值，torch.max的输出结果"></a>取每一行的最大值，torch.max的输出结果</h1><p><code>y = torch.max(x, 1)  
print(y)
torch.return_types.max(values=tensor([8, 6]),indices=tensor([1, 0])) #索引值</code></p>
<p><code>y = torch.max(x, 1, keepdim=True)[0]</code><br><code>print(y)</code><br><code>print(np.shape(y)) # keepdim=True，输出仍然是二维的</code><br><code>tensor([[8],
         [6]])torch.Size([2, 1])</code><br><code>y = torch.max(x, 1, keepdim=False)[0]</code><br><code>print(y)</code><br><code>print(np.shape(y))
keepdim=False # 输出变成了一维</code>tensor([8, 6])<br><code>torch.Size([2])</code></p>
<h3 id="ConstantPad2d的用法"><a href="#ConstantPad2d的用法" class="headerlink" title="ConstantPad2d的用法"></a>ConstantPad2d的用法</h3><p>torch.nn.ConstantPad2d(padding, value)</p>
<p>参数：padding(int, tuple)-padding的尺寸，如果是整型，那么所有的边界都使用相同的填充，如果是四元组，使用（padding_left, padding_right, padding_top, padding_bottom)</p>
<p>形状：</p>
<ul>
<li><p>输入：</p>
<script type="math/tex; mode=display">
(N, C, H_{in}, W_{in}) or (C, H_{in}, W_{in})</script></li>
<li><p>输出：</p>
<script type="math/tex; mode=display">
(N, C, H_{out}, W_{out}) or (C, H_{out}, W_{out})</script><p>其中，</p>
<script type="math/tex; mode=display">
H_{out} = H_{in}+padding_{top}+padding_{bottom}</script><script type="math/tex; mode=display">
W_{out}=W_{in}+padding_{left}+padding_{right}</script></li>
</ul>
<p>测试用例：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">n1 = nn.ConstantPad2d(2, 0)</span><br><span class="line">n2 = nn.ConstantPad2d((0, 1, 0, 1), 0)</span><br><span class="line">n3 = nn.ConstantPad2d((-1, 0, -1, 0), 0)</span><br><span class="line">input = torch.randn(1, 2, 2)</span><br><span class="line">print(input)</span><br><span class="line">t = n1(input)</span><br><span class="line">print(t)</span><br><span class="line">x = n2(input)</span><br><span class="line">y = n3(x)</span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></tbody></table></figure>
<p>结果：</p>
<p>tensor([[[ 1.0826,  0.1191],<br>         [-0.3506,  0.1677]]])<br>tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],<br>         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],<br>         [ 0.0000,  0.0000,  1.0826,  0.1191,  0.0000,  0.0000],<br>         [ 0.0000,  0.0000, -0.3506,  0.1677,  0.0000,  0.0000],<br>         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],<br>         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])<br>tensor([[[ 1.0826,  0.1191,  0.0000],<br>         [-0.3506,  0.1677,  0.0000],<br>         [ 0.0000,  0.0000,  0.0000]]])<br>tensor([[[0.1677, 0.0000],<br>         [0.0000, 0.0000]]])</p>
<p>更多详情参考<a href="https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html?highlight=constantpad2d#torch.nn.ConstantPad2d">ConstantPad2d</a></p>
<h3 id="enumerate-函数"><a href="#enumerate-函数" class="headerlink" title="enumerate()函数"></a>enumerate()函数</h3><h4 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h4><p>enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。Python 2.3. 以上版本可用，2.6 添加 start 参数。</p>
<h4 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h4><p><code>enumerate(sequence, [start=0])</code></p>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p>·sequence—一个序列、迭代器或其他支持迭代对象</p>
<p>·start—下标起始位置的值</p>
<h4 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h4><p>返回enumerate(枚举)对象</p>
<h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><p>以下展示了使用enumerate()方法的实例：</p>
<blockquote>
<p>seasons = [‘Spring’, ‘Summer’, ‘Fall’, ‘Winter’]</p>
<p>list(enumerate(seasons))</p>
<p><code>[(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]</code></p>
<p>list(enumerate(seasons, start=1))    # 下标从1开始</p>
<p><code>[(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')]</code></p>
</blockquote>
<p>普通的for循环</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">i = <span class="number">0</span></span><br><span class="line">seq = [<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">enumerate</span>(seq):</span><br><span class="line">	<span class="built_in">print</span>(i, seq[i])</span><br><span class="line">	i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">result:</span><br><span class="line"><span class="number">0</span> one</span><br><span class="line"><span class="number">1</span> two</span><br><span class="line"><span class="number">2</span> three</span><br></pre></td></tr></tbody></table></figure>
<p>for循环使用enumerate</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">seq = [<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]</span><br><span class="line"><span class="keyword">for</span> i, element <span class="keyword">in</span> <span class="built_in">enumerate</span>(seq):</span><br><span class="line">	<span class="built_in">print</span>(i, element)</span><br><span class="line"></span><br><span class="line">result:</span><br><span class="line"><span class="number">0</span> one</span><br><span class="line"><span class="number">1</span> two</span><br><span class="line"><span class="number">2</span> three</span><br></pre></td></tr></tbody></table></figure>
<h3 id="torch-clamp"><a href="#torch-clamp" class="headerlink" title="torch.clamp"></a>torch.clamp</h3><p><code>torch.clamp(input, min=None, max=None, *, out=None)-&gt;Tensor</code></p>
<p>Clamps中所有输入的元素都在[min, max]范围内，让最小值和最大值分别是min和max，将会返回：</p>
<script type="math/tex; mode=display">
y_i=min(max(x_i,min\_value_i),max\_value_i)</script><p>如果min为空，就没有下界。或者，如果max为空，没有上界。</p>
<p>注：</p>
<p>如果min大于max,<code>torch.clamp(...,min,max)</code>设置输入的所有元素为max的值。</p>
<p>参数：</p>
<p>·input(Tensor)-输入张量</p>
<p>·min(Number或Tensor,可选)-被限制范围的下界</p>
<p>·max(Number或Tensor,可选)-被限制范围的上界</p>
<p>关键字参数：</p>
<p>out(Tensor, 可选)-输出的张量</p>
<p>举例：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a = torch.randn(<span class="number">4</span>)</span><br><span class="line">&gt;&gt;&gt;a</span><br><span class="line">tensor([-<span class="number">1.7120</span>, <span class="number">0.1734</span>, -<span class="number">0.0478</span>, -<span class="number">0.0922</span>])</span><br><span class="line">&gt;&gt;&gt;torch.clamp(a, <span class="built_in">min</span>=-<span class="number">0.5</span>, <span class="built_in">max</span>=<span class="number">0.5</span>)</span><br><span class="line">tensor([-<span class="number">0.5000</span>, <span class="number">0.1734</span>, -<span class="number">0.0478</span>, -<span class="number">0.0922</span>])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;<span class="built_in">min</span> = torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, steps=<span class="number">4</span>)</span><br><span class="line">&gt;&gt;&gt;torch.clamp(a, <span class="built_in">min</span>=<span class="built_in">min</span>)</span><br><span class="line">tensor([-<span class="number">1.000</span>, <span class="number">0.1734</span>, <span class="number">0.3333</span>, <span class="number">1.0000</span>])</span><br></pre></td></tr></tbody></table></figure>
<h3 id="FPPI"><a href="#FPPI" class="headerlink" title="FPPI"></a>FPPI</h3><p><a href="https://blog.csdn.net/Bruce_0712/article/details/78462880">(68条消息) Recall/Precision/FPPI评价方式详解_Bruce_0712的博客-CSDN博客</a></p>
<h3 id="torchvision-ops-box-iou"><a href="#torchvision-ops-box-iou" class="headerlink" title="torchvision.ops.box_iou"></a>torchvision.ops.box_iou</h3><h4 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h4><p><code>torchvision.ops.box_iou(boxes1:torch.Tensor, boxes2:torch.Tensor)-&gt;torch.Tensor</code><a href="https://pytorch.org/vision/stable/_modules/torchvision/ops/boxes.html#box_iou">SOURCE</a></p>
<p>返回两个框的交并比，这两个框的形式都是<script type="math/tex">(x_1,y_1,x_2,y_2)</script>并且<script type="math/tex">0<=x_1<x_2,0<=y_1<y_2</script></p>
<h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h4><p>·boxes1(Tensor[N, 4])-&gt;第一个框</p>
<p>·boxes2(Tensor[N, 4])-&gt;第二个框</p>
<h4 id="返回"><a href="#返回" class="headerlink" title="返回"></a>返回</h4><p>返回boxes1和boxes2逐元素的配对IOU矩阵（N×M)</p>
<h4 id="返回类型"><a href="#返回类型" class="headerlink" title="返回类型"></a>返回类型</h4><p>Tensor(N,M)</p>
<h3 id="torch-argmax（）函数"><a href="#torch-argmax（）函数" class="headerlink" title="torch.argmax（）函数"></a>torch.argmax（）函数</h3><p><code>torch.argmax(input)-&gt;LongTensor</code></p>
<p>返回<code>input</code>张量所有元素的最大值序号，这是<code>torch.max()</code>返回的第二个值。</p>
<p>注：</p>
<p>如果这里有多个最大值，则会返回第一个最大值的序号。</p>
<h4 id="参数-2"><a href="#参数-2" class="headerlink" title="参数"></a>参数</h4><p><code>input(Tensor)-&gt;输入的张量</code></p>
<p>举例：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">1.3398</span>,  <span class="number">0.2663</span>, -<span class="number">0.2686</span>,  <span class="number">0.2450</span>],</span><br><span class="line">        [-<span class="number">0.7401</span>, -<span class="number">0.8805</span>, -<span class="number">0.3402</span>, -<span class="number">1.1936</span>],</span><br><span class="line">        [ <span class="number">0.4907</span>, -<span class="number">1.3948</span>, -<span class="number">1.0691</span>, -<span class="number">0.3132</span>],</span><br><span class="line">        [-<span class="number">1.6092</span>,  <span class="number">0.5419</span>, -<span class="number">0.2993</span>,  <span class="number">0.3195</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.argmax(a)</span><br><span class="line">tensor(<span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure>
<p><code>torch.argmax(input, dim, keppdim=False)-&gt;LongTensor</code></p>
<p>通过指定维度返回张量最大值的序号，这个最大值将会通过<code>torch.max()</code>返回</p>
<h4 id="参数-3"><a href="#参数-3" class="headerlink" title="参数"></a>参数</h4><p>·input(Tensor):输入的张量</p>
<p>·dim(int):减少的维度，如果没有，将返回平铺后的张量的argmax.</p>
<p>·keepdim(bool):输出的张量是否保持维度,如果<code>dim=None</code>将忽略。</p>
<p>举例：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">1.3398</span>,  <span class="number">0.2663</span>, -<span class="number">0.2686</span>,  <span class="number">0.2450</span>],</span><br><span class="line">        [-<span class="number">0.7401</span>, -<span class="number">0.8805</span>, -<span class="number">0.3402</span>, -<span class="number">1.1936</span>],</span><br><span class="line">        [ <span class="number">0.4907</span>, -<span class="number">1.3948</span>, -<span class="number">1.0691</span>, -<span class="number">0.3132</span>],</span><br><span class="line">        [-<span class="number">1.6092</span>,  <span class="number">0.5419</span>, -<span class="number">0.2993</span>,  <span class="number">0.3195</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.argmax(a, dim=<span class="number">1</span>)</span><br><span class="line">tensor([ <span class="number">0</span>,  <span class="number">2</span>,  <span class="number">0</span>,  <span class="number">1</span>])</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.tensor([</span><br><span class="line">              [</span><br><span class="line">                  [<span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">2</span>],</span><br><span class="line">                  [<span class="number">9</span>, -<span class="number">6</span>, <span class="number">2</span>, <span class="number">8</span>],</span><br><span class="line">                  [-<span class="number">3</span>, <span class="number">7</span>, -<span class="number">9</span>, <span class="number">1</span>]</span><br><span class="line">              ],</span><br><span class="line"> </span><br><span class="line">              [</span><br><span class="line">                  [-<span class="number">1</span>, <span class="number">7</span>, -<span class="number">5</span>, <span class="number">2</span>],</span><br><span class="line">                  [<span class="number">9</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">8</span>],</span><br><span class="line">                  [<span class="number">3</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">1</span>]</span><br><span class="line">              ]])</span><br><span class="line">b = torch.argmax(a, dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"> </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 0, 1],</span></span><br><span class="line"><span class="string">        [1, 1, 1, 1],</span></span><br><span class="line"><span class="string">        [1, 1, 1, 1]])</span></span><br><span class="line"><span class="string">torch.Size([2, 3, 4])"""</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># dim=0,即将第一个维度消除，也就是将两个[3*4]矩阵只保留一个，因此要在两组中作比较，即将上下两个[3*4]的矩阵分别在对应的位置上比较大小</span></span><br><span class="line"> </span><br><span class="line">b = torch.argmax(a, dim=<span class="number">1</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1, 2, 0, 1],</span></span><br><span class="line"><span class="string">        [1, 2, 2, 1]])</span></span><br><span class="line"><span class="string">torch.Size([2, 3, 4])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># dim=1，即将第二个维度消除,这么理解：矩阵维度变为[2*4];</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[1, 5, 5, 2],</span></span><br><span class="line"><span class="string">[9, -6, 2, 8],</span></span><br><span class="line"><span class="string">[-3, 7, -9, 1];</span></span><br><span class="line"><span class="string">纵向压缩成一维，因此变为[1,2,0,1];同理得到[1,2,2,1];</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">b = torch.argmax(a,dim=<span class="number">2</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[2, 0, 1],</span></span><br><span class="line"><span class="string">        [1, 0, 2]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># dim=2,即将第三个维度消除，这么理解：矩阵维度变为[2*3]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">   [1, 5, 5, 2],</span></span><br><span class="line"><span class="string">   [9, -6, 2, 8],</span></span><br><span class="line"><span class="string">   [-3, 7, -9, 1];</span></span><br><span class="line"><span class="string">横向压缩成一维</span></span><br><span class="line"><span class="string">[2,0,1],同理得到下面的"""</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="python-call-方法"><a href="#python-call-方法" class="headerlink" title="python _call_()方法"></a>python _<em>call_</em>()方法</h3><p>本节再介绍 <a href="http://c.biancheng.net/python/">Python</a> 类中一个非常特殊的实例方法，即 _<em>call_</em>()。该方法的功能类似于在类中重载 () 运算符，使得类实例对象可以像调用普通函数那样，以“对象名()”的形式使用。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 引用来自C语言中文网，详情请参考：http://c.biancheng.net/view/2380.html</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CLanguage</span>:</span><br><span class="line">    <span class="comment"># 定义__call__方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self,name,add</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"调用__call__()方法"</span>,name,add)</span><br><span class="line"></span><br><span class="line">clangs = CLanguage()</span><br><span class="line">clangs(<span class="string">"C语言中文网"</span>,<span class="string">"http://c.biancheng.net"</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>程序执行结果：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">调用__call__()方法 C语言中文网 http://c.biancheng.net</span><br></pre></td></tr></tbody></table></figure>
<p>可以看到，通过在 CLanguage 类中实现 _<em>call_</em>() 方法，使的 clangs 实例对象变为了可调用对象。</p>
<blockquote>
<p>Python 中，凡是可以将 () 直接应用到自身并执行，都称为可调用对象。可调用对象包括自定义的函数、Python 内置函数以及本节所讲的类实例对象。</p>
</blockquote>
<p>对于可调用对象，实际上“名称()”可以理解为是“名称.<strong>call</strong>()”的简写。仍以上面程序中定义的 clangs 实例对象为例，其最后一行代码还可以改写为如下形式：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">clangs.__call__("C语言中文网","http://c.biancheng.net")</span><br></pre></td></tr></tbody></table></figure>
<p>运行程序会发现，其运行结果和之前完全相同。</p>
<p>自定义函数：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">say</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Python教程：http://c.biancheng.net/python"</span>)</span><br><span class="line">say()</span><br><span class="line">say.__call__()</span><br></pre></td></tr></tbody></table></figure>
<p>程序执行结果：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">Python教程：http://c.biancheng.net/python</span><br><span class="line">Python教程：http://c.biancheng.net/python</span><br></pre></td></tr></tbody></table></figure>
<h4 id="用-call-弥补-hasattr-函数的短板"><a href="#用-call-弥补-hasattr-函数的短板" class="headerlink" title="用 call() 弥补 hasattr() 函数的短板"></a>用 <strong>call</strong>() 弥补 hasattr() 函数的短板</h4><p>前面章节介绍了 hasattr() 函数的用法，该函数的功能是查找类的实例对象中是否包含指定名称的属性或者方法，但该函数有一个缺陷，即它无法判断该指定的名称，到底是类属性还是类方法。</p>
<p>要解决这个问题，我们可以借助可调用对象的概念。要知道，类实例对象包含的方法，其实也属于可调用对象，但类属性却不是。举个例子：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CLanguage</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span> (self):</span><br><span class="line">        self.name = <span class="string">"C语言中文网"</span></span><br><span class="line">        self.add = <span class="string">"http://c.biancheng.net"</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">say</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"我正在学Python"</span>)</span><br><span class="line"></span><br><span class="line">clangs = CLanguage()</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">hasattr</span>(clangs,<span class="string">"name"</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">hasattr</span>(clangs.name,<span class="string">"__call__"</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"**********"</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">hasattr</span>(clangs,<span class="string">"say"</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">hasattr</span>(clangs.say,<span class="string">"__call__"</span>))</span><br></pre></td></tr></tbody></table></figure>
<p>程序执行结果：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="literal">False</span></span><br><span class="line">**********</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></tbody></table></figure>
<p>可以看到，由于 name 是类属性，它没有以 <strong>call</strong> 为名的 <strong>call</strong>() 方法；而 say 是类方法，它是可调用对象，因此它有 <strong>call</strong>() 方法。</p>
<h3 id="argparse-函数"><a href="#argparse-函数" class="headerlink" title="argparse()函数"></a>argparse()函数</h3><p><a href="https://docs.python.org/zh-cn/2/library/argparse.html#module-argparse"><code>argparse</code></a> 模块可以让人轻松编写用户友好的命令行接口。程序定义它需要的参数，然后 <a href="https://docs.python.org/zh-cn/2/library/argparse.html#module-argparse"><code>argparse</code></a> 将弄清如何从 <a href="https://docs.python.org/zh-cn/2/library/sys.html#sys.argv"><code>sys.argv</code></a> 解析出那些参数。 <a href="https://docs.python.org/zh-cn/2/library/argparse.html#module-argparse"><code>argparse</code></a> 模块还会自动生成帮助和使用手册，并在用户给程序传入无效参数时报出错误信息。</p>
<p>以下代码是一个 Python 程序，它获取一个整数列表并计算总和或者最大值：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'Process some integers.'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'integers'</span>, metavar=<span class="string">'N'</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, nargs=<span class="string">'+'</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">'an integer for the accumulator'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--sum'</span>, dest=<span class="string">'accumulate'</span>, action=<span class="string">'store_const'</span>,</span><br><span class="line">                    const=<span class="built_in">sum</span>, default=<span class="built_in">max</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">'sum the integers (default: find the max)'</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="built_in">print</span> args.accumulate(args.integers)</span><br></pre></td></tr></tbody></table></figure>
<p>假设上面的 Python 代码保存在名为 <code>prog.py</code> 的文件中，它可以在命令行运行并提供有用的帮助信息：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">$ python prog.py -h</span><br><span class="line">usage: prog.py [-h] [--<span class="built_in">sum</span>] N [N ...]</span><br><span class="line"></span><br><span class="line">Process some integers.</span><br><span class="line"></span><br><span class="line">positional arguments:</span><br><span class="line"> N           an integer <span class="keyword">for</span> the accumulator</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line"> -h, --<span class="built_in">help</span>  show this <span class="built_in">help</span> message <span class="keyword">and</span> exit</span><br><span class="line"> --<span class="built_in">sum</span>       <span class="built_in">sum</span> the integers (default: find the <span class="built_in">max</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>当使用适当的参数运行时，它会输出命令行传入整数的总和或者最大值：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">$ python prog.py <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"></span><br><span class="line">$ python prog.py <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> --<span class="built_in">sum</span></span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></tbody></table></figure>
<p>如果传入无效参数，则会报出错误：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">$ python prog.py a b c</span><br><span class="line">usage: prog.py [-h] [--<span class="built_in">sum</span>] N [N ...]</span><br><span class="line">prog.py: error: argument N: invalid <span class="built_in">int</span> value: <span class="string">'a'</span></span><br></pre></td></tr></tbody></table></figure>
<p>更多详情参考python文档，网址：<a href="https://docs.python.org/zh-cn/2/library/argparse.html">15.4. argparse — 命令行选项、参数和子命令解析器 — Python 2.7.18 文档</a></p>
<h3 id="Distuils"><a href="#Distuils" class="headerlink" title="Distuils"></a>Distuils</h3><p>大部分Python程序员都知道，有很多第三方包管理器供选择，包括setuptools、distribute等等。 有些是为了替代标准库中的distutils。</p>
<p>1.1概念和术语</p>
<p>对于模块开发者以及需要安装模块的使用者来说，Distutils的使用都很简单，作为一个开发者，除了编写源码之外，还需要：</p>
<p>·编写setup脚本（一般是setup.py）；</p>
<p>·编写一个setup配置文件（可选）；</p>
<p>·创建一个源码发布；</p>
<p>·创建一个或多个构建（二进制）发布（可选）;</p>
<p>有些模块开发者在开发时不会考虑多个平台发布，所以就有了packagers的角色，它们从模块开发者那取得源码发布，然后在多个平台上面进行构建，并发布多个平台的构建版本。</p>
<p>1.2简单例子</p>
<p>由python编写的setup脚本一般都非常简单。作为autoconf类型的配置脚本，setup脚本可以在构建和安装模块发布时运行多次。</p>
<p>比如，如果需要发布一个叫做foo的模块，它包含在一个文件foo.py，那setup脚本可以这样写：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup  </span><br><span class="line">setup(name=<span class="string">'foo'</span>,  </span><br><span class="line">       version=<span class="string">'1.0'</span>,  </span><br><span class="line">       py_modules=[<span class="string">'foo'</span>],  </span><br><span class="line">      )  </span><br></pre></td></tr></tbody></table></figure>
<p>setup函数的参数表示提供给Distutils的信息，这些参数分为两类：包的元数据（包名、版本号）以及包的信息（本例中是一个Python模块的列表）；模块由模块名表示，而不是文件名（对于包和扩展而言也是这样）；建议可以提供更多的元数据，比如你的名字，email地址和项目的URL地址。</p>
<p>编写好setup.py之后，就可以创建该模块的源码发布了：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">python setup.py sdist  </span><br></pre></td></tr></tbody></table></figure>
<p>sdist命令会创建一个archive 文件（比如Unix上的tar文件，Windows上的zip文件），它包含setup.py， foo.py。该archive文件命名为foo-1.0.tar.gz(zip)，解压之后的目录名是foo-1.0。</p>
<p>如果一个用户希望安装foo模块，他只需要下载foo-1.0.tar.gz，解压，进入foo-1.0目录，然后运行：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">python setup.py install  </span><br></pre></td></tr></tbody></table></figure>
<p>该命令最终会将foo.py复制到Python环境存放第三方模块的目录中。在linux环境下，运行该命令的输出是：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># python setup.py install  </span></span><br><span class="line">running install  </span><br><span class="line">running build  </span><br><span class="line">running build_py  </span><br><span class="line">creating build  </span><br><span class="line">creating build/lib  </span><br><span class="line">copying foo.py -&gt; build/lib  </span><br><span class="line">running install_lib  </span><br><span class="line">copying build/lib/foo.py -&gt; /usr/lib/python2<span class="number">.7</span>/site-packages  </span><br><span class="line">byte-compiling /usr/lib/python2<span class="number">.7</span>/site-packages/foo.py to foo.pyc  </span><br><span class="line">running install_egg_info  </span><br><span class="line">Writing /usr/lib/python2<span class="number">.7</span>/site-packages/foo-<span class="number">1.0</span>-py2<span class="number">.7</span>.egg-info  </span><br></pre></td></tr></tbody></table></figure>
<p>该命令生成的文件是：</p>
<p><code>/usr/lib/python2.7/site-packages/foo-1.0-py2.7.egg-info</code></p>
<p><code>/usr/lib/python2.7/site-packages/foo.py</code></p>
<p><code>/usr/lib/python2.7/site-packages/foo.pyc</code></p>
<h3 id="图片缩放方式"><a href="#图片缩放方式" class="headerlink" title="图片缩放方式"></a>图片缩放方式</h3><p>对图像进行预处理操作的时候，一般有两种缩放方式。</p>
<ul>
<li>一种是直接宽、高缩放至想要的宽、高，这种方式快捷，但可能会导致图像变形<br> step1: 计算宽高缩放比例，选择较小的那个缩放系数；<br> step2: 计算缩放后的尺寸: 原始图片的长宽都乘以较小的缩放系数；<br> step3：计算短边需要填充的灰边数，将短边的两边各自填充一半的灰行即可。</li>
<li>一种是等比例缩放，然后用灰色边缘填充</li>
</ul>
<h4 id="直接缩放"><a href="#直接缩放" class="headerlink" title="直接缩放"></a>直接缩放</h4><p>代码实现如下：</p>
<p><code>new_image = image.resize((target_w, target_h), Image.BICUBIC)</code></p>
<p><style>.flevdojiesbi{}</style><img src="/zh-CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/image-20220406191420560.png" class="lazyload" data-srcset="/zh-CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/image-20220406191420560.png" srcset="data:image/png;base64,666" class="flevdojiesbi lazyload"></p>
<h4 id="不变形缩放，两端填充灰边"><a href="#不变形缩放，两端填充灰边" class="headerlink" title="不变形缩放，两端填充灰边"></a>不变形缩放，两端填充灰边</h4><p><style>.wbdcxxsbsjdi{}</style><img src="/zh-CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/image-20220406191519097.png" class="lazyload" data-srcset="/zh-CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/image-20220406191519097.png" srcset="data:image/png;base64,666" class="wbdcxxsbsjdi lazyload"></p>
<h4 id="不变形缩放，一端填充灰边"><a href="#不变形缩放，一端填充灰边" class="headerlink" title="不变形缩放，一端填充灰边"></a>不变形缩放，一端填充灰边</h4><p><style>.oycnxsrsobur{}</style><img src="/zh-CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/image-20220406191620889.png" class="lazyload" data-srcset="/zh-CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/image-20220406191620889.png" srcset="data:image/png;base64,666" class="oycnxsrsobur lazyload"></p>
<p>很多图片的长宽比不同导致缩放填充后，两端的黑边大小都不同。而如果填充的比较多，则存在信息冗余，影响推理速度。YOLOv5作者对letterbox的缩放策略进行了修改，对原图自适应的添加最少的黑边。<br> <strong>计算方法：</strong><br> 1.计算原始图片宽高与输入尺寸的缩放比例rw和rh，选取r = min(rw,rh)后把原图按r进行缩放<br> 2.原图宽和高中一定有一边完全贴合输入尺寸，没有达到输入尺寸的一边计算与输入尺寸的差值，然后进行上下（or左右）的填充。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------------------------------#</span></span><br><span class="line"><span class="comment">#   对输入图像进行resize,他人测试发现，不用letterbox_image直接resize的效果更好</span></span><br><span class="line"><span class="comment"># ---------------------------------------------------#</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resize_image</span>(<span class="params">image, size, letterbox_image</span>):</span><br><span class="line">    iw, ih  = image.size</span><br><span class="line">    w, h    = size      <span class="comment"># w=200, h=300</span></span><br><span class="line">    <span class="keyword">if</span> letterbox_image:</span><br><span class="line">        scale   = <span class="built_in">min</span>(w/iw, h/ih)</span><br><span class="line">        nw      = <span class="built_in">int</span>(iw*scale)</span><br><span class="line">        nh      = <span class="built_in">int</span>(ih*scale)</span><br><span class="line"></span><br><span class="line">        image   = image.resize((nw,nh), Image.BICUBIC)</span><br><span class="line">        new_image = Image.new(<span class="string">'RGB'</span>, size, (<span class="number">128</span>,<span class="number">128</span>,<span class="number">128</span>))       <span class="comment"># 新建一张image，第二个参数表示尺寸，第三个参数表示颜色</span></span><br><span class="line">        <span class="comment"># --------------------------------------------------#</span></span><br><span class="line">        <span class="comment">#   image.paste函数表示将一张图片覆盖到另一张图片的指定位置去</span></span><br><span class="line">        <span class="comment">#   a.paste(b, (50,50))   将b的左上顶点贴到a的坐标为（50，50）的位置，左上顶点为(0,0), b超出a的部分会被自动舍弃</span></span><br><span class="line">        <span class="comment"># ---------------------------------------------------#</span></span><br><span class="line">        <span class="comment"># new_image.paste(image, ((w-nw)//2, (h-nh)//2))    # 不变形resize，两端填充灰边</span></span><br><span class="line">        new_image.paste(image, (<span class="number">0</span>, <span class="number">0</span>))      <span class="comment"># 不变形resize，一端填充灰边</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        new_image = image.resize((w, h), Image.BICUBIC)</span><br><span class="line">    <span class="keyword">return</span> new_image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img_PIL = Image.<span class="built_in">open</span>(<span class="string">"Avatar.jpg"</span>)</span><br><span class="line">img = resize_image(img_PIL, (<span class="number">200</span>, <span class="number">300</span>), <span class="literal">True</span>)   <span class="comment"># 第二参数表示目标尺寸，第三参数表示是否使用letterbox</span></span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 作者：寻找永不遗憾</span></span><br><span class="line"><span class="comment"># 链接：https://www.jianshu.com/p/2ae3a497f5f4</span></span><br><span class="line"><span class="comment"># 来源：简书</span></span><br><span class="line"><span class="comment"># 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="图像resize插值方式比较"><a href="#图像resize插值方式比较" class="headerlink" title="图像resize插值方式比较"></a>图像resize插值方式比较</h3><p>resize函数说明：</p>
<p><code>void resize(InputArray src, OutputArray dst,  Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR)</code></p>
<p>参数说明：</p>
<p><code>src</code>：输入，原图像，即待改变大小的图像；<br><code>dst</code>：输出，改变大小之后的图像，这个图像和原图像具有相同的内容，只是大小和原图像不一样而已；<br><code>dsize</code>：输出图像的大小。如果这个参数不为0，那么就代表将原图像缩放到这个Size(width，height)指定的大小；如果这个参数为0，那么原图像缩放之后的大小就要通过下面的公式来计算：</p>
<script type="math/tex; mode=display">
dsize = Size(round(fx*src.cols), round(fy*src.rows))</script><p> 其中，fx和fy就是下面要说的两个参数，是图像width方向和height方向的缩放比例。</p>
<p><code>fx</code>：width方向的缩放比例，如果它是0，那么它就会按照(double)dsize.width/src.cols来计算；<br><code>fy</code>：height方向的缩放比例，如果它是0，那么它就会按照(double)dsize.height/src.rows来计算；<br><code>interpolation</code>：这个是指定插值的方式，图像缩放之后，肯定像素要进行重新计算的，就靠这个参数来指定重新计算像素的方式，有以下几种：<br>      ·<code>INTER_NEAREST</code> - 最邻近插值<br>      ·<code>INTER_LINEAR</code> - 双线性插值，如果最后一个参数你不指定，默认使用这种方法<br>      ·<code>INTER_AREA</code>-区域插值<br>      ·<code>INTER_CUBIC</code> - 4x4像素邻域内的双立方插值<br>     ·<code>INTER_LANCZOS4</code>- 8x8像素邻域内的Lanczos插值</p>
<p>各种插值方式比较：</p>
<p>每种插值算法的前部分代码是相同的，如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">cv::Mat matSrc, matDst1, matDst2;</span><br><span class="line"></span><br><span class="line">matSrc = cv::imread(<span class="string">"lena.jpg"</span>, <span class="number">2</span> | <span class="number">4</span>);</span><br><span class="line">matDst1 = cv::Mat(cv::Size(<span class="number">800</span>, <span class="number">1000</span>), matSrc.<span class="built_in">type</span>(), cv::Scalar::<span class="built_in">all</span>(<span class="number">0</span>));</span><br><span class="line">matDst2 = cv::Mat(matDst1.size(), matSrc.<span class="built_in">type</span>(), cv::Scalar::<span class="built_in">all</span>(<span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">double scale_x = (double)matSrc.cols / matDst1.cols;</span><br><span class="line">double scale_y = (double)matSrc.rows / matDst1.rows;</span><br></pre></td></tr></tbody></table></figure>
<p>最近邻：</p>
<script type="math/tex; mode=display">
X_{src}=X_{dst}*(Width_{src}/Width_{dst})\\
Y_{src}=Y_{dst}*(Height_{src}/Height_{dst})</script><p>实现代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; matDst1.cols; ++i)</span><br><span class="line">{</span><br><span class="line">	<span class="built_in">int</span> sx = cvFloor(i * scale_x);</span><br><span class="line">	sx = std::<span class="built_in">min</span>(sx, matSrc.cols - <span class="number">1</span>);</span><br><span class="line">	<span class="keyword">for</span> (<span class="built_in">int</span> j = <span class="number">0</span>; j &lt; matDst1.rows; ++j)</span><br><span class="line">	{</span><br><span class="line">		<span class="built_in">int</span> sy = cvFloor(j * scale_y);</span><br><span class="line">		sy = std::<span class="built_in">min</span>(sy, matSrc.rows - <span class="number">1</span>);</span><br><span class="line">		matDst1.at&lt;cv::Vec3b&gt;(j, i) = matSrc.at&lt;cv::Vec3b&gt;(sy, sx);</span><br><span class="line">	}</span><br><span class="line">}</span><br><span class="line">cv::imwrite(<span class="string">"nearest_1.jpg"</span>, matDst1);</span><br><span class="line"></span><br><span class="line">cv::resize(matSrc, matDst2, matDst1.size(), <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">cv::imwrite(<span class="string">"nearest_2.jpg"</span>, matDst2);</span><br></pre></td></tr></tbody></table></figure>
<p>双线性：</p>
<script type="math/tex; mode=display">
Dst(X, Y)=(1-u)*(1-v)*Src(X',Y')+(1-u)*v*Src(X', Y'+1)+\\    
        u*(1-v)*Src(X'+1, Y')+u*v*Src(X'+1, Y'+1)</script><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">uchar* dataDst = matDst1.data;</span><br><span class="line"><span class="built_in">int</span> stepDst = matDst1.step;</span><br><span class="line">uchar* dataSrc = matSrc.data;</span><br><span class="line"><span class="built_in">int</span> stepSrc = matSrc.step;</span><br><span class="line"><span class="built_in">int</span> iWidthSrc = matSrc.cols;</span><br><span class="line"><span class="built_in">int</span> iHiehgtSrc = matSrc.rows;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">int</span> j = <span class="number">0</span>; j &lt; matDst1.rows; ++j)</span><br><span class="line">{</span><br><span class="line">	<span class="built_in">float</span> fy = (<span class="built_in">float</span>)((j + <span class="number">0.5</span>) * scale_y - <span class="number">0.5</span>);</span><br><span class="line">	<span class="built_in">int</span> sy = cvFloor(fy);</span><br><span class="line">	fy -= sy;</span><br><span class="line">	sy = std::<span class="built_in">min</span>(sy, iHiehgtSrc - <span class="number">2</span>);</span><br><span class="line">	sy = std::<span class="built_in">max</span>(<span class="number">0</span>, sy);</span><br><span class="line"></span><br><span class="line">	short cbufy[<span class="number">2</span>];</span><br><span class="line">	cbufy[<span class="number">0</span>] = cv::saturate_cast&lt;short&gt;((<span class="number">1.</span>f - fy) * <span class="number">2048</span>);</span><br><span class="line">	cbufy[<span class="number">1</span>] = <span class="number">2048</span> - cbufy[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; matDst1.cols; ++i)</span><br><span class="line">	{</span><br><span class="line">		<span class="built_in">float</span> fx = (<span class="built_in">float</span>)((i + <span class="number">0.5</span>) * scale_x - <span class="number">0.5</span>);</span><br><span class="line">		<span class="built_in">int</span> sx = cvFloor(fx);</span><br><span class="line">		fx -= sx;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (sx &lt; <span class="number">0</span>) {</span><br><span class="line">			fx = <span class="number">0</span>, sx = <span class="number">0</span>;</span><br><span class="line">		}</span><br><span class="line">		<span class="keyword">if</span> (sx &gt;= iWidthSrc - <span class="number">1</span>) {</span><br><span class="line">			fx = <span class="number">0</span>, sx = iWidthSrc - <span class="number">2</span>;</span><br><span class="line">		}</span><br><span class="line"></span><br><span class="line">		short cbufx[<span class="number">2</span>];</span><br><span class="line">		cbufx[<span class="number">0</span>] = cv::saturate_cast&lt;short&gt;((<span class="number">1.</span>f - fx) * <span class="number">2048</span>);</span><br><span class="line">		cbufx[<span class="number">1</span>] = <span class="number">2048</span> - cbufx[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (<span class="built_in">int</span> k = <span class="number">0</span>; k &lt; matSrc.channels(); ++k)</span><br><span class="line">		{</span><br><span class="line">			*(dataDst+ j*stepDst + <span class="number">3</span>*i + k) = (*(dataSrc + sy*stepSrc + <span class="number">3</span>*sx + k) * cbufx[<span class="number">0</span>] * cbufy[<span class="number">0</span>] + </span><br><span class="line">				*(dataSrc + (sy+<span class="number">1</span>)*stepSrc + <span class="number">3</span>*sx + k) * cbufx[<span class="number">0</span>] * cbufy[<span class="number">1</span>] + </span><br><span class="line">				*(dataSrc + sy*stepSrc + <span class="number">3</span>*(sx+<span class="number">1</span>) + k) * cbufx[<span class="number">1</span>] * cbufy[<span class="number">0</span>] + </span><br><span class="line">				*(dataSrc + (sy+<span class="number">1</span>)*stepSrc + <span class="number">3</span>*(sx+<span class="number">1</span>) + k) * cbufx[<span class="number">1</span>] * cbufy[<span class="number">1</span>]) &gt;&gt; <span class="number">22</span>;</span><br><span class="line">		}</span><br><span class="line">	}</span><br><span class="line">}</span><br><span class="line">cv::imwrite(<span class="string">"linear_1.jpg"</span>, matDst1);</span><br><span class="line"></span><br><span class="line">cv::resize(matSrc, matDst2, matDst1.size(), <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">cv::imwrite(<span class="string">"linear_2.jpg"</span>, matDst2);</span><br></pre></td></tr></tbody></table></figure>
<p>双三次：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">int</span> iscale_x = cv::saturate_cast&lt;<span class="built_in">int</span>&gt;(scale_x);</span><br><span class="line"><span class="built_in">int</span> iscale_y = cv::saturate_cast&lt;<span class="built_in">int</span>&gt;(scale_y);</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">int</span> j = <span class="number">0</span>; j &lt; matDst1.rows; ++j)</span><br><span class="line">{</span><br><span class="line">	<span class="built_in">float</span> fy = (<span class="built_in">float</span>)((j + <span class="number">0.5</span>) * scale_y - <span class="number">0.5</span>);</span><br><span class="line">	<span class="built_in">int</span> sy = cvFloor(fy);</span><br><span class="line">	fy -= sy;</span><br><span class="line">	sy = std::<span class="built_in">min</span>(sy, matSrc.rows - <span class="number">3</span>);</span><br><span class="line">	sy = std::<span class="built_in">max</span>(<span class="number">1</span>, sy);</span><br><span class="line"> </span><br><span class="line">	const <span class="built_in">float</span> A = -<span class="number">0.75</span>f;</span><br><span class="line"> </span><br><span class="line">	<span class="built_in">float</span> coeffsY[<span class="number">4</span>];</span><br><span class="line">	coeffsY[<span class="number">0</span>] = ((A*(fy + <span class="number">1</span>) - <span class="number">5</span>*A)*(fy + <span class="number">1</span>) + <span class="number">8</span>*A)*(fy + <span class="number">1</span>) - <span class="number">4</span>*A;</span><br><span class="line">	coeffsY[<span class="number">1</span>] = ((A + <span class="number">2</span>)*fy - (A + <span class="number">3</span>))*fy*fy + <span class="number">1</span>;</span><br><span class="line">	coeffsY[<span class="number">2</span>] = ((A + <span class="number">2</span>)*(<span class="number">1</span> - fy) - (A + <span class="number">3</span>))*(<span class="number">1</span> - fy)*(<span class="number">1</span> - fy) + <span class="number">1</span>;</span><br><span class="line">	coeffsY[<span class="number">3</span>] = <span class="number">1.</span>f - coeffsY[<span class="number">0</span>] - coeffsY[<span class="number">1</span>] - coeffsY[<span class="number">2</span>];</span><br><span class="line"> </span><br><span class="line">	short cbufY[<span class="number">4</span>];</span><br><span class="line">	cbufY[<span class="number">0</span>] = cv::saturate_cast&lt;short&gt;(coeffsY[<span class="number">0</span>] * <span class="number">2048</span>);</span><br><span class="line">	cbufY[<span class="number">1</span>] = cv::saturate_cast&lt;short&gt;(coeffsY[<span class="number">1</span>] * <span class="number">2048</span>);</span><br><span class="line">	cbufY[<span class="number">2</span>] = cv::saturate_cast&lt;short&gt;(coeffsY[<span class="number">2</span>] * <span class="number">2048</span>);</span><br><span class="line">	cbufY[<span class="number">3</span>] = cv::saturate_cast&lt;short&gt;(coeffsY[<span class="number">3</span>] * <span class="number">2048</span>);</span><br><span class="line"> </span><br><span class="line">	<span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; matDst1.cols; ++i)</span><br><span class="line">	{</span><br><span class="line">		<span class="built_in">float</span> fx = (<span class="built_in">float</span>)((i + <span class="number">0.5</span>) * scale_x - <span class="number">0.5</span>);</span><br><span class="line">		<span class="built_in">int</span> sx = cvFloor(fx);</span><br><span class="line">		fx -= sx;</span><br><span class="line"> </span><br><span class="line">		<span class="keyword">if</span> (sx &lt; <span class="number">1</span>) {</span><br><span class="line">			fx = <span class="number">0</span>, sx = <span class="number">1</span>;</span><br><span class="line">		}</span><br><span class="line">		<span class="keyword">if</span> (sx &gt;= matSrc.cols - <span class="number">3</span>) {</span><br><span class="line">			fx = <span class="number">0</span>, sx = matSrc.cols - <span class="number">3</span>;</span><br><span class="line">		}</span><br><span class="line"> </span><br><span class="line">		<span class="built_in">float</span> coeffsX[<span class="number">4</span>];</span><br><span class="line">		coeffsX[<span class="number">0</span>] = ((A*(fx + <span class="number">1</span>) - <span class="number">5</span>*A)*(fx + <span class="number">1</span>) + <span class="number">8</span>*A)*(fx + <span class="number">1</span>) - <span class="number">4</span>*A;</span><br><span class="line">		coeffsX[<span class="number">1</span>] = ((A + <span class="number">2</span>)*fx - (A + <span class="number">3</span>))*fx*fx + <span class="number">1</span>;</span><br><span class="line">		coeffsX[<span class="number">2</span>] = ((A + <span class="number">2</span>)*(<span class="number">1</span> - fx) - (A + <span class="number">3</span>))*(<span class="number">1</span> - fx)*(<span class="number">1</span> - fx) + <span class="number">1</span>;</span><br><span class="line">		coeffsX[<span class="number">3</span>] = <span class="number">1.</span>f - coeffsX[<span class="number">0</span>] - coeffsX[<span class="number">1</span>] - coeffsX[<span class="number">2</span>];</span><br><span class="line"> </span><br><span class="line">		short cbufX[<span class="number">4</span>];</span><br><span class="line">		cbufX[<span class="number">0</span>] = cv::saturate_cast&lt;short&gt;(coeffsX[<span class="number">0</span>] * <span class="number">2048</span>);</span><br><span class="line">		cbufX[<span class="number">1</span>] = cv::saturate_cast&lt;short&gt;(coeffsX[<span class="number">1</span>] * <span class="number">2048</span>);</span><br><span class="line">		cbufX[<span class="number">2</span>] = cv::saturate_cast&lt;short&gt;(coeffsX[<span class="number">2</span>] * <span class="number">2048</span>);</span><br><span class="line">		cbufX[<span class="number">3</span>] = cv::saturate_cast&lt;short&gt;(coeffsX[<span class="number">3</span>] * <span class="number">2048</span>);</span><br><span class="line"> </span><br><span class="line">		<span class="keyword">for</span> (<span class="built_in">int</span> k = <span class="number">0</span>; k &lt; matSrc.channels(); ++k)</span><br><span class="line">		{</span><br><span class="line">			matDst1.at&lt;cv::Vec3b&gt;(j, i)[k] = <span class="built_in">abs</span>((matSrc.at&lt;cv::Vec3b&gt;(sy-<span class="number">1</span>, sx-<span class="number">1</span>)[k] * cbufX[<span class="number">0</span>] * cbufY[<span class="number">0</span>] + matSrc.at&lt;cv::Vec3b&gt;(sy, sx-<span class="number">1</span>)[k] * cbufX[<span class="number">0</span>] * cbufY[<span class="number">1</span>] +</span><br><span class="line">				matSrc.at&lt;cv::Vec3b&gt;(sy+<span class="number">1</span>, sx-<span class="number">1</span>)[k] * cbufX[<span class="number">0</span>] * cbufY[<span class="number">2</span>] + matSrc.at&lt;cv::Vec3b&gt;(sy+<span class="number">2</span>, sx-<span class="number">1</span>)[k] * cbufX[<span class="number">0</span>] * cbufY[<span class="number">3</span>] +</span><br><span class="line">				matSrc.at&lt;cv::Vec3b&gt;(sy-<span class="number">1</span>, sx)[k] * cbufX[<span class="number">1</span>] * cbufY[<span class="number">0</span>] + matSrc.at&lt;cv::Vec3b&gt;(sy, sx)[k] * cbufX[<span class="number">1</span>] * cbufY[<span class="number">1</span>] +</span><br><span class="line">				matSrc.at&lt;cv::Vec3b&gt;(sy+<span class="number">1</span>, sx)[k] * cbufX[<span class="number">1</span>] * cbufY[<span class="number">2</span>] + matSrc.at&lt;cv::Vec3b&gt;(sy+<span class="number">2</span>, sx)[k] * cbufX[<span class="number">1</span>] * cbufY[<span class="number">3</span>] +</span><br><span class="line">				matSrc.at&lt;cv::Vec3b&gt;(sy-<span class="number">1</span>, sx+<span class="number">1</span>)[k] * cbufX[<span class="number">2</span>] * cbufY[<span class="number">0</span>] + matSrc.at&lt;cv::Vec3b&gt;(sy, sx+<span class="number">1</span>)[k] * cbufX[<span class="number">2</span>] * cbufY[<span class="number">1</span>] +</span><br><span class="line">				matSrc.at&lt;cv::Vec3b&gt;(sy+<span class="number">1</span>, sx+<span class="number">1</span>)[k] * cbufX[<span class="number">2</span>] * cbufY[<span class="number">2</span>] + matSrc.at&lt;cv::Vec3b&gt;(sy+<span class="number">2</span>, sx+<span class="number">1</span>)[k] * cbufX[<span class="number">2</span>] * cbufY[<span class="number">3</span>] +</span><br><span class="line">				matSrc.at&lt;cv::Vec3b&gt;(sy-<span class="number">1</span>, sx+<span class="number">2</span>)[k] * cbufX[<span class="number">3</span>] * cbufY[<span class="number">0</span>] + matSrc.at&lt;cv::Vec3b&gt;(sy, sx+<span class="number">2</span>)[k] * cbufX[<span class="number">3</span>] * cbufY[<span class="number">1</span>] +</span><br><span class="line">				matSrc.at&lt;cv::Vec3b&gt;(sy+<span class="number">1</span>, sx+<span class="number">2</span>)[k] * cbufX[<span class="number">3</span>] * cbufY[<span class="number">2</span>] + matSrc.at&lt;cv::Vec3b&gt;(sy+<span class="number">2</span>, sx+<span class="number">2</span>)[k] * cbufX[<span class="number">3</span>] * cbufY[<span class="number">3</span>] ) &gt;&gt; <span class="number">22</span>);</span><br><span class="line">		}</span><br><span class="line">	}</span><br><span class="line">}</span><br><span class="line">cv::imwrite(<span class="string">"cubic_1.jpg"</span>, matDst1);</span><br><span class="line"> </span><br><span class="line">cv::resize(matSrc, matDst2, matDst1.size(), <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>);</span><br><span class="line">cv::imwrite(<span class="string">"cubic_2.jpg"</span>, matDst2);</span><br></pre></td></tr></tbody></table></figure>
<p>基于像素区域关系：共分三种情况，图像放大时类似于双线性插值，图像缩小(x轴、y轴同时缩小)又分两种情况，此情况下可以避免波纹出现</p>
<p>具体实现代码可以参考<a href="https://github.com/fengbingchun/OpenCV_Test/blob/master/src/fbc_cv/include/resize.hpp，用法如下：">https://github.com/fengbingchun/OpenCV_Test/blob/master/src/fbc_cv/include/resize.hpp，用法如下：</a></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">fbc::Mat3BGR src(matSrc.rows, matSrc.cols, matSrc.data);</span><br><span class="line">fbc::Mat3BGR dst(matDst1.rows, matDst1.cols, matDst1.data);</span><br><span class="line">fbc::resize(src, dst, <span class="number">3</span>);</span><br></pre></td></tr></tbody></table></figure>
<p>兰索斯插值：略</p>
<p>测试代码：</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span>  millisecond 1000000</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DEBUG_PRINT(...)  printf( __VA_ARGS__); printf(<span class="string">"\n"</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DEBUG_TIME(time_) auto time_ =std::chrono::high_resolution_clock::now()</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> RUN_TIME(time_)  (double)(time_).count()/millisecond</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"> </span><br><span class="line"><span class="function">cv::Mat <span class="title">image_resize</span><span class="params">(cv::Mat image, <span class="type">int</span> width, <span class="type">int</span> height, <span class="type">int</span> interpolation, <span class="type">int</span> num)</span> </span>{</span><br><span class="line">    cv::Mat dest;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num; ++i) {</span><br><span class="line">        cv::<span class="built_in">resize</span>(image, dest, cv::<span class="built_in">Size</span>(width, height), <span class="number">0</span>, <span class="number">0</span>, interpolation);<span class="comment">//最近邻插值</span></span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> dest;</span><br><span class="line">}</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>{</span><br><span class="line">    string path = <span class="string">"../1.jpg"</span>;</span><br><span class="line">    cv::Mat image = cv::<span class="built_in">imread</span>(path);</span><br><span class="line">    cv::<span class="built_in">resize</span>(image, image, cv::<span class="built_in">Size</span>(<span class="number">1000</span>, <span class="number">1000</span>));</span><br><span class="line">    <span class="type">int</span> re_width = <span class="number">900</span>;</span><br><span class="line">    <span class="type">int</span> re_height = <span class="number">900</span>;</span><br><span class="line">    <span class="type">int</span>  num=<span class="number">10</span>;</span><br><span class="line">    cv::Mat image2X_INTER_NEAREST;</span><br><span class="line">    cv::Mat image2X_INTER_LINEAR;</span><br><span class="line">    cv::Mat image2X_INTER_AREA;</span><br><span class="line">    cv::Mat image2X_INTER_CUBIC;</span><br><span class="line">    cv::Mat initMat;</span><br><span class="line">    <span class="built_in">DEBUG_PRINT</span>(<span class="string">"image input size:%dx%d"</span>, image.rows, image.cols);</span><br><span class="line">    <span class="built_in">DEBUG_TIME</span>(T0);</span><br><span class="line">    image2X_INTER_NEAREST=<span class="built_in">image_resize</span>(image, re_width, re_height, cv::INTER_NEAREST, num);</span><br><span class="line">    <span class="built_in">DEBUG_TIME</span>(T1);</span><br><span class="line">    image2X_INTER_LINEAR=<span class="built_in">image_resize</span>(image, re_width, re_height, cv::INTER_LINEAR, num);</span><br><span class="line">    <span class="built_in">DEBUG_TIME</span>(T2);</span><br><span class="line">    image2X_INTER_AREA=<span class="built_in">image_resize</span>(image, re_width, re_height, cv::INTER_AREA, num);</span><br><span class="line">    <span class="built_in">DEBUG_TIME</span>(T3);</span><br><span class="line">    image2X_INTER_CUBIC=<span class="built_in">image_resize</span>(image, re_width, re_height, cv::INTER_CUBIC, num);</span><br><span class="line">    <span class="built_in">DEBUG_TIME</span>(T4);</span><br><span class="line">    <span class="built_in">DEBUG_PRINT</span>(<span class="string">"resize_image:%dx%d,INTER_NEAREST:%3.3fms"</span>,</span><br><span class="line">            image2X_INTER_NEAREST.rows,</span><br><span class="line">            image2X_INTER_NEAREST.cols,</span><br><span class="line">            <span class="built_in">RUN_TIME</span>(T1 - T0)/num);</span><br><span class="line">    <span class="built_in">DEBUG_PRINT</span>(<span class="string">"resize_image:%dx%d,INTER_LINEAR :%3.3fms"</span>,</span><br><span class="line">            image2X_INTER_LINEAR.rows,</span><br><span class="line">            image2X_INTER_LINEAR.cols,</span><br><span class="line">            <span class="built_in">RUN_TIME</span>(T2 - T1)/num);</span><br><span class="line">    <span class="built_in">DEBUG_PRINT</span>(<span class="string">"resize_image:%dx%d,INTER_AREA   :%3.3fms"</span>,</span><br><span class="line">            image2X_INTER_AREA.rows,</span><br><span class="line">            image2X_INTER_AREA.cols,</span><br><span class="line">            <span class="built_in">RUN_TIME</span>(T3 - T2)/num);</span><br><span class="line">    <span class="built_in">DEBUG_PRINT</span>(<span class="string">"resize_image:%dx%d,INTER_CUBIC  :%3.3fms"</span>,</span><br><span class="line">            image2X_INTER_CUBIC.rows,</span><br><span class="line">            image2X_INTER_CUBIC.cols,</span><br><span class="line">            <span class="built_in">RUN_TIME</span>(T4 - T3)/num);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line">版权声明：本文为CSDN博主「pan_jinquan」的原创文章，遵循CC <span class="number">4.0</span> BY-SA版权协议，转载请附上原文出处链接及本声明。</span><br><span class="line">原文链接：https:<span class="comment">//blog.csdn.net/guyuealian/article/details/85097633</span></span><br></pre></td></tr></tbody></table></figure>
<p>运行结果：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">image input size:1000x1000</span><br><span class="line">resize_image:900x900,INTER_NEAREST:0.389ms</span><br><span class="line">resize_image:900x900,INTER_LINEAR :0.605ms</span><br><span class="line">resize_image:900x900,INTER_AREA   :2.611ms</span><br><span class="line">resize_image:900x900,INTER_CUBIC  :1.920ms</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>总结：</p>
<figure class="highlight markdown"><table><tbody><tr><td class="code"><pre><span class="line"> 速度比较：INTER<span class="emphasis">_NEAREST（最近邻插值)&gt;INTER_</span>LINEAR(线性插值)&gt;INTER<span class="emphasis">_CUBIC(三次样条插值)&gt;INTER_</span>AREA  (区域插值)</span><br><span class="line">对图像进行缩小时，为了避免出现波纹现象，推荐采用INTER<span class="emphasis">_AREA 区域插值方法。</span></span><br><span class="line"><span class="emphasis">OpenCV推荐：如果要缩小图像，通常推荐使用#INTER_</span>AREA插值效果最好，而要放大图像，通常使用INTER<span class="emphasis">_CUBIC(速度较慢，但效果最好)，或者使用INTER_</span>LINEAR(速度较快，效果还可以)。至于最近邻插值INTER<span class="emphasis">_NEAREST，一般不推荐使用</span></span><br><span class="line"><span class="emphasis"></span></span><br></pre></td></tr></tbody></table></figure>
<p>更多详情参考：<a href="https://docs.opencv.org/3.2.0/da/d54/group__imgproc__transform.html#ga47a974309e9102f5f08231edc7e7529d">OpenCV: Geometric Image Transformations</a></p>
<h3 id="torch-nn-Identity"><a href="#torch-nn-Identity" class="headerlink" title="torch.nn.Identity()"></a>torch.nn.Identity()</h3><p>dentity模块如果不改变输入，直接返回输入</p>
<p>一种编码技巧吧，比如我们要加深网络，有些层是不改变输入数据的维度的，</p>
<p>在增减网络的过程中我们就可以用identity占个位置，这样网络整体层数永远不变，</p>
<p>应用：</p>
<p>例如此时：如果此时我们使用了se_layer，那么就SELayer(dim)，否则就输入什么就输出什么（什么都不做）</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 定义操作</span></span><br><span class="line">self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">self.proj = nn.Linear(dim, dim)</span><br><span class="line">self.se_layer = SELayer(dim) <span class="keyword">if</span> se_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line">self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"><span class="comment"># dropout</span></span><br><span class="line">attn = self.attn_drop(attn)</span><br><span class="line"><span class="comment"># 与矩阵V相乘</span></span><br><span class="line">x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B_, N, C)</span><br><span class="line"><span class="comment"># 经过一层全连接层</span></span><br><span class="line">x = self.proj(x)</span><br><span class="line"><span class="comment"># nn.Identity():建立一个输入模块，什么都不做</span></span><br><span class="line">x = self.se_layer(x)</span><br><span class="line"><span class="comment"># dropout</span></span><br><span class="line">x = self.proj_drop(x)</span><br><span class="line"><span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>
<p>更多详情参考<a href="https://pytorch.org/docs/stable/generated/torch.nn.Identity.html?highlight=identity#torch.nn.Identity">Identity — PyTorch 1.11.0 documentation</a></p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>函数库</tag>
      </tags>
  </entry>
  <entry>
    <title>马裤先生</title>
    <url>/zh-CN/%E9%A9%AC%E8%A3%A4%E5%85%88%E7%94%9F/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>火车在北平东站还没开，同屋那位睡上铺的穿马裤，戴平光的眼镜，青缎子洋服上身，胸袋插着小楷羊毫，足登青绒快靴的先生发了问：「你也是从北平上车？」很和气的。</p>
<p>火车还没动呢，不从北平上车，由哪儿呢？我只好反攻了：「你从哪儿上车？」</p>
<p>他没言语。看了看铺位，用尽全身的力气喊了声，「茶房！」</p>
<p>茶房跑来了。</p>
<p>「拿毯子！」马裤先生喊。</p>
<p>「请少待一会儿，先生。」茶房很和气的说，</p>
<p>「拿枕头！」</p>
<p>「先生，您等我忙过这会儿去，毯子和枕头就一齐全到。」茶房说的很快，可依然是很和气。</p>
<p>马裤先生没任何的表示。茶房故意地笑了笑，表示歉意。然后搭讪着慢慢地转身，腿刚预备好要走，背后打了个霹雳：「茶房！<br>」</p>
<p>茶房不是假装没听见，便是耳朵已经震聋，竟自没回头，一直地快步走开。</p>
<p>「茶房！茶房！茶房！」马裤先生连喊，一声比一声高：站台上送客的跑过一群来，以为车上失了火，要不然便是出了人命。茶<br>房始终没回头。马裤先生又挖了鼻孔一下，坐在我的床上。</p>
<p>茶房从门前走过。</p>
<p>「茶房！拿手巾把！」</p>
<p>「等等。」茶房似乎下了抵抗的决心。</p>
<p>马裤先生把领带解开，摘下领子来，分别挂在铁钩上：所有的钩子都被占了，他的帽子，大衣，已占了两个。车开了，他爬上了<br>上铺，在我的头上脱靴子，并且击打靴底上的土。枕着个手提箱，车还没到永定门，他睡着了。我心中安坦了许多。</p>
<p>到了丰台，车还没站住，上面出了声：「茶房！」</p>
<p>没等茶房答应，他又睡着了；大概这次是梦话。</p>
<p>过了丰台，大概还没到廊房，上面又打了雷：「茶房！」</p>
<p>茶房来了，眉毛拧得好像要把谁吃了才痛快。</p>
<p>「干吗？先——生——」</p>
<p>「拿茶！」</p>
<p>「好吧！」茶房的眉毛拧得直往下落毛。</p>
<p>马裤先生又入了梦乡，呼声只比「茶房」小一点。有时呼声稍低一点，用咬牙来补上。</p>
<p>到了天津。又上来些旅客。车好容易又从天津开走。刚一开车，茶房给马裤先生拿来头一份毯子枕头和手巾把。马裤先生用手巾<br>把耳鼻孔全钻得到家，这一把手巾擦了至少有一刻钟，最后用手巾擦了擦手提箱上的土。我给他数着，从老站到总站的十来分钟之间，他又喊了四五十声茶房。茶房只来了一次，他的问题是火车向哪面走呢？茶房的回答是不知道；于是又引起他的建议，车上总该有人知道，茶房应当负责去问。茶房说，连驶车的也不晓得东西南北。于是他几乎变了颜色，万一车走迷了路？！</p>
<p>茶房没再回答，可是又掉了几根眉毛。</p>
<p>他又睡了，这次是在头上摔了摔袜子，可是一口痰并没往下唾，而是照顾了车顶。</p>
<p>我的目的地是德州，天将亮就到了。谢天谢地！</p>
<p>我雇好车，进了城，还清清楚楚地听见「茶房！」</p>
<p>一个多礼拜了，我还惦记着茶房的眉毛呢。</p>
]]></content>
      <categories>
        <category>文学黑洞</category>
      </categories>
      <tags>
        <tag>今日故事</tag>
      </tags>
  </entry>
  <entry>
    <title>数学基础</title>
    <url>/zh-TW/ch01_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h1><pre><code>深度学习通常又需要哪些数学基础？深度学习里的数学到底难在哪里？通常初学者都会有这些问题，在网络推荐及书本推荐里，经常看到会列出一系列数学科目，比如微积分、线性代数、概率论、复变函数、数值计算、优化理论、信息论等等。这些数学知识有相关性，但实际上按照这样的知识范围来学习，学习成本会很久，而且会很枯燥，本章我们通过选举一些数学基础里容易混淆的一些概念做以介绍，帮助大家更好的理清这些易混淆概念之间的关系。
</code></pre><h2 id="向量和矩阵"><a href="#向量和矩阵" class="headerlink" title="向量和矩阵"></a>向量和矩阵</h2><h3 id="标量、向量、矩阵、张量之间的联系"><a href="#标量、向量、矩阵、张量之间的联系" class="headerlink" title="标量、向量、矩阵、张量之间的联系"></a>标量、向量、矩阵、张量之间的联系</h3><p><strong>标量（scalar）</strong><br>    一个标量表示一个单独的数，它不同于线性代数中研究的其他大部分对象（通常是多个数的数组）。我们用斜体表示标量。标量通常被赋予小写的变量名称。 </p>
<p><strong>向量（vector）</strong><br>    一个向量表示一组有序排列的数。通过次序中的索引，我们可以确定每个单独的数。通常我们赋予向量粗体的小写变量名称，比如xx。向量中的元素可以通过带脚标的斜体表示。向量$X$的第一个元素是$X_1$，第二个元素是$X_2$，以此类推。我们也会注明存储在向量中的元素的类型（实数、虚数等）。</p>
<p><strong>矩阵（matrix）</strong><br>    矩阵是具有相同特征和纬度的对象的集合，表现为一张二维数据表。其意义是一个对象表示为矩阵中的一行，一个特征表示为矩阵中的一列，每个特征都有数值型的取值。通常会赋予矩阵粗体的大写变量名称，比如$A$。</p>
<p><strong>张量（tensor）</strong><br>    在某些情况下，我们会讨论坐标超过两维的数组。一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们将其称之为张量。使用 $A$ 来表示张量“A”。张量$A$中坐标为$(i,j,k)$的元素记作$A_{(i,j,k)}$。 </p>
<p><strong>四者之间关系</strong>  </p>
<blockquote>
<p>标量是0阶张量，向量是一阶张量。举例：<br>​标量就是知道棍子的长度，但是你不会知道棍子指向哪儿。<br>​向量就是不但知道棍子的长度，还知道棍子指向前面还是后面。<br>​张量就是不但知道棍子的长度，也知道棍子指向前面还是后面，还能知道这棍子又向上/下和左/右偏转了多少。</p>
</blockquote>
<h3 id="张量与矩阵的区别"><a href="#张量与矩阵的区别" class="headerlink" title="张量与矩阵的区别"></a>张量与矩阵的区别</h3><pre><code>- 从代数角度讲， 矩阵它是向量的推广。向量可以看成一维的“表格”（即分量按照顺序排成一排）， 矩阵是二维的“表格”（分量按照纵横位置排列）， 那么$n$阶张量就是所谓的$n$维的“表格”。 张量的严格定义是利用线性映射来描述。
- 从几何角度讲， 矩阵是一个真正的几何量，也就是说，它是一个不随参照系的坐标变换而变化的东西。向量也具有这种特性。
- 张量可以用3×3矩阵形式来表达。 
- 表示标量的数和表示向量的三维数组也可分别看作1×1，1×3的矩阵。 
</code></pre><h3 id="矩阵和向量相乘结果"><a href="#矩阵和向量相乘结果" class="headerlink" title="矩阵和向量相乘结果"></a>矩阵和向量相乘结果</h3><pre><code>若使用爱因斯坦求和约定（Einstein summation convention），矩阵$A$, $B$相乘得到矩阵$C$可以用下式表示：
</code></pre><script type="math/tex; mode=display">a_{ik}*b_{kj}=c_{ij} \tag{1.3-1}</script><pre><code>其中，$a_{ik}$, $b_{kj}$, $c_{ij}$分别表示矩阵$A, B, C$的元素，$k$出现两次，是一个哑变量（Dummy Variables）表示对该参数进行遍历求和。
而矩阵和向量相乘可以看成是矩阵相乘的一个特殊情况，例如：矩阵$B$是一个$n \times 1$的矩阵。
</code></pre><h3 id="向量和矩阵的范数归纳"><a href="#向量和矩阵的范数归纳" class="headerlink" title="向量和矩阵的范数归纳"></a>向量和矩阵的范数归纳</h3><p><strong>向量的范数(norm)</strong><br>​    定义一个向量为：$\vec{a}=[-5, 6, 8, -10]$。任意一组向量设为$\vec{x}=(x_1,x_2,…,x_N)$。其不同范数求解如下：</p>
<ul>
<li>向量的1范数：向量的各个元素的绝对值之和，上述向量$\vec{a}$的1范数结果就是：29。</li>
</ul>
<script type="math/tex; mode=display">
\Vert\vec{x}\Vert_1=\sum_{i=1}^N\vert{x_i}\vert</script><ul>
<li>向量的2范数：向量的每个元素的平方和再开平方根，上述$\vec{a}$的2范数结果就是：15。</li>
</ul>
<script type="math/tex; mode=display">
\Vert\vec{x}\Vert_2=\sqrt{\sum_{i=1}^N{\vert{x_i}\vert}^2}</script><ul>
<li>向量的负无穷范数：向量的所有元素的绝对值中最小的：上述向量$\vec{a}$的负无穷范数结果就是：5。  </li>
</ul>
<script type="math/tex; mode=display">
\Vert\vec{x}\Vert_{-\infty}=\min{|{x_i}|}</script><ul>
<li>向量的正无穷范数：向量的所有元素的绝对值中最大的：上述向量$\vec{a}$的正无穷范数结果就是：10。 </li>
</ul>
<script type="math/tex; mode=display">
\Vert\vec{x}\Vert_{+\infty}=\max{|{x_i}|}</script><ul>
<li>向量的p范数：</li>
</ul>
<script type="math/tex; mode=display">
L_p=\Vert\vec{x}\Vert_p=\sqrt[p]{\sum_{i=1}^{N}|{x_i}|^p}</script><p><strong>矩阵的范数</strong>  </p>
<p>定义一个矩阵$A=[-1, 2, -3; 4, -6, 6]$。 任意矩阵定义为：$A<em>{m\times n}$，其元素为 $a</em>{ij}$。</p>
<p>矩阵的范数定义为</p>
<script type="math/tex; mode=display">
\Vert{A}\Vert_p :=\sup_{x\neq 0}\frac{\Vert{Ax}\Vert_p}{\Vert{x}\Vert_p}</script><p>当向量取不同范数时, 相应得到了不同的矩阵范数。</p>
<ul>
<li><strong>矩阵的1范数（列范数）</strong>：矩阵的每一列上的元</li>
</ul>
<p>  素绝对值先求和，再从中取个最大的,（列和最大），上述矩阵$A$的1范数先得到$[5,8,9]$，再取最大的最终结果就是：9。</p>
<script type="math/tex; mode=display">
\Vert A\Vert_1=\max_{1\le j\le n}\sum_{i=1}^m|{a_{ij}}|</script><ul>
<li><strong>矩阵的2范数</strong>：矩阵$A^TA$的最大特征值开平方根，上述矩阵$A$的2范数得到的最终结果是：10.0623。 </li>
</ul>
<script type="math/tex; mode=display">
\Vert A\Vert_2=\sqrt{\lambda_{max}(A^T A)}</script><p>其中， $\lambda_{max}(A^T A)$ 为 $A^T A$ 的特征值绝对值的最大值。</p>
<ul>
<li><p><strong>矩阵的无穷范数（行范数）</strong>：矩阵的每一行上的元素绝对值先求和，再从中取个最大的，（行和最大），上述矩阵$A$的行范数先得到$[6；16]$，再取最大的最终结果就是：16。 </p>
<script type="math/tex; mode=display">
\Vert A\Vert_{\infty}=\max_{1\le i \le m}\sum_{j=1}^n |{a_{ij}}|</script></li>
<li><p><strong>矩阵的核范数</strong>：矩阵的奇异值（将矩阵svd分解）之和，这个范数可以用来低秩表示（因为最小化核范数，相当于最小化矩阵的秩——低秩），上述矩阵A最终结果就是：10.9287。  </p>
</li>
<li><p><strong>矩阵的L0范数</strong>：矩阵的非0元素的个数，通常用它来表示稀疏，L0范数越小0元素越多，也就越稀疏，上述矩阵$A$最终结果就是：6。</p>
</li>
<li><strong>矩阵的L1范数</strong>：矩阵中的每个元素绝对值之和，它是L0范数的最优凸近似，因此它也可以表示稀疏，上述矩阵$A$最终结果就是：22。  </li>
<li><strong>矩阵的F范数</strong>：矩阵的各个元素平方之和再开平方根，它通常也叫做矩阵的L2范数，它的优点在于它是一个凸函数，可以求导求解，易于计算，上述矩阵A最终结果就是：10.0995。  </li>
</ul>
<script type="math/tex; mode=display">
\Vert A\Vert_F=\sqrt{(\sum_{i=1}^m\sum_{j=1}^n{| a_{ij}|}^2)}</script><ul>
<li><strong>矩阵的L21范数</strong>：矩阵先以每一列为单位，求每一列的F范数（也可认为是向量的2范数），然后再将得到的结果求L1范数（也可认为是向量的1范数），很容易看出它是介于L1和L2之间的一种范数，上述矩阵$A$最终结果就是：17.1559。 </li>
<li><strong>矩阵的 p范数</strong> </li>
</ul>
<script type="math/tex; mode=display">
\Vert A\Vert_p=\sqrt[p]{(\sum_{i=1}^m\sum_{j=1}^n{| a_{ij}|}^p)}</script><h3 id="如何判断一个矩阵为正定"><a href="#如何判断一个矩阵为正定" class="headerlink" title="如何判断一个矩阵为正定"></a>如何判断一个矩阵为正定</h3><p>判定一个矩阵是否为正定，通常有以下几个方面：  </p>
<ul>
<li>顺序主子式全大于0；  </li>
<li>存在可逆矩阵$C$使$C^TC$等于该矩阵；</li>
<li>正惯性指数等于$n$；</li>
<li>合同于单位矩阵$E$（即：规范形为$E$）</li>
<li>标准形中主对角元素全为正；</li>
<li>特征值全为正；</li>
<li>是某基的度量矩阵。</li>
</ul>
<h2 id="导数和偏导数"><a href="#导数和偏导数" class="headerlink" title="导数和偏导数"></a>导数和偏导数</h2><h3 id="导数偏导计算"><a href="#导数偏导计算" class="headerlink" title="导数偏导计算"></a>导数偏导计算</h3><p><strong>导数定义</strong>:</p>
<p>导数(derivative)代表了在自变量变化趋于无穷小的时候，函数值的变化与自变量的变化的比值。几何意义是这个点的切线。物理意义是该时刻的（瞬时）变化率。<br>​</p>
<p><em>注意</em>：在一元函数中，只有一个自变量变动，也就是说只存在一个方向的变化率，这也就是为什么一元函数没有偏导数的原因。在物理学中有平均速度和瞬时速度之说。平均速度有</p>
<script type="math/tex; mode=display">
v=\frac{s}{t}</script><p>其中$v$表示平均速度，$s$表示路程，$t$表示时间。这个公式可以改写为</p>
<script type="math/tex; mode=display">
\bar{v}=\frac{\Delta s}{\Delta t}=\frac{s(t_0+\Delta t)-s(t_0)}{\Delta t}</script><p>其中$\Delta s$表示两点之间的距离，而$\Delta t$表示走过这段距离需要花费的时间。当$\Delta t$趋向于0（$\Delta t \to 0$）时，也就是时间变得很短时，平均速度也就变成了在$t_0$时刻的瞬时速度，表示成如下形式：</p>
<script type="math/tex; mode=display">
v(t_0)=\lim_{\Delta t \to 0}{\bar{v}}=\lim_{\Delta t \to 0}{\frac{\Delta s}{\Delta t}}=\lim_{\Delta t \to 0}{\frac{s(t_0+\Delta t)-s(t_0)}{\Delta t}}</script><p>实际上，上式表示的是路程$s$关于时间$t$的函数在$t=t_0$处的导数。一般的，这样定义导数：如果平均变化率的极限存在，即有</p>
<script type="math/tex; mode=display">
\lim_{\Delta x \to 0}{\frac{\Delta y}{\Delta x}}=\lim_{\Delta x \to 0}{\frac{f(x_0+\Delta x)-f(x_0)}{\Delta x}}</script><p>则称此极限为函数 $y=f(x)$ 在点 $x<em>0$ 处的导数。记作 $f’(x_0)$ 或 $y’\vert</em>{x=x<em>0}$ 或 $\frac{dy}{dx}\vert</em>{x=x<em>0}$ 或 $\frac{df(x)}{dx}\vert</em>{x=x_0}$。</p>
<p>通俗地说，导数就是曲线在某一点切线的斜率。</p>
<p><strong>偏导数</strong>:</p>
<p>既然谈到偏导数(partial derivative)，那就至少涉及到两个自变量。以两个自变量为例，$z=f(x,y)$，从导数到偏导数，也就是从曲线来到了曲面。曲线上的一点，其切线只有一条。但是曲面上的一点，切线有无数条。而偏导数就是指多元函数沿着坐标轴的变化率。 </p>
<p><em>注意</em>：直观地说，偏导数也就是函数在某一点上沿坐标轴正方向的的变化率。</p>
<p>设函数$z=f(x,y)$在点$(x_0,y_0)$的领域内有定义，当$y=y_0$时，$z$可以看作关于$x$的一元函数$f(x,y_0)$，若该一元函数在$x=x_0$处可导，即有</p>
<script type="math/tex; mode=display">
\lim_{\Delta x \to 0}{\frac{f(x_0+\Delta x,y_0)-f(x_0,y_0)}{\Delta x}}=A</script><p>函数的极限$A$存在。那么称$A$为函数$z=f(x,y)$在点$(x<em>0,y_0)$处关于自变量$x$的偏导数，记作$f_x(x_0,y_0)$或$\frac{\partial z}{\partial x}\vert</em>{y=y<em>0}^{x=x_0}$或$\frac{\partial f}{\partial x}\vert</em>{y=y<em>0}^{x=x_0}$或$z_x\vert</em>{y=y_0}^{x=x_0}$。</p>
<p>偏导数在求解时可以将另外一个变量看做常数，利用普通的求导方式求解，比如$z=3x^2+xy$关于$x$的偏导数就为$z_x=6x+y$，这个时候$y$相当于$x$的系数。</p>
<p>某点$(x_0,y_0)$处的偏导数的几何意义为曲面$z=f(x,y)$与面$x=x_0$或面$y=y_0$交线在$y=y_0$或$x=x_0$处切线的斜率。  </p>
<h3 id="导数和偏导数有什么区别"><a href="#导数和偏导数有什么区别" class="headerlink" title="导数和偏导数有什么区别"></a>导数和偏导数有什么区别</h3><p>导数和偏导没有本质区别，如果极限存在，都是当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限。  </p>
<blockquote>
<ul>
<li>一元函数，一个$y$对应一个$x$，导数只有一个。  </li>
<li>二元函数，一个$z$对应一个$x$和一个$y$，有两个导数：一个是$z$对$x$的导数，一个是$z$对$y$的导数，称之为偏导。  </li>
<li>求偏导时要注意，对一个变量求导，则视另一个变量为常数，只对改变量求导，从而将偏导的求解转化成了一元函数的求导。</li>
</ul>
</blockquote>
<h2 id="特征值和特征向量"><a href="#特征值和特征向量" class="headerlink" title="特征值和特征向量"></a>特征值和特征向量</h2><h3 id="特征值分解与特征向量"><a href="#特征值分解与特征向量" class="headerlink" title="特征值分解与特征向量"></a>特征值分解与特征向量</h3><ul>
<li><p>特征值分解可以得到特征值(eigenvalues)与特征向量(eigenvectors)；</p>
</li>
<li><p>特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么。  </p>
<p>如果说一个向量$\vec{v}$是方阵$A$的特征向量，将一定可以表示成下面的形式：</p>
</li>
</ul>
<script type="math/tex; mode=display">
A\nu = \lambda \nu</script><p>$\lambda$为特征向量$\vec{v}$对应的特征值。特征值分解是将一个矩阵分解为如下形式： </p>
<script type="math/tex; mode=display">
A=Q\sum Q^{-1}</script><p>其中，$Q$是这个矩阵$A$的特征向量组成的矩阵，$\sum$是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。也就是说矩阵$A$的信息可以由其特征值和特征向量表示。</p>
<h3 id="奇异值与特征值有什么关系"><a href="#奇异值与特征值有什么关系" class="headerlink" title="奇异值与特征值有什么关系"></a>奇异值与特征值有什么关系</h3><pre><code>那么奇异值和特征值是怎么对应起来的呢？我们将一个矩阵$A$的转置乘以$A$，并对$A^TA$求特征值，则有下面的形式：
</code></pre><script type="math/tex; mode=display">
(A^TA)V = \lambda V</script><p>这里$V$就是上面的右奇异向量，另外还有：</p>
<script type="math/tex; mode=display">
\sigma_i = \sqrt{\lambda_i}, u_i=\frac{1}{\sigma_i}A\mu_i</script><p>这里的$\sigma$就是奇异值，$u$就是上面说的左奇异向量。【证明那个哥们也没给】<br>​奇异值$\sigma$跟特征值类似，在矩阵$\sum$中也是从大到小排列，而且$\sigma$的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前$r$（$r$远小于$m、n$）个的奇异值来近似描述矩阵，即部分奇异值分解：</p>
<script type="math/tex; mode=display">
A_{m\times n}\approx U_{m \times r}\sum_{r\times r}V_{r \times n}^T</script><p>右边的三个矩阵相乘的结果将会是一个接近于$A$的矩阵，在这儿，$r$越接近于$n$，则相乘的结果越接近于$A$。</p>
<h2 id="概率分布与随机变量"><a href="#概率分布与随机变量" class="headerlink" title="概率分布与随机变量"></a>概率分布与随机变量</h2><h3 id="机器学习为什么要使用概率"><a href="#机器学习为什么要使用概率" class="headerlink" title="机器学习为什么要使用概率"></a>机器学习为什么要使用概率</h3><p>事件的概率是衡量该事件发生的可能性的量度。虽然在一次随机试验中某个事件的发生是带有偶然性的，但那些可在相同条件下大量重复的随机试验却往往呈现出明显的数量规律。<br>​机器学习除了处理不确定量，也需处理随机量。不确定性和随机性可能来自多个方面，使用概率论来量化不确定性。<br>​概率论在机器学习中扮演着一个核心角色，因为机器学习算法的设计通常依赖于对数据的概率假设。  </p>
<blockquote>
<p>​    例如在机器学习（Andrew Ng）的课中，会有一个朴素贝叶斯假设就是条件独立的一个例子。该学习算法对内容做出假设，用来分辨电子邮件是否为垃圾邮件。假设无论邮件是否为垃圾邮件，单词x出现在邮件中的概率条件独立于单词y。很明显这个假设不是不失一般性的，因为某些单词几乎总是同时出现。然而，最终结果是，这个简单的假设对结果的影响并不大，且无论如何都可以让我们快速判别垃圾邮件。</p>
</blockquote>
<h3 id="变量与随机变量有什么区别"><a href="#变量与随机变量有什么区别" class="headerlink" title="变量与随机变量有什么区别"></a>变量与随机变量有什么区别</h3><p><strong>随机变量</strong>（random variable）</p>
<p>表示随机现象（在一定条件下，并不总是出现相同结果的现象称为随机现象）中各种结果的实值函数（一切可能的样本点）。例如某一时间内公共汽车站等车乘客人数，电话交换台在一定时间内收到的呼叫次数等，都是随机变量的实例。<br>​随机变量与模糊变量的不确定性的本质差别在于，后者的测定结果仍具有不确定性，即模糊性。</p>
<p><strong>变量与随机变量的区别：</strong><br>​当变量的取值的概率不是1时,变量就变成了随机变量；当随机变量取值的概率为1时,随机变量就变成了变量。</p>
<blockquote>
<p>比如：<br>​    当变量$x$值为100的概率为1的话,那么$x=100$就是确定了的,不会再有变化,除非有进一步运算.<br>​    当变量$x$的值为100的概率不为1,比如为50的概率是0.5,为100的概率是0.5,那么这个变量就是会随不同条件而变化的,是随机变量,取到50或者100的概率都是0.5,即50%。  </p>
</blockquote>
<h3 id="随机变量与概率分布的联系"><a href="#随机变量与概率分布的联系" class="headerlink" title="随机变量与概率分布的联系"></a>随机变量与概率分布的联系</h3><p>一个随机变量仅仅表示一个可能取得的状态，还必须给定与之相伴的概率分布来制定每个状态的可能性。用来描述随机变量或一簇随机变量的每一个可能的状态的可能性大小的方法，就是 <strong>概率分布(probability distribution)</strong>.</p>
<p>随机变量可以分为离散型随机变量和连续型随机变量。</p>
<p>相应的描述其概率分布的函数是 </p>
<p>概率质量函数(Probability Mass Function, PMF):描述离散型随机变量的概率分布，通常用大写字母 $P$表示。</p>
<p>概率密度函数(Probability Density Function, PDF):描述连续型随机变量的概率分布，通常用小写字母$p$表示。</p>
<h3 id="离散型随机变量和概率质量函数"><a href="#离散型随机变量和概率质量函数" class="headerlink" title="离散型随机变量和概率质量函数"></a>离散型随机变量和概率质量函数</h3><p>PMF 将随机变量能够取得的每个状态映射到随机变量取得该状态的概率。</p>
<ul>
<li>一般而言，$P(x)$ 表示时$X=x$的概率.</li>
<li>有时候为了防止混淆，要明确写出随机变量的名称$P($x$=x)$ </li>
<li>有时候需要先定义一个随机变量，然后制定它遵循的概率分布x服从$P($x$)$ </li>
</ul>
<p>PMF 可以同时作用于多个随机变量，即联合概率分布(joint probability distribution) $P(X=x,Y=y)$*表示 $X=x$和$Y=y$同时发生的概率，也可以简写成 $P(x,y)$.</p>
<p>如果一个函数$P$是随机变量 $X$ 的 PMF， 那么它必须满足如下三个条件</p>
<ul>
<li>$P$的定义域必须是的所有可能状态的集合</li>
<li>$∀x∈$x, $0 \leq P(x) \leq 1 $. </li>
<li>$∑_{x∈X} P(x)=1$. 我们把这一条性质称之为 归一化的(normalized)</li>
</ul>
<h3 id="连续型随机变量和概率密度函数"><a href="#连续型随机变量和概率密度函数" class="headerlink" title="连续型随机变量和概率密度函数"></a>连续型随机变量和概率密度函数</h3><p>如果一个函数$p$是x的PDF，那么它必须满足如下几个条件</p>
<ul>
<li>$p$的定义域必须是 xx 的所有可能状态的集合。</li>
<li>$∀x∈X,p(x)≥0$. 注意，我们并不要求$ p(x)≤1$，因为此处 $p(x)$不是表示的对应此状态具体的概率，而是概率的一个相对大小(密度)。具体的概率，需要积分去求。</li>
<li>$∫p(x)dx=1$, 积分下来，总和还是1，概率之和还是1.</li>
</ul>
<p>注：PDF$p(x)$并没有直接对特定的状态给出概率，给出的是密度，相对的，它给出了落在面积为 $δx$的无线小的区域内的概率为$ p(x)δx$. 由此，我们无法求得具体某个状态的概率，我们可以求得的是 某个状态 $x$ 落在 某个区间$[a,b]$内的概率为$ \int_{a}^{b}p(x)dx$.</p>
<h3 id="举例理解条件概率"><a href="#举例理解条件概率" class="headerlink" title="举例理解条件概率"></a>举例理解条件概率</h3><p>条件概率公式如下：</p>
<script type="math/tex; mode=display">
P(A|B) = P(A\cap B) / P(B)</script><p>说明：在同一个样本空间$\Omega$中的事件或者子集$A$与$B$，如果随机从$\Omega$中选出的一个元素属于$B$，那么下一个随机选择的元素属于$A$ 的概率就定义为在$B$的前提下$A$的条件概率。条件概率文氏图示意如图1.1所示。<br><img src="/zh-TW/ch01_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/ch1/conditional_probability.jpg" class="lazyload" data-srcset="/zh-TW/ch01_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/ch1/conditional_probability.jpg" srcset="data:image/png;base64,666"></p>
<p>图1.1 条件概率文氏图示意</p>
<p>根据文氏图，可以很清楚地看到在事件B发生的情况下，事件A发生的概率就是$P(A\bigcap B)$除以$P(B)$。<br>​举例：一对夫妻有两个小孩，已知其中一个是女孩，则另一个是女孩子的概率是多少？（面试、笔试都碰到过）<br>​<strong>穷举法</strong>：已知其中一个是女孩，那么样本空间为男女，女女，女男，则另外一个仍然是女生的概率就是1/3。<br>​<strong>条件概率法</strong>：$P(女|女)=P(女女)/P(女)$,夫妻有两个小孩，那么它的样本空间为女女，男女，女男，男男，则$P(女女)$为1/4，$P（女）= 1-P(男男)=3/4$,所以最后$1/3$。<br>这里大家可能会误解，男女和女男是同一种情况，但实际上类似姐弟和兄妹是不同情况。 </p>
<h3 id="联合概率与边缘概率联系区别"><a href="#联合概率与边缘概率联系区别" class="headerlink" title="联合概率与边缘概率联系区别"></a>联合概率与边缘概率联系区别</h3><p><strong>区别：</strong><br>​联合概率：联合概率指类似于$P(X=a,Y=b)$这样，包含多个条件，且所有条件同时成立的概率。联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率。<br>​边缘概率：边缘概率是某个事件发生的概率，而与其它事件无关。边缘概率指类似于$P(X=a)$，$P(Y=b)$这样，仅与单个随机变量有关的概率。</p>
<p><strong>联系：</strong><br>​联合分布可求边缘分布，但若只知道边缘分布，无法求得联合分布。  </p>
<h3 id="条件概率的链式法则"><a href="#条件概率的链式法则" class="headerlink" title="条件概率的链式法则"></a>条件概率的链式法则</h3><p>由条件概率的定义，可直接得出下面的乘法公式：<br>​乘法公式 设$A, B$是两个事件，并且$P(A) &gt; 0$, 则有 </p>
<script type="math/tex; mode=display">
P(AB) = P(B|A)P(A)</script><p>推广 </p>
<script type="math/tex; mode=display">
P(ABC)=P(C|AB)P(B|A)P(A)</script><p>一般地，用归纳法可证：若$P(A_1A_2…A_n)&gt;0$，则有</p>
<script type="math/tex; mode=display">
P(A_1A_2...A_n)=P(A_n|A_1A_2...A_{n-1})P(A_{n-1}|A_1A_2...A_{n-2})...P(A_2|A_1)P(A_1)
=P(A_1)\prod_{i=2}^{n}P(A_i|A_1A_2...A_{i-1})</script><p>任何多维随机变量联合概率分布，都可以分解成只有一个变量的条件概率相乘形式。 </p>
<h3 id="独立性和条件独立性"><a href="#独立性和条件独立性" class="headerlink" title="独立性和条件独立性"></a>独立性和条件独立性</h3><p><strong>独立性</strong><br>​两个随机变量$x$和$y$，概率分布表示成两个因子乘积形式，一个因子只包含$x$，另一个因子只包含$y$，两个随机变量相互独立(independent)。<br>​条件有时为不独立的事件之间带来独立，有时也会把本来独立的事件，因为此条件的存在，而失去独立性。<br>​举例：$P(XY)=P(X)P(Y)$, 事件$X$和事件$Y$独立。此时给定$Z$，</p>
<script type="math/tex; mode=display">
P(X,Y|Z) \not = P(X|Z)P(Y|Z)</script><p>事件独立时，联合概率等于概率的乘积。这是一个非常好的数学性质，然而不幸的是，无条件的独立是十分稀少的，因为大部分情况下，事件之间都是互相影响的。 </p>
<p><strong>条件独立性</strong><br>​给定$Z$的情况下,$X$和$Y$条件独立，当且仅当</p>
<script type="math/tex; mode=display">
X\bot Y|Z \iff P(X,Y|Z) = P(X|Z)P(Y|Z)</script><p>$X$和$Y$的关系依赖于$Z$，而不是直接产生。  </p>
<blockquote>
<p><strong>举例</strong>定义如下事件：<br>$X$：明天下雨；<br>$Y$：今天的地面是湿的；<br>$Z$：今天是否下雨；<br>$Z$事件的成立，对$X$和$Y$均有影响，然而，在$Z$事件成立的前提下，今天的地面情况对明天是否下雨没有影响。 </p>
</blockquote>
<h2 id="常见概率分布"><a href="#常见概率分布" class="headerlink" title="常见概率分布"></a>常见概率分布</h2><h3 id="Bernoulli分布"><a href="#Bernoulli分布" class="headerlink" title="Bernoulli分布"></a>Bernoulli分布</h3><p><strong>Bernoulli分布</strong>是单个二值随机变量分布, 单参数$\phi$∈[0,1]控制,$\phi$给出随机变量等于1的概率. 主要性质有: </p>
<script type="math/tex; mode=display">
\begin{align*}
P(x=1) &= \phi \\
P(x=0) &= 1-\phi  \\
P(x=x) &= \phi^x(1-\phi)^{1-x} \\
\end{align*}</script><p>其期望和方差为：</p>
<script type="math/tex; mode=display">
\begin{align*}
E_x[x] &= \phi \\
Var_x(x) &= \phi{(1-\phi)}
\end{align*}</script><p><strong>Multinoulli分布</strong>也叫<strong>范畴分布</strong>, 是单个<em>k</em>值随机分布,经常用来表示<strong>对象分类的分布</strong>. 其中$k$是有限值.Multinoulli分布由向量$\vec{p}\in[0,1]^{k-1}$参数化,每个分量$p_i$表示第$i$个状态的概率, 且$p_k=1-1^Tp$.</p>
<p><strong>适用范围</strong>: <strong>伯努利分布</strong>适合对<strong>离散型</strong>随机变量建模.</p>
<h3 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h3><p>高斯也叫正态分布(Normal Distribution), 概率度函数如下:  </p>
<script type="math/tex; mode=display">
N(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi\sigma^2}}exp\left ( -\frac{1}{2\sigma^2}(x-\mu)^2 \right )</script><p>其中, $\mu$和$\sigma$分别是均值和方差, 中心峰值x坐标由$\mu$给出, 峰的宽度受$\sigma$控制, 最大点在$x=\mu$处取得, 拐点为$x=\mu\pm\sigma$</p>
<p>正态分布中，±1$\sigma$、±2$\sigma$、±3$\sigma$下的概率分别是68.3%、95.5%、99.73%，这3个数最好记住。 </p>
<p>此外, 令$\mu=0,\sigma=1$高斯分布即简化为标准正态分布: </p>
<script type="math/tex; mode=display">
N(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi}}exp\left ( -\frac{1}{2}x^2 \right )</script><p>对概率密度函数高效求值: </p>
<script type="math/tex; mode=display">
N(x;\mu,\beta^{-1})=\sqrt{\frac{\beta}{2\pi}}exp\left(-\frac{1}{2}\beta(x-\mu)^2\right)</script><p>其中，$\beta=\frac{1}{\sigma^2}$通过参数$\beta∈（0，\infty）$来控制分布精度。</p>
<h3 id="何时采用正态分布"><a href="#何时采用正态分布" class="headerlink" title="何时采用正态分布"></a>何时采用正态分布</h3><p>问: 何时采用正态分布?<br>答: 缺乏实数上分布的先验知识, 不知选择何种形式时, 默认选择正态分布总是不会错的, 理由如下: </p>
<ol>
<li>中心极限定理告诉我们, 很多独立随机变量均近似服从正态分布, 现实中很多复杂系统都可以被建模成正态分布的噪声, 即使该系统可以被结构化分解. </li>
<li>正态分布是具有相同方差的所有概率分布中, 不确定性最大的分布, 换句话说, 正态分布是对模型加入先验知识最少的分布.</li>
</ol>
<p>正态分布的推广:<br>正态分布可以推广到$R^n$空间, 此时称为<strong>多位正态分布</strong>, 其参数是一个正定对称矩阵$\Sigma$: </p>
<script type="math/tex; mode=display">
N(x;\vec\mu,\Sigma)=\sqrt{\frac{1}{(2\pi)^ndet(\Sigma)}}exp\left(-\frac{1}{2}(\vec{x}-\vec{\mu})^T\Sigma^{-1}(\vec{x}-\vec{\mu})\right)</script><p>对多为正态分布概率密度高效求值: </p>
<script type="math/tex; mode=display">
N(x;\vec{\mu},\vec\beta^{-1}) = \sqrt{det(\vec\beta)}{(2\pi)^n}exp\left(-\frac{1}{2}(\vec{x}-\vec\mu)^T\beta(\vec{x}-\vec\mu)\right)</script><p>此处，$\vec\beta$是一个精度矩阵。</p>
<h3 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a>指数分布</h3><p>深度学习中, 指数分布用来描述在$x=0$点处取得边界点的分布, 指数分布定义如下:</p>
<script type="math/tex; mode=display">
p(x;\lambda)=\lambda I_{x\geq 0}exp(-\lambda{x})</script><p>指数分布用指示函数$I_{x\geq 0}$来使$x$取负值时的概率为零。</p>
<h3 id="Laplace-分布"><a href="#Laplace-分布" class="headerlink" title="Laplace 分布"></a>Laplace 分布</h3><p>一个联系紧密的概率分布是 Laplace 分布（Laplace distribution），它允许我们在任意一点 $\mu$处设置概率质量的峰值</p>
<script type="math/tex; mode=display">
Laplace(x;\mu;\gamma)=\frac{1}{2\gamma}exp\left(-\frac{|x-\mu|}{\gamma}\right)</script><h3 id="Dirac分布和经验分布"><a href="#Dirac分布和经验分布" class="headerlink" title="Dirac分布和经验分布"></a>Dirac分布和经验分布</h3><p>Dirac分布可保证概率分布中所有质量都集中在一个点上. Diract分布的狄拉克$\delta$函数(也称为<strong>单位脉冲函数</strong>)定义如下: </p>
<script type="math/tex; mode=display">
p(x)=\delta(x-\mu), x\neq \mu</script><script type="math/tex; mode=display">
\int_{a}^{b}\delta(x-\mu)dx = 1, a < \mu < b</script><p>Dirac 分布经常作为 经验分布（empirical distribution）的一个组成部分出现</p>
<script type="math/tex; mode=display">
\hat{p}(\vec{x})=\frac{1}{m}\sum_{i=1}^{m}\delta(\vec{x}-{\vec{x}}^{(i)})</script><p>, 其中, m个点$x^{1},…,x^{m}$是给定的数据集, <strong>经验分布</strong>将概率密度$\frac{1}{m}$赋给了这些点.</p>
<p>当我们在训练集上训练模型时, 可以认为从这个训练集上得到的经验分布指明了<strong>采样来源</strong>.</p>
<p><strong>适用范围</strong>: 狄拉克δ函数适合对<strong>连续型</strong>随机变量的经验分布.</p>
<p>&gt;</p>
<h2 id="期望、方差、协方差、相关系数"><a href="#期望、方差、协方差、相关系数" class="headerlink" title="期望、方差、协方差、相关系数"></a>期望、方差、协方差、相关系数</h2><h3 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h3><pre><code>在概率论和统计学中，数学期望（或均值，亦简称期望）是试验中每次可能结果的概率乘以其结果的总和。它反映随机变量平均取值的大小。
</code></pre><ul>
<li>线性运算： $E(ax+by+c) = aE(x)+bE(y)+c$  </li>
<li>推广形式： $E(\sum<em>{k=1}^{n}{a_ix_i+c}) = \sum</em>{k=1}^{n}{a_iE(x_i)+c}$ </li>
<li>函数期望：设$f(x)$为$x$的函数，则$f(x)$的期望为<ul>
<li>离散函数： $E(f(x))=\sum_{k=1}^{n}{f(x_k)P(x_k)}$</li>
<li>连续函数： $E(f(x))=\int_{-\infty}^{+\infty}{f(x)p(x)dx}$</li>
</ul>
</li>
</ul>
<blockquote>
<p>注意：</p>
<ul>
<li>函数的期望大于等于期望的函数（Jensen不等式），即$E(f(x))\geqslant f(E(x))$  </li>
<li>一般情况下，乘积的期望不等于期望的乘积。  </li>
<li>如果$X$和$Y$相互独立，则$E(xy)=E(x)E(y)$。  </li>
</ul>
</blockquote>
<h3 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h3><pre><code>概率论中方差用来度量随机变量和其数学期望（即均值）之间的偏离程度。方差是一种特殊的期望。定义为：
</code></pre><script type="math/tex; mode=display">
Var(x) = E((x-E(x))^2)</script><blockquote>
<p>方差性质：  </p>
<p>1）$Var(x) = E(x^2) -E(x)^2$<br>2）常数的方差为0;<br>3）方差不满足线性性质;<br>4）如果$X$和$Y$相互独立, $Var(ax+by)=a^2Var(x)+b^2Var(y)$   </p>
</blockquote>
<h3 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h3><pre><code>协方差是衡量两个变量线性相关性强度及变量尺度。  两个随机变量的协方差定义为：
</code></pre><script type="math/tex; mode=display">
Cov(x,y)=E((x-E(x))(y-E(y)))</script><pre><code>方差是一种特殊的协方差。当$X=Y$时，$Cov(x,y)=Var(x)=Var(y)$。
</code></pre><blockquote>
<p>协方差性质：  </p>
<p>1）独立变量的协方差为0。<br>2）协方差计算公式：</p>
</blockquote>
<script type="math/tex; mode=display">
Cov(\sum_{i=1}^{m}{a_ix_i}, \sum_{j=1}^{m}{b_jy_j}) = \sum_{i=1}^{m} \sum_{j=1}^{m}{a_ib_jCov(x_iy_i)}</script><p>&gt;</p>
<blockquote>
<p>3）特殊情况：</p>
</blockquote>
<script type="math/tex; mode=display">
Cov(a+bx, c+dy) = bdCov(x, y)</script><h3 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a>相关系数</h3><pre><code>相关系数是研究变量之间线性相关程度的量。两个随机变量的相关系数定义为：
</code></pre><script type="math/tex; mode=display">
Corr(x,y) = \frac{Cov(x,y)}{\sqrt{Var(x)Var(y)}}</script><blockquote>
<p>相关系数的性质：<br>1）有界性。相关系数的取值范围是 [-1,1]，可以看成无量纲的协方差。<br>2）值越接近1，说明两个变量正相关性（线性）越强。越接近-1，说明负相关性越强，当为0时，表示两个变量没有相关性。  </p>
</blockquote>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习基础</title>
    <url>/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h1><p>​    机器学习起源于上世纪50年代，1959年在IBM工作的Arthur Samuel设计了一个下棋程序，这个程序具有学习的能力，它可以在不断的对弈中提高自己。由此提出了“机器学习”这个概念，它是一个结合了多个学科如概率论，优化理论，统计等，最终在计算机上实现自我获取新知识，学习改善自己的这样一个研究领域。机器学习是人工智能的一个子集，目前已经发展出许多有用的方法，比如支持向量机，回归，决策树，随机森林，强化方法，集成学习，深度学习等等，一定程度上可以帮助人们完成一些数据预测，自动化，自动决策，最优化等初步替代脑力的任务。本章我们主要介绍下机器学习的基本概念、监督学习、分类算法、逻辑回归、代价函数、损失函数、LDA、PCA、决策树、支持向量机、EM算法、聚类和降维以及模型评估有哪些方法、指标等等。</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="大话理解机器学习本质"><a href="#大话理解机器学习本质" class="headerlink" title="大话理解机器学习本质"></a>大话理解机器学习本质</h3><p>​    机器学习(Machine Learning, ML)，顾名思义，让机器去学习。这里，机器指的是计算机，是算法运行的物理载体，你也可以把各种算法本身当做一个有输入和输出的机器。那么到底让计算机去学习什么呢？对于一个任务及其表现的度量方法，设计一种算法，让算法能够提取中数据所蕴含的规律，这就叫机器学习。如果输入机器的数据是带有标签的，就称作有监督学习。如果数据是无标签的，就是无监督学习。</p>
<h3 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h3><p>​    神经网络就是按照一定规则将多个神经元连接起来的网络。不同的神经网络，具有不同的连接规则。例如全连接(Full Connected, FC)神经网络，它的规则包括：</p>
<p>（1）有三种层：输入层，输出层，隐藏层。</p>
<p>（2）同一层的神经元之间没有连接。</p>
<p>（3）fully connected的含义：第 N 层的每个神经元和第 N-1 层的所有神经元相连，第 N-1 层神经元的输出就是第 N 层神经元的输入。</p>
<p>（4）每个连接都有一个权值。</p>
<p><strong>神经网络架构</strong><br>​    图2-1就是一个神经网络系统，它由很多层组成。输入层负责接收信息，比如一只猫的图片。输出层是计算机对这个输入信息的判断结果，它是不是猫。隐藏层就是对输入信息的传递和加工处理。<br><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.5.1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.5.1.png" srcset="data:image/png;base64,666" alt="图2-2 神经网络系统"></p>
<p>​                                图2-1 神经网络系统</p>
<h3 id="2-1-3-各种常见算法图示"><a href="#2-1-3-各种常见算法图示" class="headerlink" title="2.1.3 各种常见算法图示"></a>2.1.3 各种常见算法图示</h3><p>​    日常使用机器学习的任务中，我们经常会遇见各种算法，图2-2是各种常见算法的图示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">回归算法</th>
<th style="text-align:center">聚类算法</th>
<th style="text-align:center">正则化方法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.1/1.jpg" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.1/1.jpg" srcset="data:image/png;base64,666" alt=""></td>
<td style="text-align:center"><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.1/2.jpg" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.1/2.jpg" srcset="data:image/png;base64,666" alt=""></td>
<td style="text-align:center"><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.1/3.jpg" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.1/3.jpg" srcset="data:image/png;base64,666" alt=""></td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">决策树学习</th>
<th style="text-align:center">贝叶斯方法</th>
<th style="text-align:center">基于核的算法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.2.4.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.2.4.png" srcset="data:image/png;base64,666" alt=""></td>
<td style="text-align:center"><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.1/5.jpg" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.1/5.jpg" srcset="data:image/png;base64,666" alt=""></td>
<td style="text-align:center"><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.1/6.jpg" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.1/6.jpg" srcset="data:image/png;base64,666" alt=""></td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">聚类算法</th>
<th style="text-align:center">关联规则学习</th>
<th style="text-align:center">人工神经网络</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.1/7.jpg" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.1/7.jpg" srcset="data:image/png;base64,666" alt=""></td>
<td style="text-align:center"><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.2.8.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.2.8.png" srcset="data:image/png;base64,666" alt=""></td>
<td style="text-align:center"><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.2.09.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.2.09.png" srcset="data:image/png;base64,666" alt=""></td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">深度学习</th>
<th style="text-align:center">降低维度算法</th>
<th style="text-align:center">集成算法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.2.10.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.2.10.png" srcset="data:image/png;base64,666" alt=""></td>
<td style="text-align:center"><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.2.11.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.2.11.png" srcset="data:image/png;base64,666" alt=""></td>
<td style="text-align:center"><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.2.12.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.2.12.png" srcset="data:image/png;base64,666" alt=""></td>
</tr>
</tbody>
</table>
</div>
<p>​                                        图2-2 各种常见算法图示</p>
<h3 id="计算图的导数计算"><a href="#计算图的导数计算" class="headerlink" title="计算图的导数计算"></a>计算图的导数计算</h3><p>​    计算图导数计算是反向传播，利用链式法则和隐式函数求导。</p>
<p>​    假设 $z = f(u,v)$ 在点 $(u,v)$ 处偏导连续，$(u,v)$是关于 $t$ 的函数，在 $t$ 点可导，求 $z$ 在 $t$ 点的导数。</p>
<p>根据链式法则有</p>
<script type="math/tex; mode=display">
\frac{dz}{dt}=\frac{\partial z}{\partial u}.\frac{du}{dt}+\frac{\partial z}{\partial v}
                .\frac{dv}{dt}</script><p>​    链式法则用文字描述:“由两个函数凑起来的复合函数，其导数等于里边函数代入外边函数的值之导数，乘以里边函数的导数。<br>​    为了便于理解，下面举例说明：</p>
<script type="math/tex; mode=display">
f(x)=x^2,g(x)=2x+1</script><p>​    则:</p>
<script type="math/tex; mode=display">
{f[g(x)]}'=2[g(x)] \times g'(x)=2[2x+1] \times 2=8x+4</script><h3 id="理解局部最优与全局最优"><a href="#理解局部最优与全局最优" class="headerlink" title="理解局部最优与全局最优"></a>理解局部最优与全局最优</h3><p>​    笑谈局部最优和全局最优</p>
<blockquote>
<p>​    柏拉图有一天问老师苏格拉底什么是爱情？苏格拉底叫他到麦田走一次，摘一颗最大的麦穗回来，不许回头，只可摘一次。柏拉图空着手出来了，他的理由是，看见不错的，却不知道是不是最好的，一次次侥幸，走到尽头时，才发现还不如前面的，于是放弃。苏格拉底告诉他：“这就是爱情。”这故事让我们明白了一个道理，因为生命的一些不确定性，所以全局最优解是很难寻找到的，或者说根本就不存在，我们应该设置一些限定条件，然后在这个范围内寻找最优解，也就是局部最优解——有所斩获总比空手而归强，哪怕这种斩获只是一次有趣的经历。<br>​    柏拉图有一天又问什么是婚姻？苏格拉底叫他到树林走一次,选一棵最好的树做圣诞树，也是不许回头，只许选一次。这次他一身疲惫地拖了一棵看起来直挺、翠绿，却有点稀疏的杉树回来，他的理由是，有了上回的教训，好不容易看见一棵看似不错的，又发现时间、体力已经快不够用了，也不管是不是最好的，就拿回来了。苏格拉底告诉他：“这就是婚姻。”</p>
</blockquote>
<p>​    优化问题一般分为局部最优和全局最优。其中，</p>
<p>（1）局部最优，就是在函数值空间的一个有限区域内寻找最小值；而全局最优，是在函数值空间整个区域寻找最小值问题。</p>
<p>（2）函数局部最小点是它的函数值小于或等于附近点的点，但是有可能大于较远距离的点。</p>
<p>（3）全局最小点是那种它的函数值小于或等于所有的可行点。</p>
<h3 id="大数据与深度学习之间的关系"><a href="#大数据与深度学习之间的关系" class="headerlink" title="大数据与深度学习之间的关系"></a>大数据与深度学习之间的关系</h3><p>首先来看大数据、机器学习及数据挖掘三者简单的定义：</p>
<p><strong>大数据</strong>通常被定义为“超出常用软件工具捕获，管理和处理能力”的数据集。<br><strong>机器学习</strong>关心的问题是如何构建计算机程序使用经验自动改进。<br><strong>数据挖掘</strong>是从数据中提取模式的特定算法的应用，在数据挖掘中，重点在于算法的应用，而不是算法本身。</p>
<p><strong>机器学习和数据挖掘</strong>之间的关系如下：<br>数据挖掘是一个过程，在此过程中机器学习算法被用作提取数据集中的潜在有价值模式的工具。<br>大数据与深度学习关系总结如下：</p>
<p>（1）深度学习是一种模拟大脑的行为。可以从所学习对象的机制以及行为等等很多相关联的方面进行学习，模仿类型行为以及思维。</p>
<p>（2）深度学习对于大数据的发展有帮助。深度学习对于大数据技术开发的每一个阶段均有帮助，不管是数据的分析还是挖掘还是建模，只有深度学习，这些工作才会有可能一一得到实现。</p>
<p>（3）深度学习转变了解决问题的思维。很多时候发现问题到解决问题，走一步看一步不是一个主要的解决问题的方式了，在深度学习的基础上，要求我们从开始到最后都要基于一个目标，为了需要优化的那个最终目标去进行处理数据以及将数据放入到数据应用平台上去，这就是端到端（End to End）。</p>
<p>（4）大数据的深度学习需要一个框架。在大数据方面的深度学习都是从基础的角度出发的，深度学习需要一个框架或者一个系统。总而言之，将你的大数据通过深度分析变为现实，这就是深度学习和大数据的最直接关系。</p>
<h2 id="机器学习学习方式"><a href="#机器学习学习方式" class="headerlink" title="机器学习学习方式"></a>机器学习学习方式</h2><p>​    根据数据类型的不同，对一个问题的建模有不同的方式。依据不同的学习方式和输入数据，机器学习主要分为以下四种学习方式。</p>
<h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><p>​    特点：监督学习是使用已知正确答案的示例来训练网络。已知数据和其一一对应的标签，训练一个预测模型，将输入数据映射到标签的过程。</p>
<p>​    常见应用场景：监督式学习的常见应用场景如分类问题和回归问题。</p>
<p>​    算法举例：常见的有监督机器学习算法包括支持向量机(Support Vector Machine, SVM)，朴素贝叶斯(Naive Bayes)，逻辑回归(Logistic Regression)，K近邻(K-Nearest Neighborhood, KNN)，决策树(Decision Tree)，随机森林(Random Forest)，AdaBoost以及线性判别分析(Linear Discriminant Analysis, LDA)等。深度学习(Deep Learning)也是大多数以监督学习的方式呈现。</p>
<h3 id="非监督式学习"><a href="#非监督式学习" class="headerlink" title="非监督式学习"></a>非监督式学习</h3><p>​    定义：在非监督式学习中，数据并不被特别标识，适用于你具有数据集但无标签的情况。学习模型是为了推断出数据的一些内在结构。</p>
<p>​    常见应用场景：常见的应用场景包括关联规则的学习以及聚类等。</p>
<p>​    算法举例：常见算法包括Apriori算法以及k-Means算法。</p>
<h3 id="半监督式学习"><a href="#半监督式学习" class="headerlink" title="半监督式学习"></a>半监督式学习</h3><p>​    特点：在此学习方式下，输入数据部分被标记，部分没有被标记，这种学习模型可以用来进行预测。</p>
<p>​    常见应用场景：应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，通过对已标记数据建模，在此基础上，对未标记数据进行预测。</p>
<p>​    算法举例：常见算法如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM）等。</p>
<h3 id="弱监督学习"><a href="#弱监督学习" class="headerlink" title="弱监督学习"></a>弱监督学习</h3><p>​    特点：弱监督学习可以看做是有多个标记的数据集合，次集合可以是空集，单个元素，或包含多种情况（没有标记，有一个标记，和有多个标记）的多个元素。 数据集的标签是不可靠的，这里的不可靠可以是标记不正确，多种标记，标记不充分，局部标记等。已知数据和其一一对应的弱标签，训练一个智能算法，将输入数据映射到一组更强的标签的过程。标签的强弱指的是标签蕴含的信息量的多少，比如相对于分割的标签来说，分类的标签就是弱标签。</p>
<p>​    算法举例：举例，给出一张包含气球的图片，需要得出气球在图片中的位置及气球和背景的分割线，这就是已知弱标签学习强标签的问题。</p>
<p>​    在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。</p>
<h3 id="监督学习有哪些步骤"><a href="#监督学习有哪些步骤" class="headerlink" title="监督学习有哪些步骤"></a>监督学习有哪些步骤</h3><p>​    监督学习是使用已知正确答案的示例来训练网络，每组训练数据有一个明确的标识或结果。想象一下，我们可以训练一个网络，让其从照片库中（其中包含气球的照片）识别出气球的照片。以下就是我们在这个假设场景中所要采取的步骤。</p>
<p><strong>步骤1：数据集的创建和分类</strong><br>​    首先，浏览你的照片（数据集），确定所有包含气球的照片，并对其进行标注。然后，将所有照片分为训练集和验证集。目标就是在深度网络中找一函数，这个函数输入是任意一张照片，当照片中包含气球时，输出1，否则输出0。</p>
<p><strong>步骤2：数据增强（Data Augmentation）</strong><br>​    当原始数据搜集和标注完毕，一般搜集的数据并不一定包含目标在各种扰动下的信息。数据的好坏对于机器学习模型的预测能力至关重要，因此一般会进行数据增强。对于图像数据来说，数据增强一般包括，图像旋转，平移，颜色变换，裁剪，仿射变换等。</p>
<p><strong>步骤3：特征工程（Feature Engineering）</strong><br>​    一般来讲，特征工程包含特征提取和特征选择。常见的手工特征(Hand-Crafted Feature)有尺度不变特征变换(Scale-Invariant Feature Transform, SIFT)，方向梯度直方图(Histogram of Oriented Gradient, HOG)等。由于手工特征是启发式的，其算法设计背后的出发点不同，将这些特征组合在一起的时候有可能会产生冲突，如何将组合特征的效能发挥出来，使原始数据在特征空间中的判别性最大化，就需要用到特征选择的方法。在深度学习方法大获成功之后，人们很大一部分不再关注特征工程本身。因为，最常用到的卷积神经网络(Convolutional Neural Networks, CNNs)本身就是一种特征提取和选择的引擎。研究者提出的不同的网络结构、正则化、归一化方法实际上就是深度学习背景下的特征工程。</p>
<p><strong>步骤4：构建预测模型和损失</strong><br>​    将原始数据映射到特征空间之后，也就意味着我们得到了比较合理的输入。下一步就是构建合适的预测模型得到对应输入的输出。而如何保证模型的输出和输入标签的一致性，就需要构建模型预测和标签之间的损失函数，常见的损失函数(Loss Function)有交叉熵、均方差等。通过优化方法不断迭代，使模型从最初的初始化状态一步步变化为有预测能力的模型的过程，实际上就是学习的过程。</p>
<p><strong>步骤5：训练</strong><br>​    选择合适的模型和超参数进行初始化，其中超参数比如支持向量机中核函数、误差项惩罚权重等。当模型初始化参数设定好后，将制作好的特征数据输入到模型，通过合适的优化方法不断缩小输出与标签之间的差距，当迭代过程到了截止条件，就可以得到训练好的模型。优化方法最常见的就是梯度下降法及其变种，使用梯度下降法的前提是优化目标函数对于模型是可导的。</p>
<p><strong>步骤6：验证和模型选择</strong><br>​    训练完训练集图片后，需要进行模型测试。利用验证集来验证模型是否可以准确地挑选出含有气球在内的照片。<br>​    在此过程中，通常会通过调整和模型相关的各种事物（超参数）来重复步骤2和3，诸如里面有多少个节点，有多少层，使用怎样的激活函数和损失函数，如何在反向传播阶段积极有效地训练权值等等。</p>
<p><strong>步骤7：测试及应用</strong><br>​    当有了一个准确的模型，就可以将该模型部署到你的应用程序中。你可以将预测功能发布为API（Application Programming Interface, 应用程序编程接口）调用，并且你可以从软件中调用该API，从而进行推理并给出相应的结果。</p>
<h2 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h2><p>​    分类算法和回归算法是对真实世界不同建模的方法。分类模型是认为模型的输出是离散的，例如大自然的生物被划分为不同的种类，是离散的。回归模型的输出是连续的，例如人的身高变化过程是一个连续过程，而不是离散的。</p>
<p>​    因此，在实际建模过程时，采用分类模型还是回归模型，取决于你对任务（真实世界）的分析和理解。</p>
<h3 id="常用分类算法的优缺点？"><a href="#常用分类算法的优缺点？" class="headerlink" title="常用分类算法的优缺点？"></a>常用分类算法的优缺点？</h3><p>​    接下来我们介绍常用分类算法的优缺点，如表2-1所示。</p>
<p>​                                    表2-1 常用分类算法的优缺点<br>|算法|优点|缺点|<br>|:-|:-|:-|<br>|Bayes 贝叶斯分类法|1）所需估计的参数少，对于缺失数据不敏感。<br>2）有着坚实的数学基础，以及稳定的分类效率。|1）需要假设属性之间相互独立，这往往并不成立。（喜欢吃番茄、鸡蛋，却不喜欢吃番茄炒蛋）。<br>2）需要知道先验概率。<br>3）分类决策存在错误率。|<br>|Decision Tree决策树|1）不需要任何领域知识或参数假设。<br>2）适合高维数据。<br>3）简单易于理解。<br>4）短时间内处理大量数据，得到可行且效果较好的结果。<br>5）能够同时处理数据型和常规性属性。|1）对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。<br>2）易于过拟合。<br>3）忽略属性之间的相关性。<br>4）不支持在线学习。|<br>|SVM支持向量机|1）可以解决小样本下机器学习的问题。<br>2）提高泛化性能。<br>3）可以解决高维、非线性问题。超高维文本分类仍受欢迎。<br>4）避免神经网络结构选择和局部极小的问题。|1）对缺失数据敏感。<br>2）内存消耗大，难以解释。<br>3）运行和调参略烦人。|<br>|KNN K近邻|1）思想简单，理论成熟，既可以用来做分类也可以用来做回归； <br>2）可用于非线性分类；<br> 3）训练时间复杂度为O(n)； <br>4）准确度高，对数据没有假设，对outlier不敏感；|1）计算量太大。<br>2）对于样本分类不均衡的问题，会产生误判。<br>3）需要大量的内存。<br>4）输出的可解释性不强。|<br>|Logistic Regression逻辑回归|1）速度快。<br>2）简单易于理解，直接看到各个特征的权重。<br>3）能容易地更新模型吸收新的数据。<br>4）如果想要一个概率框架，动态调整分类阀值。|特征处理复杂。需要归一化和较多的特征工程。|<br>|Neural Network 神经网络|1）分类准确率高。<br>2）并行处理能力强。<br>3）分布式存储和学习能力强。<br>4）鲁棒性较强，不易受噪声影响。|1）需要大量参数（网络拓扑、阀值、阈值）。<br>2）结果难以解释。<br>3）训练时间过长。|<br>|Adaboosting|1）adaboost是一种有很高精度的分类器。<br>2）可以使用各种方法构建子分类器，Adaboost算法提供的是框架。<br>3）当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单。<br>4）简单，不用做特征筛选。<br>5）不用担心overfitting。|对outlier比较敏感|</p>
<h3 id="分类算法的评估方法"><a href="#分类算法的评估方法" class="headerlink" title="分类算法的评估方法"></a>分类算法的评估方法</h3><p>​    分类评估方法主要功能是用来评估分类算法的好坏，而评估一个分类器算法的好坏又包括许多项指标。了解各种评估方法，在实际应用中选择正确的评估方法是十分重要的。</p>
<ul>
<li><p><strong>几个常用术语</strong><br>​    这里首先介绍几个常见的模型评价术语，现在假设我们的分类目标只有两类，计为正例（positive）和负例（negative）分别是：<br> 1) True positives(TP):  被正确地划分为正例的个数，即实际为正例且被分类器划分为正例的实例数；<br> 2) False positives(FP): 被错误地划分为正例的个数，即实际为负例但被分类器划分为正例的实例数；<br> 3) False negatives(FN):被错误地划分为负例的个数，即实际为正例但被分类器划分为负例的实例数；<br> 4) True negatives(TN): 被正确地划分为负例的个数，即实际为负例且被分类器划分为负例的实例数。　</p>
<p>​                                    表2-2 四个术语的混淆矩阵</p>
</li>
</ul>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.9/1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.9/1.png" srcset="data:image/png;base64,666" alt="图2-3 术语的混淆矩阵"></p>
<p>表2-2是这四个术语的混淆矩阵，做以下说明：<br>    1）P=TP+FN表示实际为正例的样本个数。<br>    2）True、False描述的是分类器是否判断正确。<br>    3）Positive、Negative是分类器的分类结果，如果正例计为1、负例计为-1，即positive=1、negative=-1。用1表示True，-1表示False，那么实际的类标=TF*PN，TF为true或false，PN为positive或negative。<br>    4）例如True positives(TP)的实际类标=1*1=1为正例，False positives(FP)的实际类标=(-1)*1=-1为负例，False negatives(FN)的实际类标=(-1)*(-1)=1为正例，True negatives(TN)的实际类标=1*(-1)=-1为负例。</p>
<ul>
<li><strong>评价指标</strong><br>1) 正确率（accuracy）<pre><code>正确率是我们最常见的评价指标，accuracy = (TP+TN)/(P+N)，正确率是被分对的样本数在所有样本数中的占比，通常来说，正确率越高，分类器越好。
</code></pre>2) 错误率（error rate)<pre><code>错误率则与正确率相反，描述被分类器错分的比例，error rate = (FP+FN)/(P+N)，对某一个实例来说，分对与分错是互斥事件，所以accuracy =1 -  error rate。
</code></pre>3) 灵敏度（sensitivity）<pre><code>sensitivity = TP/P，表示的是所有正例中被分对的比例，衡量了分类器对正例的识别能力。
</code></pre>4) 特异性（specificity)<pre><code>specificity = TN/N，表示的是所有负例中被分对的比例，衡量了分类器对负例的识别能力。
</code></pre>5) 精度（precision）<pre><code>precision=TP/(TP+FP)，精度是精确性的度量，表示被分为正例的示例中实际为正例的比例。
</code></pre>6) 召回率（recall）<pre><code>召回率是覆盖面的度量，度量有多个正例被分为正例，recall=TP/(TP+FN)=TP/P=sensitivity，可以看到召回率与灵敏度是一样的。
</code></pre>7) 其他评价指标<pre><code>计算速度：分类器训练和预测需要的时间；
鲁棒性：处理缺失值和异常值的能力；
可扩展性：处理大数据集的能力；
可解释性：分类器的预测标准的可理解性，像决策树产生的规则就是很容易理解的，而神经网络的一堆参数就不好理解，我们只好把它看成一个黑盒子。
</code></pre>8) 精度和召回率反映了分类器分类性能的两个方面。如果综合考虑查准率与查全率，可以得到新的评价指标F1-score，也称为综合分类率：$F1=\frac{2 \times precision \times recall}{precision + recall}$。</li>
</ul>
<p>​    为了综合多个类别的分类情况，评测系统整体性能，经常采用的还有微平均F1（micro-averaging）和宏平均F1（macro-averaging ）两种指标。</p>
<p>​    （1）宏平均F1与微平均F1是以两种不同的平均方式求的全局F1指标。</p>
<p>​    （2）宏平均F1的计算方法先对每个类别单独计算F1值，再取这些F1值的算术平均值作为全局指标。</p>
<p>​    （3）微平均F1的计算方法是先累加计算各个类别的a、b、c、d的值，再由这些值求出F1值。</p>
<p>​    （4）由两种平均F1的计算方式不难看出，宏平均F1平等对待每一个类别，所以它的值主要受到稀有类别的影响，而微平均F1平等考虑文档集中的每一个文档，所以它的值受到常见类别的影响比较大。</p>
<ul>
<li><strong>ROC曲线和PR曲线</strong></li>
</ul>
<p>​    如图2-3，ROC曲线是（Receiver Operating Characteristic Curve，受试者工作特征曲线）的简称，是以灵敏度（真阳性率）为纵坐标，以1减去特异性（假阳性率）为横坐标绘制的性能评价曲线。可以将不同模型对同一数据集的ROC曲线绘制在同一笛卡尔坐标系中，ROC曲线越靠近左上角，说明其对应模型越可靠。也可以通过ROC曲线下面的面积（Area Under Curve, AUC）来评价模型，AUC越大，模型越可靠。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.7.3.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.7.3.png" srcset="data:image/png;base64,666"></p>
<p>​                                                                                                     图2-3 ROC曲线</p>
<p>​    PR曲线是Precision Recall Curve的简称，描述的是precision和recall之间的关系，以recall为横坐标，precision为纵坐标绘制的曲线。该曲线的所对应的面积AUC实际上是目标检测中常用的评价指标平均精度（Average Precision, AP）。AP越高，说明模型性能越好。</p>
<h3 id="正确率能很好的评估分类算法吗"><a href="#正确率能很好的评估分类算法吗" class="headerlink" title="正确率能很好的评估分类算法吗"></a>正确率能很好的评估分类算法吗</h3><p>​    不同算法有不同特点，在不同数据集上有不同的表现效果，根据特定的任务选择不同的算法。如何评价分类算法的好坏，要做具体任务具体分析。对于决策树，主要用正确率去评估，但是其他算法，只用正确率能很好的评估吗？<br>​    答案是否定的。<br>​    正确率确实是一个很直观很好的评价指标，但是有时候正确率高并不能完全代表一个算法就好。比如对某个地区进行地震预测，地震分类属性分为0：不发生地震、1发生地震。我们都知道，不发生的概率是极大的，对于分类器而言，如果分类器不加思考，对每一个测试样例的类别都划分为0，达到99%的正确率，但是，问题来了，如果真的发生地震时，这个分类器毫无察觉，那带来的后果将是巨大的。很显然，99%正确率的分类器并不是我们想要的。出现这种现象的原因主要是数据分布不均衡，类别为1的数据太少，错分了类别1但达到了很高的正确率缺忽视了研究者本身最为关注的情况。</p>
<h3 id="什么样的分类器是最好的"><a href="#什么样的分类器是最好的" class="headerlink" title="什么样的分类器是最好的"></a>什么样的分类器是最好的</h3><p>​    对某一个任务，某个具体的分类器不可能同时满足或提高所有上面介绍的指标。<br>​    如果一个分类器能正确分对所有的实例，那么各项指标都已经达到最优，但这样的分类器往往不存在。比如之前说的地震预测，既然不能百分百预测地震的发生，但实际情况中能容忍一定程度的误报。假设在1000次预测中，共有5次预测发生了地震，真实情况中有一次发生了地震，其他4次则为误报。正确率由原来的999/1000=99.9下降为996/1000=99.6。召回率由0/1=0%上升为1/1=100%。对此解释为，虽然预测失误了4次，但真的地震发生前，分类器能预测对，没有错过，这样的分类器实际意义更为重大，正是我们想要的。在这种情况下，在一定正确率前提下，要求分类器的召回率尽量高。</p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><h3 id="回归划分"><a href="#回归划分" class="headerlink" title="回归划分"></a>回归划分</h3><p>广义线性模型家族里，依据因变量不同，可以有如下划分：</p>
<p>（1）如果是连续的，就是多重线性回归。</p>
<p>（2）如果是二项分布，就是逻辑回归。</p>
<p>（3）如果是泊松（Poisson）分布，就是泊松回归。</p>
<p>（4）如果是负二项分布，就是负二项回归。</p>
<p>（5）逻辑回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最常用的就是二分类的逻辑回归。</p>
<h3 id="逻辑回归适用性"><a href="#逻辑回归适用性" class="headerlink" title="逻辑回归适用性"></a>逻辑回归适用性</h3><p>逻辑回归可用于以下几个方面：</p>
<p>（1）用于概率预测。用于可能性预测时，得到的结果有可比性。比如根据模型进而预测在不同的自变量情况下，发生某病或某种情况的概率有多大。</p>
<p>（2）用于分类。实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。进行分类时，仅需要设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类。</p>
<p>（3）寻找危险因素。寻找某一疾病的危险因素等。</p>
<p>（4）仅能用于线性问题。只有当目标和特征是线性关系时，才能用逻辑回归。在应用逻辑回归时注意两点：一是当知道模型是非线性时，不适用逻辑回归；二是当使用逻辑回归时，应注意选择和目标为线性关系的特征。</p>
<p>（5）各特征之间不需要满足条件独立假设，但各个特征的贡献独立计算。</p>
<h3 id="逻辑回归与朴素贝叶斯有什么区别"><a href="#逻辑回归与朴素贝叶斯有什么区别" class="headerlink" title="逻辑回归与朴素贝叶斯有什么区别"></a>逻辑回归与朴素贝叶斯有什么区别</h3><p>逻辑回归与朴素贝叶斯区别有以下几个方面：</p>
<p>（1）逻辑回归是判别模型， 朴素贝叶斯是生成模型，所以生成和判别的所有区别它们都有。</p>
<p>（2）朴素贝叶斯属于贝叶斯，逻辑回归是最大似然，两种概率哲学间的区别。</p>
<p>（3）朴素贝叶斯需要条件独立假设。</p>
<p>（4）逻辑回归需要求特征参数间是线性的。</p>
<h3 id="线性回归与逻辑回归的区别"><a href="#线性回归与逻辑回归的区别" class="headerlink" title="线性回归与逻辑回归的区别"></a>线性回归与逻辑回归的区别</h3><p>线性回归与逻辑回归的区别如下描述：</p>
<p>（1）线性回归的样本的输出，都是连续值，$ y\in (-\infty ,+\infty )$，而逻辑回归中$y\in (0,1)$，只能取0和1。</p>
<p>（2）对于拟合函数也有本质上的差别： </p>
<p>​    线性回归：$f(x)=\theta ^{T}x=\theta <em>{1}x </em>{1}+\theta <em>{2}x </em>{2}+…+\theta <em>{n}x </em>{n}$</p>
<p>​    逻辑回归：$f(x)=P(y=1|x;\theta )=g(\theta ^{T}x)$，其中，$g(z)=\frac{1}{1+e^{-z}}$</p>
<p>​    可以看出，线性回归的拟合函数，是对f(x)的输出变量y的拟合，而逻辑回归的拟合函数是对为1类样本的概率的拟合。</p>
<p>​    那么，为什么要以1类样本的概率进行拟合呢，为什么可以这样拟合呢？ </p>
<p>​    $\theta ^{T}x=0$就相当于是1类和0类的决策边界： </p>
<p>​    当$\theta ^{T}x&gt;0$，则y&gt;0.5；若$\theta ^{T}x\rightarrow +\infty $，则$y \rightarrow  1 $，即y为1类; </p>
<p>​    当$\theta ^{T}x&lt;0$，则y&lt;0.5；若$\theta ^{T}x\rightarrow -\infty $，则$y \rightarrow  0 $，即y为0类; </p>
<p>这个时候就能看出区别，在线性回归中$\theta ^{T}x$为预测值的拟合函数；而在逻辑回归中$\theta ^{T}x$为决策边界。下表2-3为线性回归和逻辑回归的区别。</p>
<p>​                                    表2-3 线性回归和逻辑回归的区别</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">线性回归</th>
<th style="text-align:center">逻辑回归</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">目的</td>
<td style="text-align:center">预测</td>
<td style="text-align:center">分类</td>
</tr>
<tr>
<td style="text-align:center"><script type="math/tex">y^{(i)}</script></td>
<td style="text-align:center">未知</td>
<td style="text-align:center">（0,1）</td>
</tr>
<tr>
<td style="text-align:center">函数</td>
<td style="text-align:center">拟合函数</td>
<td style="text-align:center">预测函数</td>
</tr>
<tr>
<td style="text-align:center">参数计算方式</td>
<td style="text-align:center">最小二乘法</td>
<td style="text-align:center">极大似然估计</td>
</tr>
</tbody>
</table>
</div>
<p>下面具体解释一下： </p>
<ol>
<li>拟合函数和预测函数什么关系呢？简单来说就是将拟合函数做了一个逻辑函数的转换，转换后使得$y^{(i)} \in (0,1)$;</li>
<li>最小二乘和最大似然估计可以相互替代吗？回答当然是不行了。我们来看看两者依仗的原理：最大似然估计是计算使得数据出现的可能性最大的参数，依仗的自然是Probability。而最小二乘是计算误差损失。</li>
</ol>
<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><h3 id="为什么需要代价函数"><a href="#为什么需要代价函数" class="headerlink" title="为什么需要代价函数"></a>为什么需要代价函数</h3><ol>
<li>为了得到训练逻辑回归模型的参数，需要一个代价函数，通过训练代价函数来得到参数。</li>
<li>用于找到最优解的目的函数。</li>
</ol>
<h3 id="2-10-2-代价函数作用原理"><a href="#2-10-2-代价函数作用原理" class="headerlink" title="2.10.2 代价函数作用原理"></a>2.10.2 代价函数作用原理</h3><p>​    在回归问题中，通过代价函数来求解最优解，常用的是平方误差代价函数。假设函数图像如图2-4所示，当参数发生变化时，假设函数状态也会随着变化。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16/1.jpg" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16/1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                        图2-4  $h(x) = A + Bx$函数示意图</p>
<p>​    想要拟合图中的离散点，我们需要尽可能找到最优的$A$和$B$来使这条直线更能代表所有数据。如何找到最优解呢，这就需要使用代价函数来求解，以平方误差代价函数为例，假设函数为$h(x)=\theta_0x$。<br>​    <strong>平方误差代价函数的主要思想</strong>就是将实际数据给出的值与拟合出的线的对应值做差，求出拟合出的直线与实际的差距。在实际应用中，为了避免因个别极端数据产生的影响，采用类似方差再取二分之一的方式来减小个别数据的影响。因此，引出代价函数：</p>
<script type="math/tex; mode=display">
J(\theta_0, \theta_1) = \frac{1}{m}\sum_{i=1}^m(h(x^{(i)})-y^{(i)})^2</script><p>​    <strong>最优解即为代价函数的最小值</strong>$\min J(\theta_0, \theta_1)$。如果是1个参数，代价函数一般通过二维曲线便可直观看出。如果是2个参数，代价函数通过三维图像可看出效果，参数越多，越复杂。<br>当参数为2个时，代价函数是三维图像，如下图2-5所示。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16/2.jpg" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16/2.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                        图2-5  代价函数三维图像</p>
<h3 id="为什么代价函数要非负"><a href="#为什么代价函数要非负" class="headerlink" title="为什么代价函数要非负"></a>为什么代价函数要非负</h3><p>​    目标函数存在一个下界，在优化过程当中，如果优化算法能够使目标函数不断减小，根据单调有界准则，这个优化算法就能证明是收敛有效的。<br>​    只要设计的目标函数有下界，基本上都可以，代价函数非负更为方便。</p>
<h3 id="常见代价函数"><a href="#常见代价函数" class="headerlink" title="常见代价函数"></a>常见代价函数</h3><p>（1）<strong>二次代价函数（quadratic cost）</strong>：</p>
<script type="math/tex; mode=display">
J = \frac{1}{2n}\sum_x\Vert y(x)-a^L(x)\Vert^2</script><p>​    其中，$J$表示代价函数，$x$表示样本，$y$表示实际值，$a$表示输出值，$n$表示样本的总数。使用一个样本为例简单说明，此时二次代价函数为：</p>
<script type="math/tex; mode=display">
J = \frac{(y-a)^2}{2}</script><p>​    假如使用梯度下降法（Gradient descent）来调整权值参数的大小，权值$w$和偏置$b$的梯度推导如下：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial b}=(a-y)\sigma'(z)</script><p>其中，$z$表示神经元的输入，$\sigma$表示激活函数。权值$w$和偏置$b$的梯度跟激活函数的梯度成正比，激活函数的梯度越大，权值$w$和偏置$b$的大小调整得越快，训练收敛得就越快。</p>
<p><em>注</em>：神经网络常用的激活函数为sigmoid函数，该函数的曲线如下图2-6所示：</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.18/1.jpg" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.18/1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                                图2-6 sigmoid函数曲线</p>
<p>如上图所示，对0.88和0.98两个点进行比较：<br>​    假设目标是收敛到1.0。0.88离目标1.0比较远，梯度比较大，权值调整比较大。0.98离目标1.0比较近，梯度比较小，权值调整比较小。调整方案合理。<br>​    假如目标是收敛到0。0.88离目标0比较近，梯度比较大，权值调整比较大。0.98离目标0比较远，梯度比较小，权值调整比较小。调整方案不合理。<br>​    原因：在使用sigmoid函数的情况下, 初始的代价（误差）越大，导致训练越慢。</p>
<p>（2）<strong>交叉熵代价函数（cross-entropy）</strong>：</p>
<script type="math/tex; mode=display">
J = -\frac{1}{n}\sum_x[y\ln a + (1-y)\ln{(1-a)}]</script><p>其中，$J$表示代价函数，$x$表示样本，$y$表示实际值，$a$表示输出值，$n$表示样本的总数。<br>    权值$w$和偏置$b$的梯度推导如下：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w_j}=\frac{1}{n}\sum_{x}x_j(\sigma{(z)}-y)\;，
\frac{\partial J}{\partial b}=\frac{1}{n}\sum_{x}(\sigma{(z)}-y)</script><p>当误差越大时，梯度就越大，权值$w$和偏置$b$调整就越快，训练的速度也就越快。<br><strong>二次代价函数适合输出神经元是线性的情况，交叉熵代价函数适合输出神经元是S型函数的情况。</strong></p>
<p>（3）<strong>对数似然代价函数（log-likelihood cost）</strong>：<br>对数似然函数常用来作为softmax回归的代价函数。深度学习中普遍的做法是将softmax作为最后一层，此时常用的代价函数是对数似然代价函数。<br>    对数似然代价函数与softmax的组合和交叉熵与sigmoid函数的组合非常相似。对数似然代价函数在二分类时可以化简为交叉熵代价函数的形式。<br>在tensorflow中：<br>    与sigmoid搭配使用的交叉熵函数：<code>tf.nn.sigmoid_cross_entropy_with_logits()</code>。<br>    与softmax搭配使用的交叉熵函数：<code>tf.nn.softmax_cross_entropy_with_logits()</code>。<br>在pytorch中：<br>        与sigmoid搭配使用的交叉熵函数：<code>torch.nn.BCEWithLogitsLoss()</code>。<br>    与softmax搭配使用的交叉熵函数：<code>torch.nn.CrossEntropyLoss()</code>。</p>
<h3 id="为什么用交叉熵代替二次代价函数"><a href="#为什么用交叉熵代替二次代价函数" class="headerlink" title="为什么用交叉熵代替二次代价函数"></a>为什么用交叉熵代替二次代价函数</h3><p>（1）<strong>为什么不用二次方代价函数</strong><br>由上一节可知，权值$w$和偏置$b$的偏导数为$\frac{\partial J}{\partial w}=(a-y)\sigma’(z)x$，$\frac{\partial J}{\partial b}=(a-y)\sigma’(z)$， 偏导数受激活函数的导数影响，sigmoid函数导数在输出接近0和1时非常小，会导致一些实例在刚开始训练时学习得非常慢。</p>
<p>（2）<strong>为什么要用交叉熵</strong><br>交叉熵函数权值$w$和偏置$b$的梯度推导为：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w_j}=\frac{1}{n}\sum_{x}x_j(\sigma{(z)}-y)\;，
\frac{\partial J}{\partial b}=\frac{1}{n}\sum_{x}(\sigma{(z)}-y)</script><p>由以上公式可知，权重学习的速度受到$\sigma{(z)}-y$影响，更大的误差，就有更快的学习速度，避免了二次代价函数方程中因$\sigma’{(z)}$导致的学习缓慢的情况。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="什么是损失函数"><a href="#什么是损失函数" class="headerlink" title="什么是损失函数"></a>什么是损失函数</h3><p>​    损失函数（Loss Function）又叫做误差函数，用来衡量算法的运行情况，估量模型的预测值与真实值的不一致程度，是一个非负实值函数，通常使用$<br>L(Y, f(x))$来表示。损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。</p>
<h3 id="常见的损失函数"><a href="#常见的损失函数" class="headerlink" title="常见的损失函数"></a>常见的损失函数</h3><p>​    机器学习通过对算法中的目标函数进行不断求解优化，得到最终想要的结果。分类和回归问题中，通常使用损失函数或代价函数作为目标函数。<br>​    损失函数用来评价预测值和真实值不一样的程度。通常损失函数越好，模型的性能也越好。<br>​    损失函数可分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是在经验风险损失函数上加上正则项。<br>​    下面介绍常用的损失函数：</p>
<p>（1）<strong>0-1损失函数</strong><br>如果预测值和目标值相等，值为0，如果不相等，值为1。</p>
<script type="math/tex; mode=display">
L(Y, f(x)) =
\begin{cases}
1,& Y\ne f(x)\\
0,& Y = f(x)
\end{cases}</script><p>一般的在实际使用中，相等的条件过于严格，可适当放宽条件：</p>
<script type="math/tex; mode=display">
L(Y, f(x)) =
\begin{cases}
1,& |Y-f(x)|\geqslant T\\
0,& |Y-f(x)|< T
\end{cases}</script><p>（2）<strong>绝对值损失函数</strong><br>和0-1损失函数相似，绝对值损失函数表示为：</p>
<script type="math/tex; mode=display">
L(Y, f(x)) = |Y-f(x)|​</script><p>（3）<strong>平方损失函数</strong></p>
<script type="math/tex; mode=display">
L(Y, f(x)) = \sum_N{(Y-f(x))}^2</script><p>这点可从最小二乘法和欧几里得距离角度理解。最小二乘法的原理是，最优拟合曲线应该使所有点到回归直线的距离和最小。</p>
<p>（4）<strong>对数损失函数</strong></p>
<script type="math/tex; mode=display">
L(Y, P(Y|X)) = -\log{P(Y|X)}</script><p>​    常见的逻辑回归使用的就是对数损失函数，有很多人认为逻辑回归的损失函数是平方损失，其实不然。逻辑回归它假设样本服从伯努利分布（0-1分布），进而求得满足该分布的似然函数，接着取对数求极值等。逻辑回归推导出的经验风险函数是最小化负的似然函数，从损失函数的角度看，就是对数损失函数。</p>
<p>（6）<strong>指数损失函数</strong><br>指数损失函数的标准形式为：</p>
<script type="math/tex; mode=display">
L(Y, f(x)) = \exp(-Yf(x))</script><p>例如AdaBoost就是以指数损失函数为损失函数。</p>
<p>（7）<strong>Hinge损失函数</strong><br>Hinge损失函数的标准形式如下：</p>
<script type="math/tex; mode=display">
L(y) = \max{(0, 1-ty)}</script><p>统一的形式：</p>
<script type="math/tex; mode=display">
L(Y, f(x)) = \max{(0, Yf(x))}</script><p>其中y是预测值，范围为(-1,1)，t为目标值，其为-1或1。</p>
<p>在线性支持向量机中，最优化问题可等价于</p>
<script type="math/tex; mode=display">
\underset{\min}{w,b}\sum_{i=1}^N (1-y_i(wx_i+b))+\lambda\Vert w\Vert ^2</script><p>上式相似于下式</p>
<script type="math/tex; mode=display">
\frac{1}{m}\sum_{i=1}^{N}l(wx_i+by_i) + \Vert w\Vert ^2</script><p>其中$l(wx_i+by_i)$是Hinge损失函数，$\Vert w\Vert ^2$可看做为正则化项。</p>
<h3 id="逻辑回归为什么使用对数损失函数"><a href="#逻辑回归为什么使用对数损失函数" class="headerlink" title="逻辑回归为什么使用对数损失函数"></a>逻辑回归为什么使用对数损失函数</h3><p>假设逻辑回归模型</p>
<script type="math/tex; mode=display">
P(y=1|x;\theta)=\frac{1}{1+e^{-\theta^{T}x}}</script><p>假设逻辑回归模型的概率分布是伯努利分布，其概率质量函数为：</p>
<script type="math/tex; mode=display">
P(X=n)=
\begin{cases}
1-p, n=0\\
 p,n=1
\end{cases}</script><p>其似然函数为：</p>
<script type="math/tex; mode=display">
L(\theta)=\prod_{i=1}^{m}
P(y=1|x_i)^{y_i}P(y=0|x_i)^{1-y_i}</script><p>对数似然函数为：</p>
<script type="math/tex; mode=display">
\ln L(\theta)=\sum_{i=1}^{m}[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln{P(y=0|x_i)}]\\
  =\sum_{i=1}^m[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln(1-P(y=1|x_i))]</script><p>对数函数在单个数据点上的定义为：</p>
<script type="math/tex; mode=display">
cost(y,p(y|x))=-y\ln{p(y|x)-(1-y)\ln(1-p(y|x))}</script><p>则全局样本损失函数为：</p>
<script type="math/tex; mode=display">
cost(y,p(y|x)) = -\sum_{i=1}^m[y_i\ln p(y_i|x_i)+(1-y_i)\ln(1-p(y_i|x_i))]</script><p>由此可看出，对数损失函数与极大似然估计的对数似然函数本质上是相同的。所以逻辑回归直接采用对数损失函数。</p>
<h3 id="对数损失函数是如何度量损失的"><a href="#对数损失函数是如何度量损失的" class="headerlink" title="对数损失函数是如何度量损失的"></a>对数损失函数是如何度量损失的</h3><p>​    例如，在高斯分布中，我们需要确定均值和标准差。<br>​    如何确定这两个参数？最大似然估计是比较常用的方法。最大似然的目标是找到一些参数值，这些参数值对应的分布可以最大化观测到数据的概率。<br>​    因为需要计算观测到所有数据的全概率，即所有观测到的数据点的联合概率。现考虑如下简化情况：</p>
<p>（1）假设观测到每个数据点的概率和其他数据点的概率是独立的。</p>
<p>（2）取自然对数。<br>假设观测到单个数据点$x_i(i=1,2,…n)$的概率为：</p>
<script type="math/tex; mode=display">
P(x_i;\mu,\sigma)=\frac{1}{\sigma \sqrt{2\pi}}\exp 
        \left( - \frac{(x_i-\mu)^2}{2\sigma^2} \right)</script><p>（3）其联合概率为：</p>
<script type="math/tex; mode=display">
P(x_1,x_2,...,x_n;\mu,\sigma)=\frac{1}{\sigma \sqrt{2\pi}}\exp 
        \left( - \frac{(x_1-\mu)^2}{2\sigma^2} \right) \\ \times
         \frac{1}{\sigma \sqrt{2\pi}}\exp 
        \left( - \frac{(x_2-\mu)^2}{2\sigma^2} \right) \times ... \times
        \frac{1}{\sigma \sqrt{2\pi}}\exp 
        \left( - \frac{(x_n-\mu)^2}{2\sigma^2} \right)</script><p>​    对上式取自然对数，可得：</p>
<script type="math/tex; mode=display">
 \ln(P(x_1,x_2,...x_n;\mu,\sigma))=
         \ln \left(\frac{1}{\sigma \sqrt{2\pi}} \right) 
          - \frac{(x_1-\mu)^2}{2\sigma^2}  \\ +
          \ln \left( \frac{1}{\sigma \sqrt{2\pi}} \right) 
          - \frac{(x_2-\mu)^2}{2\sigma^2} +...+
          \ln \left( \frac{1}{\sigma \sqrt{2\pi}} \right) 
          - \frac{(x_n-\mu)^2}{2\sigma^2}</script><p>根据对数定律，上式可以化简为：</p>
<script type="math/tex; mode=display">
\ln(P(x_1,x_2,...x_n;\mu,\sigma))=-n\ln(\sigma)-\frac{n}{2} \ln(2\pi)\\
         -\frac{1}{2\sigma^2}[(x_1-\mu)^2+(x_2-\mu)^2+...+(x_n-\mu)^2]</script><p>然后求导为：</p>
<script type="math/tex; mode=display">
\frac{\partial\ln(P(x_1,x_2,...,x_n;\mu,\sigma))}{\partial\mu}=
                 \frac{n}{\sigma^2}[\mu - (x_1+x_2+...+x_n)]</script><p>​     上式左半部分为对数损失函数。损失函数越小越好，因此我们令等式左半的对数损失函数为0，可得：</p>
<script type="math/tex; mode=display">
\mu=\frac{x_1+x_2+...+x_n}{n}</script><p>同理，可计算$\sigma $。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h3 id="机器学习中为什么需要梯度下降"><a href="#机器学习中为什么需要梯度下降" class="headerlink" title="机器学习中为什么需要梯度下降"></a>机器学习中为什么需要梯度下降</h3><p>梯度下降是机器学习中常见优化算法之一，梯度下降法有以下几个作用：</p>
<p>（1）梯度下降是迭代法的一种，可以用于求解最小二乘问题。</p>
<p>（2）在求解机器学习算法的模型参数，即无约束优化问题时，主要有梯度下降法（Gradient Descent）和最小二乘法。</p>
<p>（3）在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。</p>
<p>（4）如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。梯度下降法和梯度上升法可相互转换。</p>
<p>（5）在机器学习中，梯度下降法主要有随机梯度下降法和批量梯度下降法。</p>
<h3 id="梯度下降法缺点"><a href="#梯度下降法缺点" class="headerlink" title="梯度下降法缺点"></a>梯度下降法缺点</h3><p>梯度下降法缺点有以下几点：</p>
<p>（1）靠近极小值时收敛速度减慢。</p>
<p>（2）直线搜索时可能会产生一些问题。</p>
<p>（3）可能会“之字形”地下降。</p>
<p>梯度概念也有需注意的地方：</p>
<p>（1）梯度是一个向量，即有方向有大小。 </p>
<p>（2）梯度的方向是最大方向导数的方向。 </p>
<p>（3）梯度的值是最大方向导数的值。</p>
<h3 id="梯度下降法直观理解"><a href="#梯度下降法直观理解" class="headerlink" title="梯度下降法直观理解"></a>梯度下降法直观理解</h3><p>梯度下降法经典图示如下图2.7所示：</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.25/1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.25/1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                    图2.7 梯度下降法经典图示</p>
<p>​    形象化举例，由上图2.7所示，假如最开始，我们在一座大山上的某处位置，因为到处都是陌生的，不知道下山的路，所以只能摸索着根据直觉，走一步算一步，在此过程中，每走到一个位置的时候，都会求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。不断循环求梯度，就这样一步步地走下去，一直走到我们觉得已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山势低处。<br>​    由此，从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部的最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p>
<p><strong>核心思想归纳</strong>：</p>
<p>（1）初始化参数，随机选取取值范围内的任意数；</p>
<p>（2）迭代操作：<br>    a）计算当前梯度；<br>    b）修改新的变量；<br>    c）计算朝最陡的下坡方向走一步；<br>    d）判断是否需要终止，如否，返回a）；</p>
<p>（3）得到全局最优解或者接近全局最优解。</p>
<h3 id="梯度下降法算法描述"><a href="#梯度下降法算法描述" class="headerlink" title="梯度下降法算法描述"></a>梯度下降法算法描述</h3><p>梯度下降法算法步骤如下：</p>
<p>（1）确定优化模型的假设函数及损失函数。<br>​    举例，对于线性回归，假设函数为：</p>
<script type="math/tex; mode=display">
  h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n</script><p>  其中，$\theta_i,x_i(i=0,1,2,…,n)$分别为模型参数、每个样本的特征值。<br>  对于假设函数，损失函数为：</p>
<script type="math/tex; mode=display">
  J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^{m}_{j=0}(h_\theta (x^{(j)}_0
      ,x^{(j)}_1,...,x^{(j)}_n)-y_j)^2</script><p>（2）相关参数初始化。<br>​    主要初始化${\theta}_i$、算法迭代步长${\alpha} $、终止距离${\zeta} $。初始化时可以根据经验初始化，即${\theta} $初始化为0，步长${\alpha} $初始化为1。当前步长记为${\varphi}_i $。当然，也可随机初始化。</p>
<p>（3）迭代计算。</p>
<p>​    1）计算当前位置时损失函数的梯度，对${\theta}_i $，其梯度表示为：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{2m}\sum^{m}_{j=0}(h_\theta (x^{(j)}_0
    ,x^{(j)}_1,...,x^{(j)}_n)-y_j)^2</script><p>​    2）计算当前位置下降的距离。</p>
<script type="math/tex; mode=display">
{\varphi}_i={\alpha} \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)</script><p>​    3）判断是否终止。<br>​    确定是否所有${\theta}_i$梯度下降的距离${\varphi}_i$都小于终止距离${\zeta}$，如果都小于${\zeta}$，则算法终止，当然的值即为最终结果，否则进入下一步。<br>​    4）更新所有的${\theta}_i$，更新后的表达式为：</p>
<script type="math/tex; mode=display">
{\theta}_i={\theta}_i-\alpha \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)</script><script type="math/tex; mode=display">
\theta_i=\theta_i - \alpha \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{(j)}_0
    ,x^{(j)}_1,...,x^{(j)}_n)-y_j)x^{(j)}_i</script><p>​    5）令上式$x^{(j)}_0=1$，更新完毕后转入1)。<br>​    由此，可看出，当前位置的梯度方向由所有样本决定，上式中 $\frac{1}{m}$、$\alpha \frac{1}{m}$ 的目的是为了便于理解。</p>
<h3 id="如何对梯度下降法进行调优"><a href="#如何对梯度下降法进行调优" class="headerlink" title="如何对梯度下降法进行调优"></a>如何对梯度下降法进行调优</h3><p>实际使用梯度下降法时，各项参数指标不能一步就达到理想状态，对梯度下降法调优主要体现在以下几个方面：</p>
<p>（1）<strong>算法迭代步长$\alpha$选择。</strong><br>    在算法参数初始化时，有时根据经验将步长初始化为1。实际取值取决于数据样本。可以从大到小，多取一些值，分别运行算法看迭代效果，如果损失函数在变小，则取值有效。如果取值无效，说明要增大步长。但步长太大，有时会导致迭代速度过快，错过最优解。步长太小，迭代速度慢，算法运行时间长。</p>
<p>（2）<strong>参数的初始值选择。</strong><br>    初始值不同，获得的最小值也有可能不同，梯度下降有可能得到的是局部最小值。如果损失函数是凸函数，则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p>
<p>（3）<strong>标准化处理。</strong><br>    由于样本不同，特征取值范围也不同，导致迭代速度慢。为了减少特征取值的影响，可对特征数据标准化，使新期望为0，新方差为1，可节省算法运行时间。</p>
<h3 id="随机梯度和批量梯度区别"><a href="#随机梯度和批量梯度区别" class="headerlink" title="随机梯度和批量梯度区别"></a>随机梯度和批量梯度区别</h3><p>​    随机梯度下降（SDG）和批量梯度下降（BDG）是两种主要梯度下降法，其目的是增加某些限制来加速运算求解。<br>下面通过介绍两种梯度下降法的求解思路，对其进行比较。<br>假设函数为：</p>
<script type="math/tex; mode=display">
h_\theta (x_0,x_1,...,x_3) = \theta_0 x_0 + \theta_1 x_1 + ... + \theta_n x_n</script><p>损失函数为：</p>
<script type="math/tex; mode=display">
J(\theta_0, \theta_1, ... , \theta_n) = 
            \frac{1}{2m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
    ,x^{j}_1,...,x^{j}_n)-y^j)^2</script><p>其中，$m$为样本个数，$j$为参数个数。</p>
<p>1、 <strong>批量梯度下降的求解思路如下：</strong><br>a) 得到每个$ \theta $对应的梯度：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{m}\sum^{m}_{j=0}(h_\theta (x^{j}_0
    ,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i</script><p>b) 由于是求最小化风险函数，所以按每个参数 $ \theta $ 的梯度负方向更新 $ \theta_i $ ：</p>
<script type="math/tex; mode=display">
\theta_i=\theta_i - \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
    ,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i</script><p>c) 从上式可以注意到，它得到的虽然是一个全局最优解，但每迭代一步，都要用到训练集所有的数据，如果样本数据很大，这种方法迭代速度就很慢。<br>相比而言，随机梯度下降可避免这种问题。</p>
<p>2、<strong>随机梯度下降的求解思路如下：</strong><br>a) 相比批量梯度下降对应所有的训练样本，随机梯度下降法中损失函数对应的是训练集中每个样本的粒度。<br>损失函数可以写成如下这种形式，</p>
<script type="math/tex; mode=display">
J(\theta_0, \theta_1, ... , \theta_n) = 
            \frac{1}{m} \sum^{m}_{j=0}(y^j - h_\theta (x^{j}_0
            ,x^{j}_1,...,x^{j}_n))^2 = 
            \frac{1}{m} \sum^{m}_{j=0} cost(\theta,(x^j,y^j))</script><p>b）对每个参数 $ \theta$ 按梯度方向更新 $ \theta$：</p>
<script type="math/tex; mode=display">
\theta_i = \theta_i + (y^j - h_\theta (x^{j}_0, x^{j}_1, ... ,x^{j}_n))</script><p>c) 随机梯度下降是通过每个样本来迭代更新一次。<br>随机梯度下降伴随的一个问题是噪音较批量梯度下降要多，使得随机梯度下降并不是每次迭代都向着整体最优化方向。</p>
<p><strong>小结：</strong><br>随机梯度下降法、批量梯度下降法相对来说都比较极端，简单对比如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">方法</th>
<th style="text-align:left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">批量梯度下降</td>
<td style="text-align:left">a）采用所有数据来梯度下降。<br>b）批量梯度下降法在样本量很大的时候，训练速度慢。</td>
</tr>
<tr>
<td style="text-align:center">随机梯度下降</td>
<td style="text-align:left">a）随机梯度下降用一个样本来梯度下降。<br>b）训练速度很快。<br>c）随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。<br>d）收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</td>
</tr>
</tbody>
</table>
</div>
<p>下面介绍能结合两种方法优点的小批量梯度下降法。</p>
<p>3、 <strong>小批量（Mini-Batch）梯度下降的求解思路如下</strong><br>对于总数为$m$个样本的数据，根据样本的数据，选取其中的$n(1&lt; n&lt; m)$个子样本来迭代。其参数$\theta$按梯度方向更新$\theta_i$公式如下：</p>
<script type="math/tex; mode=display">
\theta_i = \theta_i - \alpha \sum^{t+n-1}_{j=t}
        ( h_\theta (x^{j}_{0}, x^{j}_{1}, ... , x^{j}_{n} ) - y^j ) x^{j}_{i}</script><h3 id="各种梯度下降法性能比较"><a href="#各种梯度下降法性能比较" class="headerlink" title="各种梯度下降法性能比较"></a>各种梯度下降法性能比较</h3><p>​    下表简单对比随机梯度下降（SGD）、批量梯度下降（BGD）、小批量梯度下降（Mini-batch GD）、和Online GD的区别：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">BGD</th>
<th style="text-align:center">SGD</th>
<th style="text-align:center">Mini-batch GD</th>
<th style="text-align:center">Online GD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">训练集</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">实时更新</td>
</tr>
<tr>
<td style="text-align:center">单次迭代样本数</td>
<td style="text-align:center">整个训练集</td>
<td style="text-align:center">单个样本</td>
<td style="text-align:center">训练集的子集</td>
<td style="text-align:center">根据具体算法定</td>
</tr>
<tr>
<td style="text-align:center">算法复杂度</td>
<td style="text-align:center">高</td>
<td style="text-align:center">低</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">低</td>
</tr>
<tr>
<td style="text-align:center">时效性</td>
<td style="text-align:center">低</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">高</td>
</tr>
<tr>
<td style="text-align:center">收敛性</td>
<td style="text-align:center">稳定</td>
<td style="text-align:center">不稳定</td>
<td style="text-align:center">较稳定</td>
<td style="text-align:center">不稳定</td>
</tr>
</tbody>
</table>
</div>
<p>BGD、SGD、Mini-batch GD，前面均已讨论过，这里介绍一下Online GD。</p>
<p>​    Online GD于Mini-batch GD/SGD的区别在于，所有训练数据只用一次，然后丢弃。这样做的优点在于可预测最终模型的变化趋势。</p>
<p>​    Online GD在互联网领域用的较多，比如搜索广告的点击率（CTR）预估模型，网民的点击行为会随着时间改变。用普通的BGD算法（每天更新一次）一方面耗时较长（需要对所有历史数据重新训练）；另一方面，无法及时反馈用户的点击行为迁移。而Online GD算法可以实时的依据网民的点击行为进行迁移。</p>
<h2 id="线性判别分析（LDA）"><a href="#线性判别分析（LDA）" class="headerlink" title="线性判别分析（LDA）"></a>线性判别分析（LDA）</h2><h3 id="LDA思想总结"><a href="#LDA思想总结" class="headerlink" title="LDA思想总结"></a>LDA思想总结</h3><p>​    线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。  </p>
<p>LDA分类思想简单总结如下：  </p>
<ol>
<li>多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。  </li>
<li>对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。  </li>
<li>对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。  </li>
</ol>
<p>如果用一句话概括LDA思想，即“投影后类内方差最小，类间方差最大”。</p>
<h3 id="图解LDA核心思想"><a href="#图解LDA核心思想" class="headerlink" title="图解LDA核心思想"></a>图解LDA核心思想</h3><p>​    假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.29/1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.29/1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>左图和右图是两种不同的投影方式。</p>
<p>​    左图思路：让不同类别的平均点距离最远的投影方式。</p>
<p>​    右图思路：让同类别的数据挨得最近的投影方式。</p>
<p>​    从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。</p>
<p>​    以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。</p>
<h3 id="二类LDA算法原理"><a href="#二类LDA算法原理" class="headerlink" title="二类LDA算法原理"></a>二类LDA算法原理</h3><p>​    输入：数据集 $D={(\boldsymbol x_1,\boldsymbol y_1),(\boldsymbol x_2,\boldsymbol y_2),…,(\boldsymbol x_m,\boldsymbol y_m)}$，其中样本 $\boldsymbol x_i $ 是n维向量，$\boldsymbol y_i  \epsilon {0, 1}$，降维后的目标维度 $d$。定义</p>
<p>​    $N_j(j=0,1)$ 为第 $j$ 类样本个数；</p>
<p>​    $X_j(j=0,1)$ 为第 $j$ 类样本的集合；</p>
<p>​    $u_j(j=0,1)$ 为第 $j$ 类样本的均值向量；</p>
<p>​    $\sum_j(j=0,1)$ 为第 $j$ 类样本的协方差矩阵。</p>
<p>​    其中</p>
<script type="math/tex; mode=display">
u_j = \frac{1}{N_j} \sum_{\boldsymbol x\epsilon X_j}\boldsymbol x(j=0,1)， 
\sum_j = \sum_{\boldsymbol x\epsilon X_j}(\boldsymbol x-u_j)(\boldsymbol x-u_j)^T(j=0,1)</script><p>​    假设投影直线是向量 $\boldsymbol w$，对任意样本 $\boldsymbol x_i$，它在直线 $w$上的投影为 $\boldsymbol w^Tx_i$，两个类别的中心点 $u_0$, $u_1 $在直线 $w$ 的投影分别为 $\boldsymbol w^Tu_0$ 、$\boldsymbol w^Tu_1$。</p>
<p>​    LDA的目标是让两类别的数据中心间的距离 $| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 |^2_2$ 尽量大，与此同时，希望同类样本投影点的协方差$\boldsymbol w^T \sum_0 \boldsymbol w$、$\boldsymbol w^T \sum_1 \boldsymbol w$ 尽量小，最小化 $\boldsymbol w^T \sum_0 \boldsymbol w - \boldsymbol w^T \sum_1 \boldsymbol w$ 。<br>​    定义<br>​    类内散度矩阵</p>
<script type="math/tex; mode=display">
S_w = \sum_0 + \sum_1 = 
    \sum_{\boldsymbol x\epsilon X_0}(\boldsymbol x-u_0)(\boldsymbol x-u_0)^T + 
    \sum_{\boldsymbol x\epsilon X_1}(\boldsymbol x-u_1)(\boldsymbol x-u_1)^T</script><p>​    类间散度矩阵 $S_b = (u_0 - u_1)(u_0 - u_1)^T$</p>
<p>​    据上分析，优化目标为</p>
<script type="math/tex; mode=display">
\mathop{\arg\max}_\boldsymbol w J(\boldsymbol w) = \frac{\| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 \|^2_2}{\boldsymbol w^T \sum_0\boldsymbol w + \boldsymbol w^T \sum_1\boldsymbol w} = 
\frac{\boldsymbol w^T(u_0-u_1)(u_0-u_1)^T\boldsymbol w}{\boldsymbol w^T(\sum_0 + \sum_1)\boldsymbol w} =
\frac{\boldsymbol w^TS_b\boldsymbol w}{\boldsymbol w^TS_w\boldsymbol w}</script><p>​    根据广义瑞利商的性质，矩阵 $S^{-1}<em>{w} S_b$ 的最大特征值为 $J(\boldsymbol w)$ 的最大值，矩阵 $S^{-1}</em>{w} S_b$ 的最大特征值对应的特征向量即为 $\boldsymbol w$。</p>
<h3 id="LDA算法流程总结"><a href="#LDA算法流程总结" class="headerlink" title="LDA算法流程总结"></a>LDA算法流程总结</h3><p>LDA算法降维流程如下：</p>
<p>​    输入：数据集 $D = { (x_1,y_1),(x_2,y_2), … ,(x_m,y_m) }$，其中样本 $x_i $ 是n维向量，$y_i  \epsilon {C_1, C_2, …, C_k}$，降维后的目标维度 $d$ 。</p>
<p>​    输出：降维后的数据集 $\overline{D} $ 。</p>
<p>步骤：</p>
<ol>
<li>计算类内散度矩阵 $S_w$。</li>
<li>计算类间散度矩阵 $S_b$ 。</li>
<li>计算矩阵 $S^{-1}_wS_b$ 。</li>
<li>计算矩阵 $S^{-1}_wS_b$ 的最大的 d 个特征值。</li>
<li>计算 d 个特征值对应的 d 个特征向量，记投影矩阵为 W 。</li>
<li>转化样本集的每个样本，得到新样本 $P_i = W^Tx_i$ 。</li>
<li>输出新样本集 $\overline{D} = { (p_1,y_1),(p_2,y_2),…,(p_m,y_m) }$</li>
</ol>
<h3 id="LDA和PCA区别"><a href="#LDA和PCA区别" class="headerlink" title="LDA和PCA区别"></a>LDA和PCA区别</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">异同点</th>
<th style="text-align:left">LDA</th>
<th style="text-align:left">PCA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">相同点</td>
<td style="text-align:left">1. 两者均可以对数据进行降维；<br>2. 两者在降维时均使用了矩阵特征分解的思想；<br>3. 两者都假设数据符合高斯分布；</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:center">不同点</td>
<td style="text-align:left">有监督的降维方法；</td>
<td style="text-align:left">无监督的降维方法；</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">降维最多降到k-1维；</td>
<td style="text-align:left">降维多少没有限制；</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">可以用于降维，还可以用于分类；</td>
<td style="text-align:left">只用于降维；</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">选择分类性能最好的投影方向；</td>
<td style="text-align:left">选择样本点投影具有最大方差的方向；</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">更明确，更能反映样本间差异；</td>
<td style="text-align:left">目的较为模糊；</td>
</tr>
</tbody>
</table>
</div>
<h3 id="LDA优缺点"><a href="#LDA优缺点" class="headerlink" title="LDA优缺点"></a>LDA优缺点</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">优缺点</th>
<th style="text-align:left">简要说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">优点</td>
<td style="text-align:left">1. 可以使用类别的先验知识；<br>2. 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；</td>
</tr>
<tr>
<td style="text-align:center">缺点</td>
<td style="text-align:left">1. LDA不适合对非高斯分布样本进行降维；<br>2. LDA降维最多降到分类数k-1维；<br>3. LDA在样本分类信息依赖方差而不是均值时，降维效果不好；<br>4. LDA可能过度拟合数据。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h2><h3 id="主成分分析（PCA）思想总结"><a href="#主成分分析（PCA）思想总结" class="headerlink" title="主成分分析（PCA）思想总结"></a>主成分分析（PCA）思想总结</h3><ol>
<li>PCA就是将高维的数据通过线性变换投影到低维空间上去。</li>
<li>投影思想：找出最能够代表原始数据的投影方法。被PCA降掉的那些维度只能是那些噪声或是冗余的数据。</li>
<li>去冗余：去除可以被其他向量代表的线性相关向量，这部分信息量是多余的。</li>
<li>去噪声，去除较小特征值对应的特征向量，特征值的大小反映了变换后在特征向量方向上变换的幅度，幅度越大，说明这个方向上的元素差异也越大，要保留。</li>
<li>对角化矩阵，寻找极大线性无关组，保留较大的特征值，去除较小特征值，组成一个投影矩阵，对原始样本矩阵进行投影，得到降维后的新样本矩阵。</li>
<li>完成PCA的关键是——协方差矩阵。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。</li>
<li>之所以对角化，因为对角化之后非对角上的元素都是0，达到去噪声的目的。对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。所以我们只取那些含有较大能量(特征值)的维度，其余的就舍掉，即去冗余。</li>
</ol>
<h3 id="图解PCA核心思想"><a href="#图解PCA核心思想" class="headerlink" title="图解PCA核心思想"></a>图解PCA核心思想</h3><p>​    PCA可解决训练数据中存在数据特征过多或特征累赘的问题。核心思想是将m维特征映射到n维（n &lt; m），这n维形成主元，是重构出来最能代表原始数据的正交特征。</p>
<p>​    假设数据集是m个n维，$(\boldsymbol x^{(1)}, \boldsymbol x^{(2)}, \cdots, \boldsymbol x^{(m)})$。如果$n=2$，需要降维到$n’=1$，现在想找到某一维度方向代表这两个维度的数据。下图有$u_1, u_2$两个向量方向，但是哪个向量才是我们所想要的，可以更好代表原始数据集的呢？</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.34/1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.34/1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>从图可看出，$u_1$比$u_2$好，为什么呢？有以下两个主要评价指标：</p>
<ol>
<li>样本点到这个直线的距离足够近。</li>
<li>样本点在这个直线上的投影能尽可能的分开。</li>
</ol>
<p>如果我们需要降维的目标维数是其他任意维，则：</p>
<ol>
<li>样本点到这个超平面的距离足够近。</li>
<li>样本点在这个超平面上的投影能尽可能的分开。</li>
</ol>
<h3 id="PCA算法推理"><a href="#PCA算法推理" class="headerlink" title="PCA算法推理"></a>PCA算法推理</h3><p>下面以基于最小投影距离为评价指标推理：</p>
<p>​    假设数据集是m个n维，$(x^{(1)}, x^{(2)},…,x^{(m)})$，且数据进行了中心化。经过投影变换得到新坐标为 ${w_1,w_2,…,w_n}$，其中 $w$ 是标准正交基，即 $| w |_2 = 1$，$w^T_iw_j = 0$。</p>
<p>​    经过降维后，新坐标为 ${ w<em>1,w_2,…,w_n }$，其中 $n’$ 是降维后的目标维数。样本点 $x^{(i)}$ 在新坐标系下的投影为 $z^{(i)} = \left(z^{(i)}_1, z^{(i)}_2, …, z^{(i)}</em>{n’}   \right)$，其中 $z^{(i)}_j = w^T_j x^{(i)}$ 是 $x^{(i)} $ 在低维坐标系里第 j 维的坐标。</p>
<p>​    如果用 $z^{(i)} $ 去恢复 $x^{(i)} $ ，则得到的恢复数据为 $\widehat{x}^{(i)} = \sum^{n’}_{j=1} x^{(i)}_j w_j = Wz^{(i)}$，其中 $W$为标准正交基组成的矩阵。</p>
<p>​    考虑到整个样本集，样本点到这个超平面的距离足够近，目标变为最小化 $\sum^m_{i=1} | \hat{x}^{(i)} - x^{(i)} |^2_2$ 。对此式进行推理，可得：</p>
<script type="math/tex; mode=display">
\sum^m_{i=1} \| \hat{x}^{(i)} - x^{(i)} \|^2_2 = 
    \sum^m_{i=1} \| Wz^{(i)} - x^{(i)} \|^2_2 \\
    = \sum^m_{i=1} \left( Wz^{(i)} \right)^T \left( Wz^{(i)} \right)
    - 2\sum^m_{i=1} \left( Wz^{(i)} \right)^T x^{(i)}
    + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
    = \sum^m_{i=1} \left( z^{(i)} \right)^T \left( z^{(i)} \right)
    - 2\sum^m_{i=1} \left( z^{(i)} \right)^T x^{(i)}
    + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
    = - \sum^m_{i=1} \left( z^{(i)} \right)^T \left( z^{(i)} \right)
    + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
    = -tr \left( W^T \left( \sum^m_{i=1} x^{(i)} \left( x^{(i)} \right)^T \right)W \right)
    + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
    = -tr \left( W^TXX^TW \right)
    + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)}</script><p>​    在推导过程中，分别用到了 $\overline{x}^{(i)} = Wz^{(i)}$ ，矩阵转置公式 $(AB)^T = B^TA^T$，$W^TW = I$，$z^{(i)} = W^Tx^{(i)}$ 以及矩阵的迹，最后两步是将代数和转为矩阵形式。<br>​    由于 $W$ 的每一个向量 $w<em>j$ 是标准正交基，$\sum^m</em>{i=1} x^{(i)} \left(  x^{(i)} \right)^T$ 是数据集的协方差矩阵，$\sum^m<em>{i=1} \left(  x^{(i)} \right)^T x^{(i)} $ 是一个常量。最小化 $\sum^m</em>{i=1} | \hat{x}^{(i)} - x^{(i)} |^2_2$ 又可等价于</p>
<script type="math/tex; mode=display">
\underbrace{\arg \min}_W - tr \left( W^TXX^TW \right) s.t.W^TW = I</script><p>利用拉格朗日函数可得到</p>
<script type="math/tex; mode=display">
J(W) = -tr(W^TXX^TW) + \lambda(W^TW - I)</script><p>​    对 $W$ 求导，可得 $-XX^TW + \lambda W = 0 $ ，也即 $ XX^TW = \lambda W $ 。 $ XX^T $ 是 $ n’ $ 个特征向量组成的矩阵，$\lambda$ 为$ XX^T $ 的特征值。$W$ 即为我们想要的矩阵。<br>​    对于原始数据，只需要 $z^{(i)} = W^TX^{(i)}$ ，就可把原始数据集降维到最小投影距离的 $n’$ 维数据集。</p>
<p>​    基于最大投影方差的推导，这里就不再赘述，有兴趣的同仁可自行查阅资料。</p>
<h3 id="PCA算法流程总结"><a href="#PCA算法流程总结" class="headerlink" title="PCA算法流程总结"></a>PCA算法流程总结</h3><p>输入：$n$ 维样本集 $D = \left( x^{(1)},x^{(2)},…,x^{(m)} \right)$ ，目标降维的维数 $n’$ 。</p>
<p>输出：降维后的新样本集 $D’  = \left( z^{(1)},z^{(2)},…,z^{(m)} \right)$ 。</p>
<p>主要步骤如下：</p>
<ol>
<li>对所有的样本进行中心化，$ x^{(i)} = x^{(i)} - \frac{1}{m} \sum^m_{j=1} x^{(j)} $ 。</li>
<li>计算样本的协方差矩阵 $XX^T$ 。</li>
<li>对协方差矩阵 $XX^T$ 进行特征值分解。</li>
<li>取出最大的 $n’ $ 个特征值对应的特征向量 ${ w<em>1,w_2,…,w</em>{n’} }$ 。</li>
<li>标准化特征向量，得到特征向量矩阵 $W$ 。</li>
<li>转化样本集中的每个样本 $z^{(i)} = W^T x^{(i)}$ 。</li>
<li>得到输出矩阵 $D’ = \left( z^{(1)},z^{(2)},…,z^{(n)} \right)$ 。<br><em>注</em>：在降维时，有时不明确目标维数，而是指定降维到的主成分比重阈值 $k(k \epsilon(0,1])$ 。假设 $n$ 个特征值为 $\lambda<em>1 \geqslant \lambda_2 \geqslant … \geqslant \lambda_n$ ，则 $n’$ 可从 $\sum^{n’}</em>{i=1} \lambda<em>i \geqslant k \times \sum^n</em>{i=1} \lambda_i $ 得到。</li>
</ol>
<h3 id="PCA算法主要优缺点"><a href="#PCA算法主要优缺点" class="headerlink" title="PCA算法主要优缺点"></a>PCA算法主要优缺点</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">优缺点</th>
<th style="text-align:left">简要说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">优点</td>
<td style="text-align:left">1. 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　2.各主成分之间正交，可消除原始数据成分间的相互影响的因素。3. 计算方法简单，主要运算是特征值分解，易于实现。</td>
</tr>
<tr>
<td style="text-align:center">缺点</td>
<td style="text-align:left">1.主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。2. 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="降维的必要性及目的"><a href="#降维的必要性及目的" class="headerlink" title="降维的必要性及目的"></a>降维的必要性及目的</h3><p><strong>降维的必要性</strong>：</p>
<ol>
<li>多重共线性和预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。</li>
<li>高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有2%。</li>
<li>过多的变量，对查找规律造成冗余麻烦。</li>
<li>仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。</li>
</ol>
<p><strong>降维的目的</strong>：</p>
<ol>
<li>减少预测变量的个数。</li>
<li>确保这些变量是相互独立的。</li>
<li>提供一个框架来解释结果。相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示。</li>
<li>数据在低维下更容易处理、更容易使用。</li>
<li>去除数据噪声。</li>
<li>降低算法运算开销。</li>
</ol>
<h3 id="KPCA与PCA的区别"><a href="#KPCA与PCA的区别" class="headerlink" title="KPCA与PCA的区别"></a>KPCA与PCA的区别</h3><p>​    应用PCA算法前提是假设存在一个线性超平面，进而投影。那如果数据不是线性的呢？该怎么办？这时候就需要KPCA，数据集从 $n$ 维映射到线性可分的高维 $N &gt;n$，然后再从 $N$ 维降维到一个低维度 $n’(n’&lt;n&lt;N)$ 。</p>
<p>​    KPCA用到了核函数思想，使用了核函数的主成分分析一般称为核主成分分析(Kernelized PCA, 简称KPCA）。</p>
<p>假设高维空间数据由 $n$ 维空间的数据通过映射 $\phi$ 产生。</p>
<p>​    $n$ 维空间的特征分解为：</p>
<script type="math/tex; mode=display">
\sum^m_{i=1} x^{(i)} \left( x^{(i)} \right)^T W = \lambda W</script><p>​    其映射为</p>
<script type="math/tex; mode=display">
\sum^m_{i=1} \phi \left( x^{(i)} \right) \phi \left( x^{(i)} \right)^T W = \lambda W</script><p>​    通过在高维空间进行协方差矩阵的特征值分解，然后用和PCA一样的方法进行降维。由于KPCA需要核函数的运算，因此它的计算量要比PCA大很多。</p>
<h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><h3 id="模型评估常用方法？"><a href="#模型评估常用方法？" class="headerlink" title="模型评估常用方法？"></a>模型评估常用方法？</h3><p>​    一般情况来说，单一评分标准无法完全评估一个机器学习模型。只用good和bad偏离真实场景去评估某个模型，都是一种欠妥的评估方式。下面介绍常用的分类模型和回归模型评估方法。</p>
<p><strong>分类模型常用评估方法：</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">指标</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Accuracy</td>
<td style="text-align:center">准确率</td>
</tr>
<tr>
<td style="text-align:center">Precision</td>
<td style="text-align:center">精准度/查准率</td>
</tr>
<tr>
<td style="text-align:center">Recall</td>
<td style="text-align:center">召回率/查全率</td>
</tr>
<tr>
<td style="text-align:center">P-R曲线</td>
<td style="text-align:center">查准率为纵轴，查全率为横轴，作图</td>
</tr>
<tr>
<td style="text-align:center">F1</td>
<td style="text-align:center">F1值</td>
</tr>
<tr>
<td style="text-align:center">Confusion Matrix</td>
<td style="text-align:center">混淆矩阵</td>
</tr>
<tr>
<td style="text-align:center">ROC</td>
<td style="text-align:center">ROC曲线</td>
</tr>
<tr>
<td style="text-align:center">AUC</td>
<td style="text-align:center">ROC曲线下的面积</td>
</tr>
</tbody>
</table>
</div>
<p><strong>回归模型常用评估方法：</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">指标</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Mean Square Error (MSE, RMSE)</td>
<td style="text-align:center">平均方差</td>
</tr>
<tr>
<td style="text-align:center">Absolute Error (MAE, RAE)</td>
<td style="text-align:center">绝对误差</td>
</tr>
<tr>
<td style="text-align:center">R-Squared</td>
<td style="text-align:center">R平方值</td>
</tr>
</tbody>
</table>
</div>
<h3 id="误差、偏差和方差有什么区别和联系"><a href="#误差、偏差和方差有什么区别和联系" class="headerlink" title="误差、偏差和方差有什么区别和联系"></a>误差、偏差和方差有什么区别和联系</h3><p>在机器学习中，Bias(偏差)，Error(误差)，和Variance(方差)存在以下区别和联系：</p>
<p><strong>对于Error </strong>：</p>
<ul>
<li><p>误差（error）：一般地，我们把学习器的实际预测输出与样本的真是输出之间的差异称为“误差”。</p>
</li>
<li><p>Error = Bias + Variance + Noise，Error反映的是整个模型的准确度。</p>
</li>
</ul>
<p><strong>对于Noise:</strong></p>
<p>噪声：描述了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</p>
<p><strong>对于Bias：</strong></p>
<ul>
<li>Bias衡量模型拟合训练数据的能力（训练数据不一定是整个 training dataset，而是只用于训练它的那一部分数据，例如：mini-batch），Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度。</li>
<li>Bias 越小，拟合能力越高（可能产生overfitting）；反之，拟合能力越低（可能产生underfitting）。</li>
<li>偏差越大，越偏离真实数据，如下图第二行所示。</li>
</ul>
<p><strong>对于Variance：</strong></p>
<ul>
<li><p>方差公式：$S<em>{N}^{2}=\frac{1}{N}\sum</em>{i=1}^{N}(x_{i}-\bar{x})^{2}$</p>
</li>
<li><p>Variance描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，模型的稳定程度越差。</p>
</li>
<li>Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。</li>
<li>Variance越小，模型的泛化的能力越高；反之，模型的泛化的能力越低。</li>
<li>如果模型在训练集上拟合效果比较优秀，但是在测试集上拟合效果比较差劣，则方差较大，说明模型的稳定程度较差，出现这种现象可能是由于模型对训练集过拟合造成的。 如下图右列所示。</li>
</ul>
<blockquote>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.20.1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.20.1.png" srcset="data:image/png;base64,666" alt=""></p>
</blockquote>
<h3 id="经验误差与泛化误差"><a href="#经验误差与泛化误差" class="headerlink" title="经验误差与泛化误差"></a>经验误差与泛化误差</h3><p>经验误差（empirical error）：也叫训练误差（training error），模型在训练集上的误差。 </p>
<p>泛化误差（generalization error）：模型在新样本集（测试集）上的误差称为“泛化误差”。</p>
<h3 id="图解欠拟合、过拟合"><a href="#图解欠拟合、过拟合" class="headerlink" title="图解欠拟合、过拟合"></a>图解欠拟合、过拟合</h3><p>根据不同的坐标方式，欠拟合与过拟合图解不同。</p>
<ol>
<li><strong>横轴为训练样本数量，纵轴为误差</strong></li>
</ol>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.4.1.jpg" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.4.1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>如上图所示，我们可以直观看出欠拟合和过拟合的区别：</p>
<p>​    模型欠拟合：在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大；</p>
<p>​    模型过拟合：在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。</p>
<p>​    模型正常：在训练集以及测试集上，同时具有相对较低的偏差以及方差。</p>
<ol>
<li><strong>横轴为模型复杂程度，纵轴为误差</strong></li>
</ol>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.4.2.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.4.2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                    红线为测试集上的Error,蓝线为训练集上的Error</p>
<p>​    模型欠拟合：模型在点A处，在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大。</p>
<p>​    模型过拟合：模型在点C处，在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。 </p>
<p>​    模型正常：模型复杂程度控制在点B处为最优。</p>
<ol>
<li><strong>横轴为正则项系数，纵轴为误差</strong></li>
</ol>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.4.3.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.4.3.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                             红线为测试集上的Error,蓝线为训练集上的Error</p>
<p>​    模型欠拟合：模型在点C处，在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大。</p>
<p>​    模型过拟合：模型在点A处，在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。 它通常发生在模型过于复杂的情况下，如参数过多等，会使得模型的预测性能变弱，并且增加数据的波动性。虽然模型在训练时的效果可以表现的很完美，基本上记住了数据的全部特点，但这种模型在未知数据的表现能力会大减折扣，因为简单的模型泛化能力通常都是很弱的。</p>
<p>​    模型正常：模型复杂程度控制在点B处为最优。</p>
<h3 id="如何解决过拟合与欠拟合"><a href="#如何解决过拟合与欠拟合" class="headerlink" title="如何解决过拟合与欠拟合"></a>如何解决过拟合与欠拟合</h3><p><strong>如何解决欠拟合：</strong></p>
<ol>
<li>添加其他特征项。组合、泛化、相关性、上下文特征、平台特征等特征是特征添加的重要手段，有时候特征项不够会导致模型欠拟合。</li>
<li>添加多项式特征。例如将线性模型添加二次项或三次项使模型泛化能力更强。例如，FM（Factorization Machine）模型、FFM（Field-aware Factorization Machine）模型，其实就是线性模型，增加了二阶多项式，保证了模型一定的拟合程度。</li>
<li>可以增加模型的复杂程度。</li>
<li>减小正则化系数。正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。</li>
</ol>
<p><strong>如何解决过拟合：</strong></p>
<ol>
<li>重新清洗数据，数据不纯会导致过拟合，此类情况需要重新清洗数据。 </li>
<li>增加训练样本数量。 </li>
<li>降低模型复杂程度。 </li>
<li>增大正则项系数。 </li>
<li>采用dropout方法，dropout方法，通俗的讲就是在训练的时候让神经元以一定的概率不工作。 </li>
<li>early stopping。 </li>
<li>减少迭代次数。 </li>
<li>增大学习率。 </li>
<li>添加噪声数据。 </li>
<li>树结构中，可以对树进行剪枝。 </li>
<li>减少特征项。</li>
</ol>
<p>欠拟合和过拟合这些方法，需要根据实际问题，实际模型，进行选择。</p>
<h3 id="交叉验证的主要作用"><a href="#交叉验证的主要作用" class="headerlink" title="交叉验证的主要作用"></a>交叉验证的主要作用</h3><p>​    为了得到更为稳健可靠的模型，对模型的泛化误差进行评估，得到模型泛化误差的近似值。当有多个模型可以选择时，我们通常选择“泛化误差”最小的模型。 </p>
<p>​    交叉验证的方法有许多种，但是最常用的是：留一交叉验证、k折交叉验证。</p>
<h3 id="理解k折交叉验证"><a href="#理解k折交叉验证" class="headerlink" title="理解k折交叉验证"></a>理解k折交叉验证</h3><ol>
<li>将含有N个样本的数据集，分成K份，每份含有N/K个样本。选择其中1份作为测试集，另外K-1份作为训练集，测试集就有K种情况。 </li>
<li>在每种情况中，用训练集训练模型，用测试集测试模型，计算模型的泛化误差。 </li>
<li>交叉验证重复K次，每份验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测，得到模型最终的泛化误差。 </li>
<li>将K种情况下，模型的泛化误差取均值，得到模型最终的泛化误差。  </li>
<li>一般$2\leqslant K \leqslant10$。 k折交叉验证的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。 </li>
<li>训练集中样本数量要足够多，一般至少大于总样本数的50%。 </li>
<li>训练集和测试集必须从完整的数据集中均匀取样。均匀取样的目的是希望减少训练集、测试集与原数据集之间的偏差。当样本数量足够多时，通过随机取样，便可以实现均匀取样的效果。 </li>
</ol>
<h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><p>第一种混淆矩阵:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">真实情况T or F</th>
<th style="text-align:left">预测为正例1，P</th>
<th style="text-align:left">预测为负例0，N</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">本来label标记为1，预测结果真为T、假为F</td>
<td style="text-align:left">TP(预测为1，实际为1)</td>
<td style="text-align:left">FN(预测为0，实际为1)</td>
</tr>
<tr>
<td style="text-align:center">本来label标记为0，预测结果真为T、假为F</td>
<td style="text-align:left">FP(预测为1，实际为0)</td>
<td style="text-align:left">TN(预测为0，实际也为0)</td>
</tr>
</tbody>
</table>
</div>
<p>第二种混淆矩阵:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">预测情况P or N</th>
<th style="text-align:left">实际label为1,预测对了为T</th>
<th style="text-align:left">实际label为0,预测对了为T</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">预测为正例1，P</td>
<td style="text-align:left">TP(预测为1，实际为1)</td>
<td style="text-align:left">FP(预测为1，实际为0)</td>
</tr>
<tr>
<td style="text-align:center">预测为负例0，N</td>
<td style="text-align:left">FN(预测为0，实际为1)</td>
<td style="text-align:left">TN(预测为0，实际也为0)</td>
</tr>
</tbody>
</table>
</div>
<h3 id="错误率及精度"><a href="#错误率及精度" class="headerlink" title="错误率及精度"></a>错误率及精度</h3><ol>
<li>错误率（Error Rate）：分类错误的样本数占样本总数的比例。</li>
<li>精度（accuracy）：分类正确的样本数占样本总数的比例。</li>
</ol>
<h3 id="查准率与查全率"><a href="#查准率与查全率" class="headerlink" title="查准率与查全率"></a>查准率与查全率</h3><p>将算法预测的结果分成四种情况： </p>
<ol>
<li>正确肯定（True Positive,TP）：预测为真，实际为真 </li>
<li>正确否定（True Negative,TN）：预测为假，实际为假 </li>
<li>错误肯定（False Positive,FP）：预测为真，实际为假 </li>
<li>错误否定（False Negative,FN）：预测为假，实际为真</li>
</ol>
<p>则： </p>
<p>查准率（Precision）=TP/（TP+FP）</p>
<p><strong>理解</strong>：预测出为阳性的样本中，正确的有多少。区别准确率（正确预测出的样本，包括正确预测为阳性、阴性，占总样本比例）。<br>例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 </p>
<p>查全率（Recall）=TP/（TP+FN）</p>
<p><strong>理解</strong>：正确预测为阳性的数量占总样本中阳性数量的比例。<br>例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 </p>
<h3 id="ROC与AUC"><a href="#ROC与AUC" class="headerlink" title="ROC与AUC"></a>ROC与AUC</h3><p>​    ROC全称是“受试者工作特征”（Receiver Operating Characteristic）。</p>
<p>​    ROC曲线的面积就是AUC（Area Under Curve）。</p>
<p>​    AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。</p>
<p>​    ROC曲线，通过将连续变量设定出多个不同的临界值，从而计算出一系列真正率和假正率，再以假正率为横坐标、真正率为纵坐标绘制成曲线，曲线下面积越大，推断准确性越高。在ROC曲线上，最靠近坐标图左上方的点为假正率和真正率均较高的临界值。 </p>
<p>​    对于分类器，或者说分类算法，评价指标主要有Precision，Recall，F-score。下图是一个ROC曲线的示例。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.40.10/1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.40.10/1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>ROC曲线的横坐标为False Positive Rate（FPR），纵坐标为True Positive Rate（TPR）。其中</p>
<script type="math/tex; mode=display">
TPR = \frac{TP}{TP+FN} ,FPR = \frac{FP}{FP+TN}</script><p>​    下面着重介绍ROC曲线图中的四个点和一条线。<br>​    第一个点(0,1)，即FPR=0, TPR=1，这意味着FN（False Negative）=0，并且FP（False Positive）=0。意味着这是一个完美的分类器，它将所有的样本都正确分类。<br>​    第二个点(1,0)，即FPR=1，TPR=0，意味着这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。<br>​    第三个点(0,0)，即FPR=TPR=0，即FP（False Positive）=TP（True Positive）=0，可以发现该分类器预测所有的样本都为负样本（Negative）。<br>​    第四个点(1,1)，即FPR=TPR=1，分类器实际上预测所有的样本都为正样本。<br>​    经过以上分析，ROC曲线越接近左上角，该分类器的性能越好。</p>
<p>​    ROC曲线所覆盖的面积称为AUC（Area Under Curve），可以更直观的判断学习器的性能，AUC越大则性能越好。  </p>
<h3 id="如何画ROC曲线"><a href="#如何画ROC曲线" class="headerlink" title="如何画ROC曲线"></a>如何画ROC曲线</h3><p>​    下图是一个示例，图中共有20个测试样本，“Class”一栏表示每个测试样本真正的标签（p表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。</p>
<p>步骤：<br>    1、假设已经得出一系列样本被划分为正类的概率，按照大小排序。<br>    2、从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。<br>    3、每次选取一个不同的threshold，得到一组FPR和TPR，即ROC曲线上的一点。以此共得到20组FPR和TPR的值。<br>    4、根据3、中的每个坐标点，画图。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.40.11/1.jpg" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.40.11/1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="如何计算TPR，FPR"><a href="#如何计算TPR，FPR" class="headerlink" title="如何计算TPR，FPR"></a>如何计算TPR，FPR</h3><p>1、分析数据<br>y_true = [0, 0, 1, 1]；scores = [0.1, 0.4, 0.35, 0.8]；<br>2、列表</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>样本</th>
<th>预测属于P的概率(score)</th>
<th>真实类别</th>
</tr>
</thead>
<tbody>
<tr>
<td>y[0]</td>
<td>0.1</td>
<td>N</td>
</tr>
<tr>
<td>y[1]</td>
<td>0.4</td>
<td>N</td>
</tr>
<tr>
<td>y[2]</td>
<td>0.35</td>
<td>P</td>
</tr>
<tr>
<td>y[3]</td>
<td>0.8</td>
<td>P</td>
</tr>
</tbody>
</table>
</div>
<p>3、将截断点依次取为score值，计算TPR和FPR。<br>当截断点为0.1时：<br>说明只要score&gt;=0.1，它的预测类别就是正例。 因为4个样本的score都大于等于0.1，所以，所有样本的预测类别都为P。<br>scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [1, 1, 1, 1]；<br>正例与反例信息如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正例</strong></td>
<td>TP=2</td>
<td>FN=0</td>
</tr>
<tr>
<td><strong>反例</strong></td>
<td>FP=2</td>
<td>TN=0</td>
</tr>
</tbody>
</table>
</div>
<p>由此可得：<br>TPR = TP/(TP+FN) = 1； FPR = FP/(TN+FP) = 1；</p>
<p>当截断点为0.35时：<br>scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 1, 1, 1];<br>正例与反例信息如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正例</strong></td>
<td>TP=2</td>
<td>FN=0</td>
</tr>
<tr>
<td><strong>反例</strong></td>
<td>FP=1</td>
<td>TN=1</td>
</tr>
</tbody>
</table>
</div>
<p>由此可得：<br>TPR = TP/(TP+FN) = 1； FPR = FP/(TN+FP) = 0.5；</p>
<p>当截断点为0.4时：<br>scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 1, 0, 1]；<br>正例与反例信息如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正例</strong></td>
<td>TP=1</td>
<td>FN=1</td>
</tr>
<tr>
<td><strong>反例</strong></td>
<td>FP=1</td>
<td>TN=1</td>
</tr>
</tbody>
</table>
</div>
<p>由此可得：<br>TPR = TP/(TP+FN) = 0.5； FPR = FP/(TN+FP) = 0.5；</p>
<p>当截断点为0.8时：<br>scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 0, 0, 1]；</p>
<p>正例与反例信息如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正例</strong></td>
<td>TP=1</td>
<td>FN=1</td>
</tr>
<tr>
<td><strong>反例</strong></td>
<td>FP=0</td>
<td>TN=2</td>
</tr>
</tbody>
</table>
</div>
<p>由此可得：<br>TPR = TP/(TP+FN) = 0.5； FPR = FP/(TN+FP) = 0；</p>
<p>4、根据TPR、FPR值，以FPR为横轴，TPR为纵轴画图。</p>
<h3 id="如何计算AUC"><a href="#如何计算AUC" class="headerlink" title="如何计算AUC"></a>如何计算AUC</h3><ul>
<li>将坐标点按照横坐标FPR排序 。</li>
<li>计算第$i$个坐标点和第$i+1$个坐标点的间距$dx$ 。 </li>
<li>获取第$i$或者$i+1$个坐标点的纵坐标y。</li>
<li>计算面积微元$ds=ydx$。</li>
<li>对面积微元进行累加，得到AUC。</li>
</ul>
<h3 id="为什么使用Roc和Auc评价分类器"><a href="#为什么使用Roc和Auc评价分类器" class="headerlink" title="为什么使用Roc和Auc评价分类器"></a>为什么使用Roc和Auc评价分类器</h3><p>​    模型有很多评估方法，为什么还要使用ROC和AUC呢？<br>​    因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。</p>
<h3 id="直观理解AUC"><a href="#直观理解AUC" class="headerlink" title="直观理解AUC"></a>直观理解AUC</h3><p>​    下图展现了三种AUC的值： </p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.40.15/1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.40.15/1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    AUC是衡量二分类模型优劣的一种评价指标，表示正例排在负例前面的概率。其他评价指标有精确度、准确率、召回率，而AUC比这三者更为常用。<br>​    一般在分类模型中，预测结果都是以概率的形式表现，如果要计算准确率，通常都会手动设置一个阈值来将对应的概率转化成类别，这个阈值也就很大程度上影响了模型准确率的计算。<br>​    举例：<br>​    现在假设有一个训练好的二分类器对10个正负样本（正例5个，负例5个）预测，得分按高到低排序得到的最好预测结果为[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]，即5个正例均排在5个负例前面，正例排在负例前面的概率为100%。然后绘制其ROC曲线，由于是10个样本，除去原点我们需要描10个点，如下：</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.17-1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.17-1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    描点方式按照样本预测结果的得分高低从左至右开始遍历。从原点开始，每遇到1便向y轴正方向移动y轴最小步长1个单位，这里是1/5=0.2；每遇到0则向x轴正方向移动x轴最小步长1个单位，这里也是0.2。不难看出，上图的AUC等于1，印证了正例排在负例前面的概率的确为100%。</p>
<p>​    假设预测结果序列为[1, 1, 1, 1, 0, 1, 0, 0, 0, 0]。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.17-2.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.17-2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    计算上图的AUC为0.96与计算正例与排在负例前面的概率0.8 × 1 + 0.2 × 0.8 = 0.96相等，而左上角阴影部分的面积则是负例排在正例前面的概率0.2 × 0.2 = 0.04。</p>
<p>​    假设预测结果序列为[1, 1, 1, 0, 1, 0, 1, 0, 0, 0]。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.17-3.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.17-3.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    计算上图的AUC为0.88与计算正例与排在负例前面的概率0.6 × 1 + 0.2 × 0.8 + 0.2 × 0.6 = 0.88相等，左上角阴影部分的面积是负例排在正例前面的概率0.2 × 0.2 × 3 = 0.12。</p>
<h3 id="代价敏感错误率与代价曲线"><a href="#代价敏感错误率与代价曲线" class="headerlink" title="代价敏感错误率与代价曲线"></a>代价敏感错误率与代价曲线</h3><p>不同的错误会产生不同代价。以二分法为例，设置代价矩阵如下：</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2-1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2-1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>当判断正确的时候，值为0，不正确的时候，分别为$Cost<em>{01}$和$Cost</em>{10}$ 。</p>
<p>$Cost_{10}$:表示实际为反例但预测成正例的代价。</p>
<p>$Cost_{01}$:表示实际为正例但是预测为反例的代价。</p>
<p><strong>代价敏感错误率</strong>=样本中由模型得到的错误值与代价乘积之和 / 总样本。<br>其数学表达式为：</p>
<script type="math/tex; mode=display">
E(f;D;cost)=\frac{1}{m}\left( \sum_{x_{i} \in D^{+}}({f(x_i)\neq y_i})\times Cost_{01}+ \sum_{x_{i} \in D^{-}}({f(x_i)\neq y_i})\times Cost_{10}\right)</script><p>$D^{+}、D^{-}$分别代表样例集的正例子集和反例子集，x是预测值，y是真实值。</p>
<p><strong>代价曲线</strong>：<br>    在均等代价时，ROC曲线不能直接反应出模型的期望总体代价，而代价曲线可以。<br>代价曲线横轴为[0,1]的正例函数代价：</p>
<script type="math/tex; mode=display">
P(+)Cost=\frac{p*Cost_{01}}{p*Cost_{01}+(1-p)*Cost_{10}}</script><p>其中p是样本为正例的概率。</p>
<p>代价曲线纵轴维[0,1]的归一化代价：</p>
<script type="math/tex; mode=display">
Cost_{norm}=\frac{FNR*p*Cost_{01}+FNR*(1-p)*Cost_{10}}{p*Cost_{01}+(1-p)*Cost_{10}}</script><p>其中FPR为假阳率，FNR=1-TPR为假阴率。</p>
<p>注：ROC每个点，对应代价平面上一条线。</p>
<p>例如，ROC上(TPR,FPR),计算出FNR=1-TPR，在代价平面上绘制一条从(0,FPR)到(1,FNR)的线段，面积则为该条件下期望的总体代价。所有线段下界面积，所有条件下学习器的期望总体代价。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.18.1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.16.18.1.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="模型有哪些比较检验方法"><a href="#模型有哪些比较检验方法" class="headerlink" title="模型有哪些比较检验方法"></a>模型有哪些比较检验方法</h3><p>正确性分析：模型稳定性分析，稳健性分析，收敛性分析，变化趋势分析，极值分析等。<br>有效性分析：误差分析，参数敏感性分析，模型对比检验等。<br>有用性分析：关键数据求解，极值点，拐点，变化趋势分析，用数据验证动态模拟等。<br>高效性分析：时空复杂度分析与现有进行比较等。</p>
<h3 id="为什么使用标准差"><a href="#为什么使用标准差" class="headerlink" title="为什么使用标准差"></a>为什么使用标准差</h3><p>方差公式为：$S^2<em>{N}=\frac{1}{N}\sum</em>{i=1}^{N}(x_{i}-\bar{x})^{2}$</p>
<p>标准差公式为：$S<em>{N}=\sqrt{\frac{1}{N}\sum</em>{i=1}^{N}(x_{i}-\bar{x})^{2}}$</p>
<p>样本标准差公式为：$S<em>{N}=\sqrt{\frac{1}{N-1}\sum</em>{i=1}^{N}(x_{i}-\bar{x})^{2}}$</p>
<p>与方差相比，使用标准差来表示数据点的离散程度有3个好处：<br>1、表示离散程度的数字与样本数据点的数量级一致，更适合对数据样本形成感性认知。</p>
<p>2、表示离散程度的数字单位与样本数据的单位一致，更方便做后续的分析运算。</p>
<p>3、在样本数据大致符合正态分布的情况下，标准差具有方便估算的特性：68%的数据点落在平均值前后1个标准差的范围内、95%的数据点落在平均值前后2个标准差的范围内，而99%的数据点将会落在平均值前后3个标准差的范围内。</p>
<h3 id="类别不平衡产生原因"><a href="#类别不平衡产生原因" class="headerlink" title="类别不平衡产生原因"></a>类别不平衡产生原因</h3><p>​    类别不平衡（class-imbalance）是指分类任务中不同类别的训练样例数目差别很大的情况。 </p>
<p>产生原因：</p>
<p>​    分类学习算法通常都会假设不同类别的训练样例数目基本相同。如果不同类别的训练样例数目差别很大，则会影响学习结果，测试结果变差。例如二分类问题中有998个反例，正例有2个，那学习方法只需返回一个永远将新样本预测为反例的分类器，就能达到99.8%的精度；然而这样的分类器没有价值。</p>
<h3 id="常见的类别不平衡问题解决方法"><a href="#常见的类别不平衡问题解决方法" class="headerlink" title="常见的类别不平衡问题解决方法"></a>常见的类别不平衡问题解决方法</h3><p>  防止类别不平衡对学习造成的影响，在构建分类模型之前，需要对分类不平衡性问题进行处理。主要解决方法有：</p>
<p>1、扩大数据集</p>
<p>​    增加包含小类样本数据的数据，更多的数据能得到更多的分布信息。</p>
<p>2、对大类数据欠采样</p>
<p>​    减少大类数据样本个数，使与小样本个数接近。<br>​    缺点：欠采样操作时若随机丢弃大类样本，可能会丢失重要信息。<br>​    代表算法：EasyEnsemble。其思想是利用集成学习机制，将大类划分为若干个集合供不同的学习器使用。相当于对每个学习器都进行欠采样，但对于全局则不会丢失重要信息。</p>
<p>3、对小类数据过采样</p>
<p>​    过采样：对小类的数据样本进行采样来增加小类的数据样本个数。 </p>
<p>​    代表算法：SMOTE和ADASYN。 </p>
<p>​    SMOTE：通过对训练集中的小类数据进行插值来产生额外的小类样本数据。</p>
<p>​    新的少数类样本产生的策略：对每个少数类样本a，在a的最近邻中随机选一个样本b，然后在a、b之间的连线上随机选一点作为新合成的少数类样本。<br>​    ADASYN：根据学习难度的不同，对不同的少数类别的样本使用加权分布，对于难以学习的少数类的样本，产生更多的综合数据。 通过减少类不平衡引入的偏差和将分类决策边界自适应地转移到困难的样本两种手段，改善了数据分布。</p>
<p>4、使用新评价指标</p>
<p>​    如果当前评价指标不适用，则应寻找其他具有说服力的评价指标。比如准确度这个评价指标在类别不均衡的分类任务中并不适用，甚至进行误导。因此在类别不均衡分类任务中，需要使用更有说服力的评价指标来对分类器进行评价。</p>
<p>5、选择新算法</p>
<p>​    不同的算法适用于不同的任务与数据，应该使用不同的算法进行比较。</p>
<p>6、数据代价加权</p>
<p>​    例如当分类任务是识别小类，那么可以对分类器的小类样本数据增加权值，降低大类样本的权值，从而使得分类器将重点集中在小类样本身上。</p>
<p>7、转化问题思考角度</p>
<p>​    例如在分类问题时，把小类的样本作为异常点，将问题转化为异常点检测或变化趋势检测问题。 异常点检测即是对那些罕见事件进行识别。变化趋势检测区别于异常点检测在于其通过检测不寻常的变化趋势来识别。    </p>
<p>8、将问题细化分析</p>
<p>​    对问题进行分析与挖掘，将问题划分成多个更小的问题，看这些小问题是否更容易解决。 </p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><h3 id="决策树的基本原理"><a href="#决策树的基本原理" class="headerlink" title="决策树的基本原理"></a>决策树的基本原理</h3><p>​    决策树（Decision Tree）是一种分而治之的决策过程。一个困难的预测问题，通过树的分支节点，被划分成两个或多个较为简单的子集，从结构上划分为不同的子问题。将依规则分割数据集的过程不断递归下去（Recursive Partitioning）。随着树的深度不断增加，分支节点的子集越来越小，所需要提的问题数也逐渐简化。当分支节点的深度或者问题的简单程度满足一定的停止规则（Stopping Rule）时, 该分支节点会停止分裂，此为自上而下的停止阈值（Cutoff Threshold）法；有些决策树也使用自下而上的剪枝（Pruning）法。</p>
<h3 id="决策树的三要素？"><a href="#决策树的三要素？" class="headerlink" title="决策树的三要素？"></a>决策树的三要素？</h3><p>​    一棵决策树的生成过程主要分为下3个部分：  </p>
<p>​    1、特征选择：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准，从而衍生出不同的决策树算法。 </p>
<p>​    2、决策树生成：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则决策树停止生长。树结构来说，递归结构是最容易理解的方式。 </p>
<p>​    3、剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。</p>
<h3 id="决策树学习基本算法"><a href="#决策树学习基本算法" class="headerlink" title="决策树学习基本算法"></a>决策树学习基本算法</h3><p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2-5.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2-5.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="决策树算法优缺点"><a href="#决策树算法优缺点" class="headerlink" title="决策树算法优缺点"></a>决策树算法优缺点</h3><p><strong>决策树算法的优点</strong>：  </p>
<p>1、决策树算法易理解，机理解释起来简单。 </p>
<p>2、决策树算法可以用于小数据集。</p>
<p>3、决策树算法的时间复杂度较小，为用于训练决策树的数据点的对数。</p>
<p>4、相比于其他算法智能分析一种类型变量，决策树算法可处理数字和数据的类别。</p>
<p>5、能够处理多输出的问题。 </p>
<p>6、对缺失值不敏感。</p>
<p>7、可以处理不相关特征数据。</p>
<p>8、效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。</p>
<p><strong>决策树算法的缺点</strong>： </p>
<p>1、对连续性的字段比较难预测。</p>
<p>2、容易出现过拟合。</p>
<p>3、当类别太多时，错误可能就会增加的比较快。</p>
<p>4、在处理特征关联性比较强的数据时表现得不是太好。</p>
<p>5、对于各类别样本数量不一致的数据，在决策树当中，信息增益的结果偏向于那些具有更多数值的特征。</p>
<h3 id="熵的概念以及理解"><a href="#熵的概念以及理解" class="headerlink" title="熵的概念以及理解"></a>熵的概念以及理解</h3><p>​    熵：度量随机变量的不确定性。<br>​    定义：假设随机变量X的可能取值有$x<em>{1},x</em>{2},…,x<em>{n}$，对于每一个可能的取值$x</em>{i}$，其概率为$P(X=x<em>{i})=p</em>{i},i=1,2…,n$。随机变量的熵为：</p>
<script type="math/tex; mode=display">
H(X)=-\sum_{i=1}^{n}p_{i}log_{2}p_{i}</script><p>​       对于样本集合，假设样本有k个类别，每个类别的概率为$\frac{|C<em>{k}|}{|D|}$，其中 ${|C</em>{k}|}{|D|}$为类别为k的样本个数，$|D|$为样本总数。样本集合D的熵为：</p>
<script type="math/tex; mode=display">
H(D)=-\sum_{k=1}^{k}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}</script><h3 id="信息增益的理解"><a href="#信息增益的理解" class="headerlink" title="信息增益的理解"></a>信息增益的理解</h3><p>​    定义：以某特征划分数据集前后的熵的差值。<br>​    熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。  ​    假设划分前样本集合D的熵为H(D)。使用某个特征A划分数据集D，计算划分后的数据子集的熵为H(D|A)。<br>​    则信息增益为：</p>
<script type="math/tex; mode=display">
g(D,A)=H(D)-H(D|A)</script><p>​    <em>注：</em>在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集D。<br>​    思想：计算所有特征划分数据集D，得到多个特征划分数据集D的信息增益，从这些信息增益中选择最大的，因而当前结点的划分特征便是使信息增益最大的划分所使用的特征。<br>​    另外这里提一下信息增益比相关知识：<br>​    $信息增益比=惩罚参数\times信息增益$<br>​    信息增益比本质：在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。<br>​    惩罚参数：数据集D以特征A作为随机变量的熵的倒数。</p>
<h3 id="剪枝处理的作用及策略"><a href="#剪枝处理的作用及策略" class="headerlink" title="剪枝处理的作用及策略"></a>剪枝处理的作用及策略</h3><p>​    剪枝处理是决策树学习算法用来解决过拟合问题的一种办法。</p>
<p>​    在决策树算法中，为了尽可能正确分类训练样本， 节点划分过程不断重复， 有时候会造成决策树分支过多，以至于将训练样本集自身特点当作泛化特点， 而导致过拟合。 因此可以采用剪枝处理来去掉一些分支来降低过拟合的风险。 </p>
<p>​    剪枝的基本策略有预剪枝（pre-pruning）和后剪枝（post-pruning）。</p>
<p>​    预剪枝：在决策树生成过程中，在每个节点划分前先估计其划分后的泛化性能， 如果不能提升，则停止划分，将当前节点标记为叶结点。 </p>
<p>​    后剪枝：生成决策树以后，再自下而上对非叶结点进行考察， 若将此节点标记为叶结点可以带来泛化性能提升，则修改之。</p>
<h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><h3 id="什么是支持向量机"><a href="#什么是支持向量机" class="headerlink" title="什么是支持向量机"></a>什么是支持向量机</h3><p>​    支持向量：在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。</p>
<p>​    支持向量机（Support Vector Machine，SVM）：其含义是通过支持向量运算的分类器。</p>
<p>​    在一个二维环境中，其中点R，S，G点和其它靠近中间黑线的点可以看作为支持向量，它们可以决定分类器，即黑线的具体参数。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2-6.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2-6.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    支持向量机是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是边界最大化，最终转化为一个凸二次规划问题来求解。由简至繁的模型包括：</p>
<p>​    当训练样本线性可分时，通过硬边界（hard margin）最大化，学习一个线性可分支持向量机；</p>
<p>​    当训练样本近似线性可分时，通过软边界（soft margin）最大化，学习一个线性支持向量机；</p>
<p>​    当训练样本线性不可分时，通过核技巧和软边界最大化，学习一个非线性支持向量机；</p>
<h3 id="支持向量机能解决哪些问题"><a href="#支持向量机能解决哪些问题" class="headerlink" title="支持向量机能解决哪些问题"></a>支持向量机能解决哪些问题</h3><p><strong>线性分类</strong></p>
<p>​    在训练数据中，每个数据都有n个的属性和一个二分类类别标志，我们可以认为这些数据在一个n维空间里。我们的目标是找到一个n-1维的超平面，这个超平面可以将数据分成两部分，每部分数据都属于同一个类别。</p>
<p>​    这样的超平面有很多，假如我们要找到一个最佳的超平面。此时，增加一个约束条件：要求这个超平面到每边最近数据点的距离是最大的，成为最大边距超平面。这个分类器即为最大边距分类器。</p>
<p><strong>非线性分类</strong></p>
<p>​    SVM的一个优势是支持非线性分类。它结合使用拉格朗日乘子法（Lagrange Multiplier）和KKT（Karush Kuhn Tucker）条件，以及核函数可以生成非线性分类器。</p>
<h3 id="核函数特点及其作用"><a href="#核函数特点及其作用" class="headerlink" title="核函数特点及其作用"></a>核函数特点及其作用</h3><p>​    引入核函数目的：把原坐标系里线性不可分的数据用核函数Kernel投影到另一个空间，尽量使得数据在新的空间里线性可分。<br>​    核函数方法的广泛应用，与其特点是分不开的：  </p>
<p>1）核函数的引入避免了“维数灾难”，大大减小了计算量。而输入空间的维数n对核函数矩阵无影响。因此，核函数方法可以有效处理高维输入。</p>
<p>2）无需知道非线性变换函数Φ的形式和参数。</p>
<p>3）核函数的形式和参数的变化会隐式地改变从输入空间到特征空间的映射，进而对特征空间的性质产生影响，最终改变各种核函数方法的性能。</p>
<p>4）核函数方法可以和不同的算法相结合，形成多种不同的基于核函数技术的方法，且这两部分的设计可以单独进行，并可以为不同的应用选择不同的核函数和算法。</p>
<h3 id="SVM为什么引入对偶问题"><a href="#SVM为什么引入对偶问题" class="headerlink" title="SVM为什么引入对偶问题"></a>SVM为什么引入对偶问题</h3><p>1，对偶问题将原始问题中的约束转为了对偶问题中的等式约束，对偶问题往往更加容易求解。</p>
<p>2，可以很自然的引用核函数（拉格朗日表达式里面有内积，而核函数也是通过内积进行映射的）。</p>
<p>3，在优化理论中，目标函数 f(x) 会有多种形式：如果目标函数和约束条件都为变量 x 的线性函数，称该问题为线性规划；如果目标函数为二次函数，约束条件为线性函数，称该最优化问题为二次规划；如果目标函数或者约束条件均为非线性函数，称该最优化问题为非线性规划。每个线性规划问题都有一个与之对应的对偶问题，对偶问题有非常良好的性质，以下列举几个：</p>
<p>​    a, 对偶问题的对偶是原问题；</p>
<p>​    b, 无论原始问题是否是凸的，对偶问题都是凸优化问题；</p>
<p>​    c, 对偶问题可以给出原始问题一个下界；</p>
<p>​    d, 当满足一定条件时，原始问题与对偶问题的解是完全等价的。</p>
<h3 id="如何理解SVM中的对偶问题"><a href="#如何理解SVM中的对偶问题" class="headerlink" title="如何理解SVM中的对偶问题"></a>如何理解SVM中的对偶问题</h3><p>在硬边界支持向量机中，问题的求解可以转化为凸二次规划问题。</p>
<p>​    假设优化目标为</p>
<script type="math/tex; mode=display">
\begin{align}
&\min_{\boldsymbol w, b}\frac{1}{2}||\boldsymbol w||^2\\
&s.t. y_i(\boldsymbol w^T\boldsymbol x_i+b)\geqslant 1, i=1,2,\cdots,m.\\
\end{align}  \tag{1}</script><p><strong>step 1</strong>. 转化问题：</p>
<script type="math/tex; mode=display">
\min_{\boldsymbol w, b} \max_{\alpha_i \geqslant 0}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}  \tag{2}</script><p>上式等价于原问题，因为若满足(1)中不等式约束，则(2)式求max时,$\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))$必须取0，与(1)等价；若不满足(1)中不等式约束，(2)中求max会得到无穷大。 交换min和max获得其对偶问题:</p>
<script type="math/tex; mode=display">
\max_{\alpha_i \geqslant 0} \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}</script><p>交换之后的对偶问题和原问题并不相等，上式的解小于等于原问题的解。</p>
<p><strong>step 2</strong>.现在的问题是如何找到问题(1) 的最优值的一个最好的下界? </p>
<script type="math/tex; mode=display">
\frac{1}{2}||\boldsymbol w||^2 < v\\
1 - y_i(\boldsymbol w^T\boldsymbol x_i+b) \leqslant 0\tag{3}</script><p>若方程组(3)无解， 则v是问题(1)的一个下界。若(3)有解， 则 </p>
<script type="math/tex; mode=display">
\forall \boldsymbol \alpha >  0 , \ \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\} < v</script><p>由逆否命题得：若 </p>
<script type="math/tex; mode=display">
\exists \boldsymbol \alpha >  0 , \ \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\} \geqslant v</script><p>则(3)无解。</p>
<p>那么v是问题</p>
<p>(1)的一个下界。<br> 要求得一个好的下界，取最大值即可 </p>
<script type="math/tex; mode=display">
\max_{\alpha_i \geqslant 0}  \min_{\boldsymbol w, b} \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}</script><p><strong>step 3</strong>. 令</p>
<script type="math/tex; mode=display">
L(\boldsymbol w, b,\boldsymbol a) =   \frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))</script><p>$p^<em>$为原问题的最小值，对应的$w,b$分别为$w^</em>,b^*$,则对于任意的$a&gt;0$:</p>
<script type="math/tex; mode=display">
p^* = \frac{1}{2}||\boldsymbol w^*||^2 \geqslant  L(\boldsymbol w^*, b,\boldsymbol a) \geqslant \min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)</script><p>则 $\min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)$是问题（1）的一个下界。</p>
<p>此时，取最大值即可求得好的下界，即</p>
<script type="math/tex; mode=display">
\max_{\alpha_i \geqslant 0} \min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)</script><h3 id="常见的核函数有哪些"><a href="#常见的核函数有哪些" class="headerlink" title="常见的核函数有哪些"></a>常见的核函数有哪些</h3><div class="table-container">
<table>
<thead>
<tr>
<th>核函数</th>
<th>表达式</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear Kernel线性核</td>
<td>$k(x,y)=x^{t}y+c$</td>
<td></td>
</tr>
<tr>
<td>Polynomial Kernel多项式核</td>
<td>$k(x,y)=(ax^{t}y+c)^{d}$</td>
<td>$d\geqslant1$为多项式的次数</td>
</tr>
<tr>
<td>Exponential Kernel指数核</td>
<td>$k(x,y)=exp(-\frac{\left \</td>
<td>x-y \right \</td>
<td>}{2\sigma ^{2}})$</td>
<td>$\sigma&gt;0$</td>
</tr>
<tr>
<td>Gaussian Kernel高斯核</td>
<td>$k(x,y)=exp(-\frac{\left \</td>
<td>x-y \right \</td>
<td>^{2}}{2\sigma ^{2}})$</td>
<td>$\sigma$为高斯核的带宽，$\sigma&gt;0$,</td>
</tr>
<tr>
<td>Laplacian Kernel拉普拉斯核</td>
<td>$k(x,y)=exp(-\frac{\left \</td>
<td>x-y \right \</td>
<td>}{\sigma})$</td>
<td>$\sigma&gt;0$</td>
</tr>
<tr>
<td>ANOVA Kernel</td>
<td>$k(x,y)=exp(-\sigma(x^{k}-y^{k})^{2})^{d}$</td>
<td></td>
</tr>
<tr>
<td>Sigmoid Kernel</td>
<td>$k(x,y)=tanh(ax^{t}y+c)$</td>
<td>$tanh$为双曲正切函数，$a&gt;0,c&lt;0$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="SVM主要特点"><a href="#SVM主要特点" class="headerlink" title="SVM主要特点"></a>SVM主要特点</h3><p>特点：</p>
<p>(1)  SVM方法的理论基础是非线性映射，SVM利用内积核函数代替向高维空间的非线性映射。<br>(2)  SVM的目标是对特征空间划分得到最优超平面，SVM方法核心是最大化分类边界。<br>(3)  支持向量是SVM的训练结果，在SVM分类决策中起决定作用的是支持向量。<br>(4)  SVM是一种有坚实理论基础的新颖的适用小样本学习方法。它基本上不涉及概率测度及大数定律等，也简化了通常的分类和回归等问题。<br>(5)  SVM的最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。<br>(6)  少数支持向量决定了最终结果，这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本,而且注定了该方法不但算法简单，而且具有较好的“鲁棒性”。这种鲁棒性主要体现在：<br>​        ①增、删非支持向量样本对模型没有影响;<br>​        ②支持向量样本集具有一定的鲁棒性;<br>​        ③有些成功的应用中，SVM方法对核的选取不敏感<br>(7)  SVM学习问题可以表示为凸优化问题，因此可以利用已知的有效算法发现目标函数的全局最小值。而其他分类方法（如基于规则的分类器和人工神经网络）都采用一种基于贪心学习的策略来搜索假设空间，这种方法一般只能获得局部最优解。<br>(8)  SVM通过最大化决策边界的边缘来控制模型的能力。尽管如此，用户必须提供其他参数，如使用核函数类型和引入松弛变量等。<br>(9)  SVM在小样本训练集上能够得到比其它算法好很多的结果。SVM优化目标是结构化风险最小，而不是经验风险最小，避免了过拟合问题，通过margin的概念，得到对数据分布的结构化描述，减低了对数据规模和数据分布的要求，有优秀的泛化能力。<br>(10)  它是一个凸优化问题，因此局部最优解一定是全局最优解的优点。  </p>
<h3 id="SVM主要缺点"><a href="#SVM主要缺点" class="headerlink" title="SVM主要缺点"></a>SVM主要缺点</h3><p>(1) SVM算法对大规模训练样本难以实施<br>​        SVM的空间消耗主要是存储训练样本和核矩阵，由于SVM是借助二次规划来求解支持向量，而求解二次规划将涉及m阶矩阵的计算（m为样本的个数），当m数目很大时该矩阵的存储和计算将耗费大量的机器内存和运算时间。<br>​        如果数据量很大，SVM的训练时间就会比较长，如垃圾邮件的分类检测，没有使用SVM分类器，而是使用简单的朴素贝叶斯分类器，或者是使用逻辑回归模型分类。</p>
<p>(2) 用SVM解决多分类问题存在困难</p>
<p>​        经典的支持向量机算法只给出了二类分类的算法，而在实际应用中，一般要解决多类的分类问题。可以通过多个二类支持向量机的组合来解决。主要有一对多组合模式、一对一组合模式和SVM决策树；再就是通过构造多个分类器的组合来解决。主要原理是克服SVM固有的缺点，结合其他算法的优势，解决多类问题的分类精度。如：与粗糙集理论结合，形成一种优势互补的多类问题的组合分类器。</p>
<p>(3) 对缺失数据敏感，对参数和核函数的选择敏感</p>
<p>​        支持向量机性能的优劣主要取决于核函数的选取，所以对于一个实际问题而言，如何根据实际的数据模型选择合适的核函数从而构造SVM算法。目前比较成熟的核函数及其参数的选择都是人为的，根据经验来选取的，带有一定的随意性。在不同的问题领域，核函数应当具有不同的形式和参数，所以在选取时候应该将领域知识引入进来，但是目前还没有好的方法来解决核函数的选取问题。</p>
<h3 id="逻辑回归与SVM的异同"><a href="#逻辑回归与SVM的异同" class="headerlink" title="逻辑回归与SVM的异同"></a>逻辑回归与SVM的异同</h3><p>相同点：</p>
<ul>
<li>LR和SVM都是<strong>分类</strong>算法。</li>
<li>LR和SVM都是<strong>监督学习</strong>算法。</li>
<li>LR和SVM都是<strong>判别模型</strong>。</li>
<li>如果不考虑核函数，LR和SVM都是<strong>线性分类</strong>算法，也就是说他们的分类决策面都是线性的。<br> 说明：LR也是可以用核函数的.但LR通常不采用核函数的方法。（<strong>计算量太大</strong>）</li>
</ul>
<p>不同点：</p>
<p><strong>1、LR采用log损失，SVM采用合页(hinge)损失。</strong><br>逻辑回归的损失函数：</p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}\sum^m_{i=1}\left[y^{i}logh_{\theta}(x^{i})+ (1-y^{i})log(1-h_{\theta}(x^{i}))\right]</script><p>支持向量机的目标函数:</p>
<script type="math/tex; mode=display">
L(w,n,a)=\frac{1}{2}||w||^2-\sum^n_{i=1}\alpha_i \left( y_i(w^Tx_i+b)-1\right)</script><p>​    逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过<strong>极大似然估计</strong>的方法估计出参数的值。<br>​    支持向量机基于几何<strong>边界最大化</strong>原理，认为存在最大几何边界的分类面为最优分类面。</p>
<p>2、<strong>LR对异常值敏感，SVM对异常值不敏感</strong>。</p>
<p>​    支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局。LR模型找到的那个超平面，是尽量让所有点都远离他，而SVM寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。<br>​    支持向量机改变非支持向量样本并不会引起决策面的变化。<br>​    逻辑回归中改变任何样本都会引起决策面的变化。  </p>
<p>3、<strong>计算复杂度不同。对于海量数据，SVM的效率较低，LR效率比较高</strong></p>
<p>​    当样本较少，特征维数较低时，SVM和LR的运行时间均比较短，SVM较短一些。准确率的话，LR明显比SVM要高。当样本稍微增加些时，SVM运行时间开始增长，但是准确率赶超了LR。SVM时间虽长，但在可接受范围内。当数据量增长到20000时，特征维数增长到200时，SVM的运行时间剧烈增加，远远超过了LR的运行时间。但是准确率却和LR相差无几。(这其中主要原因是大量非支持向量参与计算，造成SVM的二次规划问题)</p>
<p>4、<strong>对非线性问题的处理方式不同</strong></p>
<p>​    LR主要靠特征构造，必须组合交叉特征，特征离散化。SVM也可以这样，还可以通过核函数kernel（因为只有支持向量参与核计算，计算复杂度不高）。由于可以利用核函数，SVM则可以通过对偶求解高效处理。LR则在特征空间维度很高时，表现较差。</p>
<p>5、<strong>SVM的损失函数就自带正则</strong>。<br>​    损失函数中的1/2||w||^2项，这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！**</p>
<p>6、SVM自带<strong>结构风险最小化</strong>，LR则是<strong>经验风险最小化</strong>。</p>
<p>7、SVM会用核函数而LR一般不用核函数。</p>
<h2 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h2><h3 id="图解极大似然估计"><a href="#图解极大似然估计" class="headerlink" title="图解极大似然估计"></a>图解极大似然估计</h3><p>极大似然估计的原理，用一张图片来说明，如下图所示：</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.19.1.1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.19.1.1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    例：有两个外形完全相同的箱子，1号箱有99只白球，1只黑球；2号箱有1只白球，99只黑球。在一次实验中，取出的是黑球，请问是从哪个箱子中取出的？</p>
<p>​    一般的根据经验想法，会猜测这只黑球最像是从2号箱取出，此时描述的“最像”就有“最大似然”的意思，这种想法常称为“最大似然原理”。</p>
<h3 id="极大似然估计原理"><a href="#极大似然估计原理" class="headerlink" title="极大似然估计原理"></a>极大似然估计原理</h3><p>​    总结起来，最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。</p>
<p>​    极大似然估计是建立在极大似然原理的基础上的一个统计方法。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</p>
<p>​    由于样本集中的样本都是独立同分布，可以只考虑一类样本集$D$，来估计参数向量$\vec\theta$。记已知的样本集为：</p>
<script type="math/tex; mode=display">
D=\vec x_{1},\vec x_{2},...,\vec x_{n}</script><p>似然函数（likelihood function）：联合概率密度函数$p(D|\vec\theta )$称为相对于$\vec x<em>{1},\vec x</em>{2},…,\vec x_{n}$的$\vec\theta$的似然函数。</p>
<script type="math/tex; mode=display">
l(\vec\theta )=p(D|\vec\theta ) =p(\vec x_{1},\vec x_{2},...,\vec x_{n}|\vec\theta )=\prod_{i=1}^{n}p(\vec x_{i}|\vec \theta )</script><p>如果$\hat{\vec\theta}$是参数空间中能使似然函数$l(\vec\theta)$最大的$\vec\theta$值，则$\hat{\vec\theta}$应该是“最可能”的参数值，那么$\hat{\vec\theta}$就是$\theta$的极大似然估计量。它是样本集的函数，记作：</p>
<script type="math/tex; mode=display">
\hat{\vec\theta}=d(D)= \mathop {\arg \max}_{\vec\theta} l(\vec\theta )</script><p>$\hat{\vec\theta}(\vec x<em>{1},\vec x</em>{2},…,\vec x_{n})$称为极大似然函数估计值。</p>
<h3 id="贝叶斯分类器基本原理"><a href="#贝叶斯分类器基本原理" class="headerlink" title="贝叶斯分类器基本原理"></a>贝叶斯分类器基本原理</h3><p>​    贝叶斯决策论通过<strong>相关概率已知</strong>的情况下利用<strong>误判损失</strong>来选择最优的类别分类。<br>假设有$N$种可能的分类标记，记为$Y={c_1,c_2,…,c_N}$，那对于样本$\boldsymbol{x}$，它属于哪一类呢？</p>
<p>计算步骤如下：</p>
<p>step 1. 算出样本$\boldsymbol{x}$属于第i个类的概率，即$P(c_i|x)$；</p>
<p>step 2. 通过比较所有的$P(c_i|\boldsymbol{x})$，得到样本$\boldsymbol{x}$所属的最佳类别。</p>
<p>step 3. 将类别$c_i$和样本$\boldsymbol{x}$代入到贝叶斯公式中，得到：</p>
<script type="math/tex; mode=display">
P(c_i|\boldsymbol{x})=\frac{P(\boldsymbol{x}|c_i)P(c_i)}{P(\boldsymbol{x})}.</script><p>​    一般来说，$P(c_i)$为先验概率，$P(\boldsymbol{x}|c_i)$为条件概率，$P(\boldsymbol{x})$是用于归一化的证据因子。对于$P(c_i)$可以通过训练样本中类别为$c_i$的样本所占的比例进行估计；此外，由于只需要找出最大的$P(\boldsymbol{x}|c_i)$，因此我们并不需要计算$P(\boldsymbol{x})$。<br>​    为了求解条件概率，基于不同假设提出了不同的方法，以下将介绍朴素贝叶斯分类器和半朴素贝叶斯分类器。</p>
<h3 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h3><p>​    假设样本$\boldsymbol{x}$包含$d$个属性，即$\boldsymbol{x}={ x_1,x_2,…,x_d}$。于是有：</p>
<script type="math/tex; mode=display">
P(\boldsymbol{x}|c_i)=P(x_1,x_2,\cdots,x_d|c_i)</script><p>这个联合概率难以从有限的训练样本中直接估计得到。于是，朴素贝叶斯（Naive Bayesian，简称NB）采用了“属性条件独立性假设”：对已知类别，假设所有属性相互独立。于是有：</p>
<script type="math/tex; mode=display">
P(x_1,x_2,\cdots,x_d|c_i)=\prod_{j=1}^d P(x_j|c_i)</script><p>这样的话，我们就可以很容易地推出相应的判定准则了：</p>
<script type="math/tex; mode=display">
h_{nb}(\boldsymbol{x})=\mathop{\arg \max}_{c_i\in Y} P(c_i)\prod_{j=1}^dP(x_j|c_i)</script><p><strong>条件概率$P(x_j|c_i)$的求解</strong></p>
<p>如果$x_j$是标签属性，那么我们可以通过计数的方法估计$P(x_j|c_i)$</p>
<script type="math/tex; mode=display">
P(x_j|c_i)=\frac{P(x_j,c_i)}{P(c_i)}\approx\frac{\#(x_j,c_i)}{\#(c_i)}</script><p>其中，$#(x<em>j,c_i)$表示在训练样本中$x_j$与$c</em>{i}$共同出现的次数。</p>
<p>如果$x<em>j$是数值属性，通常我们假设类别中$c</em>{i}$的所有样本第$j$个属性的值服从正态分布。我们首先估计这个分布的均值$μ$和方差$σ$，然后计算$x_j$在这个分布中的概率密度$P(x_j|c_i)$。</p>
<h3 id="举例理解朴素贝叶斯分类器"><a href="#举例理解朴素贝叶斯分类器" class="headerlink" title="举例理解朴素贝叶斯分类器"></a>举例理解朴素贝叶斯分类器</h3><p>使用经典的西瓜训练集如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">编号</th>
<th style="text-align:center">色泽</th>
<th style="text-align:center">根蒂</th>
<th style="text-align:center">敲声</th>
<th style="text-align:center">纹理</th>
<th style="text-align:center">脐部</th>
<th style="text-align:center">触感</th>
<th style="text-align:center">密度</th>
<th style="text-align:center">含糖率</th>
<th style="text-align:center">好瓜</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.697</td>
<td style="text-align:center">0.460</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.774</td>
<td style="text-align:center">0.376</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.634</td>
<td style="text-align:center">0.264</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.608</td>
<td style="text-align:center">0.318</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.556</td>
<td style="text-align:center">0.215</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.403</td>
<td style="text-align:center">0.237</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.481</td>
<td style="text-align:center">0.149</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.437</td>
<td style="text-align:center">0.211</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.666</td>
<td style="text-align:center">0.091</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">硬挺</td>
<td style="text-align:center">清脆</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.243</td>
<td style="text-align:center">0.267</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">硬挺</td>
<td style="text-align:center">清脆</td>
<td style="text-align:center">模糊</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.245</td>
<td style="text-align:center">0.057</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">模糊</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.343</td>
<td style="text-align:center">0.099</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">13</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.639</td>
<td style="text-align:center">0.161</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.657</td>
<td style="text-align:center">0.198</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">15</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.360</td>
<td style="text-align:center">0.370</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">模糊</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.593</td>
<td style="text-align:center">0.042</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">17</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.719</td>
<td style="text-align:center">0.103</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
</div>
<p>对下面的测试例“测1”进行 分类：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">编号</th>
<th style="text-align:center">色泽</th>
<th style="text-align:center">根蒂</th>
<th style="text-align:center">敲声</th>
<th style="text-align:center">纹理</th>
<th style="text-align:center">脐部</th>
<th style="text-align:center">触感</th>
<th style="text-align:center">密度</th>
<th style="text-align:center">含糖率</th>
<th style="text-align:center">好瓜</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">测1</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.697</td>
<td style="text-align:center">0.460</td>
<td style="text-align:center">？</td>
</tr>
</tbody>
</table>
</div>
<p>首先，估计类先验概率$P(c_j)$，有</p>
<script type="math/tex; mode=display">
\begin{align} 
&P(好瓜=是)=\frac{8}{17}=0.471 \newline 
&P(好瓜=否)=\frac{9}{17}=0.529 
\end{align}</script><p>然后，为每个属性估计条件概率（这里，对于连续属性，假定它们服从正态分布）</p>
<script type="math/tex; mode=display">
P_{青绿|是}=P（色泽=青绿|好瓜=是）=\frac{3}{8}=0.375</script><script type="math/tex; mode=display">
P_{青绿|否}=P（色泽=青绿|好瓜=否）=\frac{3}{9}\approx0.333</script><script type="math/tex; mode=display">
P_{蜷缩|是}=P（根蒂=蜷缩|好瓜=是）=\frac{5}{8}=0.625</script><script type="math/tex; mode=display">
P_{蜷缩|否}=P（根蒂=蜷缩|好瓜=否）=\frac{3}{9}=0.333</script><script type="math/tex; mode=display">
P_{浊响|是}=P（敲声=浊响|好瓜=是）=\frac{6}{8}=0.750</script><script type="math/tex; mode=display">
P_{浊响|否}=P（敲声=浊响|好瓜=否）=\frac{4}{9}\approx 0.444</script><script type="math/tex; mode=display">
P_{清晰|是}=P（纹理=清晰|好瓜=是）=\frac{7}{8}= 0.875</script><script type="math/tex; mode=display">
P_{清晰|否}=P（纹理=清晰|好瓜=否）=\frac{2}{9}\approx 0.222</script><script type="math/tex; mode=display">
P_{凹陷|是}=P（脐部=凹陷|好瓜=是）=\frac{6}{8}= 0.750</script><script type="math/tex; mode=display">
P_{凹陷|否}=P（脐部=凹陷|好瓜=否）=\frac{2}{9} \approx 0.222</script><script type="math/tex; mode=display">
P_{硬滑|是}=P（触感=硬滑|好瓜=是）=\frac{6}{8}= 0.750</script><script type="math/tex; mode=display">
P_{硬滑|否}=P（触感=硬滑|好瓜=否）=\frac{6}{9} \approx 0.667</script><script type="math/tex; mode=display">
\begin{aligned}
\rho_{密度：0.697|是}&=\rho（密度=0.697|好瓜=是）\\&=\frac{1}{\sqrt{2 \pi}\times0.129}exp\left( -\frac{(0.697-0.574)^2}{2\times0.129^2}\right) \approx 1.959
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\rho_{密度：0.697|否}&=\rho（密度=0.697|好瓜=否）\\&=\frac{1}{\sqrt{2 \pi}\times0.195}exp\left( -\frac{(0.697-0.496)^2}{2\times0.195^2}\right) \approx 1.203
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\rho_{含糖：0.460|是}&=\rho（密度=0.460|好瓜=是）\\&=\frac{1}{\sqrt{2 \pi}\times0.101}exp\left( -\frac{(0.460-0.279)^2}{2\times0.101^2}\right) \approx 0.788
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\rho_{含糖：0.460|否}&=\rho（密度=0.460|好瓜=是）\\&=\frac{1}{\sqrt{2 \pi}\times0.108}exp\left( -\frac{(0.460-0.154)^2}{2\times0.108^2}\right) \approx 0.066
\end{aligned}</script><p>于是有</p>
<script type="math/tex; mode=display">
\begin{align} 
P(&好瓜=是)\times P_{青绿|是} \times P_{蜷缩|是} \times P_{浊响|是} \times P_{清晰|是} \times P_{凹陷|是}\newline 
&\times P_{硬滑|是} \times p_{密度：0.697|是} \times p_{含糖：0.460|是} \approx 0.063 \newline\newline 
P(&好瓜=否)\times P_{青绿|否} \times P_{蜷缩|否} \times P_{浊响|否} \times P_{清晰|否} \times P_{凹陷|否}\newline 
&\times P_{硬滑|否} \times p_{密度：0.697|否} \times p_{含糖：0.460|否} \approx 6.80\times 10^{-5} 
\end{align}</script><p>由于$0.063&gt;6.80\times 10^{-5}$，因此，朴素贝叶斯分类器将测试样本“测1”判别为“好瓜”。</p>
<h3 id="半朴素贝叶斯分类器"><a href="#半朴素贝叶斯分类器" class="headerlink" title="半朴素贝叶斯分类器"></a>半朴素贝叶斯分类器</h3><p>​    朴素贝叶斯采用了“属性条件独立性假设”，半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信息。<strong>独依赖估计</strong>（One-Dependence Estimator，简称ODE）是半朴素贝叶斯分类器最常用的一种策略。顾名思义，独依赖是假设每个属性在类别之外最多依赖一个其他属性，即：</p>
<script type="math/tex; mode=display">
P(\boldsymbol{x}|c_i)=\prod_{j=1}^d P(x_j|c_i,{\rm pa}_j)</script><p>其中$pa_j$为属性$x_i$所依赖的属性，成为$x_i$的父属性。假设父属性$pa_j$已知，那么可以使用下面的公式估计$P(x_j|c_i,{\rm pa}_j)$</p>
<script type="math/tex; mode=display">
P(x_j|c_i,{\rm pa}_j)=\frac{P(x_j,c_i,{\rm pa}_j)}{P(c_i,{\rm pa}_j)}</script><h2 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h2><h3 id="EM算法基本思想"><a href="#EM算法基本思想" class="headerlink" title="EM算法基本思想"></a>EM算法基本思想</h3><p>​    最大期望算法（Expectation-Maximization algorithm, EM），是一类通过迭代进行极大似然估计的优化算法，通常作为牛顿迭代法的替代，用于对包含隐变量或缺失数据的概率模型进行参数估计。</p>
<p>​    最大期望算法基本思想是经过两个步骤交替进行计算：</p>
<p>​    第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值<strong>；</strong></p>
<p>​    第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。</p>
<p>​    M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。</p>
<h3 id="EM算法推导"><a href="#EM算法推导" class="headerlink" title="EM算法推导"></a>EM算法推导</h3><p>​    对于$m$个样本观察数据$x=(x^{1},x^{2},…,x^{m})$，现在想找出样本的模型参数$\theta$，其极大化模型分布的对数似然函数为：</p>
<script type="math/tex; mode=display">
\theta = \mathop{\arg\max}_\theta\sum\limits_{i=1}^m logP(x^{(i)};\theta)</script><p>如果得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},…z^{(m)})$，极大化模型分布的对数似然函数则为：</p>
<script type="math/tex; mode=display">
\theta =\mathop{\arg\max}_\theta\sum\limits_{i=1}^m logP(x^{(i)};\theta) = \mathop{\arg\max}_\theta\sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}, z^{(i)};\theta)  \tag{a}</script><p>由于上式不能直接求出$\theta$，采用缩放技巧：</p>
<script type="math/tex; mode=display">
\begin{align} \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}, z^{(i)};\theta)   & = \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} \\ & \geqslant  \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} \end{align}   \tag{1}</script><p>上式用到了Jensen不等式：</p>
<script type="math/tex; mode=display">
log\sum\limits_j\lambda_jy_j \geqslant \sum\limits_j\lambda_jlogy_j\;\;,  \lambda_j \geqslant 0, \sum\limits_j\lambda_j =1</script><p>并且引入了一个未知的新分布$Q_i(z^{(i)})$。</p>
<p>此时，如果需要满足Jensen不等式中的等号，所以有：</p>
<script type="math/tex; mode=display">
\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} =c, c为常数</script><p>由于$Q_i(z^{(i)})$是一个分布，所以满足</p>
<script type="math/tex; mode=display">
\sum\limits_{z}Q_i(z^{(i)}) =1</script><p>综上，可得：</p>
<script type="math/tex; mode=display">
Q_i(z^{(i)})  = \frac{P(x^{(i)}， z^{(i)};\theta)}{\sum\limits_{z}P(x^{(i)}, z^{(i)};\theta)} =  \frac{P(x^{(i)}, z^{(i)};\theta)}{P(x^{(i)};\theta)} = P( z^{(i)}|x^{(i)};\theta)</script><p>如果$Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)};\theta)$ ，则第(1)式是我们的包含隐藏数据的对数似然的一个下界。如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。即我们需要最大化下式：</p>
<script type="math/tex; mode=display">
\mathop{\arg\max}_\theta \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}， z^{(i)};\theta)}{Q_i(z^{(i)})}</script><p>简化得：</p>
<script type="math/tex; mode=display">
\mathop{\arg\max}_\theta \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}, z^{(i)};\theta)}</script><p>以上即为EM算法的M步，$\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}, z^{(i)};\theta)}$可理解为$logP(x^{(i)}, z^{(i)};\theta) $基于条件概率分布$Q_i(z^{(i)}) $的期望。以上即为EM算法中E步和M步的具体数学含义。</p>
<h3 id="图解EM算法"><a href="#图解EM算法" class="headerlink" title="图解EM算法"></a>图解EM算法</h3><p>​    考虑上一节中的（a）式，表达式中存在隐变量，直接找到参数估计比较困难，通过EM算法迭代求解下界的最大值到收敛为止。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.20.1.jpg" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.20.1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>​    图片中的紫色部分是我们的目标模型$p(x|\theta)$，该模型复杂，难以求解析解，为了消除隐变量$z^{(i)}$的影响，我们可以选择一个不包含$z^{(i)}$的模型$r(x|\theta)$，使其满足条件$r(x|\theta) \leqslant p(x|\theta) $。</p>
<p>求解步骤如下：</p>
<p>（1）选取$\theta_1$，使得$r(x|\theta_1) = p(x|\theta_1)$，然后对此时的$r$求取最大值，得到极值点$\theta_2$，实现参数的更新。</p>
<p>（2）重复以上过程到收敛为止，在更新过程中始终满足$r \leqslant p $.</p>
<h3 id="EM算法流程"><a href="#EM算法流程" class="headerlink" title="EM算法流程"></a>EM算法流程</h3><p>输入：观察数据$x=(x^{(1)},x^{(2)},…x^{(m)})$，联合分布$p(x,z ;\theta)$，条件分布$p(z|x; \theta)$，最大迭代次数$J$</p>
<p>1）随机初始化模型参数$\theta$的初值$\theta^0$。</p>
<p>2）$for \ j  \ from \ 1  \ to  \ j$：</p>
<p>​    a） E步。计算联合分布的条件概率期望：</p>
<script type="math/tex; mode=display">
Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)}, \theta^{j})</script><script type="math/tex; mode=display">
L(\theta, \theta^{j}) = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}, \theta^{j})log{P(x^{(i)}, z^{(i)};\theta)}</script><p>​    b） M步。极大化$L(\theta, \theta^{j})$，得到$\theta^{j+1}$:</p>
<script type="math/tex; mode=display">
\theta^{j+1} = \mathop{\arg\max}_\theta L(\theta, \theta^{j})</script><p>​    c） 如果$\theta^{j+1}$收敛，则算法结束。否则继续回到步骤a）进行E步迭代。</p>
<p>输出：模型参数$\theta$。</p>
<h2 id="降维和聚类"><a href="#降维和聚类" class="headerlink" title="降维和聚类"></a>降维和聚类</h2><h3 id="图解为什么会产生维数灾难"><a href="#图解为什么会产生维数灾难" class="headerlink" title="图解为什么会产生维数灾难"></a>图解为什么会产生维数灾难</h3><p>​    假如数据集包含10张照片，照片中包含三角形和圆两种形状。现在来设计一个分类器进行训练，让这个分类器对其他的照片进行正确分类（假设三角形和圆的总数是无限大），简单的，我们用一个特征进行分类：</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                            图2.21.1.a</p>
<p>​    从上图可看到，如果仅仅只有一个特征进行分类，三角形和圆几乎是均匀分布在这条线段上，很难将10张照片线性分类。那么，增加一个特征后的情况会怎么样：</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.2.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                            图2.21.1.b</p>
<p>增加一个特征后，我们发现仍然无法找到一条直线将猫和狗分开。所以，考虑需要再增加一个特征：</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.3.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.3.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                            图2.21.1.c</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.4.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.4.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                            图2.21.1.d</p>
<p>​    此时，可以找到一个平面将三角形和圆分开。</p>
<p>​    现在计算一下不同特征数是样本的密度：</p>
<p>​    （1）一个特征时，假设特征空间时长度为5的线段，则样本密度为$10 \div 5 = 2$。</p>
<p>​    （2）两个特征时，特征空间大小为$ 5\times5 = 25$，样本密度为$10 \div 25 = 0.4$。</p>
<p>​    （3）三个特征时，特征空间大小是$ 5\times5\times5 = 125$，样本密度为$10 \div 125 = 0.08$。</p>
<p>​    以此类推，如果继续增加特征数量，样本密度会越来越稀疏，此时，更容易找到一个超平面将训练样本分开。当特征数量增长至无限大时，样本密度就变得非常稀疏。</p>
<p>​    下面看一下将高维空间的分类结果映射到低维空间时，会出现什么情况？</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.5.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.5.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                        图2.21.1.e</p>
<p>​    上图是将三维特征空间映射到二维特征空间后的结果。尽管在高维特征空间时训练样本线性可分，但是映射到低维空间后，结果正好相反。事实上，增加特征数量使得高维空间线性可分，相当于在低维空间内训练一个复杂的非线性分类器。不过，这个非线性分类器太过“聪明”，仅仅学到了一些特例。如果将其用来辨别那些未曾出现在训练样本中的测试样本时，通常结果不太理想，会造成过拟合问题。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.6a.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.6a.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                        图2.21.1.f</p>
<p>​    上图所示的只采用2个特征的线性分类器分错了一些训练样本，准确率似乎没有图2.21.1.e的高，但是，采用2个特征的线性分类器的泛化能力比采用3个特征的线性分类器要强。因为，采用2个特征的线性分类器学习到的不只是特例，而是一个整体趋势，对于那些未曾出现过的样本也可以比较好地辨别开来。换句话说，通过减少特征数量，可以避免出现过拟合问题，从而避免“维数灾难”。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.6.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.6.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    上图从另一个角度诠释了“维数灾难”。假设只有一个特征时，特征的值域是0到1，每一个三角形和圆的特征值都是唯一的。如果我们希望训练样本覆盖特征值值域的20%，那么就需要三角形和圆总数的20%。我们增加一个特征后，为了继续覆盖特征值值域的20%就需要三角形和圆总数的45%($0.452^2\approx0.2$)。继续增加一个特征后，需要三角形和圆总数的58%($0.583^3\approx0.2$)。随着特征数量的增加，为了覆盖特征值值域的20%，就需要更多的训练样本。如果没有足够的训练样本，就可能会出现过拟合问题。</p>
<p>​    通过上述例子，我们可以看到特征数量越多，训练样本就会越稀疏，分类器的参数估计就会越不准确，更加容易出现过拟合问题。“维数灾难”的另一个影响是训练样本的稀疏性并不是均匀分布的。处于中心位置的训练样本比四周的训练样本更加稀疏。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.7.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.1.7.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    假设有一个二维特征空间，如上图所示的矩形，在矩形内部有一个内切的圆形。由于越接近圆心的样本越稀疏，因此，相比于圆形内的样本，那些位于矩形四角的样本更加难以分类。当维数变大时，特征超空间的容量不变，但单位圆的容量会趋于0，在高维空间中，大多数训练数据驻留在特征超空间的角落。散落在角落的数据要比处于中心的数据难于分类。</p>
<h3 id="怎样避免维数灾难"><a href="#怎样避免维数灾难" class="headerlink" title="怎样避免维数灾难"></a>怎样避免维数灾难</h3><p><strong>有待完善！！！</strong></p>
<p>解决维度灾难问题：</p>
<p>主成分分析法PCA，线性判别法LDA</p>
<p>奇异值分解简化数据、拉普拉斯特征映射</p>
<p>Lassio缩减系数法、小波分析法、</p>
<h3 id="聚类和降维有什么区别与联系"><a href="#聚类和降维有什么区别与联系" class="headerlink" title="聚类和降维有什么区别与联系"></a>聚类和降维有什么区别与联系</h3><p>​    聚类用于找寻数据内在的分布结构，既可以作为一个单独的过程，比如异常检测等等。也可作为分类等其他学习任务的前驱过程。聚类是标准的无监督学习。</p>
<p>​    1）在一些推荐系统中需确定新用户的类型，但定义“用户类型”却可能不太容易，此时往往可先对原有的用户数据进行聚类，根据聚类结果将每个簇定义为一个类,然后再基于这些类训练分类模型,用于判别新用户的类型。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.3.1.png" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2.21.3.1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    2）而降维则是为了缓解维数灾难的一个重要方法，就是通过某种数学变换将原始高维属性空间转变为一个低维“子空间”。其基于的假设就是，虽然人们平时观测到的数据样本虽然是高维的，但是实际上真正与学习任务相关的是个低维度的分布。从而通过最主要的几个特征维度就可以实现对数据的描述，对于后续的分类很有帮助。比如对于Kaggle（数据分析竞赛平台之一）上的泰坦尼克号生还问题。通过给定一个乘客的许多特征如年龄、姓名、性别、票价等，来判断其是否能在海难中生还。这就需要首先进行特征筛选，从而能够找出主要的特征，让学习到的模型有更好的泛化性。</p>
<p>​    聚类和降维都可以作为分类等问题的预处理步骤。</p>
<p><img src="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2-19.jpg" class="lazyload" data-srcset="/zh-TW/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch2/2-19.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>​    但是他们虽然都能实现对数据的约减。但是二者适用的对象不同，聚类针对的是数据点，而降维则是对于数据的特征。另外它们有着很多种实现方法。聚类中常用的有K-means、层次聚类、基于密度的聚类等；降维中常用的则PCA、Isomap、LLE等。</p>
<h3 id="有哪些聚类算法优劣衡量标准"><a href="#有哪些聚类算法优劣衡量标准" class="headerlink" title="有哪些聚类算法优劣衡量标准"></a>有哪些聚类算法优劣衡量标准</h3><p>不同聚类算法有不同的优劣和不同的适用条件。可从以下方面进行衡量判断：<br>    1、算法的处理能力：处理大的数据集的能力，即算法复杂度；处理数据噪声的能力；处理任意形状，包括有间隙的嵌套的数据的能力；<br>    2、算法是否需要预设条件：是否需要预先知道聚类个数，是否需要用户给出领域知识； </p>
<p>​    3、算法的数据输入属性：算法处理的结果与数据输入的顺序是否相关，也就是说算法是否独立于数据输入顺序；算法处理有很多属性数据的能力，也就是对数据维数是否敏感，对数据的类型有无要求。</p>
<h3 id="聚类和分类有什么区别"><a href="#聚类和分类有什么区别" class="headerlink" title="聚类和分类有什么区别"></a>聚类和分类有什么区别</h3><p><strong>聚类（Clustering） </strong><br>    聚类，简单地说就是把相似的东西分到一组，聚类的时候，我们并不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起。一个聚类算法通常只需要知道如何计算相似度就可以开始工作了，因此聚类通常并不需要使用训练数据进行学习，在机器学习中属于无监督学习。 </p>
<p><strong>分类（Classification） </strong></p>
<p>​     分类，对于一个分类器，通常需要你告诉它“这个东西被分为某某类”。一般情况下，一个分类器会从它得到的训练集中进行学习，从而具备对未知数据进行分类的能力，在机器学习中属于监督学习。</p>
<h3 id="不同聚类算法特点性能比较"><a href="#不同聚类算法特点性能比较" class="headerlink" title="不同聚类算法特点性能比较"></a>不同聚类算法特点性能比较</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">算法名称</th>
<th style="text-align:center">可伸缩性</th>
<th style="text-align:center">适合的数据类型</th>
<th style="text-align:center">高维性</th>
<th style="text-align:center">异常数据抗干扰性</th>
<th style="text-align:center">聚类形状</th>
<th style="text-align:center">算法效率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">WAVECLUSTER</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">很高</td>
</tr>
<tr>
<td style="text-align:center">ROCK</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">混合型</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:center">BIRCH</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">球形</td>
<td style="text-align:center">很高</td>
</tr>
<tr>
<td style="text-align:center">CURE</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">较高</td>
</tr>
<tr>
<td style="text-align:center">K-PROTOTYPES</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">混合型</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:center">DENCLUE</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">较高</td>
</tr>
<tr>
<td style="text-align:center">OPTIGRID</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:center">CLIQUE</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">较低</td>
</tr>
<tr>
<td style="text-align:center">DBSCAN</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:center">CLARANS</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">球形</td>
<td style="text-align:center">较低</td>
</tr>
</tbody>
</table>
</div>
<h3 id="四种常用聚类方法之比较"><a href="#四种常用聚类方法之比较" class="headerlink" title="四种常用聚类方法之比较"></a>四种常用聚类方法之比较</h3><p>​    聚类就是按照某个特定标准把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。即聚类后同一类的数据尽可能聚集到一起，不同类数据尽量分离。<br>​    主要的聚类算法可以划分为如下几类：划分方法、层次方法、基于密度的方法、基于网格的方法以及基于模型的方法。下面主要对k-means聚类算法、凝聚型层次聚类算法、神经网络聚类算法之SOM,以及模糊聚类的FCM算法通过通用测试数据集进行聚类效果的比较和分析。</p>
<h3 id="k-means聚类算法"><a href="#k-means聚类算法" class="headerlink" title="k-means聚类算法"></a>k-means聚类算法</h3><p>k-means是划分方法中较经典的聚类算法之一。由于该算法的效率高，所以在对大规模数据进行聚类时被广泛应用。目前，许多算法均围绕着该算法进行扩展和改进。<br>k-means算法以k为参数，把n个对象分成k个簇，使簇内具有较高的相似度，而簇间的相似度较低。k-means算法的处理过程如下：首先，随机地 选择k个对象，每个对象初始地代表了一个簇的平均值或中心;对剩余的每个对象，根据其与各簇中心的距离，将它赋给最近的簇;然后重新计算每个簇的平均值。 这个过程不断重复，直到准则函数收敛。通常，采用平方误差准则，其定义如下：</p>
<script type="math/tex; mode=display">
E=\sum_{i=1}^{k}\sum_{p\in C_i}\left\|p-m_i\right\|^2</script><p>　这里E是数据中所有对象的平方误差的总和，p是空间中的点，$m_i$是簇$C_i$的平均值[9]。该目标函数使生成的簇尽可能紧凑独立，使用的距离度量是欧几里得距离，当然也可以用其他距离度量。</p>
<p><strong>算法流程</strong>：<br>​    输入：包含n个对象的数据和簇的数目k；<br>​    输出：n个对象到k个簇，使平方误差准则最小。<br>​    步骤：<br>　　(1) 任意选择k个对象作为初始的簇中心；<br>　　(2) 根据簇中对象的平均值，将每个对象(重新)赋予最类似的簇；<br>　　(3) 更新簇的平均值，即计算每个簇中对象的平均值；<br>　　(4) 重复步骤(2)、(3)直到簇中心不再变化；</p>
<h3 id="层次聚类算法"><a href="#层次聚类算法" class="headerlink" title="层次聚类算法"></a>层次聚类算法</h3><p>​    根据层次分解的顺序是自底向上的还是自上向下的，层次聚类算法分为凝聚的层次聚类算法和分裂的层次聚类算法。<br>　凝聚型层次聚类的策略是先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到所有对象都在一个簇中，或者某个终结条件被满足。绝大多数层次聚类属于凝聚型层次聚类，它们只是在簇间相似度的定义上有所不同。</p>
<p><strong>算法流程</strong>：</p>
<p>注：以采用最小距离的凝聚层次聚类算法为例：</p>
<p>　(1) 将每个对象看作一类，计算两两之间的最小距离；<br>　(2) 将距离最小的两个类合并成一个新类；<br>　(3) 重新计算新类与所有类之间的距离；<br>　(4) 重复(2)、(3)，直到所有类最后合并成一类。</p>
<h3 id="SOM聚类算法"><a href="#SOM聚类算法" class="headerlink" title="SOM聚类算法"></a>SOM聚类算法</h3><p>​    SOM神经网络[11]是由芬兰神经网络专家Kohonen教授提出的，该算法假设在输入对象中存在一些拓扑结构或顺序，可以实现从输入空间(n维)到输出平面(2维)的降维映射，其映射具有拓扑特征保持性质,与实际的大脑处理有很强的理论联系。</p>
<p>​    SOM网络包含输入层和输出层。输入层对应一个高维的输入向量，输出层由一系列组织在2维网格上的有序节点构成，输入节点与输出节点通过权重向量连接。 学习过程中，找到与之距离最短的输出层单元，即获胜单元，对其更新。同时，将邻近区域的权值更新，使输出节点保持输入向量的拓扑特征。</p>
<p><strong>算法流程</strong>：</p>
<p>​    (1) 网络初始化，对输出层每个节点权重赋初值；<br>​    (2) 从输入样本中随机选取输入向量并且归一化，找到与输入向量距离最小的权重向量；<br>​    (3) 定义获胜单元，在获胜单元的邻近区域调整权重使其向输入向量靠拢；<br>​    (4) 提供新样本、进行训练；<br>​    (5) 收缩邻域半径、减小学习率、重复，直到小于允许值，输出聚类结果。</p>
<h3 id="FCM聚类算法"><a href="#FCM聚类算法" class="headerlink" title="FCM聚类算法"></a>FCM聚类算法</h3><p>​    1965年美国加州大学柏克莱分校的扎德教授第一次提出了‘集合’的概念。经过十多年的发展，模糊集合理论渐渐被应用到各个实际应用方面。为克服非此即彼的分类缺点，出现了以模糊集合论为数学基础的聚类分析。用模糊数学的方法进行聚类分析，就是模糊聚类分析[12]。<br>​    FCM算法是一种以隶属度来确定每个数据点属于某个聚类程度的算法。该聚类算法是传统硬聚类算法的一种改进。<br>​    设数据集$X={x<em>1,x_2,…,x_n}$,它的模糊$c$划分可用模糊矩阵$U=[u</em>{ij}]$表示，矩阵$U$的元素$u<em>{ij}$表示第$j(j=1,2,…,n)$个数据点属于第$i(i=1,2,…,c)$类的隶属度，$u</em>{ij}$满足如下条件：  </p>
<script type="math/tex; mode=display">
\begin{equation}
\left\{
\begin{array}{lr}
\sum_{i=1}^c u_{ij}=1 \quad\forall~j
\\u_{ij}\in[0,1] \quad\forall ~i,j
\\\sum_{j=1}^c u_{ij}>0 \quad\forall ~i
\end{array}
\right.
\end{equation}</script><p>目前被广泛使用的聚类准则是取类内加权误差平方和的极小值。即：</p>
<script type="math/tex; mode=display">
(min)J_m(U,V)=\sum^n_{j=1}\sum^c_{i=1}u^m_{ij}d^2_{ij}(x_j,v_i)</script><p>其中$V$为聚类中心，$m$为加权指数，$d_{ij}(x_j,v_i)=||v_i-x_j||$。</p>
<p><strong>算法流程</strong>：</p>
<p>　(1) 标准化数据矩阵；<br>　(2) 建立模糊相似矩阵，初始化隶属矩阵；<br>　(3) 算法开始迭代，直到目标函数收敛到极小值；<br>　(4) 根据迭代结果，由最后的隶属矩阵确定数据所属的类，显示最后的聚类结果。</p>
<h3 id="四种聚类算法试验"><a href="#四种聚类算法试验" class="headerlink" title="四种聚类算法试验"></a>四种聚类算法试验</h3><p>​    选取专门用于测试分类、聚类算法的国际通用的UCI数据库中的IRIS数据集，IRIS数据集包含150个样本数据，分别取自三种不同 的莺尾属植物setosa、versicolor和virginica的花朵样本,每个数据含有4个属性，即萼片长度、萼片宽度、花瓣长度、花瓣宽度，单位为cm。 在数据集上执行不同的聚类算法，可以得到不同精度的聚类结果。基于前面描述的各算法原理及流程，可初步得如下聚类结果。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>聚类方法</th>
<th>聚错样本数</th>
<th>运行时间/s</th>
<th>平均准确率/（%）</th>
</tr>
</thead>
<tbody>
<tr>
<td>K-means</td>
<td>17</td>
<td>0.146001</td>
<td>89</td>
</tr>
<tr>
<td>层次聚类</td>
<td>51</td>
<td>0.128744</td>
<td>66</td>
</tr>
<tr>
<td>SOM</td>
<td>22</td>
<td>5.267283</td>
<td>86</td>
</tr>
<tr>
<td>FCM</td>
<td>12</td>
<td>0.470417</td>
<td>92</td>
</tr>
</tbody>
</table>
</div>
<p><strong>注</strong>：</p>
<p>(1) 聚错样本数：总的聚错的样本数，即各类中聚错的样本数的和；<br>(2) 运行时间：即聚类整个过程所耗费的时间，单位为s；<br>(3) 平均准确度：设原数据集有k个类,用$c<em>i$表示第i类，$n_i$为$c_i$中样本的个数，$m_i$为聚类正确的个数,则$m_i/n_i$为 第i类中的精度，则平均精度为：$avg=\frac{1}{k}\sum</em>{i=1}^{k}\frac{m<em>{i}}{n</em>{i}}$。  </p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习基础</title>
    <url>/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="神经网络组成？"><a href="#神经网络组成？" class="headerlink" title="神经网络组成？"></a>神经网络组成？</h3><p>神经网络类型众多，其中最为重要的是多层感知机。为了详细地描述神经网络，我们先从最简单的神经网络说起。</p>
<p><strong>感知机</strong></p>
<p>多层感知机中的特征神经元模型称为感知机，由<em>Frank Rosenblatt</em>于1957年发明。</p>
<p>简单的感知机如下图所示：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-1.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>其中$x_1$，$x_2$，$x_3$为感知机的输入，其输出为：</p>
<script type="math/tex; mode=display">
output = \left\{
\begin{aligned}
0, \quad if \ \ \sum_i w_i x_i \leqslant threshold \\
1, \quad if \ \ \sum_i w_i x_i > threshold
\end{aligned}
\right.</script><p>假如把感知机想象成一个加权投票机制，比如 3 位评委给一个歌手打分，打分分别为$ 4 $分、$1$ 分、$-3 $分，这$ 3$ 位评分的权重分别是 $1、3、2$，则该歌手最终得分为 $4 \times 1 + 1 \times 3 + (-3) \times 2 = 1$ 。按照比赛规则，选取的 $threshold$ 为 $3$，说明只有歌手的综合评分大于$ 3$ 时，才可顺利晋级。对照感知机，该选手被淘汰，因为：</p>
<script type="math/tex; mode=display">
\sum_i w_i x_i < threshold=3, output = 0</script><p>用 $-b$  代替 $threshold$，输出变为：</p>
<script type="math/tex; mode=display">
output = \left\{
\begin{aligned}
0, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b \leqslant 0 \\
1, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b > 0
\end{aligned}
\right.</script><p>设置合适的  $\boldsymbol{x}$  和  $b$ ，一个简单的感知机单元的与非门表示如下：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-2.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>当输入为 $0$，$1$ 时，感知机输出为 $ 0 \times (-2) + 1 \times (-2) + 3 = 1$。</p>
<p>复杂一些的感知机由简单的感知机单元组合而成：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-3.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-3.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>多层感知机</strong></p>
<p>多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络。相比于单独的感知机，多层感知机的第 $ i $ 层的每个神经元和第 $ i-1 $ 层的每个神经元都有连接。</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.1.1.5.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.1.1.5.png" srcset="data:image/png;base64,666" alt=""></p>
<p>输出层可以不止有$ 1$ 个神经元。隐藏层可以只有$ 1$ 层，也可以有多层。输出层为多个神经元的神经网络例如下图所示：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.1.1.6.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.1.1.6.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="神经网络有哪些常用模型结构？"><a href="#神经网络有哪些常用模型结构？" class="headerlink" title="神经网络有哪些常用模型结构？"></a>神经网络有哪些常用模型结构？</h3><p>下图包含了大部分常用的模型：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-7.jpg" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-7.jpg" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="如何选择深度学习开发平台？"><a href="#如何选择深度学习开发平台？" class="headerlink" title="如何选择深度学习开发平台？"></a>如何选择深度学习开发平台？</h3><p>​    现有的深度学习开源平台主要有 Caffe, PyTorch, MXNet, CNTK, Theano, TensorFlow, Keras, fastai等。那如何选择一个适合自己的平台呢，下面列出一些衡量做参考。</p>
<p><strong>参考1：与现有编程平台、技能整合的难易程度</strong></p>
<p>​    主要是前期积累的开发经验和资源，比如编程语言，前期数据集存储格式等。</p>
<p><strong>参考2: 与相关机器学习、数据处理生态整合的紧密程度</strong></p>
<p>​    深度学习研究离不开各种数据处理、可视化、统计推断等软件包。考虑建模之前，是否具有方便的数据预处理工具？建模之后，是否具有方便的工具进行可视化、统计推断、数据分析。  </p>
<p><strong>参考3：对数据量及硬件的要求和支持</strong></p>
<p>​    深度学习在不同应用场景的数据量是不一样的，这也就导致我们可能需要考虑分布式计算、多GPU计算的问题。例如，对计算机图像处理研究的人员往往需要将图像文件和计算任务分部到多台计算机节点上进行执行。当下每个深度学习平台都在快速发展，每个平台对分布式计算等场景的支持也在不断演进。</p>
<p><strong>参考4：深度学习平台的成熟程度</strong></p>
<p>​    成熟程度的考量是一个比较主观的考量因素，这些因素可包括：社区的活跃程度；是否容易和开发人员进行交流；当前应用的势头。</p>
<p><strong>参考5：平台利用是否多样性？</strong></p>
<p>​    有些平台是专门为深度学习研究和应用进行开发的，有些平台对分布式计算、GPU 等构架都有强大的优化，能否用这些平台/软件做其他事情？比如有些深度学习软件是可以用来求解二次型优化；有些深度学习平台很容易被扩展，被运用在强化学习的应用中。</p>
<h3 id="为什么使用深层表示"><a href="#为什么使用深层表示" class="headerlink" title="为什么使用深层表示?"></a>为什么使用深层表示?</h3><ol>
<li>深度神经网络是一种特征递进式的学习算法，浅层的神经元直接从输入数据中学习一些低层次的简单特征，例如边缘、纹理等。而深层的特征则基于已学习到的浅层特征继续学习更高级的特征，从计算机的角度学习深层的语义信息。</li>
<li>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</li>
</ol>
<h3 id="为什么深层神经网络难以训练？"><a href="#为什么深层神经网络难以训练？" class="headerlink" title="为什么深层神经网络难以训练？"></a>为什么深层神经网络难以训练？</h3><ol>
<li><p>梯度消失<br> 梯度消失是指通过隐藏层从后向前看，梯度会变的越来越小，说明前面层的学习会显著慢于后面层的学习，所以学习会卡住，除非梯度变大。</p>
<p> ​    梯度消失的原因受到多种因素影响，例如学习率的大小，网络参数的初始化，激活函数的边缘效应等。在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。下图是不同隐含层的学习速率：</p>
</li>
</ol>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-8.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-8.png" srcset="data:image/png;base64,666" alt=""></p>
<ol>
<li><p>梯度爆炸<br> 在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；在极端情况下，权重值甚至会溢出，变为$NaN$值，再也无法更新。</p>
</li>
<li><p>权重矩阵的退化导致模型的有效自由度减少。</p>
<p> ​    参数空间中学习的退化速度减慢，导致减少了模型的有效维数，网络的可用自由度对学习中梯度范数的贡献不均衡，随着相乘矩阵的数量（即网络深度）的增加，矩阵的乘积变得越来越退化。在有硬饱和边界的非线性网络中（例如 ReLU 网络），随着深度增加，退化过程会变得越来越快。Duvenaud等人2014年的论文里展示了关于该退化过程的可视化：</p>
</li>
</ol>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-9.jpg" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-9.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>随着深度的增加，输入空间（左上角所示）会在输入空间中的每个点处被扭曲成越来越细的单丝，只有一个与细丝正交的方向影响网络的响应。沿着这个方向，网络实际上对变化变得非常敏感。</p>
<h3 id="深度学习和机器学习有什么不同？"><a href="#深度学习和机器学习有什么不同？" class="headerlink" title="深度学习和机器学习有什么不同？"></a>深度学习和机器学习有什么不同？</h3><p>​    <strong>机器学习</strong>：利用计算机、概率论、统计学等知识，输入数据，让计算机学会新知识。机器学习的过程，就是训练数据去优化目标函数。</p>
<p>​    <strong>深度学习</strong>：是一种特殊的机器学习，具有强大的能力和灵活性。它通过学习将世界表示为嵌套的层次结构，每个表示都与更简单的特征相关，而抽象的表示则用于计算更抽象的表示。</p>
<p>​    传统的机器学习需要定义一些手工特征，从而有目的的去提取目标信息， 非常依赖任务的特异性以及设计特征的专家经验。而深度学习可以从大数据中先学习简单的特征，并从其逐渐学习到更为复杂抽象的深层特征，不依赖人工的特征工程，这也是深度学习在大数据时代受欢迎的一大原因。</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.1.6.1.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.1.6.1.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-11.jpg" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-11.jpg" srcset="data:image/png;base64,666" alt=""></p>
<h2 id="网络操作与计算"><a href="#网络操作与计算" class="headerlink" title="网络操作与计算"></a>网络操作与计算</h2><h3 id="前向传播与反向传播？"><a href="#前向传播与反向传播？" class="headerlink" title="前向传播与反向传播？"></a>前向传播与反向传播？</h3><p>神经网络的计算主要有两种：前向传播（foward propagation, FP）作用于每一层的输入，通过逐层计算得到输出结果；反向传播（backward propagation, BP）作用于网络的输出，通过计算梯度由深到浅更新网络参数。</p>
<p><strong>前向传播</strong></p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.1.1.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.1.1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>假设上一层结点 $ i,j,k,… $ 等一些结点与本层的结点 $ w $ 有连接，那么结点 $ w $ 的值怎么算呢？就是通过上一层的 $ i,j,k,… $ 等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如 $ReLu$，$sigmoid$ 等函数，最后得到的结果就是本层结点 $ w $ 的输出。 </p>
<p>最终不断的通过这种方法一层层的运算，得到输出层结果。</p>
<p><strong>反向传播</strong></p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.1.2.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.1.2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>由于我们前向传播最终得到的结果，以分类为例，最终总是有误差的，那么怎么减少误差呢，当前应用广泛的一个算法就是梯度下降算法，但是求梯度就要求偏导数，下面以图中字母为例讲解一下：</p>
<p>设最终误差为 $ E $且输出层的激活函数为线性激活函数，对于输出那么 $ E $ 对于输出节点 $ y_l $ 的偏导数是 $ y_l - t_l $，其中 $ t_l $ 是真实值，$ \frac{\partial y_l}{\partial z_l} $ 是指上面提到的激活函数，$ z_l $ 是上面提到的加权和，那么这一层的 $ E $ 对于 $ z_l $ 的偏导数为 $ \frac{\partial E}{\partial z_l} = \frac{\partial E}{\partial y_l} \frac{\partial y_l}{\partial z_l} $。同理，下一层也是这么计算，只不过 $ \frac{\partial E}{\partial y_k} $ 计算方法变了，一直反向传播到输入层，最后有 $ \frac{\partial E}{\partial x_i} = \frac{\partial E}{\partial y_j} \frac{\partial y_j}{\partial z_j} $，且 $ \frac{\partial z_j}{\partial x_i} = w_i j $。然后调整这些过程中的权值，再不断进行前向传播和反向传播的过程，最终得到一个比较好的结果。</p>
<h3 id="如何计算神经网络的输出？"><a href="#如何计算神经网络的输出？" class="headerlink" title="如何计算神经网络的输出？"></a>如何计算神经网络的输出？</h3><p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.2.1.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.2.1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>如上图，输入层有三个节点，我们将其依次编号为 1、2、3；隐藏层的 4 个节点，编号依次为 4、5、6、7；最后输出层的两个节点编号为 8、9。比如，隐藏层的节点 4，它和输入层的三个节点 1、2、3 之间都有连接，其连接上的权重分别为是 $ w<em>{41}, w</em>{42}, w_{43} $。</p>
<p>为了计算节点 4 的输出值，我们必须先得到其所有上游节点（也就是节点 1、2、3）的输出值。节点 1、2、3 是输入层的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点 1、2、3 的输出值分别是 $ x_1, x_2, x_3 $。</p>
<script type="math/tex; mode=display">
a_4 = \sigma(w^T \cdot a) = \sigma(w_{41}x_4 + w_{42}x_2 + w_{43}a_3 + w_{4b})</script><p>其中 $ w_{4b} $ 是节点 4 的偏置项。</p>
<p>同样，我们可以继续计算出节点 5、6、7 的输出值 $ a_5, a_6, a_7 $。</p>
<p>计算输出层的节点 8 的输出值 $ y_1 $：</p>
<script type="math/tex; mode=display">
y_1 = \sigma(w^T \cdot a) = \sigma(w_{84}a_4 + w_{85}a_5 + w_{86}a_6 + w_{87}a_7 + w_{8b})</script><p>其中 $ w_{8b} $ 是节点 8 的偏置项。</p>
<p>同理，我们还可以计算出 $ y_2 $。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量 $ x_1, x_2, x_3, x_4 $ 时，神经网络的输出向量 $ y_1, y_2 $ 。这里我们也看到，输出向量的维度和输出层神经元个数相同。</p>
<h3 id="如何计算卷积神经网络输出值？"><a href="#如何计算卷积神经网络输出值？" class="headerlink" title="如何计算卷积神经网络输出值？"></a>如何计算卷积神经网络输出值？</h3><p>假设有一个 5*5 的图像，使用一个 3*3 的 filter 进行卷积，想得到一个 3*3 的 Feature Map，如下所示：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.3.1.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.3.1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>$ x<em>{i,j} $ 表示图像第  $ i $ 行第 $ j $ 列元素。$ w</em>{m,n} $ 表示 filter​ 第 $ m $ 行第 $ n $ 列权重。 $ w_b $ 表示 $filter$ 的偏置项。 表$a_i,_j$示 feature map 第 $ i$ 行第 $ j $ 列元素。 $f$ 表示激活函数，这里以$ ReLU$ 函数为例。</p>
<p>卷积计算公式如下：</p>
<script type="math/tex; mode=display">
a_{i,j} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{i+m, j+n} + w_b )</script><p>当步长为 $1$ 时，计算 feature map 元素 $ a_{0,0} $ 如下：</p>
<script type="math/tex; mode=display">
a_{0,0} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{0+m, 0+n} + w_b )

= relu(w_{0,0} x_{0,0} + w_{0,1} x_{0,1} + w_{0,2} x_{0,2} + w_{1,0} x_{1,0} + \\w_{1,1} x_{1,1} + w_{1,2} x_{1,2} + w_{2,0} x_{2,0} + w_{2,1} x_{2,1} + w_{2,2} x_{2,2}) \\

= 1 + 0 + 1 + 0 + 1 + 0 + 0 + 0 + 1 \\

= 4</script><p>其计算过程图示如下：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.3.2.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.3.2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>以此类推，计算出全部的Feature Map。</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.3.4.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.3.4.png" srcset="data:image/png;base64,666" alt=""></p>
<p>当步幅为 2 时，Feature Map计算如下</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.3.5.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.3.5.png" srcset="data:image/png;base64,666" alt=""></p>
<p>注：图像大小、步幅和卷积后的Feature Map大小是有关系的。它们满足下面的关系：</p>
<script type="math/tex; mode=display">
W_2 = (W_1 - F + 2P)/S + 1\\
H_2 = (H_1 - F + 2P)/S + 1</script><p>​    其中 $ W_2 $， 是卷积后 Feature Map 的宽度；$ W_1 $ 是卷积前图像的宽度；$ F $ 是 filter 的宽度；$ P $ 是 Zero Padding 数量，Zero Padding 是指在原始图像周围补几圈 $0$，如果 $P$ 的值是 $1$，那么就补 $1$ 圈 $0$；$S$ 是步幅；$ H_2 $ 卷积后 Feature Map 的高度；$ H_1 $ 是卷积前图像的宽度。</p>
<p>​    举例：假设图像宽度 $ W_1 = 5 $，filter 宽度 $ F=3 $，Zero Padding $ P=0 $，步幅 $ S=2 $，$ Z $ 则</p>
<script type="math/tex; mode=display">
W_2 = (W_1 - F + 2P)/S + 1

= (5-3+0)/2 + 1

= 2</script><p>​    说明 Feature Map 宽度是2。同样，我们也可以计算出 Feature Map 高度也是 2。</p>
<p>如果卷积前的图像深度为 $ D $，那么相应的 filter 的深度也必须为 $ D $。深度大于 1 的卷积计算公式：</p>
<script type="math/tex; mode=display">
a_{i,j} = f(\sum_{d=0}^{D-1} \sum_{m=0}^{F-1} \sum_{n=0}^{F-1} w_{d,m,n} x_{d,i+m,j+n} + w_b)</script><p>​    其中，$ D $ 是深度；$ F $ 是 filter 的大小；$ w<em>{d,m,n} $ 表示 filter 的第 $ d $ 层第 $ m $ 行第 $ n $ 列权重；$ a</em>{d,i,j} $ 表示 feature map 的第 $ d $ 层第 $ i $ 行第 $ j $ 列像素；其它的符号含义前面相同，不再赘述。</p>
<p>​    每个卷积层可以有多个 filter。每个 filter 和原始图像进行卷积后，都可以得到一个 Feature Map。卷积后 Feature Map 的深度(个数)和卷积层的 filter 个数相同。下面的图示显示了包含两个 filter 的卷积层的计算。$7<em>7</em>3$ 输入，经过两个 $3<em>3</em>3$ filter 的卷积(步幅为 $2$)，得到了 $3<em>3</em>2$ 的输出。图中的 Zero padding 是 $1$，也就是在输入元素的周围补了一圈 $0$。</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.3.6.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.3.6.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连(卷积计算规则)，且 filter 的权值对于上一层所有神经元都是一样的。对于包含两个 $ 3 <em> 3 </em> 3 $ 的 fitler 的卷积层来说，其参数数量仅有 $ (3 <em> 3 </em> 3+1) * 2 = 56 $ 个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。</p>
<h3 id="如何计算-Pooling-层输出值输出值？"><a href="#如何计算-Pooling-层输出值输出值？" class="headerlink" title="如何计算 Pooling 层输出值输出值？"></a>如何计算 Pooling 层输出值输出值？</h3><p>​    Pooling 层主要的作用是下采样，通过去掉 Feature Map 中不重要的样本，进一步减少参数数量。Pooling 的方法很多，最常用的是 Max Pooling。Max Pooling 实际上就是在 n*n 的样本中取最大值，作为采样后的样本值。下图是 2*2 max pooling：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.4.1.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.4.1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    除了 Max Pooing 之外，常用的还有 Average Pooling ——取各样本的平均值。<br>​    对于深度为 $ D $ 的 Feature Map，各层独立做 Pooling，因此 Pooling 后的深度仍然为 $ D $。</p>
<h3 id="实例理解反向传播"><a href="#实例理解反向传播" class="headerlink" title="实例理解反向传播"></a>实例理解反向传播</h3><p>​    一个典型的三层神经网络如下所示：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.5.1.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.5.1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    其中 Layer $ L_1 $ 是输入层，Layer $ L_2 $ 是隐含层，Layer $ L_3 $ 是输出层。</p>
<p>​    假设输入数据集为 $ D={x_1, x_2, …, x_n} $，输出数据集为 $ y_1, y_2, …, y_n $。</p>
<p>​    如果输入和输出是一样，即为自编码模型。如果原始数据经过映射，会得到不同于输入的输出。</p>
<p>假设有如下的网络层：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.5.2.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.5.2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    输入层包含神经元 $ i_1, i_2 $，偏置 $ b_1 $；隐含层包含神经元 $ h_1, h_2 $，偏置 $ b_2 $，输出层为  $ o_1, o_2 $，$ w_i $ 为层与层之间连接的权重，激活函数为 $sigmoid$ 函数。对以上参数取初始值，如下图所示：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.5.3.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.5.3.png" srcset="data:image/png;base64,666" alt=""></p>
<p>其中：</p>
<ul>
<li>输入数据 $ i1=0.05, i2 = 0.10 $</li>
<li>输出数据 $ o1=0.01, o2=0.99 $;</li>
<li>初始权重 $ w1=0.15, w2=0.20, w3=0.25,w4=0.30, w5=0.40, w6=0.45, w7=0.50, w8=0.55 $</li>
<li>目标：给出输入数据 $ i1,i2 $ ( $0.05$和$0.10$ )，使输出尽可能与原始输出 $ o1,o2 $，( $0.01$和$0.99$)接近。</li>
</ul>
<p><strong>前向传播</strong></p>
<ol>
<li>输入层 —&gt; 输出层</li>
</ol>
<p>计算神经元 $ h1 $ 的输入加权和：</p>
<script type="math/tex; mode=display">
net_{h1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1\\

net_{h1} = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1 = 0.3775</script><p>神经元 $ h1 $ 的输出 $ o1 $ ：（此处用到激活函数为 sigmoid 函数）：</p>
<script type="math/tex; mode=display">
out_{h1} = \frac{1}{1 + e^{-net_{h1}}} = \frac{1}{1 + e^{-0.3775}} = 0.593269992</script><p>同理，可计算出神经元 $ h2 $ 的输出 $ o1 $：</p>
<script type="math/tex; mode=display">
out_{h2} = 0.596884378</script><ol>
<li>隐含层—&gt;输出层：  　　</li>
</ol>
<p>计算输出层神经元 $ o1 $ 和 $ o2 $ 的值：</p>
<script type="math/tex; mode=display">
net_{o1} = w_5 * out_{h1} + w_6 * out_{h2} + b_2 * 1</script><script type="math/tex; mode=display">
net_{o1} = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1 = 1.105905967</script><script type="math/tex; mode=display">
out_{o1} = \frac{1}{1 + e^{-net_{o1}}} = \frac{1}{1 + e^{1.105905967}} = 0.75136079</script><p>这样前向传播的过程就结束了，我们得到输出值为 $ [0.75136079 ,  0.772928465] $，与实际值 $ [0.01 , 0.99] $ 相差还很远，现在我们对误差进行反向传播，更新权值，重新计算输出。</p>
<p><strong>反向传播 </strong></p>
<p>​    1.计算总误差</p>
<p>总误差：(这里使用Square Error)</p>
<script type="math/tex; mode=display">
E_{total} = \sum \frac{1}{2}(target - output)^2</script><p>但是有两个输出，所以分别计算 $ o1 $ 和 $ o2 $ 的误差，总误差为两者之和：</p>
<p>$E<em>{o1} = \frac{1}{2}(target</em>{o1} - out_{o1})^2<br>= \frac{1}{2}(0.01 - 0.75136507)^2 = 0.274811083$.</p>
<p>$E_{o2} = 0.023560026$.</p>
<p>$E<em>{total} = E</em>{o1} + E_{o2} = 0.274811083 + 0.023560026 = 0.298371109$.</p>
<p>​    2.隐含层 —&gt; 输出层的权值更新：</p>
<p>以权重参数 $ w5 $ 为例，如果我们想知道 $ w5 $ 对整体误差产生了多少影响，可以用整体误差对 $ w5 $ 求偏导求出：（链式法则）</p>
<script type="math/tex; mode=display">
\frac{\partial E_{total}}{\partial w5} = \frac{\partial E_{total}}{\partial out_{o1}} * \frac{\partial out_{o1}}{\partial net_{o1}} * \frac{\partial net_{o1}}{\partial w5}</script><p>下面的图可以更直观的看清楚误差是怎样反向传播的：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.5.4.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.2.5.4.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="神经网络更“深”有什么意义？"><a href="#神经网络更“深”有什么意义？" class="headerlink" title="神经网络更“深”有什么意义？"></a>神经网络更“深”有什么意义？</h3><p>前提：在一定范围内。</p>
<ul>
<li>在神经元数量相同的情况下，深层网络结构具有更大容量，分层组合带来的是指数级的表达空间，能够组合成更多不同类型的子结构，这样可以更容易地学习和表示各种特征。</li>
<li>隐藏层增加则意味着由激活函数带来的非线性变换的嵌套层数更多，就能构造更复杂的映射关系。</li>
</ul>
<h2 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h2><h3 id="什么是超参数？"><a href="#什么是超参数？" class="headerlink" title="什么是超参数？"></a>什么是超参数？</h3><p>​    <strong>超参数</strong> : 在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。</p>
<p>​    超参数通常存在于：</p>
<pre><code>1.  定义关于模型的更高层次的概念，如复杂性或学习能力。
2.  不能直接从标准模型培训过程中的数据中学习，需要预先定义。
3.  可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定
</code></pre><p>​    超参数具体来讲比如算法中的学习率（learning rate）、梯度下降法迭代的数量（iterations）、隐藏层数目（hidden layers）、隐藏层单元数目、激活函数（ activation function）都需要根据实际情况来设置，这些数字实际上控制了最后的参数和的值，所以它们被称作超参数。</p>
<h3 id="如何寻找超参数的最优值？"><a href="#如何寻找超参数的最优值？" class="headerlink" title="如何寻找超参数的最优值？"></a>如何寻找超参数的最优值？</h3><p>​    在使用机器学习算法时，总有一些难调的超参数。例如权重衰减大小，高斯核宽度等等。这些参数需要人为设置，设置的值对结果产生较大影响。常见设置超参数的方法有：</p>
<ol>
<li><p>猜测和检查：根据经验或直觉，选择参数，一直迭代。</p>
</li>
<li><p>网格搜索：让计算机尝试在一定范围内均匀分布的一组值。</p>
</li>
<li><p>随机搜索：让计算机随机挑选一组值。</p>
</li>
<li><p>贝叶斯优化：使用贝叶斯优化超参数，会遇到贝叶斯优化算法本身就需要很多的参数的困难。</p>
</li>
<li><p>MITIE方法，好初始猜测的前提下进行局部优化。它使用BOBYQA算法，并有一个精心选择的起始点。由于BOBYQA只寻找最近的局部最优解，所以这个方法是否成功很大程度上取决于是否有一个好的起点。在MITIE的情况下，我们知道一个好的起点，但这不是一个普遍的解决方案，因为通常你不会知道好的起点在哪里。从好的方面来说，这种方法非常适合寻找局部最优解。稍后我会再讨论这一点。</p>
</li>
<li><p>最新提出的LIPO的全局优化方法。这个方法没有参数，而且经验证比随机搜索方法好。</p>
</li>
</ol>
<h3 id="超参数搜索一般过程？"><a href="#超参数搜索一般过程？" class="headerlink" title="超参数搜索一般过程？"></a>超参数搜索一般过程？</h3><p>超参数搜索一般过程：</p>
<ol>
<li>将数据集划分成训练集、验证集及测试集。</li>
<li>在训练集上根据模型的性能指标对模型参数进行优化。</li>
<li>在验证集上根据模型的性能指标对模型的超参数进行搜索。</li>
<li>步骤 2 和步骤 3 交替迭代，最终确定模型的参数和超参数，在测试集中验证评价模型的优劣。</li>
</ol>
<p>其中，搜索过程需要搜索算法，一般有：网格搜索、随机搜过、启发式智能搜索、贝叶斯搜索。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="为什么需要非线性激活函数？"><a href="#为什么需要非线性激活函数？" class="headerlink" title="为什么需要非线性激活函数？"></a>为什么需要非线性激活函数？</h3><p><strong>为什么需要激活函数？</strong></p>
<ol>
<li>激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。</li>
<li>激活函数可以引入非线性因素。如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。</li>
<li>激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。</li>
</ol>
<p><strong>为什么激活函数需要非线性函数？</strong></p>
<ol>
<li>假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。</li>
<li>使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。</li>
</ol>
<h3 id="常见的激活函数及图像"><a href="#常见的激活函数及图像" class="headerlink" title="常见的激活函数及图像"></a>常见的激活函数及图像</h3><ol>
<li><p>sigmoid 激活函数</p>
<p>函数的定义为：$ f(x) = \frac{1}{1 + e^{-x}} $，其值域为 $ (0,1) $。</p>
<p>函数图像如下：</p>
</li>
</ol>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-26.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-26.png" srcset="data:image/png;base64,666" alt=""></p>
<ol>
<li><p>tanh激活函数</p>
<p>函数的定义为：$ f(x) = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $，值域为 $ (-1,1) $。</p>
<p>函数图像如下：</p>
</li>
</ol>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-27.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-27.png" srcset="data:image/png;base64,666" alt=""></p>
<ol>
<li><p>Relu激活函数</p>
<p>函数的定义为：$ f(x) = max(0, x) $  ，值域为 $ [0,+∞) $；</p>
<p>函数图像如下：</p>
</li>
</ol>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-28.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-28.png" srcset="data:image/png;base64,666" alt=""></p>
<ol>
<li><p>Leak Relu 激活函数 </p>
<p>函数定义为： $ f(x) =  \left{<br>\begin{aligned}<br>ax, \quad x&lt;0 \\ x, \quad x&gt;0<br>\end{aligned}<br>\right. $，值域为 $ (-∞,+∞) $。 <!--0--></p>
<p>图像如下（$ a = 0.5 $）：</p>
</li>
</ol>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-29.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-29.png" srcset="data:image/png;base64,666" alt=""></p>
<ol>
<li><p>SoftPlus 激活函数</p>
<p>函数的定义为：$ f(x) = ln( 1 + e^x) $，值域为 $ (0,+∞) $。</p>
<p>函数图像如下:</p>
</li>
</ol>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-30.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-30.png" srcset="data:image/png;base64,666" alt=""></p>
<ol>
<li><p>softmax 函数</p>
<p>函数定义为： $ \sigma(z)<em>j = \frac{e^{z_j}}{\sum</em>{k=1}^K e^{z_k}} $。</p>
<p>Softmax 多用于多分类神经网络输出。</p>
</li>
</ol>
<h3 id="常见激活函数的导数计算？"><a href="#常见激活函数的导数计算？" class="headerlink" title="常见激活函数的导数计算？"></a>常见激活函数的导数计算？</h3><p>对常见激活函数，导数计算如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>原函数</th>
<th>函数表达式</th>
<th>导数</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sigmoid激活函数</td>
<td>$f(x)=\frac{1}{1+e^{-x}}$</td>
<td>$f^{‘}(x)=\frac{1}{1+e^{-x}}\left( 1- \frac{1}{1+e^{-x}} \right)=f(x)(1-f(x))$</td>
<td>当$x=10$,或$x=-10​$，$f^{‘}(x) \approx0​$,当$x=0​$$f^{‘}(x) =0.25​$</td>
</tr>
<tr>
<td>Tanh激活函数</td>
<td>$f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$</td>
<td>$f^{‘}(x)=-(tanh(x))^2$</td>
<td>当$x=10$,或$x=-10$，$f^{‘}(x) \approx0$,当$x=0$$f^{`}(x) =1$</td>
</tr>
<tr>
<td>Relu激活函数</td>
<td>$f(x)=max(0,x)$</td>
<td>$c(u)=\begin{cases} 0,x&lt;0 \\ 1,x&gt;0 \ undefined,x=0\end{cases}$<!--0--></td>
<td>通常$x=0$时，给定其导数为1和0</td>
</tr>
</tbody>
</table>
</div>
<h3 id="激活函数有哪些性质？"><a href="#激活函数有哪些性质？" class="headerlink" title="激活函数有哪些性质？"></a>激活函数有哪些性质？</h3><ol>
<li>非线性： 当激活函数是线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即 $ f(x)=x $，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的；</li>
<li>可微性： 当优化方法是基于梯度的时候，就体现了该性质；</li>
<li>单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数；</li>
<li>$ f(x)≈x $： 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；</li>
<li>输出值的范围： 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。</li>
</ol>
<h3 id="如何选择激活函数？"><a href="#如何选择激活函数？" class="headerlink" title="如何选择激活函数？"></a>如何选择激活函数？</h3><p>​    选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好，就去使用它。</p>
<p>以下是常见的选择情况：</p>
<ol>
<li>如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。</li>
<li>如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。</li>
<li>sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。</li>
<li>tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。</li>
<li>ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。</li>
<li>如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。</li>
</ol>
<h3 id="使用-ReLu-激活函数的优点？"><a href="#使用-ReLu-激活函数的优点？" class="headerlink" title="使用 ReLu 激活函数的优点？"></a>使用 ReLu 激活函数的优点？</h3><ol>
<li>在区间变动很大的情况下，ReLu 激活函数的导数或者激活函数的斜率都会远大于 0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。</li>
<li>sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。</li>
<li>需注意，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会产生这个问题。</li>
</ol>
<h3 id="什么时候可以用线性激活函数？"><a href="#什么时候可以用线性激活函数？" class="headerlink" title="什么时候可以用线性激活函数？"></a>什么时候可以用线性激活函数？</h3><ol>
<li>输出层，大多使用线性激活函数。</li>
<li>在隐含层可能会使用一些线性激活函数。</li>
<li>一般用到的线性激活函数很少。</li>
</ol>
<h3 id="怎样理解-Relu（-lt-0-时）是非线性激活函数？"><a href="#怎样理解-Relu（-lt-0-时）是非线性激活函数？" class="headerlink" title="怎样理解 Relu（< 0 时）是非线性激活函数？"></a>怎样理解 Relu（&lt; 0 时）是非线性激活函数？</h3><p>Relu 激活函数图像如下：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-32.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-32.png" srcset="data:image/png;base64,666" alt=""></p>
<p>根据图像可看出具有如下特点：</p>
<ol>
<li><p>单侧抑制；</p>
</li>
<li><p>相对宽阔的兴奋边界；</p>
</li>
<li><p>稀疏激活性；</p>
<p>ReLU 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。</p>
<p>因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。</p>
<p><strong>稀疏激活性</strong>：从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 $ x&lt;0 $ 时，relu 硬饱和，而当 x&gt;0 $ 时，则不存在饱和问题。ReLU 能够在 $ x&gt;0 $ 时保持梯度不衰减，从而缓解梯度消失问题。<!--0--></p>
</li>
</ol>
<h3 id="Softmax-定义及作用"><a href="#Softmax-定义及作用" class="headerlink" title="Softmax 定义及作用"></a>Softmax 定义及作用</h3><p>Softmax 是一种形如下式的函数：</p>
<script type="math/tex; mode=display">
P(i) = \frac{exp(\theta_i^T x)}{\sum_{k=1}^{K} exp(\theta_i^T x)}</script><p>​    其中，$ \theta_i $ 和 $ x $ 是列向量，$ \theta_i^T x $ 可能被换成函数关于 $ x $ 的函数 $ f_i(x) $</p>
<p>​    通过 softmax 函数，可以使得 $ P(i) $ 的范围在 $ [0,1] $ 之间。在回归和分类问题中，通常 $ \theta $ 是待求参数，通过寻找使得 $ P(i) $ 最大的 $ \theta_i $ 作为最佳参数。</p>
<p>​    但是，使得范围在 $ [0,1] $  之间的方法有很多，为啥要在前面加上以 $ e $ 的幂函数的形式呢？参考 logistic 函数：</p>
<script type="math/tex; mode=display">
P(i) = \frac{1}{1+exp(-\theta_i^T x)}</script><p>​    这个函数的作用就是使得 $ P(i) $ 在负无穷到 0 的区间趋向于 0， 在 0 到正无穷的区间趋向 1,。同样 softmax 函数加入了 $ e $ 的幂函数正是为了两极化：正样本的结果将趋近于 1，而负样本的结果趋近于 0。这样为多类别提供了方便（可以把 $ P(i) $ 看做是样本属于类别的概率）。可以说，Softmax 函数是 logistic 函数的一种泛化。</p>
<p>​    softmax 函数可以把它的输入，通常被称为 logits 或者 logit scores，处理成 0 到 1 之间，并且能够把输出归一化到和为 1。这意味着 softmax 函数与分类的概率分布等价。它是一个网络预测多酚类问题的最佳输出激活函数。</p>
<h3 id="Softmax-函数如何应用于多分类？"><a href="#Softmax-函数如何应用于多分类？" class="headerlink" title="Softmax 函数如何应用于多分类？"></a>Softmax 函数如何应用于多分类？</h3><p>​    softmax 用于多分类过程中，它将多个神经元的输出，映射到 $ (0,1) $ 区间内，可以看成概率来理解，从而来进行多分类！</p>
<p>​    假设我们有一个数组，$ V_i $ 表示 $ V $  中的第 $ i $ 个元素，那么这个元素的 softmax 值就是</p>
<script type="math/tex; mode=display">
S_i = \frac{e^{V_i}}{\sum_j e^{V_j}}</script><p>​    从下图看，神经网络中包含了输入层，然后通过两个特征层处理，最后通过 softmax 分析器就能得到不同条件下的概率，这里需要分成三个类别，最终会得到 $ y=0, y=1, y=2 $ 的概率值。</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.4.9.1.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.4.9.1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>继续看下面的图，三个输入通过 softmax 后得到一个数组 $ [0.05 , 0.10 , 0.85] $，这就是 soft 的功能。</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.4.9.2.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.4.9.2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>更形象的映射过程如下图所示：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.4.9.3.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.4.9.3.png" srcset="data:image/png;base64,666" alt="****"></p>
<p>​    softmax 直白来说就是将原来输出是 $ 3,1,-3 $ 通过 softmax 函数一作用，就映射成为 $ (0,1) $ 的值，而这些值的累和为 $ 1 $（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标！</p>
<h3 id="交叉熵代价函数定义及其求导推导"><a href="#交叉熵代价函数定义及其求导推导" class="headerlink" title="交叉熵代价函数定义及其求导推导"></a>交叉熵代价函数定义及其求导推导</h3><p>​    神经元的输出就是 a = σ(z)，其中$z=\sum w<em>{j}i</em>{j}+b$是输⼊的带权和。</p>
<p>$C=-\frac{1}{n}\sum[ylna+(1-y)ln(1-a)]$</p>
<p>​    其中 n 是训练数据的总数，求和是在所有的训练输⼊ x 上进⾏的， y 是对应的⽬标输出。</p>
<p>​    表达式是否解决学习缓慢的问题并不明显。实际上，甚⾄将这个定义看做是代价函数也不是显⽽易⻅的！在解决学习缓慢前，我们来看看交叉熵为何能够解释成⼀个代价函数。</p>
<p>​    将交叉熵看做是代价函数有两点原因。</p>
<p>​    第⼀，它是⾮负的， C &gt; 0。可以看出：式子中的求和中的所有独⽴的项都是负数的，因为对数函数的定义域是 (0，1)，并且求和前⾯有⼀个负号，所以结果是非负。</p>
<p>​    第⼆，如果对于所有的训练输⼊ x，神经元实际的输出接近⽬标值，那么交叉熵将接近 0。</p>
<p>​    假设在这个例⼦中， y = 0 ⽽ a ≈ 0。这是我们想到得到的结果。我们看到公式中第⼀个项就消去了，因为 y = 0，⽽第⼆项实际上就是 − ln(1 − a) ≈ 0。反之， y = 1 ⽽ a ≈ 1。所以在实际输出和⽬标输出之间的差距越⼩，最终的交叉熵的值就越低了。（这里假设输出结果不是0，就是1，实际分类也是这样的）</p>
<p>​    综上所述，交叉熵是⾮负的，在神经元达到很好的正确率的时候会接近 0。这些其实就是我们想要的代价函数的特性。其实这些特性也是⼆次代价函数具备的。所以，交叉熵就是很好的选择了。但是交叉熵代价函数有⼀个⽐⼆次代价函数更好的特性就是它避免了学习速度下降的问题。为了弄清楚这个情况，我们来算算交叉熵函数关于权重的偏导数。我们将$a={\varsigma}(z)$代⼊到 公式中应⽤两次链式法则，得到：</p>
<p>$\begin{eqnarray}\frac{\partial C}{\partial w<em>{j}}&amp;=&amp;-\frac{1}{n}\sum \frac{\partial }{\partial w</em>{j}}[ylna+(1-y)ln(1-a)]\&amp;=&amp;-\frac{1}{n}\sum \frac{\partial }{\partial a}[ylna+(1-y)ln(1-a)]<em>\frac{\partial a}{\partial w_{j}}\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{a}-\frac{1-y}{1-a})</em>\frac{\partial a}{\partial w<em>{j}}\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)})\frac{\partial \varsigma(z)}{\partial w</em>{j}}\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)}){\varsigma}’(z)x_{j}\end{eqnarray}$</p>
<p>​    根据$\varsigma(z)=\frac{1}{1+e^{-z}}$ 的定义，和⼀些运算，我们可以得到 ${\varsigma}’(z)=\varsigma(z)(1-\varsigma(z))$。化简后可得：</p>
<p>$\frac{\partial C}{\partial w<em>{j}}=\frac{1}{n}\sum x</em>{j}({\varsigma}(z)-y)$</p>
<p>​    这是⼀个优美的公式。它告诉我们权重学习的速度受到$\varsigma(z)-y$，也就是输出中的误差的控制。更⼤的误差，更快的学习速度。这是我们直觉上期待的结果。特别地，这个代价函数还避免了像在⼆次代价函数中类似⽅程中${\varsigma}’(z)$导致的学习缓慢。当我们使⽤交叉熵的时候，${\varsigma}’(z)$被约掉了，所以我们不再需要关⼼它是不是变得很⼩。这种约除就是交叉熵带来的特效。实际上，这也并不是⾮常奇迹的事情。我们在后⾯可以看到，交叉熵其实只是满⾜这种特性的⼀种选择罢了。</p>
<p>​    根据类似的⽅法，我们可以计算出关于偏置的偏导数。我这⾥不再给出详细的过程，你可以轻易验证得到：</p>
<p>$\frac{\partial C}{\partial b}=\frac{1}{n}\sum ({\varsigma}(z)-y)$</p>
<p>​    再⼀次, 这避免了⼆次代价函数中类似${\varsigma}’(z)$项导致的学习缓慢。</p>
<h3 id="为什么Tanh收敛速度比Sigmoid快？"><a href="#为什么Tanh收敛速度比Sigmoid快？" class="headerlink" title="为什么Tanh收敛速度比Sigmoid快？"></a>为什么Tanh收敛速度比Sigmoid快？</h3><p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<p>首先看如下两个函数的求导：</p>
<p>$tanh^{,}(x)=1-tanh(x)^{2}\in (0,1)$</p>
<p>$s^{,}(x)=s(x)*(1-s(x))\in (0,\frac{1}{4}]$</p>
<p>由上面两个公式可知tanh(x)梯度消失的问题比sigmoid轻，所以Tanh收敛速度比Sigmoid快。</p>
<h2 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch_Size"></a>Batch_Size</h2><h3 id="为什么需要-Batch-Size？"><a href="#为什么需要-Batch-Size？" class="headerlink" title="为什么需要 Batch_Size？"></a>为什么需要 Batch_Size？</h3><p>Batch的选择，首先决定的是下降的方向。</p>
<p>如果数据集比较小，可采用全数据集的形式，好处是：</p>
<ol>
<li>由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。</li>
<li>由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full Batch Learning 可以使用 Rprop 只基于梯度符号并且针对性单独更新各权值。</li>
</ol>
<p>对于更大的数据集，假如采用全数据集的形式，坏处是：</p>
<ol>
<li>随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。</li>
<li>以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 RMSProp 的妥协方案。 </li>
</ol>
<h3 id="Batch-Size-值的选择"><a href="#Batch-Size-值的选择" class="headerlink" title="Batch_Size 值的选择"></a>Batch_Size 值的选择</h3><p>​    假如每次只训练一个样本，即 Batch_Size = 1。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。此时，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。</p>
<p>​    既然 Batch_Size 为全数据集或者Batch_Size = 1都有各自缺点，可不可以选择一个适中的Batch_Size值呢？</p>
<p>​    此时，可采用批梯度下降法（Mini-batches Learning）。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。</p>
<h3 id="在合理范围内，增大Batch-Size有何好处？"><a href="#在合理范围内，增大Batch-Size有何好处？" class="headerlink" title="在合理范围内，增大Batch_Size有何好处？"></a>在合理范围内，增大Batch_Size有何好处？</h3><ol>
<li>内存利用率提高了，大矩阵乘法的并行化效率提高。</li>
<li>跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。</li>
<li>在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。</li>
</ol>
<h3 id="盲目增大-Batch-Size-有何坏处？"><a href="#盲目增大-Batch-Size-有何坏处？" class="headerlink" title="盲目增大 Batch_Size 有何坏处？"></a>盲目增大 Batch_Size 有何坏处？</h3><ol>
<li>内存利用率提高了，但是内存容量可能撑不住了。</li>
<li>跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。</li>
<li>Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。</li>
</ol>
<h3 id="调节-Batch-Size-对训练效果影响到底如何？"><a href="#调节-Batch-Size-对训练效果影响到底如何？" class="headerlink" title="调节 Batch_Size 对训练效果影响到底如何？"></a>调节 Batch_Size 对训练效果影响到底如何？</h3><ol>
<li>Batch_Size 太小，模型表现效果极其糟糕(error飙升)。</li>
<li>随着 Batch_Size 增大，处理相同数据量的速度越快。</li>
<li>随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。</li>
<li>由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。</li>
<li>由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。 </li>
</ol>
<h2 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h2><h3 id="归一化含义"><a href="#归一化含义" class="headerlink" title="归一化含义"></a>归一化含义</h3><ol>
<li><p>归纳统一样本的统计分布性。归一化在 $ 0-1$ 之间是统计的概率分布，归一化在$ -1—+1$ 之间是统计的坐标分布。</p>
</li>
<li><p>无论是为了建模还是为了计算，首先基本度量单位要同一，神经网络是以样本在事件中的统计分别几率来进行训练（概率计算）和预测，且 sigmoid 函数的取值是 0 到 1 之间的，网络最后一个节点的输出也是如此，所以经常要对样本的输出归一化处理。</p>
</li>
<li><p>归一化是统一在 $ 0-1 $ 之间的统计概率分布，当所有样本的输入信号都为正值时，与第一隐含层神经元相连的权值只能同时增加或减小，从而导致学习速度很慢。</p>
</li>
<li><p>另外在数据中常存在奇异样本数据，奇异样本数据存在所引起的网络训练时间增加，并可能引起网络无法收敛。为了避免出现这种情况及后面数据处理的方便，加快网络学习速度，可以对输入信号进行归一化，使得所有样本的输入信号其均值接近于 0 或与其均方差相比很小。</p>
</li>
</ol>
<h3 id="为什么要归一化"><a href="#为什么要归一化" class="headerlink" title="为什么要归一化"></a>为什么要归一化</h3><ol>
<li>为了后面数据处理的方便，归一化的确可以避免一些不必要的数值问题。</li>
<li>为了程序运行时收敛加快。 </li>
<li>同一量纲。样本数据的评价标准不一样，需要对其量纲化，统一评价标准。这算是应用层面的需求。</li>
<li>避免神经元饱和。啥意思？就是当神经元的激活在接近 0 或者 1 时会饱和，在这些区域，梯度几乎为 0，这样，在反向传播过程中，局部梯度就会接近 0，这会有效地“杀死”梯度。</li>
<li>保证输出数据中数值小的不被吞食。 </li>
</ol>
<h3 id="为什么归一化能提高求解最优解速度"><a href="#为什么归一化能提高求解最优解速度" class="headerlink" title="为什么归一化能提高求解最优解速度"></a>为什么归一化能提高求解最优解速度</h3><p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.6.3.1.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.6.3.1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    上图是代表数据是否均一化的最优解寻解过程（圆圈可以理解为等高线）。左图表示未经归一化操作的寻解过程，右图表示经过归一化后的寻解过程。</p>
<p>​    当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。</p>
<p>​    因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。</p>
<h3 id="3D-图解未归一化"><a href="#3D-图解未归一化" class="headerlink" title="3D 图解未归一化"></a>3D 图解未归一化</h3><p>例子：</p>
<p>​    假设 $ w1 $ 的范围在 $ [-10, 10] $，而 $ w2 $ 的范围在 $ [-100, 100] $，梯度每次都前进 1 单位，那么在 $ w1 $ 方向上每次相当于前进了 $ 1/20 $，而在 $ w2 $ 上只相当于 $ 1/200 $！某种意义上来说，在 $ w2 $ 上前进的步长更小一些,而 $ w1 $ 在搜索过程中会比 $ w2 $ “走”得更快。</p>
<p>​    这样会导致，在搜索过程中更偏向于 $ w1 $ 的方向。走出了“L”形状，或者成为“之”字形。</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-37.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3-37.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="归一化有哪些类型"><a href="#归一化有哪些类型" class="headerlink" title="归一化有哪些类型"></a>归一化有哪些类型</h3><ol>
<li>线性归一化</li>
</ol>
<script type="math/tex; mode=display">
x^{\prime} = \frac{x-min(x)}{max(x) - min(x)}</script><p>​    适用范围：比较适用在数值比较集中的情况。</p>
<p>​    缺点：如果 max 和 min 不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。</p>
<ol>
<li>标准差标准化</li>
</ol>
<script type="math/tex; mode=display">
x^{\prime} = \frac{x-\mu}{\sigma}</script><p>​    含义：经过处理的数据符合标准正态分布，即均值为 0，标准差为 1 其中 $ \mu $ 为所有样本数据的均值，$ \sigma $ 为所有样本数据的标准差。</p>
<ol>
<li><p>非线性归一化</p>
<p>适用范围：经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 $ log $、指数，正切等。</p>
</li>
</ol>
<h3 id="局部响应归一化作用"><a href="#局部响应归一化作用" class="headerlink" title="局部响应归一化作用"></a>局部响应归一化作用</h3><p>​    LRN 是一种提高深度学习准确度的技术方法。LRN 一般是在激活、池化函数后的一种方法。</p>
<p>​    在 ALexNet 中，提出了 LRN 层，对局部神经元的活动创建竞争机制，使其中响应比较大对值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。</p>
<h3 id="理解局部响应归一化"><a href="#理解局部响应归一化" class="headerlink" title="理解局部响应归一化"></a>理解局部响应归一化</h3><p>​    局部响应归一化原理是仿造生物学上活跃的神经元对相邻神经元的抑制现象（侧抑制），其公式如下：</p>
<script type="math/tex; mode=display">
b_{x,y}^i = a_{x,y}^i / (k + \alpha \sum_{j=max(0, i-n/2)}^{min(N-1, i+n/2)}(a_{x,y}^j)^2 )^\beta</script><p>其中，<br>1) $ a $：表示卷积层（包括卷积操作和池化操作）后的输出结果，是一个四维数组[batch,height,width,channel]。</p>
<ul>
<li>batch：批次数(每一批为一张图片)。</li>
<li>height：图片高度。</li>
<li>width：图片宽度。</li>
<li>channel：通道数。可以理解成一批图片中的某一个图片经过卷积操作后输出的神经元个数，或理解为处理后的图片深度。</li>
</ul>
<p>2) $ a_{x,y}^i $ 表示在这个输出结构中的一个位置 $ [a,b,c,d] $，可以理解成在某一张图中的某一个通道下的某个高度和某个宽度位置的点，即第 $ a $ 张图的第 $ d $ 个通道下的高度为b宽度为c的点。</p>
<p>3) $ N $：论文公式中的 $ N $ 表示通道数 (channel)。</p>
<p>4) $ a $，$ n/2 $， $ k $ 分别表示函数中的 input,depth_radius,bias。参数 $ k, n, \alpha, \beta $ 都是超参数，一般设置 $ k=2, n=5, \alpha=1*e-4, \beta=0.75 $</p>
<p>5) $ \sum $：$ \sum $ 叠加的方向是沿着通道方向的，即每个点值的平方和是沿着 $ a $ 中的第 3 维 channel 方向的，也就是一个点同方向的前面 $ n/2 $ 个通道（最小为第 $ 0 $ 个通道）和后 $ n/2 $ 个通道（最大为第 $ d-1 $ 个通道）的点的平方和(共 $ n+1 $ 个点)。而函数的英文注解中也说明了把 input 当成是 $ d $ 个 3 维的矩阵，说白了就是把 input 的通道数当作 3 维矩阵的个数，叠加的方向也是在通道方向。 </p>
<p>简单的示意图如下：</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.6.7.1.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.6.7.1.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="什么是批归一化（Batch-Normalization）"><a href="#什么是批归一化（Batch-Normalization）" class="headerlink" title="什么是批归一化（Batch Normalization）"></a>什么是批归一化（Batch Normalization）</h3><p>​    以前在神经网络训练中，只是对输入层数据进行归一化处理，却没有在中间层进行归一化处理。要知道，虽然我们对输入数据进行了归一化处理，但是输入数据经过 $ \sigma(WX+b) $ 这样的矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而随着深度网络的多层运算之后，数据分布的变化将越来越大。如果我们能在网络的中间也进行归一化处理，是否对网络的训练起到改进作用呢？答案是肯定的。 </p>
<p>​    这种在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化Batch Normalization（BN）。</p>
<h3 id="批归一化（BN）算法的优点"><a href="#批归一化（BN）算法的优点" class="headerlink" title="批归一化（BN）算法的优点"></a>批归一化（BN）算法的优点</h3><p>下面我们来说一下BN算法的优点： </p>
<ol>
<li>减少了人为选择参数。在某些情况下可以取消 dropout 和 L2 正则项参数,或者采取更小的 L2 正则项约束参数； </li>
<li>减少了对学习率的要求。现在我们可以使用初始很大的学习率或者选择了较小的学习率，算法也能够快速训练收敛； </li>
<li>可以不再使用局部响应归一化。BN 本身就是归一化网络(局部响应归一化在 AlexNet 网络中存在) </li>
<li>破坏原来的数据分布，一定程度上缓解过拟合（防止每批训练中某一个样本经常被挑选到，文献说这个可以提高 1% 的精度）。 </li>
<li>减少梯度消失，加快收敛速度，提高训练精度。</li>
</ol>
<h3 id="批归一化（BN）算法流程"><a href="#批归一化（BN）算法流程" class="headerlink" title="批归一化（BN）算法流程"></a>批归一化（BN）算法流程</h3><p>下面给出 BN 算法在训练时的过程</p>
<p>输入：上一层输出结果 $ X = {x_1, x_2, …, x_m} $，学习参数 $ \gamma, \beta $</p>
<p>算法流程：</p>
<ol>
<li>计算上一层输出数据的均值</li>
</ol>
<script type="math/tex; mode=display">
\mu_{\beta} = \frac{1}{m} \sum_{i=1}^m(x_i)</script><p>其中，$ m $ 是此次训练样本 batch 的大小。</p>
<ol>
<li>计算上一层输出数据的标准差</li>
</ol>
<script type="math/tex; mode=display">
\sigma_{\beta}^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\beta})^2</script><ol>
<li>归一化处理，得到</li>
</ol>
<script type="math/tex; mode=display">
\hat x_i = \frac{x_i + \mu_{\beta}}{\sqrt{\sigma_{\beta}^2} + \epsilon}</script><p>其中 $ \epsilon $ 是为了避免分母为 0 而加进去的接近于 0 的很小值</p>
<ol>
<li>重构，对经过上面归一化处理得到的数据进行重构，得到</li>
</ol>
<script type="math/tex; mode=display">
y_i = \gamma \hat x_i + \beta</script><p>其中，$ \gamma, \beta $ 为可学习参数。</p>
<p>注：上述是 BN 训练时的过程，但是当在投入使用时，往往只是输入一个样本，没有所谓的均值 $ \mu<em>{\beta} $ 和标准差 $ \sigma</em>{\beta}^2 $。此时，均值 $ \mu<em>{\beta} $ 是计算所有 batch $ \mu</em>{\beta} $ 值的平均值得到，标准差 $ \sigma<em>{\beta}^2 $ 采用每个batch $ \sigma</em>{\beta}^2 $  的无偏估计得到。</p>
<h3 id="批归一化和群组归一化比较"><a href="#批归一化和群组归一化比较" class="headerlink" title="批归一化和群组归一化比较"></a>批归一化和群组归一化比较</h3><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th style="text-align:left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>批量归一化（Batch Normalization，以下简称 BN）</td>
<td style="text-align:left">可让各种网络并行训练。但是，批量维度进行归一化会带来一些问题——批量统计估算不准确导致批量变小时，BN 的误差会迅速增加。在训练大型网络和将特征转移到计算机视觉任务中（包括检测、分割和视频），内存消耗限制了只能使用小批量的 BN。</td>
</tr>
<tr>
<td>群组归一化 Group Normalization (简称 GN)</td>
<td style="text-align:left">GN 将通道分成组，并在每组内计算归一化的均值和方差。GN 的计算与批量大小无关，并且其准确度在各种批量大小下都很稳定。</td>
</tr>
<tr>
<td>比较</td>
<td style="text-align:left">在 ImageNet 上训练的 ResNet-50上，GN 使用批量大小为 2 时的错误率比 BN 的错误率低 10.6％ ;当使用典型的批量时，GN 与 BN 相当，并且优于其他标归一化变体。而且，GN 可以自然地从预训练迁移到微调。在进行 COCO 中的目标检测和分割以及 Kinetics 中的视频分类比赛中，GN 可以胜过其竞争对手，表明 GN 可以在各种任务中有效地取代强大的 BN。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Weight-Normalization和Batch-Normalization比较"><a href="#Weight-Normalization和Batch-Normalization比较" class="headerlink" title="Weight Normalization和Batch Normalization比较"></a>Weight Normalization和Batch Normalization比较</h3><p>​    Weight Normalization 和 Batch Normalization 都属于参数重写（Reparameterization）的方法，只是采用的方式不同。</p>
<p>​    Weight Normalization 是对网络权值$  W $ 进行 normalization，因此也称为 Weight Normalization；</p>
<p>​    Batch Normalization 是对网络某一层输入数据进行 normalization。</p>
<p>​    Weight Normalization相比Batch Normalization有以下三点优势：</p>
<ol>
<li><p>Weight Normalization 通过重写深度学习网络的权重W的方式来加速深度学习网络参数收敛，没有引入 minbatch 的依赖，适用于 RNN（LSTM）网络（Batch Normalization 不能直接用于RNN，进行 normalization 操作，原因在于：1) RNN 处理的 Sequence 是变长的；2) RNN 是基于 time step 计算，如果直接使用 Batch Normalization 处理，需要保存每个 time step 下，mini btach 的均值和方差，效率低且占内存）。</p>
</li>
<li><p>Batch Normalization 基于一个 mini batch 的数据计算均值和方差，而不是基于整个 Training set 来做，相当于进行梯度计算式引入噪声。因此，Batch Normalization 不适用于对噪声敏感的强化学习、生成模型（Generative model：GAN，VAE）使用。相反，Weight Normalization 对通过标量 $ g $ 和向量 $ v $ 对权重 $ W $ 进行重写，重写向量 $ v $ 是固定的，因此，基于 Weight Normalization 的 Normalization 可以看做比 Batch Normalization 引入更少的噪声。    </p>
</li>
<li><p>不需要额外的存储空间来保存 mini batch 的均值和方差，同时实现 Weight Normalization 时，对深度学习网络进行正向信号传播和反向梯度计算带来的额外计算开销也很小。因此，要比采用 Batch Normalization 进行 normalization 操作时，速度快。  但是 Weight Normalization 不具备 Batch Normalization 把网络每一层的输出 Y 固定在一个变化范围的作用。因此，采用 Weight Normalization 进行 Normalization 时需要特别注意参数初始值的选择。</p>
</li>
</ol>
<h3 id="Batch-Normalization在什么时候用比较合适？"><a href="#Batch-Normalization在什么时候用比较合适？" class="headerlink" title="Batch Normalization在什么时候用比较合适？"></a>Batch Normalization在什么时候用比较合适？</h3><p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<p>​    在CNN中，BN应作用在非线性映射前。在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。</p>
<p>​    BN比较适用的场景是：每个mini-batch比较大，数据分布比较接近。在进行训练之前，要做好充分的shuffle，否则效果会差很多。另外，由于BN需要在运行过程中统计每个mini-batch的一阶统计量和二阶统计量，因此不适用于动态的网络结构和RNN网络。</p>
<h2 id="预训练与微调-fine-tuning"><a href="#预训练与微调-fine-tuning" class="headerlink" title="预训练与微调(fine tuning)"></a>预训练与微调(fine tuning)</h2><h3 id="为什么无监督预训练可以帮助深度学习？"><a href="#为什么无监督预训练可以帮助深度学习？" class="headerlink" title="为什么无监督预训练可以帮助深度学习？"></a>为什么无监督预训练可以帮助深度学习？</h3><p>深度网络存在问题:</p>
<ol>
<li><p>网络越深，需要的训练样本数越多。若用监督则需大量标注样本，不然小规模样本容易造成过拟合。深层网络特征比较多，会出现的多特征问题主要有多样本问题、规则化问题、特征选择问题。</p>
</li>
<li><p>多层神经网络参数优化是个高阶非凸优化问题，经常得到收敛较差的局部解；</p>
</li>
<li><p>梯度扩散问题，BP算法计算出的梯度随着深度向前而显著下降，导致前面网络参数贡献很小，更新速度慢。</p>
</li>
</ol>
<p><strong>解决方法：</strong></p>
<p>​    逐层贪婪训练，无监督预训练（unsupervised pre-training）即训练网络的第一个隐藏层，再训练第二个…最后用这些训练好的网络参数值作为整体网络参数的初始值。</p>
<p>经过预训练最终能得到比较好的局部最优解。</p>
<h3 id="什么是模型微调fine-tuning"><a href="#什么是模型微调fine-tuning" class="headerlink" title="什么是模型微调fine tuning"></a>什么是模型微调fine tuning</h3><p>​    用别人的参数、修改后的网络和自己的数据进行训练，使得参数适应自己的数据，这样一个过程，通常称之为微调（fine tuning). </p>
<p><strong>模型的微调举例说明：</strong></p>
<p>​    我们知道，CNN 在图像识别这一领域取得了巨大的进步。如果想将 CNN 应用到我们自己的数据集上，这时通常就会面临一个问题：通常我们的 dataset 都不会特别大，一般不会超过 1 万张，甚至更少，每一类图片只有几十或者十几张。这时候，直接应用这些数据训练一个网络的想法就不可行了，因为深度学习成功的一个关键性因素就是大量带标签数据组成的训练集。如果只利用手头上这点数据，即使我们利用非常好的网络结构，也达不到很高的 performance。这时候，fine-tuning 的思想就可以很好解决我们的问题：我们通过对 ImageNet 上训练出来的模型（如CaffeNet,VGGNet,ResNet) 进行微调，然后应用到我们自己的数据集上。</p>
<h3 id="微调时候网络参数是否更新？"><a href="#微调时候网络参数是否更新？" class="headerlink" title="微调时候网络参数是否更新？"></a>微调时候网络参数是否更新？</h3><p>答案：会更新。</p>
<ol>
<li>finetune 的过程相当于继续训练，跟直接训练的区别是初始化的时候。 </li>
<li>直接训练是按照网络定义指定的方式初始化。</li>
<li>finetune是用你已经有的参数文件来初始化。</li>
</ol>
<h3 id="fine-tuning-模型的三种状态"><a href="#fine-tuning-模型的三种状态" class="headerlink" title="fine-tuning 模型的三种状态"></a>fine-tuning 模型的三种状态</h3><ol>
<li><p>状态一：只预测，不训练。<br>特点：相对快、简单，针对那些已经训练好，现在要实际对未知数据进行标注的项目，非常高效；</p>
</li>
<li><p>状态二：训练，但只训练最后分类层。<br>特点：fine-tuning的模型最终的分类以及符合要求，现在只是在他们的基础上进行类别降维。</p>
</li>
<li><p>状态三：完全训练，分类层+之前卷积层都训练<br>特点：跟状态二的差异很小，当然状态三比较耗时和需要训练GPU资源，不过非常适合fine-tuning到自己想要的模型里面，预测精度相比状态二也提高不少。</p>
</li>
</ol>
<h2 id="权重偏差初始化"><a href="#权重偏差初始化" class="headerlink" title="权重偏差初始化"></a>权重偏差初始化</h2><h3 id="全都初始化为-0"><a href="#全都初始化为-0" class="headerlink" title="全都初始化为 0"></a>全都初始化为 0</h3><p><strong>偏差初始化陷阱</strong>： 都初始化为 0。</p>
<p><strong>产生陷阱原因</strong>：因为并不知道在训练神经网络中每一个权重最后的值，但是如果进行了恰当的数据归一化后，我们可以有理由认为有一半的权重是正的，另一半是负的。令所有权重都初始化为 0，如果神经网络计算出来的输出值是一样的，神经网络在进行反向传播算法计算出来的梯度值也一样，并且参数更新值也一样。更一般地说，如果权重初始化为同一个值，网络就是对称的。</p>
<p><strong>形象化理解</strong>：在神经网络中考虑梯度下降的时候，设想你在爬山，但身处直线形的山谷中，两边是对称的山峰。由于对称性，你所在之处的梯度只能沿着山谷的方向，不会指向山峰；你走了一步之后，情况依然不变。结果就是你只能收敛到山谷中的一个极大值，而走不到山峰上去。</p>
<h3 id="全都初始化为同样的值"><a href="#全都初始化为同样的值" class="headerlink" title="全都初始化为同样的值"></a>全都初始化为同样的值</h3><p>​    偏差初始化陷阱： 都初始化为一样的值。<br>​    以一个三层网络为例：<br>首先看下结构</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.8.2.1.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.8.2.1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>它的表达式为： </p>
<script type="math/tex; mode=display">
a_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})</script><script type="math/tex; mode=display">
a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + b_2^{(1)})</script><script type="math/tex; mode=display">
a_3^{(2)} = f(W_{31}^{(1)} x_1 + W_{32}^{(1)} x_2 + W_{33}^{(1)} x_3 + b_3^{(1)})</script><script type="math/tex; mode=display">
h_{W,b}(x) = a_1^{(3)} = f(W_{11}^{(2)} a_1^{(2)} + W_{12}^{(2)} a_2^{(2)} + W_{13}^{(2)} a_3^{(2)} + b_1^{(2)})</script><script type="math/tex; mode=display">
xa_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 +</script><p>如果每个权重都一样，那么在多层网络中，从第二层开始，每一层的输入值都是相同的了也就是$ a1=a2=a3=…. $，既然都一样，就相当于一个输入了，为啥呢？？</p>
<p>如果是反向传递算法（如果这里不明白请看上面的连接），其中的偏置项和权重项的迭代的偏导数计算公式如下</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b;x,y) = a_j^{(l)} \delta_i^{(l+1)}

\frac{\partial}{\partial b_{i}^{(l)}} J(W,b;x,y) = \delta_i^{(l+1)}</script><p>$ \delta $ 的计算公式</p>
<script type="math/tex; mode=display">
\delta_i^{(l)} = (\sum_{j=1}^{s_{t+1}} W_{ji}^{(l)} \delta_j^{(l+1)} ) f^{\prime}(z_i^{(l)})</script><p>如果用的是 sigmoid 函数</p>
<script type="math/tex; mode=display">
f^{\prime}(z_i^{(l)}) = a_i^{(l)}(1-a_i^{(l)})</script><p>把后两个公式代入，可以看出所得到的梯度下降法的偏导相同，不停的迭代，不停的相同，不停的迭代，不停的相同……，最后就得到了相同的值（权重和截距）。</p>
<h3 id="初始化为小的随机数"><a href="#初始化为小的随机数" class="headerlink" title="初始化为小的随机数"></a>初始化为小的随机数</h3><p>​    将权重初始化为很小的数字是一个普遍的打破网络对称性的解决办法。这个想法是，神经元在一开始都是随机的、独一无二的，所以它们会计算出不同的更新，并将自己整合到整个网络的各个部分。一个权重矩阵的实现可能看起来像 $ W=0.01∗np.random.randn(D,H) $，其中 randn 是从均值为 0 的单位标准高斯分布进行取样。通过这个公式(函数)，每个神经元的权重向量初始化为一个从多维高斯分布取样的随机向量，所以神经元在输入空间中指向随机的方向(so the neurons point in random direction in the input space). 应该是指输入空间对于随机方向有影响)。其实也可以从均匀分布中来随机选取小数，但是在实际操作中看起来似乎对最后的表现并没有太大的影响。</p>
<p>​    备注：并不是数字越小就会表现的越好。比如，如果一个神经网络层的权重非常小，那么在反向传播算法就会计算出很小的梯度(因为梯度 gradient 是与权重成正比的)。在网络不断的反向传播过程中将极大地减少“梯度信号”，并可能成为深层网络的一个需要注意的问题。</p>
<h3 id="用-1-sqrt-n-校准方差"><a href="#用-1-sqrt-n-校准方差" class="headerlink" title="用 $ 1/\sqrt n $ 校准方差"></a>用 $ 1/\sqrt n $ 校准方差</h3><p>​    上述建议的一个问题是，随机初始化神经元的输出的分布有一个随输入量增加而变化的方差。结果证明，我们可以通过将其权重向量按其输入的平方根(即输入的数量)进行缩放，从而将每个神经元的输出的方差标准化到 1。也就是说推荐的启发式方法 (heuristic) 是将每个神经元的权重向量按下面的方法进行初始化: $ w=np.random.randn(n)/\sqrt n $，其中 n 表示输入的数量。这保证了网络中所有的神经元最初的输出分布大致相同，并在经验上提高了收敛速度。</p>
<h3 id="稀疏初始化-Sparse-Initialazation"><a href="#稀疏初始化-Sparse-Initialazation" class="headerlink" title="稀疏初始化(Sparse Initialazation)"></a>稀疏初始化(Sparse Initialazation)</h3><p>​    另一种解决未校准方差问题的方法是把所有的权重矩阵都设为零，但是为了打破对称性，每个神经元都是随机连接地(从如上面所介绍的一个小的高斯分布中抽取权重)到它下面的一个固定数量的神经元。一个典型的神经元连接的数目可能是小到 10 个。</p>
<h3 id="初始化偏差"><a href="#初始化偏差" class="headerlink" title="初始化偏差"></a>初始化偏差</h3><p>​    将偏差初始化为零是可能的，也是很常见的，因为非对称性破坏是由权重的小随机数导致的。因为 ReLU 具有非线性特点，所以有些人喜欢使用将所有的偏差设定为小的常数值如 0.01，因为这样可以确保所有的 ReLU 单元在最开始就激活触发(fire)并因此能够获得和传播一些梯度值。然而，这是否能够提供持续的改善还不太清楚(实际上一些结果表明这样做反而使得性能更加糟糕)，所以更通常的做法是简单地将偏差初始化为 0.</p>
<h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><h3 id="学习率的作用"><a href="#学习率的作用" class="headerlink" title="学习率的作用"></a>学习率的作用</h3><p>​    在机器学习中，监督式学习通过定义一个模型，并根据训练集上的数据估计最优参数。梯度下降法是一个广泛被用来最小化模型误差的参数优化算法。梯度下降法通过多次迭代，并在每一步中最小化成本函数（cost 来估计模型的参数。学习率 (learning rate)，在迭代过程中会控制模型的学习进度。</p>
<p>​    在梯度下降法中，都是给定的统一的学习率，整个优化过程中都以确定的步长进行更新， 在迭代优化的前期中，学习率较大，则前进的步长就会较长，这时便能以较快的速度进行梯度下降，而在迭代优化的后期，逐步减小学习率的值，减小步长，这样将有助于算法的收敛，更容易接近最优解。故而如何对学习率的更新成为了研究者的关注点。<br>​    在模型优化中，常用到的几种学习率衰减方法有：分段常数衰减、多项式衰减、指数衰减、自然指数衰减、余弦衰减、线性余弦衰减、噪声线性余弦衰减</p>
<h3 id="学习率衰减常用参数有哪些"><a href="#学习率衰减常用参数有哪些" class="headerlink" title="学习率衰减常用参数有哪些"></a>学习率衰减常用参数有哪些</h3><div class="table-container">
<table>
<thead>
<tr>
<th>参数名称</th>
<th>参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>learning_rate</td>
<td>初始学习率</td>
</tr>
<tr>
<td>global_step</td>
<td>用于衰减计算的全局步数，非负，用于逐步计算衰减指数</td>
</tr>
<tr>
<td>decay_steps</td>
<td>衰减步数，必须是正值，决定衰减周期</td>
</tr>
<tr>
<td>decay_rate</td>
<td>衰减率</td>
</tr>
<tr>
<td>end_learning_rate</td>
<td>最低的最终学习率</td>
</tr>
<tr>
<td>cycle</td>
<td>学习率下降后是否重新上升</td>
</tr>
<tr>
<td>alpha</td>
<td>最小学习率</td>
</tr>
<tr>
<td>num_periods</td>
<td>衰减余弦部分的周期数</td>
</tr>
<tr>
<td>initial_variance</td>
<td>噪声的初始方差</td>
</tr>
<tr>
<td>variance_decay</td>
<td>衰减噪声的方差</td>
</tr>
</tbody>
</table>
</div>
<h3 id="分段常数衰减"><a href="#分段常数衰减" class="headerlink" title="分段常数衰减"></a>分段常数衰减</h3><p>​    分段常数衰减需要事先定义好的训练次数区间，在对应区间置不同的学习率的常数值，一般情况刚开始的学习率要大一些，之后要越来越小，要根据样本量的大小设置区间的间隔大小，样本量越大，区间间隔要小一点。下图即为分段常数衰减的学习率变化图，横坐标代表训练次数，纵坐标代表学习率。</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/learnrate1.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/learnrate1.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="指数衰减"><a href="#指数衰减" class="headerlink" title="指数衰减"></a>指数衰减</h3><p>​    以指数衰减方式进行学习率的更新，学习率的大小和训练次数指数相关，其更新规则为：</p>
<script type="math/tex; mode=display">
decayed{\_}learning{\_}rate =learning{\_}rate*decay{\_}rate^{\frac{global{\_step}}{decay{\_}steps}}</script><p>​    这种衰减方式简单直接，收敛速度快，是最常用的学习率衰减方式，如下图所示，绿色的为学习率随<br>训练次数的指数衰减方式，红色的即为分段常数衰减，它在一定的训练区间内保持学习率不变。</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/learnrate2.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/learnrate2.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="自然指数衰减"><a href="#自然指数衰减" class="headerlink" title="自然指数衰减"></a>自然指数衰减</h3><p>​    它与指数衰减方式相似，不同的在于它的衰减底数是$e$，故而其收敛的速度更快，一般用于相对比较<br>容易训练的网络，便于较快的收敛，其更新规则如下</p>
<script type="math/tex; mode=display">
decayed{\_}learning{\_}rate =learning{\_}rate*e^{\frac{-decay{\_rate}}{global{\_}step}}</script><p>​    下图为为分段常数衰减、指数衰减、自然指数衰减三种方式的对比图，红色的即为分段常数衰减图，阶梯型曲线。蓝色线为指数衰减图，绿色即为自然指数衰减图，很明可以看到自然指数衰减方式下的学习率衰减程度要大于一般指数衰减方式，有助于更快的收敛。</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/learnrate3.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/learnrate3.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="多项式衰减"><a href="#多项式衰减" class="headerlink" title="多项式衰减"></a>多项式衰减</h3><p>​    应用多项式衰减的方式进行更新学习率，这里会给定初始学习率和最低学习率取值，然后将会按照<br>给定的衰减方式将学习率从初始值衰减到最低值,其更新规则如下式所示。</p>
<script type="math/tex; mode=display">
global{\_}step=min(global{\_}step,decay{\_}steps)</script><script type="math/tex; mode=display">
decayed{\_}learning{\_}rate =(learning{\_}rate-end{\_}learning{\_}rate)* \left( 1-\frac{global{\_step}}{decay{\_}steps}\right)^{power} \\
 +end{\_}learning{\_}rate</script><p>​    需要注意的是，有两个机制，降到最低学习率后，到训练结束可以一直使用最低学习率进行更新，另一个是再次将学习率调高，使用 decay_steps 的倍数，取第一个大于 global_steps 的结果，如下式所示.它是用来防止神经网络在训练的后期由于学习率过小而导致的网络一直在某个局部最小值附近震荡，这样可以通过在后期增大学习率跳出局部极小值。</p>
<script type="math/tex; mode=display">
decay{\_}steps = decay{\_}steps*ceil \left( \frac{global{\_}step}{decay{\_}steps}\right)</script><p>​    如下图所示，红色线代表学习率降低至最低后，一直保持学习率不变进行更新，绿色线代表学习率衰减到最低后，又会再次循环往复的升高降低。</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/learnrate4.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/learnrate4.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="余弦衰减"><a href="#余弦衰减" class="headerlink" title="余弦衰减"></a>余弦衰减</h3><p>​    余弦衰减就是采用余弦的相关方式进行学习率的衰减，衰减图和余弦函数相似。其更新机制如下式所示：</p>
<script type="math/tex; mode=display">
global{\_}step=min(global{\_}step,decay{\_}steps)</script><script type="math/tex; mode=display">
cosine{\_}decay=0.5*\left( 1+cos\left( \pi* \frac{global{\_}step}{decay{\_}steps}\right)\right)</script><script type="math/tex; mode=display">
decayed=(1-\alpha)*cosine{\_}decay+\alpha</script><script type="math/tex; mode=display">
decayed{\_}learning{\_}rate=learning{\_}rate*decayed</script><p>​    如下图所示，红色即为标准的余弦衰减曲线，学习率从初始值下降到最低学习率后保持不变。蓝色的线是线性余弦衰减方式曲线，它是学习率从初始学习率以线性的方式下降到最低学习率值。绿色噪声线性余弦衰减方式。</p>
<p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/learnrate5.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/learnrate5.png" srcset="data:image/png;base64,666" alt=""></p>
<h2 id="Dropout-系列问题"><a href="#Dropout-系列问题" class="headerlink" title="Dropout 系列问题"></a>Dropout 系列问题</h2><h3 id="为什么要正则化？"><a href="#为什么要正则化？" class="headerlink" title="为什么要正则化？"></a>为什么要正则化？</h3><ol>
<li>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。  </li>
<li>如果你怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高，但正则化有助于避免过度拟合，或者减少网络误差。</li>
</ol>
<h3 id="为什么正则化有利于预防过拟合？"><a href="#为什么正则化有利于预防过拟合？" class="headerlink" title="为什么正则化有利于预防过拟合？"></a>为什么正则化有利于预防过拟合？</h3><p><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.12.2.1.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.12.2.1.png" srcset="data:image/png;base64,666" alt=""><br><img src="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.12.2.2.png" class="lazyload" data-srcset="/zh-TW/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ch3/3.12.2.2.png" srcset="data:image/png;base64,666" alt=""> </p>
<p>左图是高偏差，右图是高方差，中间是Just Right，这几张图我们在前面课程中看到过。  </p>
<h3 id="理解dropout正则化"><a href="#理解dropout正则化" class="headerlink" title="理解dropout正则化"></a>理解dropout正则化</h3><p>​    Dropout可以随机删除网络中的神经单元，它为什么可以通过正则化发挥如此大的作用呢？  </p>
<p>​    直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。  </p>
<h3 id="dropout率的选择"><a href="#dropout率的选择" class="headerlink" title="dropout率的选择"></a>dropout率的选择</h3><ol>
<li>经过交叉验证，隐含节点 dropout 率等于 0.5 的时候效果最好，原因是 0.5 的时候 dropout 随机生成的网络结构最多。</li>
<li>dropout 也可以被用作一种添加噪声的方法，直接对 input 进行操作。输入层设为更接近 1 的数。使得输入变化不会太大（0.8） </li>
<li>对参数 $ w $ 的训练进行球形限制 (max-normalization)，对 dropout 的训练非常有用。</li>
<li>球形半径 $ c $ 是一个需要调整的参数，可以使用验证集进行参数调优。</li>
<li>dropout 自己虽然也很牛，但是 dropout、max-normalization、large decaying learning rates and high momentum 组合起来效果更好，比如 max-norm regularization 就可以防止大的learning rate 导致的参数 blow up。</li>
<li>使用 pretraining 方法也可以帮助 dropout 训练参数，在使用 dropout 时，要将所有参数都乘以 $ 1/p $。</li>
</ol>
<h3 id="dropout缺点"><a href="#dropout缺点" class="headerlink" title="dropout缺点"></a>dropout缺点</h3><p>​    dropout一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数J每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。我通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。我觉得你也可以尝试其它方法，虽然我们并没有关于这些方法性能的数据统计，但你可以把它们与dropout方法一起使用。  </p>
<h2 id="深度学习中常用的数据增强方法？"><a href="#深度学习中常用的数据增强方法？" class="headerlink" title="深度学习中常用的数据增强方法？"></a>深度学习中常用的数据增强方法？</h2><p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<ul>
<li><p>Color Jittering：对颜色的数据增强：图像亮度、饱和度、对比度变化（此处对色彩抖动的理解不知是否得当）；</p>
</li>
<li><p>PCA  Jittering：首先按照RGB三个颜色通道计算均值和标准差，再在整个训练集上计算协方差矩阵，进行特征分解，得到特征向量和特征值，用来做PCA Jittering；</p>
</li>
<li><p>Random Scale：尺度变换；</p>
</li>
<li><p>Random Crop：采用随机图像差值方式，对图像进行裁剪、缩放；包括Scale Jittering方法（VGG及ResNet模型使用）或者尺度和长宽比增强变换；</p>
</li>
<li><p>Horizontal/Vertical Flip：水平/垂直翻转；</p>
</li>
<li><p>Shift：平移变换；</p>
</li>
<li><p>Rotation/Reflection：旋转/仿射变换；</p>
</li>
<li><p>Noise：高斯噪声、模糊处理；</p>
</li>
<li><p>Label Shuffle：类别不平衡数据的增广；</p>
</li>
</ul>
<h2 id="如何理解-Internal-Covariate-Shift？"><a href="#如何理解-Internal-Covariate-Shift？" class="headerlink" title="如何理解 Internal Covariate Shift？"></a>如何理解 Internal Covariate Shift？</h2><p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<p>​    深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。</p>
<p>​    Google 将这一现象总结为 Internal Covariate Shift，简称 ICS。 什么是 ICS 呢？</p>
<p>​    大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同。</p>
<p>​    大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。</p>
<p><strong>那么ICS会导致什么问题？</strong></p>
<p>简而言之，每个神经元的输入数据不再是“独立同分布”。</p>
<p>其一，上层参数需要不断适应新的输入数据分布，降低学习速度。</p>
<p>其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。</p>
<p>其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>卷积神经网络</title>
    <url>/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="卷积神经网络（CNN）"><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h1><p>​    卷积神经网络是一种用来处理局部和整体相关性的计算网络结构，被应用在图像识别、自然语言处理甚至是语音识别领域，因为图像数据具有显著的局部与整体关系，其在图像识别领域的应用获得了巨大的成功。</p>
<h2 id="卷积神经网络的组成层"><a href="#卷积神经网络的组成层" class="headerlink" title="卷积神经网络的组成层"></a>卷积神经网络的组成层</h2><p>​    以图像分类任务为例，在表5.1所示卷积神经网络中，一般包含5种类型的网络层次结构：</p>
<p>​                                                                 表5.1 卷积神经网络的组成</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">CNN层次结构</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">输入层</td>
<td style="text-align:center">$W_1\times H_1\times 3$</td>
<td style="text-align:left">卷积网络的原始输入，可以是原始或预处理后的像素矩阵</td>
</tr>
<tr>
<td style="text-align:center">卷积层</td>
<td style="text-align:center">$W_1\times H_1\times K$</td>
<td style="text-align:left">参数共享、局部连接，利用平移不变性从全局特征图提取局部特征</td>
</tr>
<tr>
<td style="text-align:center">激活层</td>
<td style="text-align:center">$W_1\times H_1\times K$</td>
<td style="text-align:left">将卷积层的输出结果进行非线性映射</td>
</tr>
<tr>
<td style="text-align:center">池化层</td>
<td style="text-align:center">$W_2\times H_2\times K$</td>
<td style="text-align:left">进一步筛选特征，可以有效减少后续网络层次所需的参数量</td>
</tr>
<tr>
<td style="text-align:center">全连接层</td>
<td style="text-align:center">$(W_2 \cdot H_2 \cdot K)\times C$</td>
<td style="text-align:left">将多维特征展平为2维特征，通常低维度特征对应任务的学习目标（类别或回归值）</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>$W_1\times H_1\times 3$对应原始图像或经过预处理的像素值矩阵，3对应RGB图像的通道;$K$表示卷积层中卷积核（滤波器）的个数;$W_2\times H_2$ 为池化后特征图的尺度，在全局池化中尺度对应$1\times 1$;$(W_2 \cdot H_2 \cdot K)$是将多维特征压缩到1维之后的大小，$C$对应的则是图像类别个数。</p>
</blockquote>
<h3 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h3><p>​    输入层(Input Layer)通常是输入卷积神经网络的原始数据或经过预处理的数据，可以是图像识别领域中原始三维的多彩图像，也可以是音频识别领域中经过傅利叶变换的二维波形数据，甚至是自然语言处理中一维表示的句子向量。以图像分类任务为例，输入层输入的图像一般包含RGB三个通道，是一个由长宽分别为$H$和$W$组成的3维像素值矩阵$H\times W \times 3$，卷积网络会将输入层的数据传递到一系列卷积、池化等操作进行特征提取和转化，最终由全连接层对特征进行汇总和结果输出。根据计算能力、存储大小和模型结构的不同，卷积神经网络每次可以批量处理的图像个数不尽相同，若指定输入层接收到的图像个数为$N$，则输入层的输出数据为$N\times H\times W\times 3$。</p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>​    卷积层(Convolution Layer)通常用作对输入层输入数据进行特征提取，通过卷积核矩阵对原始数据中隐含关联性的一种抽象。卷积操作原理上其实是对两张像素矩阵进行点乘求和的数学操作，其中一个矩阵为输入的数据矩阵，另一个矩阵则为卷积核（滤波器或特征矩阵），求得的结果表示为原始图像中提取的特定局部特征。图5.1表示卷积操作过程中的不同填充策略，上半部分采用零填充，下半部分采用有效卷积（舍弃不能完整运算的边缘部分）。<br>​                                                    <img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/convolution.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/convolution.png" srcset="data:image/png;base64,666" alt="conv-same"><br>​                                                        图5.1 卷积操作示意图</p>
<h3 id="激活层"><a href="#激活层" class="headerlink" title="激活层"></a>激活层</h3><p>​    激活层(Activation Layer)负责对卷积层抽取的特征进行激活，由于卷积操作是由输入矩阵与卷积核矩阵进行相差的线性变化关系，需要激活层对其进行非线性的映射。激活层主要由激活函数组成，即在卷积层输出结果的基础上嵌套一个非线性函数，让输出的特征图具有非线性关系。卷积网络中通常采用ReLU来充当激活函数（还包括tanh和sigmoid等）ReLU的函数形式如公式（5-1）所示，能够限制小于0的值为0,同时大于等于0的值保持不变。</p>
<script type="math/tex; mode=display">
f(x)=\begin{cases}
   0 &\text{if } x<0 \\
   x &\text{if } x\ge 0
\end{cases}
\tag{5-1}</script><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>​    池化层又称为降采样层(Downsampling Layer)，作用是对感受域内的特征进行筛选，提取区域内最具代表性的特征，能够有效地降低输出特征尺度，进而减少模型所需要的参数量。按操作类型通常分为最大池化(Max Pooling)、平均池化(Average Pooling)和求和池化(Sum Pooling)，它们分别提取感受域内最大、平均与总和的特征值作为输出，最常用的是最大池化。</p>
<h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>​    全连接层(Full Connected Layer)负责对卷积神经网络学习提取到的特征进行汇总，将多维的特征输入映射为二维的特征输出，高维表示样本批次，低位常常对应任务目标。</p>
<h2 id="卷积在图像中有什么直观作用"><a href="#卷积在图像中有什么直观作用" class="headerlink" title="卷积在图像中有什么直观作用"></a>卷积在图像中有什么直观作用</h2><p>​    在卷积神经网络中，卷积常用来提取图像的特征，但不同层次的卷积操作提取到的特征类型是不相同的，特征类型粗分如表5.2所示。<br>​                                                                 表5.2 卷积提取的特征类型</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">卷积层次</th>
<th style="text-align:center">特征类型</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">浅层卷积</td>
<td style="text-align:center">边缘特征</td>
</tr>
<tr>
<td style="text-align:center">中层卷积</td>
<td style="text-align:center">局部特征</td>
</tr>
<tr>
<td style="text-align:center">深层卷积</td>
<td style="text-align:center">全局特征</td>
</tr>
</tbody>
</table>
</div>
<p>图像与不同卷积核的卷积可以用来执行边缘检测、锐化和模糊等操作。表5.3显示了应用不同类型的卷积核（滤波器）后的各种卷积图像。<br>​                                                                 表5.3 一些常见卷积核的作用</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">卷积作用</th>
<th style="text-align:center">卷积核</th>
<th style="text-align:center">卷积后图像</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">输出原图</td>
<td style="text-align:center">$\begin{bmatrix} 0 &amp; 0 &amp; 0 \ 0 &amp; 1 &amp; 0 \ 0 &amp; 0 &amp; 0 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/cat.jpg" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/cat.jpg" srcset="data:image/png;base64,666" alt="origin_img"></td>
</tr>
<tr>
<td style="text-align:center">边缘检测（突出边缘差异）</td>
<td style="text-align:center">$\begin{bmatrix} 1 &amp; 0 &amp; -1 \ 0 &amp; 0 &amp; 0 \ -1 &amp; 0 &amp; 1 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/cat-edgeDetect.jpg" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/cat-edgeDetect.jpg" srcset="data:image/png;base64,666" alt="edgeDetect-1"></td>
</tr>
<tr>
<td style="text-align:center">边缘检测（突出中间值）</td>
<td style="text-align:center">$\begin{bmatrix} -1 &amp; -1 &amp; -1 \ -1 &amp; 8 &amp; -1 \ -1 &amp; -1 &amp; -1 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/cat-edgeDetect-2.jpg" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/cat-edgeDetect-2.jpg" srcset="data:image/png;base64,666" alt="edgeDetect-2"></td>
</tr>
<tr>
<td style="text-align:center">图像锐化</td>
<td style="text-align:center">$\begin{bmatrix} 0 &amp; -1 &amp; 0 \ -1 &amp; 5 &amp; -1 \ 0 &amp; -1 &amp; 0 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/cat-sharpen.jpg" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/cat-sharpen.jpg" srcset="data:image/png;base64,666" alt="sharpen_img"></td>
</tr>
<tr>
<td style="text-align:center">方块模糊</td>
<td style="text-align:center">$\begin{bmatrix} 1 &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 \end{bmatrix} \times \frac{1}{9}$</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/cat-boxblur.jpg" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/cat-boxblur.jpg" srcset="data:image/png;base64,666" alt="box_blur"></td>
</tr>
<tr>
<td style="text-align:center">高斯模糊</td>
<td style="text-align:center">$\begin{bmatrix} 1 &amp; 2 &amp; 1 \ 2 &amp; 4 &amp; 2 \ 1 &amp; 2 &amp; 1 \end{bmatrix} \times \frac{1}{16}$</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/cat-blur-gaussian.jpg" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/cat-blur-gaussian.jpg" srcset="data:image/png;base64,666" alt="gaussian_blur"></td>
</tr>
</tbody>
</table>
</div>
<h2 id="卷积层有哪些基本参数？"><a href="#卷积层有哪些基本参数？" class="headerlink" title="卷积层有哪些基本参数？"></a>卷积层有哪些基本参数？</h2><p>​    卷积层中需要用到卷积核（滤波器或特征检测器）与图像特征矩阵进行点乘运算，利用卷积核与对应的特征感受域进行划窗式运算时，需要设定卷积核对应的大小、步长、个数以及填充的方式，如表5.4所示。</p>
<p>​                                                                         表5.4 卷积层的基本参数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:left">作用</th>
<th style="text-align:left">常见设置</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">卷积核大小 (Kernel Size)</td>
<td style="text-align:left">卷积核的大小定义了卷积的感受野</td>
<td style="text-align:left">在过去常设为5，如LeNet-5；现在多设为3，通过堆叠$3\times3$的卷积核来达到更大的感受域</td>
</tr>
<tr>
<td style="text-align:center">卷积核步长 (Stride)</td>
<td style="text-align:left">定义了卷积核在卷积过程中的步长</td>
<td style="text-align:left">常见设置为1，表示滑窗距离为1，可以覆盖所有相邻位置特征的组合；当设置为更大值时相当于对特征组合降采样</td>
</tr>
<tr>
<td style="text-align:center">填充方式 (Padding)</td>
<td style="text-align:left">在卷积核尺寸不能完美匹配输入的图像矩阵时需要进行一定的填充策略</td>
<td style="text-align:left">设置为’SAME’表示对不足卷积核大小的边界位置进行某种填充（通常零填充）以保证卷积输出维度与与输入维度一致；当设置为’VALID’时则对不足卷积尺寸的部分进行舍弃，输出维度就无法保证与输入维度一致</td>
</tr>
<tr>
<td style="text-align:center">输入通道数 (In Channels)</td>
<td style="text-align:left">指定卷积操作时卷积核的深度</td>
<td style="text-align:left">默认与输入的特征矩阵通道数（深度）一致；在某些压缩模型中会采用通道分离的卷积方式</td>
</tr>
<tr>
<td style="text-align:center">输出通道数 (Out Channels)</td>
<td style="text-align:left">指定卷积核的个数</td>
<td style="text-align:left">若设置为与输入通道数一样的大小，可以保持输入输出维度的一致性；若采用比输入通道数更小的值，则可以减少整体网络的参数量</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>卷积操作维度变换公式：</p>
<p>$O<em>d =\begin{cases} \lceil \frac{(I_d - k</em>{size})+ 1)}{s}\rceil ,&amp; \text{padding=VALID}\ \lceil \frac{I_d}{s}\rceil,&amp;\text{padding=SAME} \end{cases}$</p>
<p>其中，$I<em>d$为输入维度，$O_d$为输出维度，$k</em>{size}$为卷积核大小，$s$为步长</p>
</blockquote>
<h2 id="卷积核有什么类型？"><a href="#卷积核有什么类型？" class="headerlink" title="卷积核有什么类型？"></a>卷积核有什么类型？</h2><p>​    常见的卷积主要是由连续紧密的卷积核对输入的图像特征进行滑窗式点乘求和操作，除此之外还有其他类型的卷积核在不同的任务中会用到，具体分类如表5.5所示。<br>​                                                                     表5.5 卷积核分类</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">卷积类别</th>
<th style="text-align:center">示意图</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">标准卷积</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/img7.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/img7.png" srcset="data:image/png;base64,666" alt="image"></td>
<td style="text-align:left">最常用的卷积核，连续紧密的矩阵形式可以提取图像区域中的相邻像素之间的关联关系，$3\times3$的卷积核可以获得$3\times3$像素范围的感受视野</td>
</tr>
<tr>
<td style="text-align:center">扩张卷积（带孔卷积或空洞卷积）</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/img8.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/img8.png" srcset="data:image/png;base64,666" alt="image"></td>
<td style="text-align:left">引入一个称作扩张率（Dilation Rate）的参数，使同样尺寸的卷积核可以获得更大的感受视野，相应的在相同感受视野的前提下比普通卷积采用更少的参数。同样是$3\times3$的卷积核尺寸，扩张卷积可以提取$5\times5$范围的区域特征，在实时图像分割领域广泛应用</td>
</tr>
<tr>
<td style="text-align:center">转置卷积</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/img10.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/img10.png" srcset="data:image/png;base64,666" alt="image"></td>
<td style="text-align:left">先对原始特征矩阵进行填充使其维度扩大到适配卷积目标输出维度，然后进行普通的卷积操作的一个过程，其输入到输出的维度变换关系恰好与普通卷积的变换关系相反，但这个变换并不是真正的逆变换操作，通常称为转置卷积(Transpose Convolution)而不是反卷积(Deconvolution)。转置卷积常见于目标检测领域中对小目标的检测和图像分割领域还原输入图像尺度。</td>
</tr>
<tr>
<td style="text-align:center">可分离卷积</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/img11.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/img11.png" srcset="data:image/png;base64,666" alt="image"></td>
<td style="text-align:left">标准的卷积操作是同时对原始图像$H\times W\times C$三个方向的卷积运算，假设有$K$个相同尺寸的卷积核，这样的卷积操作需要用到的参数为$H\times W\times C\times K$个；若将长宽与深度方向的卷积操作分离出变为$H\times W$与$C$的两步卷积操作，则同样的卷积核个数$K$，只需要$(H\times W + C)\times K$个参数，便可得到同样的输出尺度。可分离卷积(Seperable Convolution)通常应用在模型压缩或一些轻量的卷积神经网络中，如MobileNet$^{[1]}$、Xception$^{[2]}$等</td>
</tr>
</tbody>
</table>
</div>
<h2 id="二维卷积与三维卷积有什么区别？"><a href="#二维卷积与三维卷积有什么区别？" class="headerlink" title="二维卷积与三维卷积有什么区别？"></a>二维卷积与三维卷积有什么区别？</h2><ul>
<li><strong>二维卷积</strong><br>二维卷积操作如图5.3所示，为了更直观的说明，分别展示在单通道和多通道输入中，对单个通道输出的卷积操作。在单通道输入的情况下，若输入卷积核尺寸为 $(k_h, k_w, 1)$，卷积核在输入图像的空间维度上进行滑窗操作，每次滑窗和 $(k_h, k_w)$窗口内的值进行卷积操作，得到输出图像中的一个值。在多通道输入的情况下，假定输入图像特征通道数为3，卷积核尺寸则为$(k_h, k_w, 3)$，每次滑窗与3个通道上的$(k_h, k_w)$窗口内的所有值进行卷积操作，得到输出图像中的一个值。</li>
</ul>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.6.1.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.6.1.png" srcset="data:image/png;base64,666" alt="image"></p>
<ul>
<li><strong>三维卷积</strong><br>3D卷积操作如图所示，同样分为单通道和多通道，且假定只使用1个卷积核，即输出图像仅有一个通道。对于单通道输入，与2D卷积不同之处在于，输入图像多了一个深度(depth)维度，卷积核也多了一个$k_d$维度，因此3D卷积核的尺寸为$(k_h, k_w, k_d)$，每次滑窗与$(k_h, k_w, k_d)$窗口内的值进行相关操作，得到输出3D图像中的一个值。对于多通道输入，则与2D卷积的操作一样，每次滑窗与3个channels上的$(k_h, k_w, k_d)$窗口内的所有值进行相关操作，得到输出3D图像中的一个值。</li>
</ul>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.6.2.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.6.2.png" srcset="data:image/png;base64,666" alt="image"></p>
<h2 id="有哪些池化方法？"><a href="#有哪些池化方法？" class="headerlink" title="有哪些池化方法？"></a>有哪些池化方法？</h2><p>​    池化操作通常也叫做子采样(Subsampling)或降采样(Downsampling)，在构建卷积神经网络时，往往会用在卷积层之后，通过池化来降低卷积层输出的特征维度，有效减少网络参数的同时还可以防止过拟合现象。池化操作可以降低图像维度的原因，本质上是因为图像具有一种“静态性”的属性，这个意思是说在一个图像区域有用的特征极有可能在另一个区域同样有用。因此，为了描述一个大的图像，很直观的想法就是对不同位置的特征进行聚合统计。例如，可以计算图像在固定区域上特征的平均值 (或最大值)来代表这个区域的特征。<br>​                                                                              表5.6 池化分类</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">池化类型</th>
<th style="text-align:center">示意图</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">一般池化(General Pooling)</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/general_pooling.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/general_pooling.png" srcset="data:image/png;base64,666" alt="max_pooling"></td>
<td style="text-align:left">通常包括最大池化(Max Pooling)和平均池化(Mean Pooling)。以最大池化为例，池化范围$(2\times2)$和滑窗步长$(stride=2)$ 相同，仅提取一次相同区域的范化特征。</td>
</tr>
<tr>
<td style="text-align:center">重叠池化(Overlapping Pooling)</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/overlap_pooling.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/overlap_pooling.png" srcset="data:image/png;base64,666" alt="overlap_pooling"></td>
<td style="text-align:left">与一般池化操作相同，但是池化范围$P<em>{size}$与滑窗步长$stride$关系为$P</em>{size}&gt;stride$，同一区域内的像素特征可以参与多次滑窗提取，得到的特征表达能力更强，但计算量更大。</td>
</tr>
<tr>
<td style="text-align:center">空间金字塔池化$^*$(Spatial Pyramid Pooling)</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/spatial_pooling.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/spatial_pooling.png" srcset="data:image/png;base64,666" alt="spatial_pooling"></td>
<td style="text-align:left">在进行多尺度目标的训练时，卷积层允许输入的图像特征尺度是可变的，紧接的池化层若采用一般的池化方法会使得不同的输入特征输出相应变化尺度的特征，而卷积神经网络中最后的全连接层则无法对可变尺度进行运算，因此需要对不同尺度的输出特征采样到相同输出尺度。</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>SPPNet$^{[3]}$就引入了空间池化的组合，对不同输出尺度采用不同的滑窗大小和步长以确保输出尺度相同$(win_{size}=\lceil \frac{in}{out}\rceil; stride=\lfloor \frac{in}{out}\rfloor; )$，同时用如金字塔式叠加的多种池化尺度组合，以提取更加丰富的图像特征。常用于多尺度训练和目标检测中的区域提议网络(Region Proposal Network)的兴趣区域(Region of Interest)提取</p>
</blockquote>
<h2 id="1-times1-卷积作用？"><a href="#1-times1-卷积作用？" class="headerlink" title="$1\times1$卷积作用？"></a>$1\times1$卷积作用？</h2><p>​    NIN(Network in Network)$^{[4]}$是第一篇探索$1\times1$卷积核的论文，这篇论文通过在卷积层中使用MLP替代传统线性的卷积核，使单层卷积层内具有非线性映射的能力，也因其网络结构中嵌套MLP子网络而得名NIN。NIN对不同通道的特征整合到MLP自网络中，让不同通道的特征能够交互整合，使通道之间的信息得以流通，其中的MLP子网络恰恰可以用$1\times1$的卷积进行代替。</p>
<p>​    GoogLeNet$^{[5]}$则采用$1\times1$卷积核来减少模型的参数量。在原始版本的Inception模块中，由于每一层网络采用了更多的卷积核，大大增加了模型的参数量。此时在每一个较大卷积核的卷积层前引入$1\times1$卷积，可以通过分离通道与宽高卷积来减少模型参数量。以图5.2为例，在不考虑参数偏置项的情况下，若输入和输出的通道数为$C_1=16$，则左半边网络模块所需的参数为$(1\times1+3\times3+5\times5+0)\times C_1\times C_1=8960$；假定右半边网络模块采用的$1\times1$卷积通道数为$C_2=8$$(满足C_1&gt;C_2)$，则右半部分的网络结构所需参数量为$(1\times1\times (3C_1+C_2)+3\times3\times C_2 +5\times5\times C_2)\times C_1=5248$ ，可以在不改变模型表达能力的前提下大大减少所使用的参数量。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.8-1.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.8-1.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>​                                图5.2 Inception模块</p>
<p>综上所述，$1\times 1$卷积的作用主要为以下两点：</p>
<ul>
<li>实现信息的跨通道交互和整合。</li>
<li>对卷积核通道数进行降维和升维，减小参数量。</li>
</ul>
<h2 id="卷积层和池化层有什么区别？"><a href="#卷积层和池化层有什么区别？" class="headerlink" title="卷积层和池化层有什么区别？"></a>卷积层和池化层有什么区别？</h2><p>​    卷积层核池化层在结构上具有一定的相似性，都是对感受域内的特征进行提取，并且根据步长设置获取到不同维度的输出，但是其内在操作是有本质区别的，如表5.7所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">卷积层</th>
<th style="text-align:center">池化层</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>结构</strong></td>
<td style="text-align:center">零填充时输出维度不变，而通道数改变</td>
<td style="text-align:center">通常特征维度会降低，通道数不变</td>
</tr>
<tr>
<td style="text-align:center"><strong>稳定性</strong></td>
<td style="text-align:center">输入特征发生细微改变时，输出结果会改变</td>
<td style="text-align:center">感受域内的细微变化不影响输出结果</td>
</tr>
<tr>
<td style="text-align:center"><strong>作用</strong></td>
<td style="text-align:center">感受域内提取局部关联特征</td>
<td style="text-align:center">感受域内提取泛化特征，降低维度</td>
</tr>
<tr>
<td style="text-align:center"><strong>参数量</strong></td>
<td style="text-align:center">与卷积核尺寸、卷积核个数相关</td>
<td style="text-align:center">不引入额外参数</td>
</tr>
</tbody>
</table>
</div>
<h2 id="卷积核是否一定越大越好？"><a href="#卷积核是否一定越大越好？" class="headerlink" title="卷积核是否一定越大越好？"></a>卷积核是否一定越大越好？</h2><p>​    在早期的卷积神经网络中（如LeNet-5、AlexNet），用到了一些较大的卷积核（$11\times11$和$5\times 5$），受限于当时的计算能力和模型结构的设计，无法将网络叠加得很深，因此卷积网络中的卷积层需要设置较大的卷积核以获取更大的感受域。但是这种大卷积核反而会导致计算量大幅增加，不利于训练更深层的模型，相应的计算性能也会降低。后来的卷积神经网络（VGG、GoogLeNet等），发现通过堆叠2个$3\times 3$卷积核可以获得与$5\times 5$卷积核相同的感受视野，同时参数量会更少（$3×3×2+1$ &lt; $ 5×5×1+1$），$3\times 3$卷积核被广泛应用在许多卷积神经网络中。因此可以认为，在大多数情况下通过堆叠较小的卷积核比直接采用单个更大的卷积核会更加有效。</p>
<p>​    但是，这并不是表示更大的卷积核就没有作用，在某些领域应用卷积神经网络时仍然可以采用较大的卷积核。譬如在自然语言处理领域，由于文本内容不像图像数据可以对特征进行很深层的抽象，往往在该领域的特征提取只需要较浅层的神经网络即可。在将卷积神经网络应用在自然语言处理领域时，通常都是较为浅层的卷积层组成，但是文本特征有时又需要有较广的感受域让模型能够组合更多的特征（如词组和字符），此时直接采用较大的卷积核将是更好的选择。</p>
<p>​    综上所述，卷积核的大小并没有绝对的优劣，需要视具体的应用场景而定，但是极大和极小的卷积核都是不合适的，单独的$1\times 1$极小卷积核只能用作分离卷积而不能对输入的原始特征进行有效的组合，极大的卷积核通常会组合过多的无意义特征从而浪费了大量的计算资源。</p>
<h2 id="每层卷积是否只能用一种尺寸的卷积核？"><a href="#每层卷积是否只能用一种尺寸的卷积核？" class="headerlink" title="每层卷积是否只能用一种尺寸的卷积核？"></a>每层卷积是否只能用一种尺寸的卷积核？</h2><p>​    经典的神经网络一般都属于层叠式网络，每层仅用一个尺寸的卷积核，如VGG结构中使用了大量的$3×3$卷积层。事实上，同一层特征图可以分别使用多个不同尺寸的卷积核，以获得不同尺度的特征，再把这些特征结合起来，得到的特征往往比使用单一卷积核的要好，如GoogLeNet、Inception系列的网络，均是每层使用了多个卷积核结构。如图5.3所示，输入的特征在同一层分别经过$1×1$、$3×3$和$5×5$三种不同尺寸的卷积核，再将分别得到的特征进行整合，得到的新特征可以看作不同感受域提取的特征组合，相比于单一卷积核会有更强的表达能力。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.11-1.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.11-1.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>​                                                                                                   图5.3 Inception模块结构</p>
<h2 id="怎样才能减少卷积层参数量？"><a href="#怎样才能减少卷积层参数量？" class="headerlink" title="怎样才能减少卷积层参数量？"></a>怎样才能减少卷积层参数量？</h2><p>减少卷积层参数量的方法可以简要地归为以下几点：</p>
<ul>
<li>使用堆叠小卷积核代替大卷积核：VGG网络中2个$3\times 3$的卷积核可以代替1个$5\times 5$的卷积核</li>
<li>使用分离卷积操作：将原本$K\times K\times C$的卷积操作分离为$K\times K\times 1$和$1\times1\times C$的两部分操作</li>
<li>添加$1\times 1$的卷积操作：与分离卷积类似，但是通道数可变，在$K\times K\times C_1$卷积前添加$1\times1\times C_2$的卷积核（满足$C_2 &lt;C_1$）</li>
<li>在卷积层前使用池化操作：池化可以降低卷积层的输入特征维度</li>
</ul>
<h2 id="在进行卷积操作时，必须同时考虑通道和区域吗？"><a href="#在进行卷积操作时，必须同时考虑通道和区域吗？" class="headerlink" title="在进行卷积操作时，必须同时考虑通道和区域吗？"></a>在进行卷积操作时，必须同时考虑通道和区域吗？</h2><p>​    标准卷积中，采用区域与通道同时处理的操作，如下图所示：</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.13-1.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.13-1.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>​    这样做可以简化卷积层内部的结构，每一个输出的特征像素都由所有通道的同一个区域提取而来。</p>
<p>​    但是这种方式缺乏灵活性，并且在深层的网络结构中使得运算变得相对低效，更为灵活的方式是使区域和通道的卷积分离开来，通道分离（深度分离）卷积网络由此诞生。如下图所示，Xception网络可解决上述问题。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.13-2.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.13-2.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>​    我们首先对每一个通道进行各自的卷积操作，有多少个通道就有多少个过滤器。得到新的通道特征矩阵之后，再对这批新通道特征进行标准的$1×1$跨通道卷积操作。</p>
<h2 id="采用宽卷积的好处有什么？"><a href="#采用宽卷积的好处有什么？" class="headerlink" title="采用宽卷积的好处有什么？"></a>采用宽卷积的好处有什么？</h2><p>​    宽卷积对应的是窄卷积，实际上并不是卷积操作的类型，指的是卷积过程中的填充方法，对应的是’SAME’填充和’VALID’填充。’SAME’填充通常采用零填充的方式对卷积核不满足整除条件的输入特征进行补全，以使卷积层的输出维度保持与输入特征维度一致；’VALID’填充的方式则相反，实际并不进行任何填充，在输入特征边缘位置若不足以进行卷积操作，则对边缘信息进行舍弃，因此在步长为1的情况下该填充方式的卷积层输出特征维度可能会略小于输入特征的维度。此外，由于前一种方式通过补零来进行完整的卷积操作，可以有效地保留原始的输入特征信息。</p>
<p>​    比如下图左部分为窄卷积。注意到越在边缘的位置被卷积的次数越少。宽卷积可以看作在卷积之前在边缘用0补充，常见有两种情况，一个是全补充，如下图右部分，这样输出大于输入的维度。另一种常用的方法是补充一一部分0值，使得输出和输入的维度一致。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.14.1.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.14.1.png" srcset="data:image/png;base64,666" alt="image"></p>
<h2 id="理解转置卷积与棋盘效应"><a href="#理解转置卷积与棋盘效应" class="headerlink" title="理解转置卷积与棋盘效应"></a>理解转置卷积与棋盘效应</h2><h3 id="标准卷积"><a href="#标准卷积" class="headerlink" title="标准卷积"></a>标准卷积</h3><p>在理解转置卷积之前，需要先理解标准卷积的运算方式。</p>
<p>首先给出一个输入输出结果</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/img32.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/img32.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>那是怎样计算的呢？</p>
<p>卷积的时候需要对卷积核进行180的旋转，同时卷积核中心与需计算的图像像素对齐，输出结构为中心对齐像素的一个新的像素值，计算例子如下：</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.19.1-2.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.19.1-2.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>这样计算出左上角(即第一行第一列)像素的卷积后像素值。</p>
<p>给出一个更直观的例子，从左到右看，原像素经过卷积由1变成-8。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.19.1-3.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.19.1-3.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>通过滑动卷积核，就可以得到整张图片的卷积结果。</p>
<h3 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h3><p>图像的deconvolution过程如下：</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.19.2-5.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.19.2-5.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>输入：2x2， 卷积核：4x4， 滑动步长：3， 输出：7x7 </p>
<p>过程如下： </p>
<ol>
<li><p>输入图片每个像素进行一次full卷积，根据full卷积大小计算可以知道每个像素的卷积后大小为 1+4-1=4， 即4x4大小的特征图，输入有4个像素所以4个4x4的特征图 </p>
</li>
<li><p>将4个特征图进行步长为3的相加； 输出的位置和输入的位置相同。步长为3是指每隔3个像素进行相加，重叠部分进行相加，即输出的第1行第4列是由红色特阵图的第一行第四列与绿色特征图的第一行第一列相加得到，其他如此类推。  </p>
<p>可以看出翻卷积的大小是由卷积核大小与滑动步长决定， in是输入大小， k是卷积核大小， s是滑动步长， out是输出大小 得到 out = (in - 1) <em> s + k 上图过程就是， (2 - 1) </em> 3 + 4 = 7。</p>
</li>
</ol>
<h3 id="棋盘效应"><a href="#棋盘效应" class="headerlink" title="棋盘效应"></a>棋盘效应</h3><h2 id="卷积神经网络的参数设置"><a href="#卷积神经网络的参数设置" class="headerlink" title="卷积神经网络的参数设置"></a>卷积神经网络的参数设置</h2><p>​    卷积神经网络中常见的参数在其他类型的神经网络中也是类似的，但是参数的设置还得结合具体的任务才能设置在合理的范围，具体的参数列表如表XX所示。<br>​                                                    表XX 卷积神经网络常见参数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:center">常见设置</th>
<th style="text-align:left">参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">学习率(Learning Rate)</td>
<td style="text-align:center">$0-1$</td>
<td style="text-align:left">反向传播网络中更新权值矩阵的步长，在一些常见的网络中会在固定迭代次数或模型不再收敛后对学习率进行指数下降(如$lr=lr\times 0.1$)。当学习率越大计算误差对权值矩阵的影响越大，容易在某个局部最优解附近震荡；越小的学习率对网络权值的更新越精细，但是需要花费更多的时间去迭代</td>
</tr>
<tr>
<td style="text-align:center">批次大小(Batch Size)</td>
<td style="text-align:center">$1-N$</td>
<td style="text-align:left">批次大小指定一次性流入模型的数据样本个数，根据任务和计算性能限制判断实际取值，在一些图像任务中往往由于计算性能和存储容量限制只能选取较小的值。在相同迭代次数的前提下，数值越大模型越稳定，泛化能力越强，损失值曲线越平滑，模型也更快地收敛，但是每次迭代需要花费更多的时间</td>
</tr>
<tr>
<td style="text-align:center">数据轮次(Epoch)</td>
<td style="text-align:center">$1-N$</td>
<td style="text-align:left">数据轮次指定所有训练数据在模型中训练的次数，根据数据集规模和分布情况会设置为不同的值。当模型较为简单或训练数据规模较小时，通常轮次不宜过高，否则模型容易过拟合；模型较为复杂或训练数据规模足够大时，可适当提高数据的训练轮次。</td>
</tr>
<tr>
<td style="text-align:center">权重衰减系数(Weight Decay)</td>
<td style="text-align:center">$0-0.001$</td>
<td style="text-align:left">模型训练过程中反向传播权值更新的权重衰减值</td>
</tr>
</tbody>
</table>
</div>
<h2 id="提高卷积神经网络的泛化能力"><a href="#提高卷积神经网络的泛化能力" class="headerlink" title="提高卷积神经网络的泛化能力"></a>提高卷积神经网络的泛化能力</h2><p>​    卷积神经网络与其他类型的神经网络类似，在采用反向传播进行训练的过程中比较依赖输入的数据分布，当数据分布较为极端的情况下容易导致模型欠拟合或过拟合，表XX记录了提高卷积网络泛化能力的方法。<br>​                                                                   表XX 提高卷积网络化能力的方法</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">方法</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">使用更多数据</td>
<td style="text-align:left">在有条件的前提下，尽可能多地获取训练数据是最理想的方法，更多的数据可以让模型得到充分的学习，也更容易提高泛化能力</td>
</tr>
<tr>
<td style="text-align:center">使用更大批次</td>
<td style="text-align:left">在相同迭代次数和学习率的条件下，每批次采用更多的数据将有助于模型更好的学习到正确的模式，模型输出结果也会更加稳定</td>
</tr>
<tr>
<td style="text-align:center">调整数据分布</td>
<td style="text-align:left">大多数场景下的数据分布是不均匀的，模型过多地学习某类数据容易导致其输出结果偏向于该类型的数据，此时通过调整输入的数据分布可以一定程度提高泛化能力</td>
</tr>
<tr>
<td style="text-align:center">调整目标函数</td>
<td style="text-align:left">在某些情况下，目标函数的选择会影响模型的泛化能力，如目标函数$f(y,y’)=</td>
<td>y-y’</td>
<td>$在某类样本已经识别较为准确而其他样本误差较大的侵害概况下，不同类别在计算损失结果的时候距离权重是相同的，若将目标函数改成$f(y,y’)=(y-y’)^2$则可以使误差小的样本计算损失的梯度比误差大的样本更小，进而有效地平衡样本作用，提高模型泛化能力</td>
</tr>
<tr>
<td style="text-align:center">调整网络结构</td>
<td style="text-align:left">在浅层卷积神经网络中，参数量较少往往使模型的泛化能力不足而导致欠拟合，此时通过叠加卷积层可以有效地增加网络参数，提高模型表达能力；在深层卷积网络中，若没有充足的训练数据则容易导致模型过拟合，此时通过简化网络结构减少卷积层数可以起到提高模型泛化能力的作用</td>
</tr>
<tr>
<td style="text-align:center">数据增强</td>
<td style="text-align:left">数据增强又叫数据增广，在有限数据的前提下通过平移、旋转、加噪声等一些列变换来增加训练数据，同类数据的表现形式也变得更多样，有助于模型提高泛化能力，需要注意的是数据变化应尽可能不破坏元数数据的主体特征(如在图像分类任务中对图像进行裁剪时不能将分类主体目标裁出边界)。</td>
</tr>
<tr>
<td style="text-align:center">权值正则化</td>
<td style="text-align:left">权值正则化就是通常意义上的正则化，一般是在损失函数中添加一项权重矩阵的正则项作为惩罚项，用来惩罚损失值较小时网络权重过大的情况，此时往往是网络权值过拟合了数据样本(如$Loss=f(WX+b,y’)+\frac{\lambda}{\eta}\sum{</td>
<td>W</td>
<td>}$)。</td>
</tr>
<tr>
<td style="text-align:center">屏蔽网络节点</td>
<td style="text-align:left">该方法可以认为是网络结构上的正则化，通过随机性地屏蔽某些神经元的输出让剩余激活的神经元作用，可以使模型的容错性更强。</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>对大多数神经网络模型同样通用</p>
</blockquote>
<h2 id="卷积神经网络在不同领域的应用"><a href="#卷积神经网络在不同领域的应用" class="headerlink" title="卷积神经网络在不同领域的应用"></a>卷积神经网络在不同领域的应用</h2><p>​    卷积神经网络中的卷积操作是其关键组成，而卷积操作只是一种数学运算方式，实际上对不同类型的数值表示数据都是通用的，尽管这些数值可能表示的是图像像素值、文本序列中单个字符或是语音片段中单字的音频。只要使原始数据能够得到有效地数值化表示，卷积神经网络能够在不同的领域中得到应用，要关注的是如何将卷积的特性更好地在不同领域中应用，如表XX所示。<br>​                                                    表XX 卷积神经网络不同领域的应用<br>| 应用领域 | 输入数据图示 | 说明 |<br>| :——-: | :—————: | :— |<br>|   图像处理   | <img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/Image-process.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/Image-process.png" srcset="data:image/png;base64,666" alt="image_process"> | 卷积神经网络在图像处理领域有非常广泛的应用，这是因为图像数据本身具有的局部完整性非常 |<br>| 自然语言处理 | <img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/NLP.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/NLP.png" srcset="data:image/png;base64,666" alt="NLP"> |  |<br>|   语音处理   | <img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/audio-recognition.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/audio-recognition.png" srcset="data:image/png;base64,666" alt="audio_process"> |  |</p>
<h3 id="联系"><a href="#联系" class="headerlink" title="联系"></a>联系</h3><p>​    自然语言处理是对一维信号（词序列）做操作。<br>​    计算机视觉是对二维（图像）或三维（视频流）信号做操作。</p>
<h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>​    自然语言处理的输入数据通常是离散取值（例如表示一个单词或字母通常表示为词典中的one hot向量），计算机视觉则是连续取值（比如归一化到0，1之间的灰度值）。CNN有两个主要特点，区域不变性(location invariance)和组合性(Compositionality)。</p>
<ol>
<li>区域不变性：滤波器在每层的输入向量(图像)上滑动，检测的是局部信息，然后通过pooling取最大值或均值。pooling这步综合了局部特征，失去了每个特征的位置信息。这很适合基于图像的任务，比如要判断一幅图里有没有猫这种生物，你可能不会去关心这只猫出现在图像的哪个区域。但是在NLP里，词语在句子或是段落里出现的位置，顺序，都是很重要的信息。</li>
<li>局部组合性：CNN中，每个滤波器都把较低层的局部特征组合生成较高层的更全局化的特征。这在CV里很好理解，像素组合成边缘，边缘生成形状，最后把各种形状组合起来得到复杂的物体表达。在语言里，当然也有类似的组合关系，但是远不如图像来的直接。而且在图像里，相邻像素必须是相关的，相邻的词语却未必相关。</li>
</ol>
<h2 id="卷积神经网络凸显共性的方法？"><a href="#卷积神经网络凸显共性的方法？" class="headerlink" title="卷积神经网络凸显共性的方法？"></a>卷积神经网络凸显共性的方法？</h2><h3 id="局部连接"><a href="#局部连接" class="headerlink" title="局部连接"></a>局部连接</h3><p>​    我们首先了解一个概念，感受野，即每个神经元仅与输入神经元相连接的一块区域。<br>在图像卷积操作中，神经元在空间维度上是局部连接，但在深度上是全连接。局部连接的思想，是受启发于生物学里的视觉系统结构，视觉皮层的神经元就是仅用局部接受信息。对于二维图像，局部像素关联性较强。这种局部连接保证了训练后的滤波器能够对局部特征有最强的响应，使神经网络可以提取数据的局部特征；<br>下图是一个很经典的图示，左边是全连接，右边是局部连接。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.27.1.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.27.1.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>对于一个1000 × 1000的输入图像而言，如果下一个隐藏层的神经元数目为10^6个，采用全连接则有1000 × 1000 × 10^6 = 10^12个权值参数，如此巨大的参数量几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中10 × 10的局部图像相连接，那么此时的权值参数数量为10 × 10 × 10^6 = 10^8，将直接减少4个数量级。</p>
<h3 id="权值共享"><a href="#权值共享" class="headerlink" title="权值共享"></a>权值共享</h3><p>​    权值共享，即计算同一深度的神经元时采用的卷积核参数是共享的。权值共享在一定程度上讲是有意义的，是由于在神经网络中，提取的底层边缘特征与其在图中的位置无关。但是在另一些场景中是无意的，如在人脸识别任务，我们期望在不同的位置学到不同的特征。<br>需要注意的是，权重只是对于同一深度切片的神经元是共享的。在卷积层中，通常采用多组卷积核提取不同的特征，即对应的是不同深度切片的特征，而不同深度切片的神经元权重是不共享。相反，偏置这一权值对于同一深度切片的所有神经元都是共享的。<br>权值共享带来的好处是大大降低了网络的训练难度。如下图，假设在局部连接中隐藏层的每一个神经元连接的是一个10 × 10的局部图像，因此有10 × 10个权值参数，将这10 × 10个权值参数共享给剩下的神经元，也就是说隐藏层中10^6个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10 × 10个权值参数（也就是卷积核的大小）。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.27.2.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.27.2.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>这里就体现了卷积神经网络的奇妙之处，使用少量的参数，却依然能有非常出色的性能。上述仅仅是提取图像一种特征的过程。如果要多提取出一些特征，可以增加多个卷积核，不同的卷积核能够得到图像不同尺度下的特征，称之为特征图（feature map）。</p>
<h3 id="池化操作"><a href="#池化操作" class="headerlink" title="池化操作"></a>池化操作</h3><p>池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图：</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.27.3.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/5.27.3.png" srcset="data:image/png;base64,666" alt="image"></p>
<h2 id="全连接、局部连接、全卷积与局部卷积"><a href="#全连接、局部连接、全卷积与局部卷积" class="headerlink" title="全连接、局部连接、全卷积与局部卷积"></a>全连接、局部连接、全卷积与局部卷积</h2><p>​    大多数神经网络中高层网络通常会采用全连接层(Global Connected Layer)，通过多对多的连接方式对特征进行全局汇总，可以有效地提取全局信息。但是全连接的方式需要大量的参数，是神经网络中最占资源的部分之一，因此就需要由局部连接(Local Connected Layer)，仅在局部区域范围内产生神经元连接，能够有效地减少参数量。根据卷积操作的作用范围可以分为全卷积(Global Convolution)和局部卷积(Local Convolution)。实际上这里所说的全卷积就是标准卷积，即在整个输入特征维度范围内采用相同的卷积核参数进行运算，全局共享参数的连接方式可以使神经元之间的连接参数大大减少;局部卷积又叫平铺卷积(Tiled Convolution)或非共享卷积(Unshared Convolution)，是局部连接与全卷积的折衷。四者的比较如表XX所示。<br>​                                                     表XX 卷积网络中连接方式的对比</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">连接方式</th>
<th style="text-align:center">示意图</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">全连接</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/full-connected.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/full-connected.png" srcset="data:image/png;base64,666" alt="full-connected"></td>
<td style="text-align:left">层间神经元完全连接，每个输出神经元可以获取到所有输入神经元的信息，有利于信息汇总，常置于网络末层；连接与连接之间独立参数，大量的连接大大增加模型的参数规模。</td>
</tr>
<tr>
<td style="text-align:center">局部连接</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/local-connected.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/local-connected.png" srcset="data:image/png;base64,666" alt="local-connected"></td>
<td style="text-align:left">层间神经元只有局部范围内的连接，在这个范围内采用全连接的方式，超过这个范围的神经元则没有连接；连接与连接之间独立参数，相比于全连接减少了感受域外的连接，有效减少参数规模</td>
</tr>
<tr>
<td style="text-align:center">全卷积</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/conv.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/conv.png" srcset="data:image/png;base64,666" alt="convolution"></td>
<td style="text-align:left">层间神经元只有局部范围内的连接，在这个范围内采用全连接的方式，连接所采用的参数在不同感受域之间共享，有利于提取特定模式的特征；相比于局部连接，共用感受域之间的参数可以进一步减少参数量。</td>
</tr>
<tr>
<td style="text-align:center">局部卷积</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/local-conv.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/local-conv.png" srcset="data:image/png;base64,666" alt="local-conv"></td>
<td style="text-align:left">层间神经元只有局部范围内的连接，感受域内采用全连接的方式，而感受域之间间隔采用局部连接与全卷积的连接方式；相比与全卷积成倍引入额外参数，但有更强的灵活性和表达能力；相比于局部连接，可以有效控制参数量</td>
</tr>
</tbody>
</table>
</div>
<h2 id="局部卷积的应用"><a href="#局部卷积的应用" class="headerlink" title="局部卷积的应用"></a>局部卷积的应用</h2><p>并不是所有的卷积都会进行权重共享，在某些特定任务中，会使用不权重共享的卷积。下面通过人脸这一任务来进行讲解。在读人脸方向的一些paper时，会发现很多都会在最后加入一个Local Connected Conv，也就是不进行权重共享的卷积层。总的来说，这一步的作用就是使用3D模型来将人脸对齐，从而使CNN发挥最大的效果。<br><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/img66.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch5/img66.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>截取论文中的一部分图，经过3D对齐以后，形成的图像均是152×152，输入到上述的网络结构中。该结构的参数如下：</p>
<p>Conv：32个11×11×3的卷积核，</p>
<p>Max-pooling: 3×3，stride=2，</p>
<p>Conv: 16个9×9的卷积核，</p>
<p>Local-Conv: 16个9×9的卷积核，</p>
<p>Local-Conv: 16个7×7的卷积核，</p>
<p>Local-Conv: 16个5×5的卷积核，</p>
<p>Fully-connected: 4096维，</p>
<p>Softmax: 4030维。</p>
<p>前三层的目的在于提取低层次的特征，比如简单的边和纹理。其中Max-pooling层使得卷积的输出对微小的偏移情况更加鲁棒。但不能使用更多的Max-pooling层，因为太多的Max-pooling层会使得网络损失图像信息。全连接层将上一层的每个单元和本层的所有单元相连，用来捕捉人脸图像不同位置特征之间的相关性。最后使用softmax层用于人脸分类。<br>中间三层都是使用参数不共享的卷积核，之所以使用参数不共享，有如下原因：</p>
<p>（1）对齐的人脸图片中，不同的区域会有不同的统计特征，因此并不存在特征的局部稳定性，所以使用相同的卷积核会导致信息的丢失。</p>
<p>（2）不共享的卷积核并不增加inference时特征的计算量，仅会增加训练时的计算量。<br>使用不共享的卷积核，由于需要训练的参数量大大增加，因此往往需要通过其他方法增加数据量。</p>
<h2 id="NetVLAD池化-（贡献者：熊楚原-中国人民大学）"><a href="#NetVLAD池化-（贡献者：熊楚原-中国人民大学）" class="headerlink" title="NetVLAD池化    （贡献者：熊楚原-中国人民大学）"></a>NetVLAD池化    （贡献者：熊楚原-中国人民大学）</h2><p>NetVLAD是论文[15]提出的一个局部特征聚合的方法。</p>
<p>在传统的网络里面，例如VGG啊，最后一层卷积层输出的特征都是类似于Batchsize x 3 x 3 x 512的这种东西，然后会经过FC聚合，或者进行一个Global Average Pooling（NIN里的做法），或者怎么样，变成一个向量型的特征，然后进行Softmax or 其他的Loss。</p>
<p>这种方法说简单点也就是输入一个图片或者什么的结构性数据，然后经过特征提取得到一个长度固定的向量，之后可以用度量的方法去进行后续的操作，比如分类啊，检索啊，相似度对比等等。</p>
<p>那么NetVLAD考虑的主要是最后一层卷积层输出的特征这里，我们不想直接进行欠采样或者全局映射得到特征，对于最后一层输出的W x H x D，设计一个新的池化，去聚合一个“局部特征“，这即是NetVLAD的作用。</p>
<p>NetVLAD的一个输入是一个W x H x D的图像特征，例如VGG-Net最后的3 x 3 x 512这样的矩阵，在网络中还需加一个维度为Batchsize。</p>
<p>NetVLAD还需要另输入一个标量K即表示VLAD的聚类中心数量，它主要是来构成一个矩阵C，是通过原数据算出来的每一个$W \times H$特征的聚类中心，C的shape即$C: K \times D$，然后根据三个输入，VLAD是计算下式的V:</p>
<script type="math/tex; mode=display">V(j, k) = \sum_{i=1}^{N}{a_k(x_i)(x_i(j) - c_k(j))}</script><p>其中j表示维度，从1到D，可以看到V的j是和输入与c对应的，对每个类别k，都对所有的x进行了计算，如果$x_i$属于当前类别k，$a_k=1$，否则$a_k=0$，计算每一个x和它聚类中心的残差，然后把残差加起来，即是每个类别k的结果，最后分别L2正则后拉成一个长向量后再做L2正则，正则非常的重要，因为这样才能统一所有聚类算出来的值，而残差和的目的主要是消减不同聚类上的分布不均，两者共同作用才能得到最后正常的输出。</p>
<p>输入与输出如下图所示：</p>
<p><img src="http://www.ecohnoch.cn/img/netvlad.jpeg" class="lazyload" data-srcset="http://www.ecohnoch.cn/img/netvlad.jpeg" srcset="data:image/png;base64,666" alt="image"></p>
<p>中间得到的K个D维向量即是对D个x都进行了与聚类中心计算残差和的过程，最终把K个D维向量合起来后进行即得到最终输出的$K \times D$长度的一维向量。</p>
<p>而VLAD本身是不可微的，因为上面的a要么是0要么是1，表示要么当前描述x是当前聚类，要么不是，是个离散的，NetVLAD为了能够在深度卷积网络里使用反向传播进行训练，对a进行了修正。</p>
<p>那么问题就是如何重构一个a，使其能够评估当前的这个x和各个聚类的关联程度？用softmax来得到：</p>
<script type="math/tex; mode=display">a_k = \frac{e^{W_k^T x_i + b_k}}{e^{W_{k'}^T x_i + b_{k'}}}</script><p>将这个把上面的a替换后，即是NetVLAD的公式，可以进行反向传播更新参数。</p>
<p>所以一共有三个可训练参数，上式a中的$W: K \times D$，上式a中的$b: K \times 1$，聚类中心$c: K \times D$，而原始VLAD只有一个参数c。</p>
<p>最终池化得到的输出是一个恒定的K x D的一维向量（经过了L2正则），如果带Batchsize，输出即为Batchsize x (K x D)的二维矩阵。</p>
<p>NetVLAD作为池化层嵌入CNN网络即如下图所示，</p>
<p><img src="http://www.ecohnoch.cn/img/netvlad_emb.png" class="lazyload" data-srcset="http://www.ecohnoch.cn/img/netvlad_emb.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>原论文中采用将传统图像检索方法VLAD进行改进后应用在CNN的池化部分作为一种另类的局部特征池化，在场景检索上取得了很好的效果。</p>
<p>后续相继又提出了ActionVLAD、ghostVLAD等改进。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>经典网络</title>
    <url>/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="经典网络解读"><a href="#经典网络解读" class="headerlink" title="经典网络解读"></a>经典网络解读</h1><h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><h3 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h3><p>​    LeNet-5是由$LeCun$ 提出的一种用于识别手写数字和机器印刷字符的卷积神经网络（Convolutional Neural Network，CNN）$^{[1]}$，其命名来源于作者$LeCun$的名字，5则是其研究成果的代号，在LeNet-5之前还有LeNet-4和LeNet-1鲜为人知。LeNet-5阐述了图像中像素特征之间的相关性能够由参数共享的卷积操作所提取，同时使用卷积、下采样（池化）和非线性映射这样的组合结构，是当前流行的大多数深度图像识别网络的基础。</p>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image1.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                                                 图4.1 LeNet-5网络结构图</p>
<p>​    如图4.1所示，LeNet-5一共包含7层（输入层不作为网络结构），分别由2个卷积层、2个下采样层和3个连接层组成，网络的参数配置如表4.1所示，其中下采样层和全连接层的核尺寸分别代表采样范围和连接矩阵的尺寸（如卷积核尺寸中的$“5\times5\times1/1,6”$表示核大小为$5\times5\times1$、步长为$1$且核个数为6的卷积核）。</p>
<p>​                                                                 表4.1 LeNet-5网络参数配置</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">网络层</th>
<th style="text-align:center">输入尺寸</th>
<th style="text-align:center">核尺寸</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:center">可训练参数量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">卷积层$C_1$</td>
<td style="text-align:center">$32\times32\times1$</td>
<td style="text-align:center">$5\times5\times1/1,6$</td>
<td style="text-align:center">$28\times28\times6$</td>
<td style="text-align:center">$(5\times5\times1+1)\times6$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_2$</td>
<td style="text-align:center">$28\times28\times6$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$14\times14\times6$</td>
<td style="text-align:center">$(1+1)\times6$ $^*$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_3$</td>
<td style="text-align:center">$14\times14\times6$</td>
<td style="text-align:center">$5\times5\times6/1,16$</td>
<td style="text-align:center">$10\times10\times16$</td>
<td style="text-align:center">$1516^*$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_4$</td>
<td style="text-align:center">$10\times10\times16$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$5\times5\times16$</td>
<td style="text-align:center">$(1+1)\times16$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_5$$^*$</td>
<td style="text-align:center">$5\times5\times16$</td>
<td style="text-align:center">$5\times5\times16/1,120$</td>
<td style="text-align:center">$1\times1\times120$</td>
<td style="text-align:center">$(5\times5\times16+1)\times120$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$F_6$</td>
<td style="text-align:center">$1\times1\times120$</td>
<td style="text-align:center">$120\times84$</td>
<td style="text-align:center">$1\times1\times84$</td>
<td style="text-align:center">$(120+1)\times84$</td>
</tr>
<tr>
<td style="text-align:center">输出层</td>
<td style="text-align:center">$1\times1\times84$</td>
<td style="text-align:center">$84\times10$</td>
<td style="text-align:center">$1\times1\times10$</td>
<td style="text-align:center">$(84+1)\times10$</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>​    $^*$ 在LeNet中，下采样操作和池化操作类似，但是在得到采样结果后会乘以一个系数和加上一个偏置项，所以下采样的参数个数是$(1+1)\times6$而不是零。</p>
<p>​    $^*$ $C_3$卷积层可训练参数并未直接连接$S_2$中所有的特征图（Feature Map），而是采用如图4.2所示的采样特征方式进行连接（稀疏连接），生成的16个通道特征图中分别按照相邻3个特征图、相邻4个特征图、非相邻4个特征图和全部6个特征图进行映射，得到的参数个数计算公式为$6\times(25\times3+1)+6\times(25\times4+1)+3\times(25\times4+1)+1\times(25\times6+1)=1516$，在原论文中解释了使用这种采样方式原因包含两点：限制了连接数不至于过大（当年的计算能力比较弱）;强制限定不同特征图的组合可以使映射得到的特征图学习到不同的特征模式。</p>
</blockquote>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/featureMap.jpg" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/featureMap.jpg" srcset="data:image/png;base64,666" alt="FeatureMap"></p>
<p>​                                                                图4.2 $S_2$与$C_3$之间的特征图稀疏连接</p>
<blockquote>
<p>​    $^*$ $C_5$卷积层在图4.1中显示为全连接层，原论文中解释这里实际采用的是卷积操作，只是刚好在$5\times5$卷积后尺寸被压缩为$1\times1$，输出结果看起来和全连接很相似。</p>
</blockquote>
<h3 id="模型特性"><a href="#模型特性" class="headerlink" title="模型特性"></a>模型特性</h3><ul>
<li>卷积网络使用一个3层的序列组合：卷积、下采样（池化）、非线性映射（LeNet-5最重要的特性，奠定了目前深层卷积网络的基础）</li>
<li>使用卷积提取空间特征</li>
<li>使用映射的空间均值进行下采样</li>
<li>使用$tanh$或$sigmoid$进行非线性映射</li>
<li>多层神经网络（MLP）作为最终的分类器</li>
<li>层间的稀疏连接矩阵以避免巨大的计算开销</li>
</ul>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><h3 id="模型介绍-1"><a href="#模型介绍-1" class="headerlink" title="模型介绍"></a>模型介绍</h3><p>​    AlexNet是由$Alex$ $Krizhevsky $提出的首个应用于图像分类的深层卷积神经网络，该网络在2012年ILSVRC（ImageNet Large Scale Visual Recognition Competition）图像分类竞赛中以15.3%的top-5测试错误率赢得第一名$^{[2]}$。AlexNet使用GPU代替CPU进行运算，使得在可接受的时间范围内模型结构能够更加复杂，它的出现证明了深层卷积神经网络在复杂模型下的有效性，使CNN在计算机视觉中流行开来，直接或间接地引发了深度学习的热潮。</p>
<h3 id="模型结构-1"><a href="#模型结构-1" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/alexnet.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/alexnet.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                                                         图4.3 AlexNet网络结构图</p>
<p>​    如图4.3所示，除去下采样（池化层）和局部响应规范化操作（Local Responsible Normalization, LRN），AlexNet一共包含8层，前5层由卷积层组成，而剩下的3层为全连接层。网络结构分为上下两层，分别对应两个GPU的操作过程，除了中间某些层（$C<em>3$卷积层和$F</em>{6-8}$全连接层会有GPU间的交互），其他层两个GPU分别计算结 果。最后一层全连接层的输出作为$softmax$的输入，得到1000个图像分类标签对应的概率值。除去GPU并行结构的设计，AlexNet网络结构与LeNet十分相似，其网络的参数配置如表4.2所示。</p>
<p>​                                    表4.2 AlexNet网络参数配置</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">网络层</th>
<th style="text-align:center">输入尺寸</th>
<th style="text-align:center">核尺寸</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:center">可训练参数量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">卷积层$C_1$ $^*$</td>
<td style="text-align:center">$224\times224\times3$</td>
<td style="text-align:center">$11\times11\times3/4,48(\times2_{GPU})$</td>
<td style="text-align:center">$55\times55\times48(\times2_{GPU})$</td>
<td style="text-align:center">$(11\times11\times3+1)\times48\times2$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max}$$^*$</td>
<td style="text-align:center">$55\times55\times48(\times2_{GPU})$</td>
<td style="text-align:center">$3\times3/2(\times2_{GPU})$</td>
<td style="text-align:center">$27\times27\times48(\times2_{GPU})$</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_2$</td>
<td style="text-align:center">$27\times27\times48(\times2_{GPU})$</td>
<td style="text-align:center">$5\times5\times48/1,128(\times2_{GPU})$</td>
<td style="text-align:center">$27\times27\times128(\times2_{GPU})$</td>
<td style="text-align:center">$(5\times5\times48+1)\times128\times2$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max}$</td>
<td style="text-align:center">$27\times27\times128(\times2_{GPU})$</td>
<td style="text-align:center">$3\times3/2(\times2_{GPU})$</td>
<td style="text-align:center">$13\times13\times128(\times2_{GPU})$</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_3$ $^*$</td>
<td style="text-align:center">$13\times13\times128\times2_{GPU}$</td>
<td style="text-align:center">$3\times3\times256/1,192(\times2_{GPU})$</td>
<td style="text-align:center">$13\times13\times192(\times2_{GPU})$</td>
<td style="text-align:center">$(3\times3\times256+1)\times192\times2$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_4$</td>
<td style="text-align:center">$13\times13\times192(\times2_{GPU})$</td>
<td style="text-align:center">$3\times3\times192/1,192(\times2_{GPU})$</td>
<td style="text-align:center">$13\times13\times192(\times2_{GPU})$</td>
<td style="text-align:center">$(3\times3\times192+1)\times192\times2$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_5$</td>
<td style="text-align:center">$13\times13\times192(\times2_{GPU})$</td>
<td style="text-align:center">$3\times3\times192/1,128(\times2_{GPU})$</td>
<td style="text-align:center">$13\times13\times128(\times2_{GPU})$</td>
<td style="text-align:center">$(3\times3\times192+1)\times128\times2$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max}$</td>
<td style="text-align:center">$13\times13\times128(\times2_{GPU})$</td>
<td style="text-align:center">$3\times3/2(\times2_{GPU})$</td>
<td style="text-align:center">$6\times6\times128(\times2_{GPU})$</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">全连接层$F_6$  $^*$</td>
<td style="text-align:center">$6\times6\times128\times2_{GPU}$</td>
<td style="text-align:center">$9216\times2048(\times2_{GPU})$</td>
<td style="text-align:center">$1\times1\times2048(\times2_{GPU})$</td>
<td style="text-align:center">$(9216+1)\times2048\times2$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$F_7$</td>
<td style="text-align:center">$1\times1\times2048\times2_{GPU}$</td>
<td style="text-align:center">$4096\times2048(\times2_{GPU})$</td>
<td style="text-align:center">$1\times1\times2048(\times2_{GPU})$</td>
<td style="text-align:center">$(4096+1)\times2048\times2$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$F_8$</td>
<td style="text-align:center">$1\times1\times2048\times2_{GPU}$</td>
<td style="text-align:center">$4096\times1000$</td>
<td style="text-align:center">$1\times1\times1000$</td>
<td style="text-align:center">$(4096+1)\times1000\times2$</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>卷积层$C_1$输入为$224\times224\times3$的图片数据，分别在两个GPU中经过核为$11\times11\times3$、步长（stride）为4的卷积卷积后，分别得到两条独立的$55\times55\times48$的输出数据。</p>
<p>下采样层$S<em>{max}$实际上是嵌套在卷积中的最大池化操作，但是为了区分没有采用最大池化的卷积层单独列出来。在$C</em>{1-2}$卷积层中的池化操作之后（ReLU激活操作之前），还有一个LRN操作，用作对相邻特征点的归一化处理。</p>
<p>卷积层$C<em>3$ 的输入与其他卷积层不同，$13\times13\times192\times2</em>{GPU}$表示汇聚了上一层网络在两个GPU上的输出结果作为输入，所以在进行卷积操作时通道上的卷积核维度为384。</p>
<p>全连接层$F_{6-8}$中输入数据尺寸也和$C_3$类似，都是融合了两个GPU流向的输出结果作为输入。</p>
</blockquote>
<h3 id="模型特性-1"><a href="#模型特性-1" class="headerlink" title="模型特性"></a>模型特性</h3><ul>
<li>所有卷积层都使用ReLU作为非线性映射函数，使模型收敛速度更快</li>
<li>在多个GPU上进行模型的训练，不但可以提高模型的训练速度，还能提升数据的使用规模</li>
<li>使用LRN对局部的特征进行归一化，结果作为ReLU激活函数的输入能有效降低错误率</li>
<li>重叠最大池化（overlapping max pooling），即池化范围z与步长s存在关系$z&gt;s$（如$S_{max}$中核尺度为$3\times3/2$），避免平均池化（average pooling）的平均效应</li>
<li>使用随机丢弃技术（dropout）选择性地忽略训练中的单个神经元，避免模型的过拟合</li>
</ul>
<h2 id="ZFNet"><a href="#ZFNet" class="headerlink" title="ZFNet"></a>ZFNet</h2><h3 id="模型介绍-2"><a href="#模型介绍-2" class="headerlink" title="模型介绍"></a>模型介绍</h3><p>​    ZFNet是由$Matthew$ $D. Zeiler$和$Rob$ $Fergus$在AlexNet基础上提出的大型卷积网络，在2013年ILSVRC图像分类竞赛中以11.19%的错误率获得冠军（实际上原ZFNet所在的队伍并不是真正的冠军，原ZFNet以13.51%错误率排在第8，真正的冠军是$Clarifai$这个队伍，而$Clarifai$这个队伍所对应的一家初创公司的CEO又是$Zeiler$，而且$Clarifai$对ZFNet的改动比较小，所以通常认为是ZFNet获得了冠军）$^{[3-4]}$。ZFNet实际上是微调（fine-tuning）了的AlexNet，并通过反卷积（Deconvolution）的方式可视化各层的输出特征图，进一步解释了卷积操作在大型网络中效果显著的原因。</p>
<h3 id="模型结构-2"><a href="#模型结构-2" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image21.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image21.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image21.jpeg" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image21.jpeg" srcset="data:image/png;base64,666" alt=""></p>
<p>​                        图4.4 ZFNet网络结构图（原始结构图与AlexNet风格结构图）</p>
<p>​    如图4.4所示，ZFNet与AlexNet类似，都是由8层网络组成的卷积神经网络，其中包含5层卷积层和3层全连接层。两个网络结构最大的不同在于，ZFNet第一层卷积采用了$7\times7\times3/2$的卷积核替代了AlexNet中第一层卷积核$11\times11\times3/4$的卷积核。图4.5中ZFNet相比于AlexNet在第一层输出的特征图中包含更多中间频率的信息，而AlexNet第一层输出的特征图大多是低频或高频的信息，对中间频率特征的缺失导致后续网络层次如图4.5（c）能够学习到的特征不够细致，而导致这个问题的根本原因在于AlexNet在第一层中采用的卷积核和步长过大。</p>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/zfnet-layer1.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/zfnet-layer1.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/zfnet-layer2.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/zfnet-layer2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    图4.5 （a）ZFNet第一层输出的特征图（b）AlexNet第一层输出的特征图（c）AlexNet第二层输出的特征图（d）ZFNet第二层输出的特征图</p>
<p>​                                    表4.3 ZFNet网络参数配置<br>|        网络层         |               输入尺寸               |                  核尺寸                  |               输出尺寸               |              可训练参数量               |<br>| :—————————-: | :—————————————————: | :———————————————————: | :—————————————————: | :——————————————————-: |<br>|   卷积层$C<em>1$ $^*$  |        $224\times224\times3$         |          $7\times7\times3/2,96$          |        $110\times110\times96$        |      $(7\times7\times3+1)\times96$      |<br>| 下采样层$S</em>{max}$ |        $110\times110\times96$        |               $3\times3/2$               |         $55\times55\times96$         |                    0                    |<br>|      卷积层$C<em>2$ $^*$      |         $55\times55\times96$         |         $5\times5\times96/2,256$         |        $26\times26\times256$        | $(5\times5\times96+1)\times256$ |<br>|   下采样层$S</em>{max}$   | $26\times26\times256$ |       $3\times3/2$       | $13\times13\times256$ |                    0                    |<br>|   卷积层$C<em>3$  |  $13\times13\times256$  | $3\times3\times256/1,384$ | $13\times13\times384$ | $(3\times3\times256+1)\times384$ |<br>|      卷积层$C_4$      | $13\times13\times384$ | $3\times3\times384/1,384$ | $13\times13\times384$ | $(3\times3\times384+1)\times384$ |<br>|      卷积层$C_5$      | $13\times13\times384$ | $3\times3\times384/1,256$ | $13\times13\times256$ | $(3\times3\times384+1)\times256$ |<br>|   下采样层$S</em>{max}$   | $13\times13\times256$ |       $3\times3/2$       |  $6\times6\times256$  |                    0                    |<br>|  全连接层$F_6$  |   $6\times6\times256$   |     $9216\times4096$     | $1\times1\times4096$ |       $(9216+1)\times4096$       |<br>|     全连接层$F_7$     |  $1\times1\times4096$  |     $4096\times4096$     | $1\times1\times4096$ |       $(4096+1)\times4096$       |<br>|     全连接层$F_8$     | $1\times1\times4096$ |             $4096\times1000$             |         $1\times1\times1000$         |       $(4096+1)\times1000$       |</p>
<blockquote>
<p>卷积层$C_1$与AlexNet中的$C_1$有所不同，采用$7\times7\times3/2$的卷积核代替$11\times11\times3/4$，使第一层卷积输出的结果可以包含更多的中频率特征，对后续网络层中多样化的特征组合提供更多选择，有利于捕捉更细致的特征。</p>
<p>卷积层$C_2$采用了步长2的卷积核，区别于AlexNet中$C_2$的卷积核步长，所以输出的维度有所差异。</p>
</blockquote>
<h3 id="模型特性-2"><a href="#模型特性-2" class="headerlink" title="模型特性"></a>模型特性</h3><p>​    ZFNet与AlexNet在结构上几乎相同，此部分虽属于模型特性，但准确地说应该是ZFNet原论文中可视化技术的贡献。</p>
<ul>
<li>可视化技术揭露了激发模型中每层单独的特征图。</li>
<li>可视化技术允许观察在训练阶段特征的演变过程且诊断出模型的潜在问题。</li>
<li>可视化技术用到了多层解卷积网络，即由特征激活返回到输入像素空间。</li>
<li>可视化技术进行了分类器输出的敏感性分析，即通过阻止部分输入图像来揭示那部分对于分类是重要的。</li>
<li>可视化技术提供了一个非参数的不变性来展示来自训练集的哪一块激活哪个特征图，不仅需要裁剪输入图片，而且自上而下的投影来揭露来自每块的结构激活一个特征图。</li>
<li>可视化技术依赖于解卷积操作，即卷积操作的逆过程，将特征映射到像素上。</li>
</ul>
<h2 id="Network-in-Network"><a href="#Network-in-Network" class="headerlink" title="Network in Network"></a>Network in Network</h2><h3 id="模型介绍-3"><a href="#模型介绍-3" class="headerlink" title="模型介绍"></a>模型介绍</h3><p>​    Network In Network (NIN)是由$Min Lin$等人提出，在CIFAR-10和CIFAR-100分类任务中达到当时的最好水平，因其网络结构是由三个多层感知机堆叠而被成为NIN$^{[5]}$。NIN以一种全新的角度审视了卷积神经网络中的卷积核设计，通过引入子网络结构代替纯卷积中的线性映射部分，这种形式的网络结构激发了更复杂的卷积神经网络的结构设计，其中下一节中介绍的GoogLeNet的Inception结构就是来源于这个思想。</p>
<h3 id="模型结构-3"><a href="#模型结构-3" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image23.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image23.png" srcset="data:image/png;base64,666" alt=""><br>​                                    图 4.6 NIN网络结构图</p>
<p>​    NIN由三层的多层感知卷积层（MLPConv Layer）构成，每一层多层感知卷积层内部由若干层的局部全连接层和非线性激活函数组成，代替了传统卷积层中采用的线性卷积核。在网络推理（inference）时，这个多层感知器会对输入特征图的局部特征进行划窗计算，并且每个划窗的局部特征图对应的乘积的权重是共享的，这两点是和传统卷积操作完全一致的，最大的不同在于多层感知器对局部特征进行了非线性的映射，而传统卷积的方式是线性的。NIN的网络参数配置表4.4所示（原论文并未给出网络参数，表中参数为编者结合网络结构图和CIFAR-100数据集以$3\times3$卷积为例给出）。</p>
<p>​                    表4.4 NIN网络参数配置（结合原论文NIN结构和CIFAR-100数据给出）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">网络层</th>
<th style="text-align:center">输入尺寸</th>
<th style="text-align:center">核尺寸</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:center">参数个数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">局部全连接层$L_{11}$ $^*$</td>
<td style="text-align:center">$32\times32\times3$</td>
<td style="text-align:center">$(3\times3)\times16/1$</td>
<td style="text-align:center">$30\times30\times16$</td>
<td style="text-align:center">$(3\times3\times3+1)\times16$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$L_{12}$ $^*$</td>
<td style="text-align:center">$30\times30\times16$</td>
<td style="text-align:center">$16\times16$</td>
<td style="text-align:center">$30\times30\times16$</td>
<td style="text-align:center">$((16+1)\times16)$</td>
</tr>
<tr>
<td style="text-align:center">局部全连接层$L_{21}$</td>
<td style="text-align:center">$30\times30\times16$</td>
<td style="text-align:center">$(3\times3)\times64/1$</td>
<td style="text-align:center">$28\times28\times64$</td>
<td style="text-align:center">$(3\times3\times16+1)\times64$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$L_{22}$</td>
<td style="text-align:center">$28\times28\times64$</td>
<td style="text-align:center">$64\times64$</td>
<td style="text-align:center">$28\times28\times64$</td>
<td style="text-align:center">$((64+1)\times64)$</td>
</tr>
<tr>
<td style="text-align:center">局部全连接层$L_{31}$</td>
<td style="text-align:center">$28\times28\times64$</td>
<td style="text-align:center">$(3\times3)\times100/1$</td>
<td style="text-align:center">$26\times26\times100$</td>
<td style="text-align:center">$(3\times3\times64+1)\times100$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$L_{32}$</td>
<td style="text-align:center">$26\times26\times100$</td>
<td style="text-align:center">$100\times100$</td>
<td style="text-align:center">$26\times26\times100$</td>
<td style="text-align:center">$((100+1)\times100)$</td>
</tr>
<tr>
<td style="text-align:center">全局平均采样$GAP$ $^*$</td>
<td style="text-align:center">$26\times26\times100$</td>
<td style="text-align:center">$26\times26\times100/1$</td>
<td style="text-align:center">$1\times1\times100$</td>
<td style="text-align:center">$0$</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>局部全连接层$L<em>{11}$实际上是对原始输入图像进行划窗式的全连接操作，因此划窗得到的输出特征尺寸为$30\times30$（$\frac{32-3_k+1}{1</em>{stride}}=30$）<br>全连接层$L<em>{12}$是紧跟$L</em>{11}$后的全连接操作，输入的特征是划窗后经过激活的局部响应特征，因此仅需连接$L<em>{11}$和$L</em>{12}$的节点即可，而每个局部全连接层和紧接的全连接层构成代替卷积操作的多层感知卷积层（MLPConv）。<br>全局平均采样层或全局平均池化层$GAP$（Global Average Pooling）将$L_{32}$输出的每一个特征图进行全局的平均池化操作，直接得到最后的类别数，可以有效地减少参数量。</p>
</blockquote>
<h3 id="模型特点"><a href="#模型特点" class="headerlink" title="模型特点"></a>模型特点</h3><ul>
<li>使用多层感知机结构来代替卷积的滤波操作，不但有效减少卷积核数过多而导致的参数量暴涨问题，还能通过引入非线性的映射来提高模型对特征的抽象能力。</li>
<li>使用全局平均池化来代替最后一个全连接层，能够有效地减少参数量（没有可训练参数），同时池化用到了整个特征图的信息，对空间信息的转换更加鲁棒，最后得到的输出结果可直接作为对应类别的置信度。</li>
</ul>
<h2 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h2><h3 id="模型介绍-4"><a href="#模型介绍-4" class="headerlink" title="模型介绍"></a>模型介绍</h3><p>​    VGGNet是由牛津大学视觉几何小组（Visual Geometry Group, VGG）提出的一种深层卷积网络结构，他们以7.32%的错误率赢得了2014年ILSVRC分类任务的亚军（冠军由GoogLeNet以6.65%的错误率夺得）和25.32%的错误率夺得定位任务（Localization）的第一名（GoogLeNet错误率为26.44%）$^{[5]}$，网络名称VGGNet取自该小组名缩写。VGGNet是首批把图像分类的错误率降低到10%以内模型，同时该网络所采用的$3\times3$卷积核的思想是后来许多模型的基础，该模型发表在2015年国际学习表征会议（International Conference On Learning Representations, ICLR）后至今被引用的次数已经超过1万4千余次。</p>
<h3 id="模型结构-4"><a href="#模型结构-4" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/vgg16.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/vgg16.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                图 4.7 VGG16网络结构图</p>
<p>​    在原论文中的VGGNet包含了6个版本的演进，分别对应VGG11、VGG11-LRN、VGG13、VGG16-1、VGG16-3和VGG19，不同的后缀数值表示不同的网络层数（VGG11-LRN表示在第一层中采用了LRN的VGG11，VGG16-1表示后三组卷积块中最后一层卷积采用卷积核尺寸为$1\times1$，相应的VGG16-3表示卷积核尺寸为$3\times3$），本节介绍的VGG16为VGG16-3。图4.7中的VGG16体现了VGGNet的核心思路，使用$3\times3$的卷积组合代替大尺寸的卷积（2个$3\times3卷积即可与$$5\times5$卷积拥有相同的感受视野），网络参数设置如表4.5所示。</p>
<p>​                                表4.5 VGG16网络参数配置</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">网络层</th>
<th style="text-align:center">输入尺寸</th>
<th style="text-align:center">核尺寸</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:center">参数个数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">卷积层$C_{11}$</td>
<td style="text-align:center">$224\times224\times3$</td>
<td style="text-align:center">$3\times3\times64/1$</td>
<td style="text-align:center">$224\times224\times64$</td>
<td style="text-align:center">$(3\times3\times3+1)\times64$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{12}$</td>
<td style="text-align:center">$224\times224\times64$</td>
<td style="text-align:center">$3\times3\times64/1$</td>
<td style="text-align:center">$224\times224\times64$</td>
<td style="text-align:center">$(3\times3\times64+1)\times64$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max1}$</td>
<td style="text-align:center">$224\times224\times64$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$112\times112\times64$</td>
<td style="text-align:center">$0$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{21}$</td>
<td style="text-align:center">$112\times112\times64$</td>
<td style="text-align:center">$3\times3\times128/1$</td>
<td style="text-align:center">$112\times112\times128$</td>
<td style="text-align:center">$(3\times3\times64+1)\times128$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{22}$</td>
<td style="text-align:center">$112\times112\times128$</td>
<td style="text-align:center">$3\times3\times128/1$</td>
<td style="text-align:center">$112\times112\times128$</td>
<td style="text-align:center">$(3\times3\times128+1)\times128$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max2}$</td>
<td style="text-align:center">$112\times112\times128$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$56\times56\times128$</td>
<td style="text-align:center">$0$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{31}$</td>
<td style="text-align:center">$56\times56\times128$</td>
<td style="text-align:center">$3\times3\times256/1$</td>
<td style="text-align:center">$56\times56\times256$</td>
<td style="text-align:center">$(3\times3\times128+1)\times256$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{32}$</td>
<td style="text-align:center">$56\times56\times256$</td>
<td style="text-align:center">$3\times3\times256/1$</td>
<td style="text-align:center">$56\times56\times256$</td>
<td style="text-align:center">$(3\times3\times256+1)\times256$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{33}$</td>
<td style="text-align:center">$56\times56\times256$</td>
<td style="text-align:center">$3\times3\times256/1$</td>
<td style="text-align:center">$56\times56\times256$</td>
<td style="text-align:center">$(3\times3\times256+1)\times256$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max3}$</td>
<td style="text-align:center">$56\times56\times256$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$28\times28\times256$</td>
<td style="text-align:center">$0$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{41}$</td>
<td style="text-align:center">$28\times28\times256$</td>
<td style="text-align:center">$3\times3\times512/1$</td>
<td style="text-align:center">$28\times28\times512$</td>
<td style="text-align:center">$(3\times3\times256+1)\times512$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{42}$</td>
<td style="text-align:center">$28\times28\times512$</td>
<td style="text-align:center">$3\times3\times512/1$</td>
<td style="text-align:center">$28\times28\times512$</td>
<td style="text-align:center">$(3\times3\times512+1)\times512$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{43}$</td>
<td style="text-align:center">$28\times28\times512$</td>
<td style="text-align:center">$3\times3\times512/1$</td>
<td style="text-align:center">$28\times28\times512$</td>
<td style="text-align:center">$(3\times3\times512+1)\times512$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max4}$</td>
<td style="text-align:center">$28\times28\times512$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$0$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{51}$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$3\times3\times512/1$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$(3\times3\times512+1)\times512$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{52}$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$3\times3\times512/1$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$(3\times3\times512+1)\times512$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{53}$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$3\times3\times512/1$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$(3\times3\times512+1)\times512$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max5}$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$7\times7\times512$</td>
<td style="text-align:center">$0$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$FC_{1}$</td>
<td style="text-align:center">$7\times7\times512$</td>
<td style="text-align:center">$(7\times7\times512)\times4096$</td>
<td style="text-align:center">$1\times4096$</td>
<td style="text-align:center">$(7\times7\times512+1)\times4096$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$FC_{2}$</td>
<td style="text-align:center">$1\times4096$</td>
<td style="text-align:center">$4096\times4096$</td>
<td style="text-align:center">$1\times4096$</td>
<td style="text-align:center">$(4096+1)\times4096$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$FC_{3}$</td>
<td style="text-align:center">$1\times4096$</td>
<td style="text-align:center">$4096\times1000$</td>
<td style="text-align:center">$1\times1000$</td>
<td style="text-align:center">$(4096+1)\times1000$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="模型特性-3"><a href="#模型特性-3" class="headerlink" title="模型特性"></a>模型特性</h3><ul>
<li>整个网络都使用了同样大小的卷积核尺寸$3\times3$和最大池化尺寸$2\times2$。</li>
<li>$1\times1$卷积的意义主要在于线性变换，而输入通道数和输出通道数不变，没有发生降维。</li>
<li>两个$3\times3$的卷积层串联相当于1个$5\times5$的卷积层，感受野大小为$5\times5$。同样地，3个$3\times3$的卷积层串联的效果则相当于1个$7\times7$的卷积层。这样的连接方式使得网络参数量更小，而且多层的激活函数令网络对特征的学习能力更强。</li>
<li>VGGNet在训练时有一个小技巧，先训练浅层的的简单网络VGG11，再复用VGG11的权重来初始化VGG13，如此反复训练并初始化VGG19，能够使训练时收敛的速度更快。</li>
<li>在训练过程中使用多尺度的变换对原始数据做数据增强，使得模型不易过拟合。</li>
</ul>
<h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><h3 id="模型介绍-5"><a href="#模型介绍-5" class="headerlink" title="模型介绍"></a>模型介绍</h3><p>​    GoogLeNet作为2014年ILSVRC在分类任务上的冠军，以6.65%的错误率力压VGGNet等模型，在分类的准确率上面相比过去两届冠军ZFNet和AlexNet都有很大的提升。从名字<strong>GoogLe</strong>Net可以知道这是来自谷歌工程师所设计的网络结构，而名字中Goog<strong>LeNet</strong>更是致敬了LeNet$^{[0]}$。GoogLeNet中最核心的部分是其内部子网络结构Inception，该结构灵感来源于NIN，至今已经经历了四次版本迭代（Inception$_{v1-4}$）。</p>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/img_inception_01.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/img_inception_01.png" srcset="data:image/png;base64,666" alt=""><br>​                    图 4.8 Inception性能比较图</p>
<h3 id="模型结构-5"><a href="#模型结构-5" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image25.jpeg" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image25.jpeg" srcset="data:image/png;base64,666" alt=""><br>​                    图 4.9 GoogLeNet网络结构图<br>​    如图4.9中所示，GoogLeNet相比于以前的卷积神经网络结构，除了在深度上进行了延伸，还对网络的宽度进行了扩展，整个网络由许多块状子网络的堆叠而成，这个子网络构成了Inception结构。图4.9为Inception的四个版本：$Inception<em>{v1}$在同一层中采用不同的卷积核，并对卷积结果进行合并;$Inception</em>{v2}$组合不同卷积核的堆叠形式，并对卷积结果进行合并;$Inception<em>{v3}$则在$v_2$基础上进行深度组合的尝试;$Inception</em>{v4}$结构相比于前面的版本更加复杂，子网络中嵌套着子网络。</p>
<p>$Inception_{v1}$</p>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image27.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image27.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image28.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image28.png" srcset="data:image/png;base64,666" alt=""></p>
<p>$Inception_{v2}$</p>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image34.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image34.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image36.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image36.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image38.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image38.png" srcset="data:image/png;base64,666" alt=""></p>
<p>$Inception_{v3}$</p>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image37.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image37.png" srcset="data:image/png;base64,666" alt=""></p>
<p>$Inception_{v4}$</p>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image46.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image46.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image47.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image47.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image63.png" class="lazyload" data-srcset="/zh-TW/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/ch4/image63.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                    图 4.10 Inception$_{v1-4}$结构图</p>
<p>​                    表 4.6 GoogLeNet中Inception$_{v1}$网络参数配置</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">网络层</th>
<th style="text-align:center">输入尺寸</th>
<th style="text-align:center">核尺寸</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:center">参数个数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">卷积层$C_{11}$</td>
<td style="text-align:center">$H\times{W}\times{C_1}$</td>
<td style="text-align:center">$1\times1\times{C_2}/2$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}$</td>
<td style="text-align:center">$(1\times1\times{C_1}+1)\times{C_2}$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{21}$</td>
<td style="text-align:center">$H\times{W}\times{C_2}$</td>
<td style="text-align:center">$1\times1\times{C_2}/2$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}$</td>
<td style="text-align:center">$(1\times1\times{C_2}+1)\times{C_2}$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{22}$</td>
<td style="text-align:center">$H\times{W}\times{C_2}$</td>
<td style="text-align:center">$3\times3\times{C_2}/1$</td>
<td style="text-align:center">$H\times{W}\times{C_2}/1$</td>
<td style="text-align:center">$(3\times3\times{C_2}+1)\times{C_2}$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{31}$</td>
<td style="text-align:center">$H\times{W}\times{C_1}$</td>
<td style="text-align:center">$1\times1\times{C_2}/2$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}$</td>
<td style="text-align:center">$(1\times1\times{C_1}+1)\times{C_2}$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{32}$</td>
<td style="text-align:center">$H\times{W}\times{C_2}$</td>
<td style="text-align:center">$5\times5\times{C_2}/1$</td>
<td style="text-align:center">$H\times{W}\times{C_2}/1$</td>
<td style="text-align:center">$(5\times5\times{C_2}+1)\times{C_2}$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{41}$</td>
<td style="text-align:center">$H\times{W}\times{C_1}$</td>
<td style="text-align:center">$3\times3/2$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}$</td>
<td style="text-align:center">$0$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{42}$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}$</td>
<td style="text-align:center">$1\times1\times{C_2}/1$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}$</td>
<td style="text-align:center">$(3\times3\times{C_2}+1)\times{C_2}$</td>
</tr>
<tr>
<td style="text-align:center">合并层$M$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}(\times4)$</td>
<td style="text-align:center">拼接</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times({C_2}\times4)$</td>
<td style="text-align:center">$0$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="模型特性-4"><a href="#模型特性-4" class="headerlink" title="模型特性"></a>模型特性</h3><ul>
<li><p>采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合； </p>
</li>
<li><p>之所以卷积核大小采用1、3和5，主要是为了方便对齐。设定卷积步长stride=1之后，只要分别设定pad=0、1、2，那么卷积之后便可以得到相同维度的特征，然后这些特征就可以直接拼接在一起了；</p>
</li>
<li><p>网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积的比例也要增加。但是，使用5x5的卷积核仍然会带来巨大的计算量。 为此，文章借鉴NIN2，采用1x1卷积核来进行降维。</p>
<h1 id=""><a href="#" class="headerlink" title=" "></a> </h1></li>
</ul>
<h2 id="Restnet"><a href="#Restnet" class="headerlink" title="Restnet"></a>Restnet</h2><h2 id="Densenet"><a href="#Densenet" class="headerlink" title="Densenet"></a>Densenet</h2><h2 id="为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？"><a href="#为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？" class="headerlink" title="为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？"></a>为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？</h2><ul>
<li>评测对比：为了让自己的结果更有说服力，在发表自己成果的时候会同一个标准的baseline及在baseline上改进而进行比较，常见的比如各种检测分割的问题都会基于VGG或者Resnet101这样的基础网络。</li>
<li>时间和精力有限：在科研压力和工作压力中，时间和精力只允许大家在有限的范围探索。</li>
<li>模型创新难度大：进行基本模型的改进需要大量的实验和尝试，并且需要大量的实验积累和强大灵感，很有可能投入产出比比较小。</li>
<li>资源限制：创造一个新的模型需要大量的时间和计算资源，往往在学校和小型商业团队不可行。</li>
<li>在实际的应用场景中，其实是有大量的非标准模型的配置。</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>卷积神经网络</title>
    <url>/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="卷积神经网络（CNN）"><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h1><p>​    卷积神经网络是一种用来处理局部和整体相关性的计算网络结构，被应用在图像识别、自然语言处理甚至是语音识别领域，因为图像数据具有显著的局部与整体关系，其在图像识别领域的应用获得了巨大的成功。</p>
<h2 id="卷积神经网络的组成层"><a href="#卷积神经网络的组成层" class="headerlink" title="卷积神经网络的组成层"></a>卷积神经网络的组成层</h2><p>​    以图像分类任务为例，在表5.1所示卷积神经网络中，一般包含5种类型的网络层次结构：</p>
<p>​                                                                 表5.1 卷积神经网络的组成</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">CNN层次结构</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">输入层</td>
<td style="text-align:center">$W_1\times H_1\times 3$</td>
<td style="text-align:left">卷积网络的原始输入，可以是原始或预处理后的像素矩阵</td>
</tr>
<tr>
<td style="text-align:center">卷积层</td>
<td style="text-align:center">$W_1\times H_1\times K$</td>
<td style="text-align:left">参数共享、局部连接，利用平移不变性从全局特征图提取局部特征</td>
</tr>
<tr>
<td style="text-align:center">激活层</td>
<td style="text-align:center">$W_1\times H_1\times K$</td>
<td style="text-align:left">将卷积层的输出结果进行非线性映射</td>
</tr>
<tr>
<td style="text-align:center">池化层</td>
<td style="text-align:center">$W_2\times H_2\times K$</td>
<td style="text-align:left">进一步筛选特征，可以有效减少后续网络层次所需的参数量</td>
</tr>
<tr>
<td style="text-align:center">全连接层</td>
<td style="text-align:center">$(W_2 \cdot H_2 \cdot K)\times C$</td>
<td style="text-align:left">将多维特征展平为2维特征，通常低维度特征对应任务的学习目标（类别或回归值）</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>$W_1\times H_1\times 3$对应原始图像或经过预处理的像素值矩阵，3对应RGB图像的通道;$K$表示卷积层中卷积核（滤波器）的个数;$W_2\times H_2$ 为池化后特征图的尺度，在全局池化中尺度对应$1\times 1$;$(W_2 \cdot H_2 \cdot K)$是将多维特征压缩到1维之后的大小，$C$对应的则是图像类别个数。</p>
</blockquote>
<h3 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h3><p>​    输入层(Input Layer)通常是输入卷积神经网络的原始数据或经过预处理的数据，可以是图像识别领域中原始三维的多彩图像，也可以是音频识别领域中经过傅利叶变换的二维波形数据，甚至是自然语言处理中一维表示的句子向量。以图像分类任务为例，输入层输入的图像一般包含RGB三个通道，是一个由长宽分别为$H$和$W$组成的3维像素值矩阵$H\times W \times 3$，卷积网络会将输入层的数据传递到一系列卷积、池化等操作进行特征提取和转化，最终由全连接层对特征进行汇总和结果输出。根据计算能力、存储大小和模型结构的不同，卷积神经网络每次可以批量处理的图像个数不尽相同，若指定输入层接收到的图像个数为$N$，则输入层的输出数据为$N\times H\times W\times 3$。</p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>​    卷积层(Convolution Layer)通常用作对输入层输入数据进行特征提取，通过卷积核矩阵对原始数据中隐含关联性的一种抽象。卷积操作原理上其实是对两张像素矩阵进行点乘求和的数学操作，其中一个矩阵为输入的数据矩阵，另一个矩阵则为卷积核（滤波器或特征矩阵），求得的结果表示为原始图像中提取的特定局部特征。图5.1表示卷积操作过程中的不同填充策略，上半部分采用零填充，下半部分采用有效卷积（舍弃不能完整运算的边缘部分）。<br>​                                                    <img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/convolution.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/convolution.png" srcset="data:image/png;base64,666" alt="conv-same"><br>​                                                        图5.1 卷积操作示意图</p>
<h3 id="激活层"><a href="#激活层" class="headerlink" title="激活层"></a>激活层</h3><p>​    激活层(Activation Layer)负责对卷积层抽取的特征进行激活，由于卷积操作是由输入矩阵与卷积核矩阵进行相差的线性变化关系，需要激活层对其进行非线性的映射。激活层主要由激活函数组成，即在卷积层输出结果的基础上嵌套一个非线性函数，让输出的特征图具有非线性关系。卷积网络中通常采用ReLU来充当激活函数（还包括tanh和sigmoid等）ReLU的函数形式如公式（5-1）所示，能够限制小于0的值为0,同时大于等于0的值保持不变。</p>
<script type="math/tex; mode=display">
f(x)=\begin{cases}
   0 &\text{if } x<0 \\
   x &\text{if } x\ge 0
\end{cases}
\tag{5-1}</script><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>​    池化层又称为降采样层(Downsampling Layer)，作用是对感受域内的特征进行筛选，提取区域内最具代表性的特征，能够有效地降低输出特征尺度，进而减少模型所需要的参数量。按操作类型通常分为最大池化(Max Pooling)、平均池化(Average Pooling)和求和池化(Sum Pooling)，它们分别提取感受域内最大、平均与总和的特征值作为输出，最常用的是最大池化。</p>
<h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>​    全连接层(Full Connected Layer)负责对卷积神经网络学习提取到的特征进行汇总，将多维的特征输入映射为二维的特征输出，高维表示样本批次，低位常常对应任务目标。</p>
<h2 id="卷积在图像中有什么直观作用"><a href="#卷积在图像中有什么直观作用" class="headerlink" title="卷积在图像中有什么直观作用"></a>卷积在图像中有什么直观作用</h2><p>​    在卷积神经网络中，卷积常用来提取图像的特征，但不同层次的卷积操作提取到的特征类型是不相同的，特征类型粗分如表5.2所示。<br>​                                                                 表5.2 卷积提取的特征类型</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">卷积层次</th>
<th style="text-align:center">特征类型</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">浅层卷积</td>
<td style="text-align:center">边缘特征</td>
</tr>
<tr>
<td style="text-align:center">中层卷积</td>
<td style="text-align:center">局部特征</td>
</tr>
<tr>
<td style="text-align:center">深层卷积</td>
<td style="text-align:center">全局特征</td>
</tr>
</tbody>
</table>
</div>
<p>图像与不同卷积核的卷积可以用来执行边缘检测、锐化和模糊等操作。表5.3显示了应用不同类型的卷积核（滤波器）后的各种卷积图像。<br>​                                                                 表5.3 一些常见卷积核的作用</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">卷积作用</th>
<th style="text-align:center">卷积核</th>
<th style="text-align:center">卷积后图像</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">输出原图</td>
<td style="text-align:center">$\begin{bmatrix} 0 &amp; 0 &amp; 0 \ 0 &amp; 1 &amp; 0 \ 0 &amp; 0 &amp; 0 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/cat.jpg" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/cat.jpg" srcset="data:image/png;base64,666" alt="origin_img"></td>
</tr>
<tr>
<td style="text-align:center">边缘检测（突出边缘差异）</td>
<td style="text-align:center">$\begin{bmatrix} 1 &amp; 0 &amp; -1 \ 0 &amp; 0 &amp; 0 \ -1 &amp; 0 &amp; 1 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/cat-edgeDetect.jpg" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/cat-edgeDetect.jpg" srcset="data:image/png;base64,666" alt="edgeDetect-1"></td>
</tr>
<tr>
<td style="text-align:center">边缘检测（突出中间值）</td>
<td style="text-align:center">$\begin{bmatrix} -1 &amp; -1 &amp; -1 \ -1 &amp; 8 &amp; -1 \ -1 &amp; -1 &amp; -1 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/cat-edgeDetect-2.jpg" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/cat-edgeDetect-2.jpg" srcset="data:image/png;base64,666" alt="edgeDetect-2"></td>
</tr>
<tr>
<td style="text-align:center">图像锐化</td>
<td style="text-align:center">$\begin{bmatrix} 0 &amp; -1 &amp; 0 \ -1 &amp; 5 &amp; -1 \ 0 &amp; -1 &amp; 0 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/cat-sharpen.jpg" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/cat-sharpen.jpg" srcset="data:image/png;base64,666" alt="sharpen_img"></td>
</tr>
<tr>
<td style="text-align:center">方块模糊</td>
<td style="text-align:center">$\begin{bmatrix} 1 &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 \end{bmatrix} \times \frac{1}{9}$</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/cat-boxblur.jpg" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/cat-boxblur.jpg" srcset="data:image/png;base64,666" alt="box_blur"></td>
</tr>
<tr>
<td style="text-align:center">高斯模糊</td>
<td style="text-align:center">$\begin{bmatrix} 1 &amp; 2 &amp; 1 \ 2 &amp; 4 &amp; 2 \ 1 &amp; 2 &amp; 1 \end{bmatrix} \times \frac{1}{16}$</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/cat-blur-gaussian.jpg" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/cat-blur-gaussian.jpg" srcset="data:image/png;base64,666" alt="gaussian_blur"></td>
</tr>
</tbody>
</table>
</div>
<h2 id="卷积层有哪些基本参数？"><a href="#卷积层有哪些基本参数？" class="headerlink" title="卷积层有哪些基本参数？"></a>卷积层有哪些基本参数？</h2><p>​    卷积层中需要用到卷积核（滤波器或特征检测器）与图像特征矩阵进行点乘运算，利用卷积核与对应的特征感受域进行划窗式运算时，需要设定卷积核对应的大小、步长、个数以及填充的方式，如表5.4所示。</p>
<p>​                                                                         表5.4 卷积层的基本参数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:left">作用</th>
<th style="text-align:left">常见设置</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">卷积核大小 (Kernel Size)</td>
<td style="text-align:left">卷积核的大小定义了卷积的感受野</td>
<td style="text-align:left">在过去常设为5，如LeNet-5；现在多设为3，通过堆叠$3\times3$的卷积核来达到更大的感受域</td>
</tr>
<tr>
<td style="text-align:center">卷积核步长 (Stride)</td>
<td style="text-align:left">定义了卷积核在卷积过程中的步长</td>
<td style="text-align:left">常见设置为1，表示滑窗距离为1，可以覆盖所有相邻位置特征的组合；当设置为更大值时相当于对特征组合降采样</td>
</tr>
<tr>
<td style="text-align:center">填充方式 (Padding)</td>
<td style="text-align:left">在卷积核尺寸不能完美匹配输入的图像矩阵时需要进行一定的填充策略</td>
<td style="text-align:left">设置为’SAME’表示对不足卷积核大小的边界位置进行某种填充（通常零填充）以保证卷积输出维度与与输入维度一致；当设置为’VALID’时则对不足卷积尺寸的部分进行舍弃，输出维度就无法保证与输入维度一致</td>
</tr>
<tr>
<td style="text-align:center">输入通道数 (In Channels)</td>
<td style="text-align:left">指定卷积操作时卷积核的深度</td>
<td style="text-align:left">默认与输入的特征矩阵通道数（深度）一致；在某些压缩模型中会采用通道分离的卷积方式</td>
</tr>
<tr>
<td style="text-align:center">输出通道数 (Out Channels)</td>
<td style="text-align:left">指定卷积核的个数</td>
<td style="text-align:left">若设置为与输入通道数一样的大小，可以保持输入输出维度的一致性；若采用比输入通道数更小的值，则可以减少整体网络的参数量</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>卷积操作维度变换公式：</p>
<p>$O<em>d =\begin{cases} \lceil \frac{(I_d - k</em>{size})+ 1)}{s}\rceil ,&amp; \text{padding=VALID}\ \lceil \frac{I_d}{s}\rceil,&amp;\text{padding=SAME} \end{cases}$</p>
<p>其中，$I<em>d$为输入维度，$O_d$为输出维度，$k</em>{size}$为卷积核大小，$s$为步长</p>
</blockquote>
<h2 id="卷积核有什么类型？"><a href="#卷积核有什么类型？" class="headerlink" title="卷积核有什么类型？"></a>卷积核有什么类型？</h2><p>​    常见的卷积主要是由连续紧密的卷积核对输入的图像特征进行滑窗式点乘求和操作，除此之外还有其他类型的卷积核在不同的任务中会用到，具体分类如表5.5所示。<br>​                                                                     表5.5 卷积核分类</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">卷积类别</th>
<th style="text-align:center">示意图</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">标准卷积</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/img7.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/img7.png" srcset="data:image/png;base64,666" alt="image"></td>
<td style="text-align:left">最常用的卷积核，连续紧密的矩阵形式可以提取图像区域中的相邻像素之间的关联关系，$3\times3$的卷积核可以获得$3\times3$像素范围的感受视野</td>
</tr>
<tr>
<td style="text-align:center">扩张卷积（带孔卷积或空洞卷积）</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/img8.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/img8.png" srcset="data:image/png;base64,666" alt="image"></td>
<td style="text-align:left">引入一个称作扩张率（Dilation Rate）的参数，使同样尺寸的卷积核可以获得更大的感受视野，相应的在相同感受视野的前提下比普通卷积采用更少的参数。同样是$3\times3$的卷积核尺寸，扩张卷积可以提取$5\times5$范围的区域特征，在实时图像分割领域广泛应用</td>
</tr>
<tr>
<td style="text-align:center">转置卷积</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/img10.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/img10.png" srcset="data:image/png;base64,666" alt="image"></td>
<td style="text-align:left">先对原始特征矩阵进行填充使其维度扩大到适配卷积目标输出维度，然后进行普通的卷积操作的一个过程，其输入到输出的维度变换关系恰好与普通卷积的变换关系相反，但这个变换并不是真正的逆变换操作，通常称为转置卷积(Transpose Convolution)而不是反卷积(Deconvolution)。转置卷积常见于目标检测领域中对小目标的检测和图像分割领域还原输入图像尺度。</td>
</tr>
<tr>
<td style="text-align:center">可分离卷积</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/img11.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/img11.png" srcset="data:image/png;base64,666" alt="image"></td>
<td style="text-align:left">标准的卷积操作是同时对原始图像$H\times W\times C$三个方向的卷积运算，假设有$K$个相同尺寸的卷积核，这样的卷积操作需要用到的参数为$H\times W\times C\times K$个；若将长宽与深度方向的卷积操作分离出变为$H\times W$与$C$的两步卷积操作，则同样的卷积核个数$K$，只需要$(H\times W + C)\times K$个参数，便可得到同样的输出尺度。可分离卷积(Seperable Convolution)通常应用在模型压缩或一些轻量的卷积神经网络中，如MobileNet$^{[1]}$、Xception$^{[2]}$等</td>
</tr>
</tbody>
</table>
</div>
<h2 id="二维卷积与三维卷积有什么区别？"><a href="#二维卷积与三维卷积有什么区别？" class="headerlink" title="二维卷积与三维卷积有什么区别？"></a>二维卷积与三维卷积有什么区别？</h2><ul>
<li><strong>二维卷积</strong><br>二维卷积操作如图5.3所示，为了更直观的说明，分别展示在单通道和多通道输入中，对单个通道输出的卷积操作。在单通道输入的情况下，若输入卷积核尺寸为 $(k_h, k_w, 1)$，卷积核在输入图像的空间维度上进行滑窗操作，每次滑窗和 $(k_h, k_w)$窗口内的值进行卷积操作，得到输出图像中的一个值。在多通道输入的情况下，假定输入图像特征通道数为3，卷积核尺寸则为$(k_h, k_w, 3)$，每次滑窗与3个通道上的$(k_h, k_w)$窗口内的所有值进行卷积操作，得到输出图像中的一个值。</li>
</ul>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.6.1.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.6.1.png" srcset="data:image/png;base64,666" alt="image"></p>
<ul>
<li><strong>三维卷积</strong><br>3D卷积操作如图所示，同样分为单通道和多通道，且假定只使用1个卷积核，即输出图像仅有一个通道。对于单通道输入，与2D卷积不同之处在于，输入图像多了一个深度(depth)维度，卷积核也多了一个$k_d$维度，因此3D卷积核的尺寸为$(k_h, k_w, k_d)$，每次滑窗与$(k_h, k_w, k_d)$窗口内的值进行相关操作，得到输出3D图像中的一个值。对于多通道输入，则与2D卷积的操作一样，每次滑窗与3个channels上的$(k_h, k_w, k_d)$窗口内的所有值进行相关操作，得到输出3D图像中的一个值。</li>
</ul>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.6.2.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.6.2.png" srcset="data:image/png;base64,666" alt="image"></p>
<h2 id="池化方法"><a href="#池化方法" class="headerlink" title="池化方法"></a>池化方法</h2><p>​    池化操作通常也叫做子采样(Subsampling)或降采样(Downsampling)，在构建卷积神经网络时，往往会用在卷积层之后，通过池化来降低卷积层输出的特征维度，有效减少网络参数的同时还可以防止过拟合现象。池化操作可以降低图像维度的原因，本质上是因为图像具有一种“静态性”的属性，这个意思是说在一个图像区域有用的特征极有可能在另一个区域同样有用。因此，为了描述一个大的图像，很直观的想法就是对不同位置的特征进行聚合统计。例如，可以计算图像在固定区域上特征的平均值 (或最大值)来代表这个区域的特征。<br>​                                                                              表5.6 池化分类</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">池化类型</th>
<th style="text-align:center">示意图</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">一般池化(General Pooling)</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/general_pooling.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/general_pooling.png" srcset="data:image/png;base64,666" alt="max_pooling"></td>
<td style="text-align:left">通常包括最大池化(Max Pooling)和平均池化(Mean Pooling)。以最大池化为例，池化范围$(2\times2)$和滑窗步长$(stride=2)$ 相同，仅提取一次相同区域的范化特征。</td>
</tr>
<tr>
<td style="text-align:center">重叠池化(Overlapping Pooling)</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/overlap_pooling.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/overlap_pooling.png" srcset="data:image/png;base64,666" alt="overlap_pooling"></td>
<td style="text-align:left">与一般池化操作相同，但是池化范围$P<em>{size}$与滑窗步长$stride$关系为$P</em>{size}&gt;stride$，同一区域内的像素特征可以参与多次滑窗提取，得到的特征表达能力更强，但计算量更大。</td>
</tr>
<tr>
<td style="text-align:center">空间金字塔池化$^*$(Spatial Pyramid Pooling)</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/spatial_pooling.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/spatial_pooling.png" srcset="data:image/png;base64,666" alt="spatial_pooling"></td>
<td style="text-align:left">在进行多尺度目标的训练时，卷积层允许输入的图像特征尺度是可变的，紧接的池化层若采用一般的池化方法会使得不同的输入特征输出相应变化尺度的特征，而卷积神经网络中最后的全连接层则无法对可变尺度进行运算，因此需要对不同尺度的输出特征采样到相同输出尺度。</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>SPPNet$^{[3]}$就引入了空间池化的组合，对不同输出尺度采用不同的滑窗大小和步长以确保输出尺度相同$(win_{size}=\lceil \frac{in}{out}\rceil; stride=\lfloor \frac{in}{out}\rfloor; )$，同时用如金字塔式叠加的多种池化尺度组合，以提取更加丰富的图像特征。常用于多尺度训练和目标检测中的区域提议网络(Region Proposal Network)的兴趣区域(Region of Interest)提取</p>
</blockquote>
<h2 id="1-times1-卷积的作用"><a href="#1-times1-卷积的作用" class="headerlink" title="$1\times1$卷积的作用"></a>$1\times1$卷积的作用</h2><p>​    NIN(Network in Network)$^{[4]}$是第一篇探索$1\times1$卷积核的论文，这篇论文通过在卷积层中使用MLP替代传统线性的卷积核，使单层卷积层内具有非线性映射的能力，也因其网络结构中嵌套MLP子网络而得名NIN。NIN对不同通道的特征整合到MLP自网络中，让不同通道的特征能够交互整合，使通道之间的信息得以流通，其中的MLP子网络恰恰可以用$1\times1$的卷积进行代替。</p>
<p>​    GoogLeNet$^{[5]}$则采用$1\times1$卷积核来减少模型的参数量。在原始版本的Inception模块中，由于每一层网络采用了更多的卷积核，大大增加了模型的参数量。此时在每一个较大卷积核的卷积层前引入$1\times1$卷积，可以通过分离通道与宽高卷积来减少模型参数量。以图5.2为例，在不考虑参数偏置项的情况下，若输入和输出的通道数为$C_1=16$，则左半边网络模块所需的参数为$(1\times1+3\times3+5\times5+0)\times C_1\times C_1=8960$；假定右半边网络模块采用的$1\times1$卷积通道数为$C_2=8$$(满足C_1&gt;C_2)$，则右半部分的网络结构所需参数量为$(1\times1\times (3C_1+C_2)+3\times3\times C_2 +5\times5\times C_2)\times C_1=5248$ ，可以在不改变模型表达能力的前提下大大减少所使用的参数量。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.8-1.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.8-1.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>​                                图5.2 Inception模块</p>
<p>综上所述，$1\times 1$卷积的作用主要为以下两点：</p>
<ul>
<li>实现信息的跨通道交互和整合。</li>
<li>对卷积核通道数进行降维和升维，减小参数量。</li>
</ul>
<h2 id="卷积层和池化层的区别"><a href="#卷积层和池化层的区别" class="headerlink" title="卷积层和池化层的区别"></a>卷积层和池化层的区别</h2><p>​    卷积层核池化层在结构上具有一定的相似性，都是对感受域内的特征进行提取，并且根据步长设置获取到不同维度的输出，但是其内在操作是有本质区别的，如表5.7所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">卷积层</th>
<th style="text-align:center">池化层</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>结构</strong></td>
<td style="text-align:center">零填充时输出维度不变，而通道数改变</td>
<td style="text-align:center">通常特征维度会降低，通道数不变</td>
</tr>
<tr>
<td style="text-align:center"><strong>稳定性</strong></td>
<td style="text-align:center">输入特征发生细微改变时，输出结果会改变</td>
<td style="text-align:center">感受域内的细微变化不影响输出结果</td>
</tr>
<tr>
<td style="text-align:center"><strong>作用</strong></td>
<td style="text-align:center">感受域内提取局部关联特征</td>
<td style="text-align:center">感受域内提取泛化特征，降低维度</td>
</tr>
<tr>
<td style="text-align:center"><strong>参数量</strong></td>
<td style="text-align:center">与卷积核尺寸、卷积核个数相关</td>
<td style="text-align:center">不引入额外参数</td>
</tr>
</tbody>
</table>
</div>
<h2 id="卷积核是否一定越大越好？"><a href="#卷积核是否一定越大越好？" class="headerlink" title="卷积核是否一定越大越好？"></a>卷积核是否一定越大越好？</h2><p>​    在早期的卷积神经网络中（如LeNet-5、AlexNet），用到了一些较大的卷积核（$11\times11$和$5\times 5$），受限于当时的计算能力和模型结构的设计，无法将网络叠加得很深，因此卷积网络中的卷积层需要设置较大的卷积核以获取更大的感受域。但是这种大卷积核反而会导致计算量大幅增加，不利于训练更深层的模型，相应的计算性能也会降低。后来的卷积神经网络（VGG、GoogLeNet等），发现通过堆叠2个$3\times 3$卷积核可以获得与$5\times 5$卷积核相同的感受视野，同时参数量会更少（$3×3×2+1$ &lt; $ 5×5×1+1$），$3\times 3$卷积核被广泛应用在许多卷积神经网络中。因此可以认为，在大多数情况下通过堆叠较小的卷积核比直接采用单个更大的卷积核会更加有效。</p>
<p>​    但是，这并不是表示更大的卷积核就没有作用，在某些领域应用卷积神经网络时仍然可以采用较大的卷积核。譬如在自然语言处理领域，由于文本内容不像图像数据可以对特征进行很深层的抽象，往往在该领域的特征提取只需要较浅层的神经网络即可。在将卷积神经网络应用在自然语言处理领域时，通常都是较为浅层的卷积层组成，但是文本特征有时又需要有较广的感受域让模型能够组合更多的特征（如词组和字符），此时直接采用较大的卷积核将是更好的选择。</p>
<p>​    综上所述，卷积核的大小并没有绝对的优劣，需要视具体的应用场景而定，但是极大和极小的卷积核都是不合适的，单独的$1\times 1$极小卷积核只能用作分离卷积而不能对输入的原始特征进行有效的组合，极大的卷积核通常会组合过多的无意义特征从而浪费了大量的计算资源。</p>
<h2 id="每层卷积是否只能用一种尺寸的卷积核？"><a href="#每层卷积是否只能用一种尺寸的卷积核？" class="headerlink" title="每层卷积是否只能用一种尺寸的卷积核？"></a>每层卷积是否只能用一种尺寸的卷积核？</h2><p>​    经典的神经网络一般都属于层叠式网络，每层仅用一个尺寸的卷积核，如VGG结构中使用了大量的$3×3$卷积层。事实上，同一层特征图可以分别使用多个不同尺寸的卷积核，以获得不同尺度的特征，再把这些特征结合起来，得到的特征往往比使用单一卷积核的要好，如GoogLeNet、Inception系列的网络，均是每层使用了多个卷积核结构。如图5.3所示，输入的特征在同一层分别经过$1×1$、$3×3$和$5×5$三种不同尺寸的卷积核，再将分别得到的特征进行整合，得到的新特征可以看作不同感受域提取的特征组合，相比于单一卷积核会有更强的表达能力。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.11-1.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.11-1.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>​                                图5.3 Inception模块结构</p>
<h2 id="怎样才能减少卷积层参数量？"><a href="#怎样才能减少卷积层参数量？" class="headerlink" title="怎样才能减少卷积层参数量？"></a>怎样才能减少卷积层参数量？</h2><p>减少卷积层参数量的方法可以简要地归为以下几点：</p>
<ul>
<li>使用堆叠小卷积核代替大卷积核：VGG网络中2个$3\times 3$的卷积核可以代替1个$5\times 5$的卷积核</li>
<li>使用分离卷积操作：将原本$K\times K\times C$的卷积操作分离为$K\times K\times 1$和$1\times1\times C$的两部分操作</li>
<li>添加$1\times 1$的卷积操作：与分离卷积类似，但是通道数可变，在$K\times K\times C_1$卷积前添加$1\times1\times C_2$的卷积核（满足$C_2 &lt;C_1$）</li>
<li>在卷积层前使用池化操作：池化可以降低卷积层的输入特征维度</li>
</ul>
<h2 id="在进行卷积操作时，必须同时考虑通道和区域吗？"><a href="#在进行卷积操作时，必须同时考虑通道和区域吗？" class="headerlink" title="在进行卷积操作时，必须同时考虑通道和区域吗？"></a>在进行卷积操作时，必须同时考虑通道和区域吗？</h2><p>​    标准卷积中，采用区域与通道同时处理的操作，如下图所示：</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.13-1.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.13-1.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>​    这样做可以简化卷积层内部的结构，每一个输出的特征像素都由所有通道的同一个区域提取而来。</p>
<p>​    但是这种方式缺乏灵活性，并且在深层的网络结构中使得运算变得相对低效，更为灵活的方式是使区域和通道的卷积分离开来，通道分离（深度分离）卷积网络由此诞生。如下图所示，Xception网络可解决上述问题。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.13-2.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.13-2.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>​    我们首先对每一个通道进行各自的卷积操作，有多少个通道就有多少个过滤器。得到新的通道特征矩阵之后，再对这批新通道特征进行标准的$1×1$跨通道卷积操作。</p>
<h2 id="采用宽卷积的好处有什么？"><a href="#采用宽卷积的好处有什么？" class="headerlink" title="采用宽卷积的好处有什么？"></a>采用宽卷积的好处有什么？</h2><p>​    宽卷积对应的是窄卷积，实际上并不是卷积操作的类型，指的是卷积过程中的填充方法，对应的是’SAME’填充和’VALID’填充。’SAME’填充通常采用零填充的方式对卷积核不满足整除条件的输入特征进行补全，以使卷积层的输出维度保持与输入特征维度一致；’VALID’填充的方式则相反，实际并不进行任何填充，在输入特征边缘位置若不足以进行卷积操作，则对边缘信息进行舍弃，因此在步长为1的情况下该填充方式的卷积层输出特征维度可能会略小于输入特征的维度。此外，由于前一种方式通过补零来进行完整的卷积操作，可以有效地保留原始的输入特征信息。</p>
<p>​    比如下图左部分为窄卷积。注意到越在边缘的位置被卷积的次数越少。宽卷积可以看作在卷积之前在边缘用0补充，常见有两种情况，一个是全补充，如下图右部分，这样输出大于输入的维度。另一种常用的方法是补充一一部分0值，使得输出和输入的维度一致。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.14.1.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.14.1.png" srcset="data:image/png;base64,666" alt="image"></p>
<h2 id="理解转置卷积与棋盘效应"><a href="#理解转置卷积与棋盘效应" class="headerlink" title="理解转置卷积与棋盘效应"></a>理解转置卷积与棋盘效应</h2><h3 id="标准卷积"><a href="#标准卷积" class="headerlink" title="标准卷积"></a>标准卷积</h3><p>在理解转置卷积之前，需要先理解标准卷积的运算方式。</p>
<p>首先给出一个输入输出结果</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/img32.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/img32.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>那是怎样计算的呢？</p>
<p>卷积的时候需要对卷积核进行180的旋转，同时卷积核中心与需计算的图像像素对齐，输出结构为中心对齐像素的一个新的像素值，计算例子如下：</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.19.1-2.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.19.1-2.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>这样计算出左上角(即第一行第一列)像素的卷积后像素值。</p>
<p>给出一个更直观的例子，从左到右看，原像素经过卷积由1变成-8。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.19.1-3.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.19.1-3.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>通过滑动卷积核，就可以得到整张图片的卷积结果。</p>
<h3 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h3><p>图像的deconvolution过程如下：</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.19.2-5.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.19.2-5.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>输入：2x2， 卷积核：4x4， 滑动步长：3， 输出：7x7 </p>
<p>过程如下： </p>
<ol>
<li><p>输入图片每个像素进行一次full卷积，根据full卷积大小计算可以知道每个像素的卷积后大小为 1+4-1=4， 即4x4大小的特征图，输入有4个像素所以4个4x4的特征图 </p>
</li>
<li><p>将4个特征图进行步长为3的相加； 输出的位置和输入的位置相同。步长为3是指每隔3个像素进行相加，重叠部分进行相加，即输出的第1行第4列是由红色特阵图的第一行第四列与绿色特征图的第一行第一列相加得到，其他如此类推。  </p>
<p>可以看出翻卷积的大小是由卷积核大小与滑动步长决定， in是输入大小， k是卷积核大小， s是滑动步长， out是输出大小 得到 out = (in - 1) <em> s + k 上图过程就是， (2 - 1) </em> 3 + 4 = 7。</p>
</li>
</ol>
<h3 id="棋盘效应"><a href="#棋盘效应" class="headerlink" title="棋盘效应"></a>棋盘效应</h3><h2 id="卷积神经网络的参数设置"><a href="#卷积神经网络的参数设置" class="headerlink" title="卷积神经网络的参数设置"></a>卷积神经网络的参数设置</h2><p>​    卷积神经网络中常见的参数在其他类型的神经网络中也是类似的，但是参数的设置还得结合具体的任务才能设置在合理的范围，具体的参数列表如表XX所示。<br>​                                                    表XX 卷积神经网络常见参数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:center">常见设置</th>
<th style="text-align:left">参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">学习率(Learning Rate)</td>
<td style="text-align:center">$0-1$</td>
<td style="text-align:left">反向传播网络中更新权值矩阵的步长，在一些常见的网络中会在固定迭代次数或模型不再收敛后对学习率进行指数下降(如$lr=lr\times 0.1$)。当学习率越大计算误差对权值矩阵的影响越大，容易在某个局部最优解附近震荡；越小的学习率对网络权值的更新越精细，但是需要花费更多的时间去迭代</td>
</tr>
<tr>
<td style="text-align:center">批次大小(Batch Size)</td>
<td style="text-align:center">$1-N$</td>
<td style="text-align:left">批次大小指定一次性流入模型的数据样本个数，根据任务和计算性能限制判断实际取值，在一些图像任务中往往由于计算性能和存储容量限制只能选取较小的值。在相同迭代次数的前提下，数值越大模型越稳定，泛化能力越强，损失值曲线越平滑，模型也更快地收敛，但是每次迭代需要花费更多的时间</td>
</tr>
<tr>
<td style="text-align:center">数据轮次(Epoch)</td>
<td style="text-align:center">$1-N$</td>
<td style="text-align:left">数据轮次指定所有训练数据在模型中训练的次数，根据数据集规模和分布情况会设置为不同的值。当模型较为简单或训练数据规模较小时，通常轮次不宜过高，否则模型容易过拟合；模型较为复杂或训练数据规模足够大时，可适当提高数据的训练轮次。</td>
</tr>
<tr>
<td style="text-align:center">权重衰减系数(Weight Decay)</td>
<td style="text-align:center">$0-0.001$</td>
<td style="text-align:left">模型训练过程中反向传播权值更新的权重衰减值</td>
</tr>
</tbody>
</table>
</div>
<h2 id="提高卷积神经网络的泛化能力"><a href="#提高卷积神经网络的泛化能力" class="headerlink" title="提高卷积神经网络的泛化能力"></a>提高卷积神经网络的泛化能力</h2><p>​    卷积神经网络与其他类型的神经网络类似，在采用反向传播进行训练的过程中比较依赖输入的数据分布，当数据分布较为极端的情况下容易导致模型欠拟合或过拟合，表XX记录了提高卷积网络泛化能力的方法。<br>​                                                                   表XX 提高卷积网络化能力的方法</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">方法</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">使用更多数据</td>
<td style="text-align:left">在有条件的前提下，尽可能多地获取训练数据是最理想的方法，更多的数据可以让模型得到充分的学习，也更容易提高泛化能力</td>
</tr>
<tr>
<td style="text-align:center">使用更大批次</td>
<td style="text-align:left">在相同迭代次数和学习率的条件下，每批次采用更多的数据将有助于模型更好的学习到正确的模式，模型输出结果也会更加稳定</td>
</tr>
<tr>
<td style="text-align:center">调整数据分布</td>
<td style="text-align:left">大多数场景下的数据分布是不均匀的，模型过多地学习某类数据容易导致其输出结果偏向于该类型的数据，此时通过调整输入的数据分布可以一定程度提高泛化能力</td>
</tr>
<tr>
<td style="text-align:center">调整目标函数</td>
<td style="text-align:left">在某些情况下，目标函数的选择会影响模型的泛化能力，如目标函数$f(y,y’)=</td>
<td>y-y’</td>
<td>$在某类样本已经识别较为准确而其他样本误差较大的侵害概况下，不同类别在计算损失结果的时候距离权重是相同的，若将目标函数改成$f(y,y’)=(y-y’)^2$则可以使误差小的样本计算损失的梯度比误差大的样本更小，进而有效地平衡样本作用，提高模型泛化能力</td>
</tr>
<tr>
<td style="text-align:center">调整网络结构</td>
<td style="text-align:left">在浅层卷积神经网络中，参数量较少往往使模型的泛化能力不足而导致欠拟合，此时通过叠加卷积层可以有效地增加网络参数，提高模型表达能力；在深层卷积网络中，若没有充足的训练数据则容易导致模型过拟合，此时通过简化网络结构减少卷积层数可以起到提高模型泛化能力的作用</td>
</tr>
<tr>
<td style="text-align:center">数据增强</td>
<td style="text-align:left">数据增强又叫数据增广，在有限数据的前提下通过平移、旋转、加噪声等一些列变换来增加训练数据，同类数据的表现形式也变得更多样，有助于模型提高泛化能力，需要注意的是数据变化应尽可能不破坏元数数据的主体特征(如在图像分类任务中对图像进行裁剪时不能将分类主体目标裁出边界)。</td>
</tr>
<tr>
<td style="text-align:center">权值正则化</td>
<td style="text-align:left">权值正则化就是通常意义上的正则化，一般是在损失函数中添加一项权重矩阵的正则项作为惩罚项，用来惩罚损失值较小时网络权重过大的情况，此时往往是网络权值过拟合了数据样本(如$Loss=f(WX+b,y’)+\frac{\lambda}{\eta}\sum{</td>
<td>W</td>
<td>}$)。</td>
</tr>
<tr>
<td style="text-align:center">屏蔽网络节点</td>
<td style="text-align:left">该方法可以认为是网络结构上的正则化，通过随机性地屏蔽某些神经元的输出让剩余激活的神经元作用，可以使模型的容错性更强。</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>对大多数神经网络模型同样通用</p>
</blockquote>
<h2 id="卷积神经网络在不同领域的应用"><a href="#卷积神经网络在不同领域的应用" class="headerlink" title="卷积神经网络在不同领域的应用"></a>卷积神经网络在不同领域的应用</h2><p>​    卷积神经网络中的卷积操作是其关键组成，而卷积操作只是一种数学运算方式，实际上对不同类型的数值表示数据都是通用的，尽管这些数值可能表示的是图像像素值、文本序列中单个字符或是语音片段中单字的音频。只要使原始数据能够得到有效地数值化表示，卷积神经网络能够在不同的领域中得到应用，要关注的是如何将卷积的特性更好地在不同领域中应用，如表XX所示。<br>​                                                    表XX 卷积神经网络不同领域的应用<br>| 应用领域 | 输入数据图示 | 说明 |<br>| :——-: | :—————: | :— |<br>|   图像处理   | <img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/Image-process.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/Image-process.png" srcset="data:image/png;base64,666" alt="image_process"> | 卷积神经网络在图像处理领域有非常广泛的应用，这是因为图像数据本身具有的局部完整性非常 |<br>| 自然语言处理 | <img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/NLP.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/NLP.png" srcset="data:image/png;base64,666" alt="NLP"> |  |<br>|   语音处理   | <img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/audio-recognition.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/audio-recognition.png" srcset="data:image/png;base64,666" alt="audio_process"> |  |</p>
<h3 id="联系"><a href="#联系" class="headerlink" title="联系"></a>联系</h3><p>​    自然语言处理是对一维信号（词序列）做操作。<br>​    计算机视觉是对二维（图像）或三维（视频流）信号做操作。</p>
<h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>​    自然语言处理的输入数据通常是离散取值（例如表示一个单词或字母通常表示为词典中的one hot向量），计算机视觉则是连续取值（比如归一化到0，1之间的灰度值）。CNN有两个主要特点，区域不变性(location invariance)和组合性(Compositionality)。</p>
<ol>
<li>区域不变性：滤波器在每层的输入向量(图像)上滑动，检测的是局部信息，然后通过pooling取最大值或均值。pooling这步综合了局部特征，失去了每个特征的位置信息。这很适合基于图像的任务，比如要判断一幅图里有没有猫这种生物，你可能不会去关心这只猫出现在图像的哪个区域。但是在NLP里，词语在句子或是段落里出现的位置，顺序，都是很重要的信息。</li>
<li>局部组合性：CNN中，每个滤波器都把较低层的局部特征组合生成较高层的更全局化的特征。这在CV里很好理解，像素组合成边缘，边缘生成形状，最后把各种形状组合起来得到复杂的物体表达。在语言里，当然也有类似的组合关系，但是远不如图像来的直接。而且在图像里，相邻像素必须是相关的，相邻的词语却未必相关。</li>
</ol>
<h2 id="卷积神经网络凸显共性的方法？"><a href="#卷积神经网络凸显共性的方法？" class="headerlink" title="卷积神经网络凸显共性的方法？"></a>卷积神经网络凸显共性的方法？</h2><h3 id="局部连接"><a href="#局部连接" class="headerlink" title="局部连接"></a>局部连接</h3><p>​    我们首先了解一个概念，感受野，即每个神经元仅与输入神经元相连接的一块区域。<br>在图像卷积操作中，神经元在空间维度上是局部连接，但在深度上是全连接。局部连接的思想，是受启发于生物学里的视觉系统结构，视觉皮层的神经元就是仅用局部接受信息。对于二维图像，局部像素关联性较强。这种局部连接保证了训练后的滤波器能够对局部特征有最强的响应，使神经网络可以提取数据的局部特征；<br>下图是一个很经典的图示，左边是全连接，右边是局部连接。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.27.1.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.27.1.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>对于一个1000 × 1000的输入图像而言，如果下一个隐藏层的神经元数目为10^6个，采用全连接则有1000 × 1000 × 10^6 = 10^12个权值参数，如此巨大的参数量几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中10 × 10的局部图像相连接，那么此时的权值参数数量为10 × 10 × 10^6 = 10^8，将直接减少4个数量级。</p>
<h3 id="权值共享"><a href="#权值共享" class="headerlink" title="权值共享"></a>权值共享</h3><p>​    权值共享，即计算同一深度的神经元时采用的卷积核参数是共享的。权值共享在一定程度上讲是有意义的，是由于在神经网络中，提取的底层边缘特征与其在图中的位置无关。但是在另一些场景中是无意的，如在人脸识别任务，我们期望在不同的位置学到不同的特征。<br>需要注意的是，权重只是对于同一深度切片的神经元是共享的。在卷积层中，通常采用多组卷积核提取不同的特征，即对应的是不同深度切片的特征，而不同深度切片的神经元权重是不共享。相反，偏置这一权值对于同一深度切片的所有神经元都是共享的。<br>权值共享带来的好处是大大降低了网络的训练难度。如下图，假设在局部连接中隐藏层的每一个神经元连接的是一个10 × 10的局部图像，因此有10 × 10个权值参数，将这10 × 10个权值参数共享给剩下的神经元，也就是说隐藏层中10^6个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10 × 10个权值参数（也就是卷积核的大小）。</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.27.2.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.27.2.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>这里就体现了卷积神经网络的奇妙之处，使用少量的参数，却依然能有非常出色的性能。上述仅仅是提取图像一种特征的过程。如果要多提取出一些特征，可以增加多个卷积核，不同的卷积核能够得到图像不同尺度下的特征，称之为特征图（feature map）。</p>
<h3 id="池化操作"><a href="#池化操作" class="headerlink" title="池化操作"></a>池化操作</h3><p>池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图：</p>
<p><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.27.3.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/5.27.3.png" srcset="data:image/png;base64,666" alt="image"></p>
<h2 id="全连接、局部连接、全卷积与局部卷积"><a href="#全连接、局部连接、全卷积与局部卷积" class="headerlink" title="全连接、局部连接、全卷积与局部卷积"></a>全连接、局部连接、全卷积与局部卷积</h2><p>​    大多数神经网络中高层网络通常会采用全连接层(Global Connected Layer)，通过多对多的连接方式对特征进行全局汇总，可以有效地提取全局信息。但是全连接的方式需要大量的参数，是神经网络中最占资源的部分之一，因此就需要由局部连接(Local Connected Layer)，仅在局部区域范围内产生神经元连接，能够有效地减少参数量。根据卷积操作的作用范围可以分为全卷积(Global Convolution)和局部卷积(Local Convolution)。实际上这里所说的全卷积就是标准卷积，即在整个输入特征维度范围内采用相同的卷积核参数进行运算，全局共享参数的连接方式可以使神经元之间的连接参数大大减少;局部卷积又叫平铺卷积(Tiled Convolution)或非共享卷积(Unshared Convolution)，是局部连接与全卷积的折衷。四者的比较如表XX所示。<br>​                                                     表XX 卷积网络中连接方式的对比</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">连接方式</th>
<th style="text-align:center">示意图</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">全连接</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/full-connected.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/full-connected.png" srcset="data:image/png;base64,666" alt="full-connected"></td>
<td style="text-align:left">层间神经元完全连接，每个输出神经元可以获取到所有输入神经元的信息，有利于信息汇总，常置于网络末层；连接与连接之间独立参数，大量的连接大大增加模型的参数规模。</td>
</tr>
<tr>
<td style="text-align:center">局部连接</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/local-connected.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/local-connected.png" srcset="data:image/png;base64,666" alt="local-connected"></td>
<td style="text-align:left">层间神经元只有局部范围内的连接，在这个范围内采用全连接的方式，超过这个范围的神经元则没有连接；连接与连接之间独立参数，相比于全连接减少了感受域外的连接，有效减少参数规模</td>
</tr>
<tr>
<td style="text-align:center">全卷积</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/conv.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/conv.png" srcset="data:image/png;base64,666" alt="convolution"></td>
<td style="text-align:left">层间神经元只有局部范围内的连接，在这个范围内采用全连接的方式，连接所采用的参数在不同感受域之间共享，有利于提取特定模式的特征；相比于局部连接，共用感受域之间的参数可以进一步减少参数量。</td>
</tr>
<tr>
<td style="text-align:center">局部卷积</td>
<td style="text-align:center"><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/local-conv.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/local-conv.png" srcset="data:image/png;base64,666" alt="local-conv"></td>
<td style="text-align:left">层间神经元只有局部范围内的连接，感受域内采用全连接的方式，而感受域之间间隔采用局部连接与全卷积的连接方式；相比与全卷积成倍引入额外参数，但有更强的灵活性和表达能力；相比于局部连接，可以有效控制参数量</td>
</tr>
</tbody>
</table>
</div>
<h2 id="局部卷积的应用"><a href="#局部卷积的应用" class="headerlink" title="局部卷积的应用"></a>局部卷积的应用</h2><p>并不是所有的卷积都会进行权重共享，在某些特定任务中，会使用不权重共享的卷积。下面通过人脸这一任务来进行讲解。在读人脸方向的一些paper时，会发现很多都会在最后加入一个Local Connected Conv，也就是不进行权重共享的卷积层。总的来说，这一步的作用就是使用3D模型来将人脸对齐，从而使CNN发挥最大的效果。<br><img src="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/img66.png" class="lazyload" data-srcset="/zh-TW/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/ch5/img66.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>截取论文中的一部分图，经过3D对齐以后，形成的图像均是152×152，输入到上述的网络结构中。该结构的参数如下：</p>
<p>Conv：32个11×11×3的卷积核，</p>
<p>Max-pooling: 3×3，stride=2，</p>
<p>Conv: 16个9×9的卷积核，</p>
<p>Local-Conv: 16个9×9的卷积核，</p>
<p>Local-Conv: 16个7×7的卷积核，</p>
<p>Local-Conv: 16个5×5的卷积核，</p>
<p>Fully-connected: 4096维，</p>
<p>Softmax: 4030维。</p>
<p>前三层的目的在于提取低层次的特征，比如简单的边和纹理。其中Max-pooling层使得卷积的输出对微小的偏移情况更加鲁棒。但不能使用更多的Max-pooling层，因为太多的Max-pooling层会使得网络损失图像信息。全连接层将上一层的每个单元和本层的所有单元相连，用来捕捉人脸图像不同位置特征之间的相关性。最后使用softmax层用于人脸分类。<br>中间三层都是使用参数不共享的卷积核，之所以使用参数不共享，有如下原因：</p>
<p>（1）对齐的人脸图片中，不同的区域会有不同的统计特征，因此并不存在特征的局部稳定性，所以使用相同的卷积核会导致信息的丢失。</p>
<p>（2）不共享的卷积核并不增加inference时特征的计算量，仅会增加训练时的计算量。<br>使用不共享的卷积核，由于需要训练的参数量大大增加，因此往往需要通过其他方法增加数据量。</p>
<h2 id="NetVLAD池化-（贡献者：熊楚原-中国人民大学）"><a href="#NetVLAD池化-（贡献者：熊楚原-中国人民大学）" class="headerlink" title="NetVLAD池化    （贡献者：熊楚原-中国人民大学）"></a>NetVLAD池化    （贡献者：熊楚原-中国人民大学）</h2><p>NetVLAD是论文[15]提出的一个局部特征聚合的方法。</p>
<p>在传统的网络里面，例如VGG啊，最后一层卷积层输出的特征都是类似于Batchsize x 3 x 3 x 512的这种东西，然后会经过FC聚合，或者进行一个Global Average Pooling（NIN里的做法），或者怎么样，变成一个向量型的特征，然后进行Softmax or 其他的Loss。</p>
<p>这种方法说简单点也就是输入一个图片或者什么的结构性数据，然后经过特征提取得到一个长度固定的向量，之后可以用度量的方法去进行后续的操作，比如分类啊，检索啊，相似度对比等等。</p>
<p>那么NetVLAD考虑的主要是最后一层卷积层输出的特征这里，我们不想直接进行欠采样或者全局映射得到特征，对于最后一层输出的W x H x D，设计一个新的池化，去聚合一个“局部特征“，这即是NetVLAD的作用。</p>
<p>NetVLAD的一个输入是一个W x H x D的图像特征，例如VGG-Net最后的3 x 3 x 512这样的矩阵，在网络中还需加一个维度为Batchsize。</p>
<p>NetVLAD还需要另输入一个标量K即表示VLAD的聚类中心数量，它主要是来构成一个矩阵C，是通过原数据算出来的每一个$W \times H$特征的聚类中心，C的shape即$C: K \times D$，然后根据三个输入，VLAD是计算下式的V:</p>
<script type="math/tex; mode=display">V(j, k) = \sum_{i=1}^{N}{a_k(x_i)(x_i(j) - c_k(j))}</script><p>其中j表示维度，从1到D，可以看到V的j是和输入与c对应的，对每个类别k，都对所有的x进行了计算，如果$x_i$属于当前类别k，$a_k=1$，否则$a_k=0$，计算每一个x和它聚类中心的残差，然后把残差加起来，即是每个类别k的结果，最后分别L2正则后拉成一个长向量后再做L2正则，正则非常的重要，因为这样才能统一所有聚类算出来的值，而残差和的目的主要是消减不同聚类上的分布不均，两者共同作用才能得到最后正常的输出。</p>
<p>输入与输出如下图所示：</p>
<p><img src="http://www.ecohnoch.cn/img/netvlad.jpeg" class="lazyload" data-srcset="http://www.ecohnoch.cn/img/netvlad.jpeg" srcset="data:image/png;base64,666" alt="image"></p>
<p>中间得到的K个D维向量即是对D个x都进行了与聚类中心计算残差和的过程，最终把K个D维向量合起来后进行即得到最终输出的$K \times D$长度的一维向量。</p>
<p>而VLAD本身是不可微的，因为上面的a要么是0要么是1，表示要么当前描述x是当前聚类，要么不是，是个离散的，NetVLAD为了能够在深度卷积网络里使用反向传播进行训练，对a进行了修正。</p>
<p>那么问题就是如何重构一个a，使其能够评估当前的这个x和各个聚类的关联程度？用softmax来得到：</p>
<script type="math/tex; mode=display">a_k = \frac{e^{W_k^T x_i + b_k}}{e^{W_{k'}^T x_i + b_{k'}}}</script><p>将这个把上面的a替换后，即是NetVLAD的公式，可以进行反向传播更新参数。</p>
<p>所以一共有三个可训练参数，上式a中的$W: K \times D$，上式a中的$b: K \times 1$，聚类中心$c: K \times D$，而原始VLAD只有一个参数c。</p>
<p>最终池化得到的输出是一个恒定的K x D的一维向量（经过了L2正则），如果带Batchsize，输出即为Batchsize x (K x D)的二维矩阵。</p>
<p>NetVLAD作为池化层嵌入CNN网络即如下图所示，</p>
<p><img src="http://www.ecohnoch.cn/img/netvlad_emb.png" class="lazyload" data-srcset="http://www.ecohnoch.cn/img/netvlad_emb.png" srcset="data:image/png;base64,666" alt="image"></p>
<p>原论文中采用将传统图像检索方法VLAD进行改进后应用在CNN的池化部分作为一种另类的局部特征池化，在场景检索上取得了很好的效果。</p>
<p>后续相继又提出了ActionVLAD、ghostVLAD等改进。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>目标检测</title>
    <url>/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="什么是目标检测？"><a href="#什么是目标检测？" class="headerlink" title="什么是目标检测？"></a>什么是目标检测？</h3><p>​    目标检测（Object Detection）的任务是找出图像中所有感兴趣的目标（物体），确定它们的类别和位置，是计算机视觉领域的核心问题之一。由于各类物体有不同的外观、形状和姿态，加上成像时光照、遮挡等因素的干扰，目标检测一直是计算机视觉领域最具有挑战性的问题。</p>
<p>​    计算机视觉中关于图像识别有四大类任务：</p>
<p><strong>分类-Classification</strong>：解决“是什么？”的问题，即给定一张图片或一段视频判断里面包含什么类别的目标。</p>
<p><strong>定位-Location</strong>：解决“在哪里？”的问题，即定位出这个目标的位置。</p>
<p><strong>检测-Detection</strong>：解决“是什么？在哪里？”的问题，即定位出这个目标的的位置并且知道目标物是什么。</p>
<p><strong>分割-Segmentation</strong>：分为实例分割（Instance-level）和场景分割（Scene-level），解决“每一个像素属于哪个目标物或场景”的问题。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.1.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.1.png" srcset="data:image/png;base64,666" alt="图像识别四大类任务，图像来源于cs231n 2016课件Lecture 8"></p>
<h3 id="目标检测要解决的核心问题？"><a href="#目标检测要解决的核心问题？" class="headerlink" title="目标检测要解决的核心问题？"></a>目标检测要解决的核心问题？</h3><p>除了图像分类之外，目标检测要解决的核心问题是：</p>
<p>1.目标可能出现在图像的任何位置。</p>
<p>2.目标有各种不同的大小。</p>
<p>3.目标可能有各种不同的形状。</p>
<h3 id="目标检测算法分类？"><a href="#目标检测算法分类？" class="headerlink" title="目标检测算法分类？"></a>目标检测算法分类？</h3><p>基于深度学习的目标检测算法主要分为两类：</p>
<p><strong>1.Two stage目标检测算法</strong></p>
<p>​    先进行区域生成（region proposal，RP）（一个有可能包含待检物体的预选框），再通过卷积神经网络进行样本分类。</p>
<p>​    任务：特征提取—&gt;生成RP—&gt;分类/定位回归。</p>
<p>​    常见的two stage目标检测算法有：R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN和R-FCN等。</p>
<p><strong>2.One stage目标检测算法</strong></p>
<p>​    不用RP，直接在网络中提取特征来预测物体分类和位置。</p>
<p>​    任务：特征提取—&gt;分类/定位回归。</p>
<p>​    常见的one stage目标检测算法有：OverFeat、YOLOv1、YOLOv2、YOLOv3、SSD和RetinaNet等。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.2.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.2.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="目标检测有哪些应用？"><a href="#目标检测有哪些应用？" class="headerlink" title="目标检测有哪些应用？"></a>目标检测有哪些应用？</h3><p>​    目标检测具有巨大的实用价值和应用前景。应用领域包括人脸检测、行人检测、车辆检测、飞机航拍或卫星图像中道路的检测、车载摄像机图像中的障碍物检测、医学影像的病灶检测等。还有在安防领域中，可以实现比如安全帽、安全带等动态检测，移动侦测、区域入侵检测、物品看护等功能。</p>
<h2 id="Two-Stage目标检测算法"><a href="#Two-Stage目标检测算法" class="headerlink" title="Two Stage目标检测算法"></a>Two Stage目标检测算法</h2><h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><p><strong>R-CNN有哪些创新点？</strong></p>
<ol>
<li>使用CNN（ConvNet）对 region proposals 计算 feature vectors。从经验驱动特征（SIFT、HOG）到数据驱动特征（CNN feature map），提高特征对样本的表示能力。</li>
<li>采用大样本（ILSVRC）有监督预训练和小样本（PASCAL）微调（fine-tuning）的方法解决小样本难以训练甚至过拟合等问题。</li>
</ol>
<p>注：ILSVRC其实就是众所周知的ImageNet的挑战赛，数据量极大；PASCAL数据集（包含目标检测和图像分割等），相对较小。</p>
<p><strong>R-CNN 介绍</strong></p>
<p>​    R-CNN作为R-CNN系列的第一代算法，其实没有过多的使用“深度学习”思想，而是将“深度学习”和传统的“计算机视觉”的知识相结合。比如R-CNN pipeline中的第二步和第四步其实就属于传统的“计算机视觉”技术。使用selective search提取region proposals，使用SVM实现分类。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.1-1.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.1-1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>原论文中R-CNN pipeline只有4个步骤，光看上图无法深刻理解R-CNN处理机制，下面结合图示补充相应文字</p>
<ol>
<li><p>预训练模型。选择一个预训练 （pre-trained）神经网络（如AlexNet、VGG）。</p>
</li>
<li><p>重新训练全连接层。使用需要检测的目标重新训练（re-train）最后全连接层（connected layer）。</p>
</li>
<li><p>提取 proposals并计算CNN 特征。利用选择性搜索（Selective Search）算法提取所有proposals（大约2000幅images），调整（resize/warp）它们成固定大小，以满足 CNN输入要求（因为全连接层的限制），然后将feature map 保存到本地磁盘。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.4.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.4.png" srcset="data:image/png;base64,666" alt=""></p>
</li>
<li><p>训练SVM。利用feature map 训练SVM来对目标和背景进行分类（每个类一个二进制SVM）</p>
</li>
<li><p>边界框回归（Bounding boxes Regression）。训练将输出一些校正因子的线性回归分类器</p>
</li>
</ol>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.5.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.5.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>R-CNN 实验结果</strong></p>
<p>R-CNN在VOC 2007测试集上mAP达到58.5%，打败当时所有的目标检测算法。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.6.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.6.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><p><strong>Fast R-CNN有哪些创新点？</strong></p>
<ol>
<li>只对整幅图像进行一次特征提取，避免R-CNN中的冗余特征提取</li>
<li>用RoI pooling层替换最后一层的max pooling层，同时引入建议框数据，提取相应建议框特征</li>
<li>Fast R-CNN网络末尾采用并行的不同的全连接层，可同时输出分类结果和窗口回归结果，实现了end-to-end的多任务训练【建议框提取除外】，也不需要额外的特征存储空间【R-CNN中的特征需要保持到本地，来供SVM和Bounding-box regression进行训练】</li>
<li>采用SVD对Fast R-CNN网络末尾并行的全连接层进行分解，减少计算复杂度，加快检测速度。</li>
</ol>
<p><strong>Fast R-CNN 介绍</strong></p>
<p>​    Fast R-CNN是基于R-CNN和SPPnets进行的改进。SPPnets，其创新点在于计算整幅图像的the shared feature map，然后根据object proposal在shared feature map上映射到对应的feature vector（就是不用重复计算feature map了）。当然，SPPnets也有缺点：和R-CNN一样，训练是多阶段（multiple-stage pipeline）的，速度还是不够”快”，特征还要保存到本地磁盘中。</p>
<p>将候选区域直接应用于特征图，并使用RoI池化将其转化为固定大小的特征图块。以下是Fast R-CNN的流程图</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.2-1.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.2-1.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>RoI Pooling层详解</strong></p>
<p>因为Fast R-CNN使用全连接层，所以应用RoI Pooling将不同大小的ROI转换为固定大小。</p>
<p>RoI Pooling 是Pooling层的一种，而且是针对RoI的Pooling，其特点是输入特征图尺寸不固定，但是输出特征图尺寸固定（如7x7）。</p>
<p><strong>什么是RoI呢？</strong></p>
<p>RoI是Region of Interest的简写，一般是指图像上的区域框，但这里指的是由Selective Search提取的候选框。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.2-2.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.2-2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>往往经过RPN后输出的不止一个矩形框，所以这里我们是对多个RoI进行Pooling。</p>
<p><strong>RoI Pooling的输入</strong></p>
<p>输入有两部分组成： </p>
<ol>
<li>特征图（feature map）：指的是上面所示的特征图，在Fast RCNN中，它位于RoI Pooling之前，在Faster RCNN中，它是与RPN共享那个特征图，通常我们常常称之为“share_conv”； </li>
<li>RoIs，其表示所有RoI的N*5的矩阵。其中N表示RoI的数量，第一列表示图像index，其余四列表示其余的左上角和右下角坐标。</li>
</ol>
<p>在Fast RCNN中，指的是Selective Search的输出；在Faster RCNN中指的是RPN的输出，一堆矩形候选框，形状为1x5x1x1（4个坐标+索引index），其中值得注意的是：坐标的参考系不是针对feature map这张图的，而是针对原图的（神经网络最开始的输入）。其实关于ROI的坐标理解一直很混乱，到底是根据谁的坐标来。其实很好理解，我们已知原图的大小和由Selective Search算法提取的候选框坐标，那么根据”映射关系”可以得出特征图（feature map）的大小和候选框在feature map上的映射坐标。至于如何计算，其实就是比值问题，下面会介绍。所以这里把ROI理解为原图上各个候选框（region proposals），也是可以的。</p>
<p>注：说句题外话，由Selective Search算法提取的一系列可能含有object的bounding box，这些通常称为region proposals或者region of interest（ROI）。</p>
<p><strong>RoI的具体操作</strong></p>
<ol>
<li><p>根据输入image，将ROI映射到feature map对应位置</p>
<p>注：映射规则比较简单，就是把各个坐标除以“输入图片与feature map的大小的比值”，得到了feature map上的box坐标</p>
</li>
<li><p>将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同）</p>
</li>
<li><p>对每个sections进行max pooling操作</p>
</li>
</ol>
<p>这样我们就可以从不同大小的方框得到固定大小的相应 的feature maps。值得一提的是，输出的feature maps的大小不取决于ROI和卷积feature maps大小。RoI Pooling 最大的好处就在于极大地提高了处理速度。</p>
<p><strong>RoI Pooling的输出</strong></p>
<p>输出是batch个vector，其中batch的值等于RoI的个数，vector的大小为channel <em> w </em> h；RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w * h）的矩形框。</p>
<p><strong>RoI Pooling示例</strong></p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.11.gif" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.11.gif" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><p><strong>Faster R-CNN有哪些创新点？</strong></p>
<p>Fast R-CNN依赖于外部候选区域方法，如选择性搜索。但这些算法在CPU上运行且速度很慢。在测试中，Fast R-CNN需要2.3秒来进行预测，其中2秒用于生成2000个ROI。Faster R-CNN采用与Fast R-CNN相同的设计，只是它用内部深层网络代替了候选区域方法。新的候选区域网络（RPN）在生成ROI时效率更高，并且以每幅图像10毫秒的速度运行。<br><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.3-1.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.3-1.png" srcset="data:image/png;base64,666" alt=""> </p>
<p>图8.1.13 <strong>Faster R-CNN的流程图</strong><br>Faster R-CNN的流程图与Fast R-CNN相同，采用外部候选区域方法代替了内部深层网络。<br><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.3-2.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.3-2.png" srcset="data:image/png;base64,666" alt=""> </p>
<p>图8.1.14<br><strong>候选区域网络</strong></p>
<p>候选区域网络（RPN）将第一个卷积网络的输出特征图作为输入。它在特征图上滑动一个3×3的卷积核，以使用卷积网络（如下所示的ZF网络）构建与类别无关的候选区域。其他深度网络（如VGG或ResNet）可用于更全面的特征提取，但这需要以速度为代价。ZF网络最后会输出256个值，它们将馈送到两个独立的全连接层，以预测边界框和两个objectness分数，这两个objectness分数度量了边界框是否包含目标。我们其实可以使用回归器计算单个objectness分数，但为简洁起见，Faster R-CNN使用只有两个类别的分类器：即带有目标的类别和不带有目标的类别。<br><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.3-3.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.3-3.png" srcset="data:image/png;base64,666" alt=""> </p>
<p>图8.1.15<br>对于特征图中的每一个位置，RPN会做k次预测。因此，RPN将输出4×k个坐标和每个位置上2×k个得分。下图展示了8×8的特征图，且有一个3×3的卷积核执行运算，它最后输出8×8×3个ROI（其中k=3）。下图（右）展示了单个位置的3个候选区域。<br><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.3-4.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.3-4.png" srcset="data:image/png;base64,666" alt=""></p>
<p>图8.1.16<br>假设最好涵盖不同的形状和大小。因此，Faster R-CNN不会创建随机边界框。相反，它会预测一些与左上角名为锚点的参考框相关的偏移量（如x, y）。我们限制这些偏移量的值，因此我们的猜想仍然类似于锚点。<br><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.17.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.1.17.png" srcset="data:image/png;base64,666" alt=""> </p>
<p>图8.1.17<br>要对每个位置进行k个预测，我们需要以每个位置为中心的k个锚点。每个预测与特定锚点相关联，但不同位置共享相同形状的锚点。<br><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.3-6.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.3-6.png" srcset="data:image/png;base64,666" alt=""> </p>
<p>图8.1.18<br>这些锚点是精心挑选的，因此它们是多样的，且覆盖具有不同比例和宽高比的现实目标。这使得我们可以用更好的猜想来指导初始训练，并允许每个预测专门用于特定的形状。该策略使早期训练更加稳定和简便。<br><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.3-7.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.3-7.png" srcset="data:image/png;base64,666" alt=""></p>
<p>图8.1.19<br>Faster R-CNN使用更多的锚点。它部署9个锚点框：3个不同宽高比的3个不同大小的锚点（Anchor）框。每一个位置使用9个锚点，每个位置会生成2×9个objectness分数和4×9个坐标。</p>
<h3 id="R-FCN"><a href="#R-FCN" class="headerlink" title="R-FCN"></a>R-FCN</h3><p><strong>R-FCN有哪些创新点？</strong></p>
<p>R-FCN 仍属于two-stage 目标检测算法：RPN+R-FCN</p>
<ol>
<li>Fully convolutional</li>
<li>位置敏感得分图（position-sensitive score maps）</li>
</ol>
<blockquote>
<p>our region-based detector is <strong>fully convolutional</strong> with almost all computation shared on the entire image. To achieve this goal, we propose <strong>position-sensitive score maps</strong> to address a dilemma between translation-invariance in image classification and translation-variance in object detection.</p>
</blockquote>
<p>R-FCN backbone：ResNet</p>
<p>ResNet-101+R-FCN：83.6% in PASCAL VOC 2007 test datasets</p>
<p>既提高了mAP，又加快了检测速度</p>
<pre><code>假设我们只有一个特征图用来检测右眼。那么我们可以使用它定位人脸吗？应该可以。因为右眼应该在人脸图像的左上角，所以我们可以利用这一点定位整个人脸。如果我们还有其他用来检测左眼、鼻子或嘴巴的特征图，那么我们可以将检测结果结合起来，更好地定位人脸。现在我们回顾一下所有问题。在Faster R-CNN中，检测器使用了多个全连接层进行预测。如果有2000个ROI，那么成本非常高。R-FCN通过减少每个ROI所需的工作量实现加速。上面基于区域的特征图与ROI是独立的，可以在每个ROI之外单独计算。剩下的工作就比较简单了，因此R-FCN的速度比Faster R-CNN快。
</code></pre><p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.4-1.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.4-1.png" srcset="data:image/png;base64,666" alt=""></p>
<pre><code>图8.2.1 人脸检测
现在我们来看一下5×5的特征图M，内部包含一个蓝色方块。我们将方块平均分成3×3个区域。现在，我们在M中创建了一个新的特征图，来检测方块的左上角（TL）。这个新的特征图如下图（右）所示。只有绿色的网格单元[2,2]处于激活状态。在左侧创建一个新的特征图，用于检测目标的左上角。
</code></pre><p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.4-2.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.4-2.png" srcset="data:image/png;base64,666" alt=""> </p>
<pre><code>图8.2.2 检测示例
我们将方块分成9个部分，由此创建了9个特征图，每个用来检测对应的目标区域。这些特征图叫做位置敏感得分图（position-sensitive score map），因为每个图检测目标的子区域（计算其得分）。
</code></pre><p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.4-3.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.4-3.png" srcset="data:image/png;base64,666" alt=""></p>
<pre><code>图8.2.3生成9个得分图
下图中红色虚线矩形是建议的ROI。我们将其分割成3×3个区域，并询问每个区域包含目标对应部分的概率是多少。例如，左上角ROI区域包含左眼的概率。我们将结果存储成3×3 vote数组，如下图（右）所示。例如，vote_array[0][0]包含左上角区域是否包含目标对应部分的得分。
</code></pre><p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.4-4.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.4-4.png" srcset="data:image/png;base64,666" alt=""> </p>
<pre><code>图8.2.4
将ROI应用到特征图上，输出一个3x3数组。将得分图和ROI映射到vote数组的过程叫做位置敏感ROI池化（position-sensitive ROI-pool）。该过程与前面讨论过的ROI池化非常接近。
</code></pre><p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.4-5.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.4-5.png" srcset="data:image/png;base64,666" alt=""> </p>
<pre><code>图8.2.5
将ROI的一部分叠加到对应的得分图上，计算V[i][j]。在计算出位置敏感ROI池化的所有值后，类别得分是其所有元素得分的平均值。
</code></pre><p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.6.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.6.png" srcset="data:image/png;base64,666" alt=""></p>
<pre><code>图8.2.6 ROI池化
假如我们有C个类别要检测。我们将其扩展为C+1个类别，这样就为背景（非目标）增加了一个新的类别。每个类别有3×3个得分图，因此一共有(C+1)×3×3个得分图。使用每个类别的得分图可以预测出该类别的类别得分。然后我们对这些得分应用 softmax 函数，计算出每个类别的概率。以下是数据流图，在本案例中，k=3。
</code></pre><p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.7.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.2.7.png" srcset="data:image/png;base64,666" alt=""> </p>
<pre><code>图8.2.7
</code></pre><h3 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h3><p><strong>FPN有哪些创新点？</strong></p>
<ol>
<li>多层特征</li>
<li>特征融合</li>
</ol>
<p>解决目标检测中的多尺度问题，通过简单的网络连接改变，在基本不增加原有模型计算量的情况下，大幅度提升小物体（small object）检测的性能。</p>
<p>在物体检测里面，有限计算量情况下，网络的深度（对应到感受野）与 stride 通常是一对矛盾的东西，常用的网络结构对应的 stride 一般会比较大（如 32），而图像中的小物体甚至会小于 stride 的大小，造成的结果就是小物体的检测性能急剧下降。传统解决这个问题的思路包括：</p>
<ol>
<li>图像金字塔（image pyramid），即多尺度训练和测试。但该方法计算量大，耗时较久。</li>
<li>特征分层，即每层分别预测对应的scale分辨率的检测结果，如SSD算法。该方法强行让不同层学习同样的语义信息，但实际上不同深度对应于不同层次的语义特征，浅层网络分辨率高，学到更多是细节特征，深层网络分辨率低，学到更多是语义特征。</li>
</ol>
<p>因而，目前多尺度的物体检测主要面临的挑战为：</p>
<ol>
<li>如何学习具有强语义信息的多尺度特征表示？</li>
<li>如何设计通用的特征表示来解决物体检测中的多个子问题？如 object proposal, box localization, instance segmentation.</li>
<li>如何高效计算多尺度的特征表示？</li>
</ol>
<p>FPN网络直接在Faster R-CNN单网络上做修改，每个分辨率的 feature map 引入后一分辨率缩放两倍的 feature map 做 element-wise 相加的操作。通过这样的连接，每一层预测所用的 feature map 都融合了不同分辨率、不同语义强度的特征，融合的不同分辨率的 feature map 分别做对应分辨率大小的物体检测。这样保证了每一层都有合适的分辨率以及强语义（rich semantic）特征。同时，由于此方法只是在原网络基础上加上了额外的跨层连接，在实际应用中几乎不增加额外的时间和计算量。作者接下来实验了将 FPN 应用在 Faster RCNN 上的性能，在 COCO 上达到了 state-of-the-art 的单模型精度。在RPN上，FPN增加了8.0个点的平均召回率（average recall，AR）；在后面目标检测上，对于COCO数据集，FPN增加了2.3个点的平均精确率（average precision，AP），对于VOC数据集，FPN增加了3.8个点的AP。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/FPN-01.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/FPN-01.png" srcset="data:image/png;base64,666" alt=""></p>
<p>FPN算法主要由三个模块组成，分别是：</p>
<ol>
<li>Bottom-up pathway（自底向上线路）</li>
<li>Lateral connections（横向连接）</li>
<li>Top-down path（自顶向下线路）</li>
</ol>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/FPN-02.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/FPN-02.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>Bottom-up pathway</strong></p>
<p>FPN是基于Faster R-CNN进行改进，其backbone是ResNet-101，FPN主要应用在Faster R-CNN中的RPN（用于bounding box proposal generation）和Fast R-CNN（用于object detection）两个模块中。</p>
<p>其中 RPN 和 Fast RCNN 分别关注的是召回率（recall）和精确率（precision），在这里对比的指标分别为 Average Recall(AR) 和 Average Precision(AP)。</p>
<p>注：Bottom-up可以理解为自底向上，Top-down可以理解为自顶向下。这里的下是指low-level，上是指high-level，分别对应于提取的低级（浅层）特征和高级语义（高层）特征。</p>
<p>Bottom-up pathway 是卷积网络的前向传播过程。在前向传播过程中，feature map的大小可以在某些层发生改变。一些尺度（scale）因子为2，所以后一层feature map的大小是前一层feature map大小的二分之一，根据此关系进而构成了feature pyramid（hierarchy）。</p>
<p>然而还有很多层输出的feature map是一样的大小（即不进行缩放的卷积），作者将这些层归为同一 stage。对于feature pyramid，作者为每个stage定义一个pyramid level。</p>
<p>作者将每个stage的最后一层的输出作为feature map，然后不同stage进行同一操作，便构成了feature pyramid。</p>
<p>具体来说，对于ResNets-101，作者使用了每个stage的最后一个残差结构的特征激活输出。将这些残差模块输出表示为{C2, C3, C4, C5}，对应于conv2，conv3，conv4和conv5的输出，并且注意它们相对于输入图像具有{4, 8, 16, 32}像素的步长。考虑到内存占用，没有将conv1包含在金字塔中。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/FPN-03.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/FPN-03.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>Top-down pathway and lateral connections</strong></p>
<p>Top-town pathway是上采样（upsampling）过程。而lateral connection（横向连接）是将上采样的结果和bottom-up pathway生成的相同大小的feature map进行融合（merge）。</p>
<p>注：上采样尺度因子为2，因为为了和之前下采样卷积的尺度因子=2一样。上采样是放大，下采样是缩小。</p>
<p>具体操作如下图所示，上采样（2x up）feature map与相同大小的bottom-up feature map进行逐像素相加融合（element-wise addition），其中bottom-up feature先要经过1x1卷积层，目的是为了减少通道维度（reduce channel dimensions）。</p>
<p>注：减少通道维度是为了将bottom-up feature map的通道数量与top-down feature map的通道数量保持一致，又因为两者feature map大小一致，所以可以进行对应位置像素的叠加（element-wise addition）。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/FPN-04.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/FPN-04.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h3><p><strong>Mask R-CNN有哪些创新点？</strong></p>
<ol>
<li>Backbone：ResNeXt-101+FPN</li>
<li>RoI Align替换RoI Pooling</li>
</ol>
<p>Mask R-CNN是一个实例分割（Instance segmentation）算法，主要是在目标检测的基础上再进行分割。Mask R-CNN算法主要是Faster R-CNN+FCN，更具体一点就是ResNeXt+RPN+RoI Align+Fast R-CNN+FCN。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/Mask R-CNN-01.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/Mask R-CNN-01.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>Mask R-CNN算法步骤</strong></p>
<ol>
<li>输入一幅你想处理的图片，然后进行对应的预处理操作，或者预处理后的图片；</li>
<li>将其输入到一个预训练好的神经网络中（ResNeXt等）获得对应的feature map；</li>
<li>对这个feature map中的每一点设定预定个的RoI，从而获得多个候选RoI；</li>
<li>将这些候选的RoI送入RPN网络进行二值分类（前景或背景）和BB回归，过滤掉一部分候选的RoI；</li>
<li>对这些剩下的RoI进行RoI Align操作（即先将原图和feature map的pixel对应起来，然后将feature map和固定的feature对应起来）；</li>
<li>对这些RoI进行分类（N类别分类）、BB回归和MASK生成（在每一个RoI里面进行FCN操作）。</li>
</ol>
<p><strong>RoI Pooling和RoI Align有哪些不同？</strong></p>
<p>ROI Align 是在Mask-RCNN中提出的一种区域特征聚集方式，很好地解决了RoI Pooling操作中两次量化造成的区域不匹配(mis-alignment)的问题。实验显示，在检测测任务中将 RoI Pooling 替换为 RoI Align 可以提升检测模型的准确性。</p>
<p>在常见的两级检测框架（比如Fast-RCNN，Faster-RCNN，RFCN）中，RoI Pooling 的作用是根据预选框的位置坐标在特征图中将相应区域池化为固定尺寸的特征图，以便进行后续的分类和包围框回归操作。由于预选框的位置通常是由模型回归得到的，一般来讲是浮点数，而池化后的特征图要求尺寸固定。故RoI Pooling这一操作存在两次量化的过程。</p>
<ul>
<li>将候选框边界量化为整数点坐标值。</li>
<li>将量化后的边界区域平均分割成 $k\times k$ 个单元(bin),对每一个单元的边界进行量化。</li>
</ul>
<p>事实上，经过上述两次量化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度。在论文里，作者把它总结为“不匹配问题（misalignment）”。</p>
<p>下面我们用直观的例子具体分析一下上述区域不匹配问题。如下图所示，这是一个Faster-RCNN检测框架。输入一张$800\times 800$的图片，图片上有一个$665\times 665$的包围框（框着一只狗）。图片经过主干网络提取特征后，特征图缩放步长（stride）为32。因此，图像和包围框的边长都是输入时的1/32。800正好可以被32整除变为25。但665除以32以后得到20.78，带有小数，于是RoI Pooling 直接将它量化成20。接下来需要把框内的特征池化$7\times 7$的大小，因此将上述包围框平均分割成$7\times 7$个矩形区域。显然，每个矩形区域的边长为2.86，又含有小数。于是ROI Pooling 再次把它量化到2。经过这两次量化，候选区域已经出现了较明显的偏差（如图中绿色部分所示）。更重要的是，该层特征图上0.1个像素的偏差，缩放到原图就是3.2个像素。那么0.8的偏差，在原图上就是接近30个像素点的差别，这一差别不容小觑。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/Mask R-CNN-02.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/Mask R-CNN-02.png" srcset="data:image/png;base64,666" alt=""></p>
<p>为了解决RoI Pooling的上述缺点，作者提出了RoI Align这一改进的方法(如图2)。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/Mask R-CNN-03.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/Mask R-CNN-03.png" srcset="data:image/png;base64,666" alt=""></p>
<p>RoI Align的思路很简单：取消量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值，从而将整个特征聚集过程转化为一个连续的操作。值得注意的是，在具体的算法操作上，RoI Align并不是简单地补充出候选区域边界上的坐标点，然后将这些坐标点进行池化，而是重新设计了一套比较优雅的流程，如下图所示：</p>
<ol>
<li><p>遍历每一个候选区域，保持浮点数边界不做量化。</p>
</li>
<li><p>将候选区域分割成$k\times k$个单元，每个单元的边界也不做量化。</p>
</li>
<li><p>在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。</p>
</li>
</ol>
<p>这里对上述步骤的第三点作一些说明：这个固定位置是指在每一个矩形单元（bin）中按照固定规则确定的位置。比如，如果采样点数是1，那么就是这个单元的中心点。如果采样点数是4，那么就是把这个单元平均分割成四个小方块以后它们分别的中心点。显然这些采样点的坐标通常是浮点数，所以需要使用插值的方法得到它的像素值。在相关实验中，作者发现将采样点设为4会获得最佳性能，甚至直接设为1在性能上也相差无几。事实上，RoI Align 在遍历取样点的数量上没有RoI Pooling那么多，但却可以获得更好的性能，这主要归功于解决了mis alignment的问题。值得一提的是，我在实验时发现，RoI Align在VOC 2007数据集上的提升效果并不如在COCO上明显。经过分析，造成这种区别的原因是COCO上小目标的数量更多，而小目标受mis alignment问题的影响更大（比如，同样是0.5个像素点的偏差，对于较大的目标而言显得微不足道，但是对于小目标，误差的影响就要高很多）。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/Mask R-CNN-04.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/Mask R-CNN-04.png" srcset="data:image/png;base64,666" alt=""></p>
<h2 id="One-Stage目标检测算法"><a href="#One-Stage目标检测算法" class="headerlink" title="One Stage目标检测算法"></a>One Stage目标检测算法</h2><p>我们将对单次目标检测器（包括SSD系列和YOLO系列等算法）进行综述。我们将分析FPN以理解多尺度特征图如何提高准确率，特别是小目标的检测，其在单次检测器中的检测效果通常很差。然后我们将分析Focal loss和RetinaNet，看看它们是如何解决训练过程中的类别不平衡问题的。</p>
<h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><p><strong>SSD有哪些创新点？</strong></p>
<ol>
<li>基于Faster R-CNN中的Anchor，提出了相似的先验框（Prior box）</li>
<li>从不同比例的特征图（多尺度特征）中产生不同比例的预测，并明确地按长宽比分离预测。</li>
</ol>
<p>不同于前面的R-CNN系列，SSD属于one-stage方法。SSD使用 VGG16 网络作为特征提取器（和 Faster R-CNN 中使用的 CNN 一样），将后面的全连接层替换成卷积层，并在之后添加自定义卷积层，并在最后直接采用卷积进行检测。在多个特征图上设置不同缩放比例和不同宽高比的先验框以融合多尺度特征图进行检测，靠前的大尺度特征图可以捕捉到小物体的信息，而靠后的小尺度特征图能捕捉到大物体的信息，从而提高检测的准确性和定位的准确性。如下图是SSD的网络结构图。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/SSD-01.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/SSD-01.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>1. 怎样设置default boxes？</strong><br>SSD中default box的概念有点类似于Faster R-CNN中的anchor。不同于Faster R-CNN只在最后一个特征层取anchor, SSD在多个特征层上取default box，可以得到不同尺度的default box。在特征图的每个单元上取不同宽高比的default box,一般宽高比在{1,2,3,1/2,1/3}中选取，有时还会额外增加一个宽高比为1但具有特殊尺度的box。如下图所示，在8x8的feature map和4x4的feature map上的每个单元取4个不同的default box。原文对于300x300的输入，分别在conv4_3, conv7,conv8_2,conv9_2,conv10_2,conv11_2的特征图上的每个单元取4,6,6,6,4,4个default box. 由于以上特征图的大小分别是38x38,19x19,10x10,5x5,3x3,1x1，所以一共得到38x38x4+19x19x6+10x10x6+5x5x6+<br>3x3x4+1x1x4=8732个default box.对一张300x300的图片输入网络将会针对这8732个default box预测8732个边界框。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/SSD-02.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/SSD-02.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>2. 怎样对先验框进行匹配？</strong><br>SSD在训练的时候只需要输入图像和图像中每个目标对应的ground truth. 先验框与ground truth 的匹配遵循两个原则：</p>
<p>（1）对图片中的每个ground truth, 在先验框中找到与其IOU最大的先验框，则该先验框对应的预测边界框与ground truth 匹配。</p>
<p>（2）对于（1）中每个剩下的没有与任何ground truth匹配到的先验框，找到与其IOU最大的ground truth，若其与该ground truth的IOU值大于某个阈值（一般设为0.5），则该先验框对应的预测边界框与该ground truth匹配。</p>
<p>按照这两个原则进行匹配，匹配到ground truth的先验框对应的预测边界框作为正样本，没有匹配到ground truth的先验框对应的预测边界框作为负样本。尽管一个ground truth可以与多个先验框匹配，但是ground truth的数量相对先验框还是很少，按照上面的原则进行匹配还是会造成负样本远多于正样本的情况。为了使正负样本尽量均衡（一般保证正负样本比例约为1：3），SSD采用hard negative mining, 即对负样本按照其预测背景类的置信度进行降序排列，选取置信度较小的top-k作为训练的负样本。</p>
<p><strong>3. 怎样得到预测的检测结果？</strong></p>
<p>最后分别在所选的特征层上使用3x3卷积核预测不同default boxes所属的类别分数及其预测的边界框location。由于对于每个box需要预测该box属于每个类别的置信度（假设有c类，包括背景，例如20class的数据集合，c=21）和该box对应的预测边界框的location(包含4个值，即该box的中心坐标和宽高)，则每个box需要预测c+4个值。所以对于某个所选的特征层，该层的卷积核个数为（c+4）x 该层的default box个数.最后将每个层得到的卷积结果进行拼接。对于得到的每个预测框，取其类别置信度的最大值，若该最大值大于置信度阈值，则最大值所对应的类别即为该预测框的类别，否则过滤掉此框。对于保留的预测框根据它对应的先验框进行解码得到其真实的位置参数（这里还需注意要防止预测框位置超出图片），然后根据所属类别置信度进行降序排列，取top-k个预测框，最后进行NMS，过滤掉重叠度较大的预测框，最后得到检测结果。</p>
<p>SSD优势是速度比较快，整个过程只需要一步，首先在图片不同位置按照不同尺度和宽高比进行密集抽样，然后利用CNN提取特征后直接进行分类与回归，所以速度比较快，但均匀密集采样会造成正负样本不均衡的情况使得训练比较困难，导致模型准确度有所降低。另外，SSD对小目标的检测没有大目标好，因为随着网络的加深，在高层特征图中小目标的信息丢失掉了，适当增大输入图片的尺寸可以提升小目标的检测效果。</p>
<h3 id="DSSD"><a href="#DSSD" class="headerlink" title="DSSD"></a>DSSD</h3><p><strong>DSSD有哪些创新点？</strong></p>
<ol>
<li>Backbone：将ResNet替换SSD中的VGG网络，增强了特征提取能力</li>
<li>添加了Deconvolution层，增加了大量上下文信息</li>
</ol>
<p>为了解决SSD算法检测小目标困难的问题，DSSD算法将SSD算法基础网络从VGG-16更改为ResNet-101，增强网络特征提取能力，其次参考FPN算法思路利用去Deconvolution结构将图像深层特征从高维空间传递出来，与浅层信息融合，联系不同层级之间的图像语义关系，设计预测模块结构，通过不同层级特征之间融合特征输出预测物体类别信息。</p>
<p>DSSD算法中有两个特殊的结构：Prediction模块；Deconvolution模块。前者利用提升每个子任务的表现来提高准确性，并且防止梯度直接流入ResNet主网络。后者则增加了三个Batch Normalization层和三个3×3卷积层，其中卷积层起到了缓冲的作用，防止梯度对主网络影响太剧烈，保证网络的稳定性。</p>
<p>SSD和DSSD的网络模型如下图所示：</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/DSSD-01.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/DSSD-01.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>Prediction Module</strong></p>
<p>SSD直接从多个卷积层中单独引出预测函数，预测量多达7000多，梯度计算量也很大。MS-CNN方法指出，改进每个任务的子网可以提高准确性。根据这一思想，DSSD在每一个预测层后增加残差模块，并且对于多种方案进行了对比，如下图所示。结果表明，增加残差预测模块后，高分辨率图片的检测精度比原始SSD提升明显。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/DSSD-02.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/DSSD-02.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>Deconvolution模块</strong></p>
<p>为了整合浅层特征图和deconvolution层的信息，作者引入deconvolution模块，如下图所示。作者受到论文Learning to Refine Object Segments的启发，认为用于精细网络的deconvolution模块的分解结构达到的精度可以和复杂网络一样，并且更有效率。作者对其进行了一定的修改：其一，在每个卷积层后添加批归一化（batch normalization）层；其二，使用基于学习的deconvolution层而不是简单地双线性上采样；其三，作者测试了不同的结合方式，元素求和（element-wise sum）与元素点积（element-wise product）方式，实验证明元素点积计算能得到更好的精度。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/DSSD-03.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/DSSD-03.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h3><p><strong>YOLOv1有哪些创新点？</strong></p>
<ol>
<li>将整张图作为网络的输入，直接在输出层回归bounding box的位置和所属的类别</li>
<li>速度快，one stage detection的开山之作</li>
</ol>
<p><strong>YOLOv1介绍</strong></p>
<p>YOLO（You Only Look Once: Unified, Real-Time Object Detection）是one-stage detection的开山之作。之前的物体检测方法首先需要产生大量可能包含待检测物体的先验框, 然后用分类器判断每个先验框对应的边界框里是否包含待检测物体，以及物体所属类别的概率或者置信度，同时需要后处理修正边界框，最后基于一些准则过滤掉置信度不高和重叠度较高的边界框，进而得到检测结果。这种基于先产生候选区再检测的方法虽然有相对较高的检测准确率，但运行速度较慢。</p>
<p>YOLO创造性的将物体检测任务直接当作回归问题（regression problem）来处理，将候选区和检测两个阶段合二为一。只需一眼就能知道每张图像中有哪些物体以及物体的位置。下图展示了各物体检测系统的流程图。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv1-01.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv1-01.png" srcset="data:image/png;base64,666" alt=""></p>
<p>事实上，YOLO也并没有真正的去掉候选区，而是直接将输入图片划分成7x7=49个网格，每个网格预测两个边界框，一共预测49x2=98个边界框。可以近似理解为在输入图片上粗略的选取98个候选区，这98个候选区覆盖了图片的整个区域，进而用回归预测这98个候选框对应的边界框。</p>
<p><strong>1. 网络结构是怎样的？</strong></p>
<p>YOLO网络借鉴了GoogLeNet分类网络结构，不同的是YOLO使用1x1卷积层和3x3卷积层替代inception module。如下图所示，整个检测网络包括24个卷积层和2个全连接层。其中，卷积层用来提取图像特征，全连接层用来预测图像位置和类别概率值。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv1-02.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv1-02.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>2. YOLO的输入、输出、损失函数分别是什么？</strong></p>
<p>前面说到YOLO将输入图像分成7x7的网格，最后输出是7x7xk的张量。YOLO网络最后接了两个全连接层，全连接层要求输入是固定大小的，所以YOLO要求输入图像有固定大小，论文中作者设计的输入尺寸是448x448。</p>
<p>YOLO将输入图像分成7x7的网格，每个网格预测2个边界框。若某物体的ground truth的中心落在该网格，则该网格中与这个ground truth IOU最大的边界框负责预测该物体。对每个边界框会预测5个值，分别是边界框的中心x,y（相对于所属网格的边界），边界框的宽高w,h（相对于原始输入图像的宽高的比例），以及这些边界框的confidencescores（边界框与ground truth box的IOU值）。同时每个网格还需要预测c个类条件概率 （是一个c维向量，表示某个物体object在这个网格中，且该object分别属于各个类别的概率，这里的c类物体不包含背景）。论文中的c=20，则每个网格需要预测2x5+20=30个值，这些值被映射到一个30维的向量。<br>为了让边界框坐标损失、分类损失达到很好的平衡，损失函数设计如下图所示。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv1-03.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv1-03.png" srcset="data:image/png;base64,666" alt=""></p>
<p>如上图所示，损失函数分为坐标预测（蓝色框）、含有物体的边界框的confidence预测（红色框）、不含有物体的边界框的confidence预测（黄色框）、分类预测（紫色框）四个部分。</p>
<p>由于不同大小的边界框对预测偏差的敏感度不同，小的边界框对预测偏差的敏感度更大。为了均衡不同尺寸边界框对预测偏差的敏感度的差异。作者巧妙的对边界框的w,h取均值再求L2 loss. YOLO中更重视坐标预测，赋予坐标损失更大的权重，记为 coord，在pascal voc训练中coodd=5 ，classification error部分的权重取1。</p>
<p>某边界框的置信度定义为：某边界框的confidence = 该边界框存在某类对象的概率pr(object)*该边界框与该对象的ground truth的IOU值 ，若该边界框存在某个对象pr(object)=1 ，否则pr(object)=0 。由于一幅图中大部分网格中是没有物体的，这些网格中的边界框的confidence置为0，相比于有物体的网格，这些不包含物体的网格更多，对梯度更新的贡献更大，会导致网络不稳定。为了平衡上述问题，YOLO损失函数中对没有物体的边界框的confidence error赋予较小的权重，记为 noobj，对有物体的边界框的confidence error赋予较大的权重。在pascal VOC训练中noobj=0.5 ，有物体的边界框的confidence error的权重设为1.</p>
<p><strong>3. YOLO怎样预测？</strong></p>
<p>YOLO最后采用非极大值抑制（NMS）算法从输出结果中提取最有可能的对象和其对应的边界框。</p>
<p>输入一张图片到YOLO网络将输出一个7<em>7</em>30的张量表示图片中每个网格对应的可能的两个边界框以及每个边界框的置信度和包含的对象属于各个类别的概率。由此可以计算某对象i属于类别 同时在第j个边界框中的得分：</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv1-04.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv1-04.png" srcset="data:image/png;base64,666" alt=""></p>
<p>每个网格有20个类条件概率，2个边界框置信度，相当于每个网格有40个得分，7x7个网格有1960个得分，每类对象有1960/20=98个得分，即98个候选框。</p>
<p><strong>NMS步骤如下：</strong></p>
<p>1.设置一个Score的阈值，一个IOU的阈值；</p>
<p>2.对于每类对象，遍历属于该类的所有候选框，</p>
<p>①过滤掉Score低于Score阈值的候选框；</p>
<p>②找到剩下的候选框中最大Score对应的候选框，添加到输出列表；</p>
<p>③进一步计算剩下的候选框与②中输出列表中每个候选框的IOU，若该IOU大于设置的IOU阈值，将该候选框过滤掉，否则加入输出列表中；</p>
<p>④最后输出列表中的候选框即为图片中该类对象预测的所有边界框</p>
<p>3.返回步骤2继续处理下一类对象。</p>
<p>YOLO将识别与定位合二为一，结构简便，检测速度快，更快的Fast YOLO可以达到155FPS。相对于R-CNN系列, YOLO的整个流程中都能看到整张图像的信息，因此它在检测物体时能很好的利用上下文信息，从而不容易在背景上预测出错误的物体信息。同时YOLO可以学习到高度泛化的特征，能将一个域上学到的特征迁移到不同但相关的域上，如在自然图像上做训练的YOLO，在艺术图片上可以得到较好的测试结果。</p>
<p>由于YOLO网格设置比较稀疏，且每个网格只预测2个边界框，其总体预测精度不高，略低于Fast RCNN。其对小物体的检测效果较差，尤其是对密集的小物体表现比较差。</p>
<h3 id="YOLOv2"><a href="#YOLOv2" class="headerlink" title="YOLOv2"></a>YOLOv2</h3><p><strong>YOLOv2 有哪些创新点？</strong></p>
<p>YOLOv1虽然检测速度快，但在定位方面不够准确，并且召回率较低。为了提升定位准确度，改善召回率，YOLOv2在YOLOv1的基础上提出了几种改进策略，如下图所示，可以看到，一些改进方法能有效提高模型的mAP。</p>
<ol>
<li>大尺度预训练分类</li>
<li>New Network：Darknet-19</li>
<li>加入anchor</li>
</ol>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv2-01.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv2-01.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>YOLOv2 介绍</strong></p>
<p><strong>（1）Batch Normalization</strong></p>
<p>YOLOv2中在每个卷积层后加Batch Normalization(BN)层，去掉dropout. BN层可以起到一定的正则化效果，能提升模型收敛速度，防止模型过拟合。YOLOv2通过使用BN层使得mAP提高了2%。<br><strong>（2）High Resolution Classifier</strong></p>
<p>目前的大部分检测模型都会使用主流分类网络（如vgg、resnet）在ImageNet上的预训练模型作为特征提取器,<br>而这些分类网络大部分都是以小于256x256的图片作为输入进行训练的，低分辨率会影响模型检测能力。YOLOv2将输入图片的分辨率提升至448x448，为了使网络适应新的分辨率，YOLOv2先在ImageNet上以448x448的分辨率对网络进行10个epoch的微调，让网络适应高分辨率的输入。通过使用高分辨率的输入，YOLOv2的mAP提升了约4%。</p>
<p><strong>（3）Convolutional With Anchor Boxes</strong></p>
<p>YOLOv1利用全连接层直接对边界框进行预测，导致丢失较多空间信息，定位不准。YOLOv2去掉了YOLOv1中的全连接层，使用Anchor Boxes预测边界框，同时为了得到更高分辨率的特征图，YOLOv2还去掉了一个池化层。由于图片中的物体都倾向于出现在图片的中心位置，若特征图恰好有一个中心位置，利用这个中心位置预测中心点落入该位置的物体，对这些物体的检测会更容易。所以总希望得到的特征图的宽高都为奇数。YOLOv2通过缩减网络，使用416x416的输入，模型下采样的总步长为32，最后得到13x13的特征图，然后对13x13的特征图的每个cell预测5个anchor boxes，对每个anchor box预测边界框的位置信息、置信度和一套分类概率值。使用anchor<br>boxes之后，YOLOv2可以预测13x13x5=845个边界框，模型的召回率由原来的81%提升到88%，mAP由原来的69.5%降低到69.2%.召回率提升了7%，准确率下降了0.3%。</p>
<p><strong>（4）Dimension Clusters</strong></p>
<p>在Faster R-CNN和SSD中，先验框都是手动设定的，带有一定的主观性。YOLOv2采用k-means聚类算法对训练集中的边界框做了聚类分析，选用boxes之间的IOU值作为聚类指标。综合考虑模型复杂度和召回率，最终选择5个聚类中心，得到5个先验框，发现其中中扁长的框较少，而瘦高的框更多，更符合行人特征。通过对比实验，发现用聚类分析得到的先验框比手动选择的先验框有更高的平均IOU值，这使得模型更容易训练学习。</p>
<p><strong>（5）New Network：Darknet-19</strong></p>
<p>YOLOv2采用Darknet-19，其网络结构如下图所示，包括19个卷积层和5个max pooling层，主要采用3x3卷积和1x1卷积，这里1x1卷积可以压缩特征图通道数以降低模型计算量和参数，每个卷积层后使用BN层以加快模型收敛同时防止过拟合。最终采用global avg pool 做预测。采用YOLOv2，模型的mAP值没有显著提升，但计算量减少了。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv2-02.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv2-02.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>（6）Direct location prediction</strong></p>
<p>Faster R-CNN使用anchor boxes预测边界框相对先验框的偏移量，由于没有对偏移量进行约束，每个位置预测的边界框可以落在图片任何位置，会导致模型不稳定，加长训练时间。YOLOv2沿用YOLOv1的方法，根据所在网格单元的位置来预测坐标,则Ground Truth的值介于0到1之间。网络中将得到的网络预测结果再输入sigmoid函数中，让输出结果介于0到1之间。设一个网格相对于图片左上角的偏移量是cx，cy。先验框的宽度和高度分别是pw和ph，则预测的边界框相对于特征图的中心坐标(bx，by)和宽高bw、bh的计算公式如下图所示。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv2-03.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv2-03.png" srcset="data:image/png;base64,666" alt=""></p>
<p>YOLOv2结合Dimention Clusters, 通过对边界框的位置预测进行约束，使模型更容易稳定训练，这种方式使得模型的mAP值提升了约5%。</p>
<p><strong>（7）Fine-Grained Features</strong></p>
<p>YOLOv2借鉴SSD使用多尺度的特征图做检测，提出pass through层将高分辨率的特征图与低分辨率的特征图联系在一起，从而实现多尺度检测。YOLOv2提取Darknet-19最后一个max pool层的输入，得到26x26x512的特征图。经过1x1x64的卷积以降低特征图的维度，得到26x26x64的特征图，然后经过pass through层的处理变成13x13x256的特征图（抽取原特征图每个2x2的局部区域组成新的channel，即原特征图大小降低4倍，channel增加4倍），再与13x13x1024大小的特征图连接，变成13x13x1280的特征图，最后在这些特征图上做预测。使用Fine-Grained Features，YOLOv2的性能提升了1%.</p>
<p><strong>（8）Multi-Scale Training</strong></p>
<p>YOLOv2中使用的Darknet-19网络结构中只有卷积层和池化层，所以其对输入图片的大小没有限制。YOLOv2采用多尺度输入的方式训练，在训练过程中每隔10个batches,重新随机选择输入图片的尺寸，由于Darknet-19下采样总步长为32，输入图片的尺寸一般选择32的倍数{320,352,…,608}。采用Multi-Scale Training, 可以适应不同大小的图片输入，当采用低分辨率的图片输入时，mAP值略有下降，但速度更快，当采用高分辨率的图片输入时，能得到较高mAP值，但速度有所下降。</p>
<p>YOLOv2借鉴了很多其它目标检测方法的一些技巧，如Faster R-CNN的anchor boxes, SSD中的多尺度检测。除此之外，YOLOv2在网络设计上做了很多tricks,使它能在保证速度的同时提高检测准确率，Multi-Scale Training更使得同一个模型适应不同大小的输入，从而可以在速度和精度上进行自由权衡。</p>
<p><strong>YOLOv2的训练</strong></p>
<p>YOLOv2的训练主要包括三个阶段。<br>第一阶段：先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为$224\times 224$,共训练160个epochs。<br>第二阶段：将网络的输入调整为$448\times 448$,继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。<br>第三个阶段：修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络。<br>网络修改包括（网路结构可视化）：移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个$3\times 3 \times 2014$卷积层，同时增加了一个passthrough层，最后使用$1\times 1$卷积层输出预测结果。</p>
<h3 id="YOLO9000"><a href="#YOLO9000" class="headerlink" title="YOLO9000"></a>YOLO9000</h3><p>github：<a href="http://pjreddie.com/yolo9000/">http://pjreddie.com/yolo9000/</a></p>
<p>YOLO9000是在YOLOv2的基础上提出的一种联合训练方法，可以检测超过9000个类别的模型。YOLOv2混合目标检测数据集和分类数据集，用目标检测数据集及其类别标记信息和位置标注信息训练模型学习预测目标定位和分类，用分类数据集及其类别标记信息进一步扩充模型所能识别的物体类别同时能增强模型鲁棒性。</p>
<p><strong>1. YOLO9000是怎么组织数据的？</strong></p>
<p>YOLO9000根据各个类别之间的从属关系建立一种树结WordTree, 将COCO数据集和ImageNet数据集组织起来。</p>
<p>WordTree的生成方式如下：</p>
<p>①首先遍历ImageNet中的类别名词。</p>
<p>②对每个名词，在WordNet(一种结构化概念及概念之间关系的语言数据库)上找到从它所在位置到根节点（设根节点为实体对象physical object）的最短路径，由于在WordNet中大多数同义词只有一个路径，所以先把将该路径上的词全都加到树中。</p>
<p>③迭代地检查剩下的名词，取它到根节点的最短路径，将该最短路径上的还没出现在层次树中的词加入到树中。<br>混合后的数据集形成一个有9418类的WordTree.生成的WordTree模型如下图所示。另外考虑到COCO数据集相对于ImageNet数据集数据量太少了，为了平衡两个数据集，作者进一步对COCO数据集过采样，使COCO数据集与ImageNet数据集的数据量比例接近1：4。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv2-04.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv2-04.png" srcset="data:image/png;base64,666" alt=""></p>
<p>对于物体的标签，采用one-hot编码的形式，数据集中的每个物体的类别标签被组织成1个长度为9418的向量，向量中除在WordTree中从该物体对应的名词到根节点的路径上出现的词对应的类别标号处为1，其余位置为0。</p>
<p><strong>2. YOLO9000是怎么进行联合训练的？</strong></p>
<p>YOLO9000采用YOLOv2的结构，anchorbox由原来的5调整到3，对每个anchorbox预测其对应的边界框的位置信息x,y,w,h和置信度以及所包含的物体分别属于9418类的概率，所以每个anchorbox需要预测4+1+9418=9423个值。每个网格需要预测3x9423=28269个值。在训练的过程中，当网络遇到来自检测数据集的图片时，用完整的YOLOv2loss进行反向传播计算，当网络遇到来自分类数据集的图片时，只用分类部分的loss进行反向传播。</p>
<p><strong>3. YOLO9000是怎么预测的？</strong></p>
<p>WordTree中每个节点的子节点都属于同一个子类，分层次的对每个子类中的节点进行一次softmax处理，以得到同义词集合中的每个词的下义词的概率。当需要预测属于某个类别的概率时，需要预测该类别节点的条件概率。即在WordTree上找到该类别名词到根节点的路径，计算路径上每个节点的概率之积。预测时，YOLOv2得到置信度，同时会给出边界框位置以及一个树状概率图，沿着根节点向下，沿着置信度最高的分支向下，直到达到某个阈值，最后到达的节点类别即为预测物体的类别。</p>
<p>YOLO9000使用WordTree混合目标检测数据集和分类数据集，并在其上进行联合训练，使之能实时检测出超过9000个类别的物体，其强大令人赞叹不已。YOLO9000尤其对动物的识别效果很好，但是对衣服或者设备等类别的识别效果不是很好，可能的原因是与目标检测数据集中的数据偏向有关。</p>
<h3 id="YOLOv3"><a href="#YOLOv3" class="headerlink" title="YOLOv3"></a>YOLOv3</h3><p>YOLOv3总结了自己在YOLOv2的基础上做的一些尝试性改进，有的尝试取得了成功，而有的尝试并没有提升模型性能。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用FPN架构实现多尺度检测。</p>
<p><strong>YOLOv3有哪些创新点？</strong></p>
<ol>
<li>新网络结构：DarkNet-53</li>
<li>融合FPN</li>
<li>用逻辑回归替代softmax作为分类器</li>
</ol>
<p><strong>1. YOLOv3对网络结构做了哪些改进？</strong></p>
<p>YOLOv3在之前Darknet-19的基础上引入了残差块，并进一步加深了网络，改进后的网络有53个卷积层，取名为Darknet-53，网络结构如下图所示（以256*256的输入为例）。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv3-01.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv3-01.png" srcset="data:image/png;base64,666" alt=""></p>
<p>为了比较Darknet-53与其它网络结构的性能，作者在TitanX上，采用相同的实验设置，将256x256的图片分别输入以Darknet-19，ResNet-101，ResNet-152和Darknet-53为基础网络的分类模型中，实验得到的结果如下图所示。可以看到Darknet-53比ResNet-101的性能更好，而且速度是其1.5倍，Darknet-53与ResNet-152性能相似但速度几乎是其2倍。注意到，Darknet-53相比于其它网络结构实现了每秒最高的浮点计算量，说明其网络结构能更好的利用GPU。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv3-02.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv3-02.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>2.YOLOv3中怎样实现多尺度检测？</strong></p>
<p>YOLOv3借鉴了FPN的思想，从不同尺度提取特征。相比YOLOv2，YOLOv3提取最后3层特征图，不仅在每个特征图上分别独立做预测，同时通过将小特征图上采样到与大的特征图相同大小，然后与大的特征图拼接做进一步预测。用维度聚类的思想聚类出9种尺度的anchor box，将9种尺度的anchor box均匀的分配给3种尺度的特征图.如下图是在网络结构图的基础上加上多尺度特征提取部分的示意图（以在COCO数据集(80类)上256x256的输入为例）：</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv3-03.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/YOLOv3-03.png" srcset="data:image/png;base64,666" alt=""></p>
<p>从YOLOv1到YOLOv2再到YOLO9000、YOLOv3, YOLO经历三代变革，在保持速度优势的同时，不断改进网络结构，同时汲取其它优秀的目标检测算法的各种trick，先后引入anchor box机制、引入FPN实现多尺度检测等。</p>
<h3 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h3><p><strong>研究背景</strong></p>
<ul>
<li>Two-Stage检测器（如Faster R-CNN、FPN）效果好，但速度相对慢</li>
<li>One-Stage检测器（如YOLO、SSD）速度快，但效果一般</li>
</ul>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-01.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-01.png" srcset="data:image/png;base64,666" alt=""></p>
<p>作者对one-stage检测器准确率不高的问题进行探究，发现主要问题在于正负类别不均衡（简单-难分类别不均衡）。</p>
<blockquote>
<p>We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause.</p>
</blockquote>
<p>作者建议通过重新设计标准的交叉熵损失（cross entropy loss）来解决这种类别不平衡（class inbalance）问题，即提出Focal Loss。</p>
<blockquote>
<p>We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training.</p>
</blockquote>
<p>结合Focal Loss的one-stage检测器称为RetinaNet，该检测器在COCO上mAP可以和特征金字塔网络（feature pyramid network，FPN）或者Mask R-CNN接近，</p>
<p><strong>问：什么是类别不均衡（class imbalance）？</strong></p>
<p>答：负样本的数量极大于正样本的数量，比如包含物体的区域（正样本）很少，而不包含物体的区域（负样本）很多。比如检测算法在早期会生成一大波的bbox。而一幅常规的图片中，顶多就那么几个object。这意味着，绝大多数的bbox属于background。</p>
<p><strong>问：样本的类别不均衡会带来什么问题？</strong></p>
<p>答：由于大多数都是简单易分的负样本（属于背景的样本），使得训练过程不能充分学习到属于那些有类别样本的信息；其次简单易分的负样本太多，可能掩盖了其他有类别样本的作用（这些简单易分的负样本仍产生一定幅度的loss，见下图蓝色曲线，数量多会对loss起主要贡献作用，因此就主导了梯度的更新方向，掩盖了重要的信息）</p>
<blockquote>
<p>This imbalance causes two problems: (1) training is inefficient as most locations are easy negatives that contribute no useful learning signal; (2) en masse, the easy negatives can overwhelm training and lead to degenerate models.</p>
</blockquote>
<p>简单来说，因为bbox数量爆炸。 正是因为bbox中属于background的bbox太多了，所以如果分类器无脑地把所有bbox统一归类为background，accuracy也可以刷得很高。于是乎，分类器的训练就失败了。分类器训练失败，检测精度自然就低了。</p>
<p><strong>问：为什么在two-stage检测器中，没有出现类别不均衡（class imbalamce）问题呢？</strong></p>
<p>答：因为通过RPN阶段可以减少候选目标区域，而在分类阶段，可以固定前景与背景比值（foreground-to-background ratio）为1:3，或者使用OHEM（online hard example mining）使得前景和背景的数量达到均衡。</p>
<p><strong>RetinaNet有哪些创新点？</strong></p>
<p><strong>概述：</strong></p>
<ul>
<li>New loss：提出Focal Loss函数解决class imbalance</li>
</ul>
<script type="math/tex; mode=display">
FL(p_t) = -(1-p_t)^\gamma \log(p_t)FL(pt)=−(1−pt)γlog(pt)</script><ul>
<li>New detector：RetinaNet = ResNet + FPN + Two sub-networks + Focal Loss</li>
</ul>
<p>Focal Loss更加聚焦在困难样本（hard examples）上的训练。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-02.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-02.png" srcset="data:image/png;base64,666" alt=""></p>
<p>将Focal Loss与ResNet-101-FPN backbone结合提出RetinaNet（one-stage检测器），RetinaNet在COCO test-dev上达到39.1mAP，速度为5FPS。</p>
<p>RetinaNet检测器与当时最佳的其它检测器进行比较，无论是速度上还是准确率上都是最佳：</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-03.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-03.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>详解：</strong></p>
<p>作者提出一种新的损失函数，思路是希望那些hard examples对损失的贡献变大，使网络更倾向于从这些样本上学习。</p>
<p>作者以二分类为例进行说明：</p>
<p><strong>交叉熵函数CE</strong></p>
<p>首先是我们常使用的交叉熵损失函数：</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-04.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-04.png" srcset="data:image/png;base64,666" alt=""></p>
<p>上式中，y=+1或者y=-1。p∈[0,1]是y=+1的估计概率。作者定义pt为：</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-05.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-05.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-06.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-06.png" srcset="data:image/png;base64,666" alt=""></p>
<p>注：对交叉熵函数不了解的，可以参考<a href="https://blog.csdn.net/chaipp0607/article/details/73392175">理解交叉熵作为损失函数在神经网络中的作用</a></p>
<p><strong>均衡交叉熵函数</strong></p>
<p>要对类别不均衡问题对loss的贡献进行一个控制，即加上一个控制权重即可，最初作者的想法即如下这样，对于属于少数类别的样本，增大α即可</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-07.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-07.png" srcset="data:image/png;base64,666" alt=""></p>
<p>但这样有一个问题，它仅仅解决了正负样本之间的平衡问题，并没有区分易分/难分样本，按作者的话说：</p>
<blockquote>
<p>While α balances the importance of positive/negative examples, it does not differentiate between easy/hard examples. Instead, we propose to reshape the loss function to down-weight easy examples and thus focus training on hard negatives.</p>
</blockquote>
<p>问：为什么公式(3)只解决正负样本不均衡问题？</p>
<p>答：增加了一个系数αt，跟pt的定义类似，当label=1的时候，αt=a；当label=-1的时候，αt=1-a，a的范围也是0到1。因此可以通过设定a的值（一般而言假如1这个类的样本数比-1这个类的样本数多很多，那么a会取0到0.5来增加-1这个类的样本的权重）来控制正负样本对总的loss的共享权重。</p>
<p><strong>Focal Loss</strong></p>
<p>作者一开始给交叉熵损失函数添加modulating factor：</p>
<script type="math/tex; mode=display">
(1-pt)^γ(1−pt)γ</script><p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-08.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-08.png" srcset="data:image/png;base64,666" alt=""></p>
<p>显然，样本越易分，pt就越大（pt—&gt;1），modulating factor趋近于0，则贡献的loss就越小，同样地，样本越难分，其pt就越小，modulating factor接近于1，则贡献的loss不受影响。</p>
<p>问：为什么pt越大，FL值越小？</p>
<p>答：根据公式（4）可知，FL与log(pt)中的pt成反比，与1-pt成正比，因此FL与pt的关系成反比。这是交叉熵函数的基本性质。当pt很大时（接近于1），FL值很小；而当pt很小时（接近于0），FL值会很大。</p>
<p>注：这里有个超参数—focusing parameter γ。</p>
<p>γ 放大了modulating factor的作用。</p>
<p>举原文中的一个例子，当pt=0.9时，带有modulating factor的focal loss是CE loss的100分之一，即进一步减小了正确分类的损失。</p>
<blockquote>
<p>For instance, with γ = 2, an example classified with pt = 0.9 would have 100× lower loss compared with CE and with pt ≈ 0.968 it would have 1000× lower loss. This in turn increases the importance of correcting misclassified examples (whose loss is scaled down by at most 4× for pt ≤ .5 and γ = 2).</p>
</blockquote>
<p>在实际中，作者采用如下公式，即综合了公式(3)和公式(4)的形式，这样机能调整正负样本的权重，又能控制难易分类样本的权重：</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-09.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-09.png" srcset="data:image/png;base64,666" alt=""></p>
<p>这里的两个参数 α和γ 来控制，在实验中a的选择范围也很广，一般而言当γ增加的时候，a需要减小一点，本文作者采用α=0.25，γ=2效果最好。</p>
<p><strong>RetinaNet Detector</strong></p>
<p>RetinaNet是由backbone网络和两个特殊任务的子网络（subnet）组成（属于one-stage检测器）。Backbone用来计算feature map；第一个子网络用来object classification，第二个子网络用来bounding box regression。</p>
<p><strong>Feature Pyramid Network Backbone</strong></p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-10.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-10.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>Anchor</strong></p>
<p><strong>Classification Subnet</strong></p>
<p><strong>Box Regression Subnet</strong></p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-11.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-11.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-12.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-12.png" srcset="data:image/png;base64,666" alt=""></p>
<p>RetinaNet结构注意内容：</p>
<ol>
<li>训练时FPN每一级的所有example都被用于计算Focal Loss，loss值加到一起用来训练；</li>
<li>测试时FPN每一级只选取score最大的1000个example来做nms；</li>
<li>整个结构不同层的head部分(上图中的c和d部分)共享参数，但分类和回归分支间的参数不共享；</li>
<li>分类分支的最后一级卷积的bias初始化成前面提到的-log((1-π)/π);</li>
</ol>
<p>作者：张磊_0503 链接：<a href="https://www.jianshu.com/p/204d9ad9507f">https://www.jianshu.com/p/204d9ad9507f</a> 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。</p>
<p><strong>实验结果</strong></p>
<p>Table1是关于RetinaNet和Focal Loss的一些实验结果。（a）是在交叉熵的基础上加上参数a，a=0.5就表示传统的交叉熵，可以看出当a=0.75的时候效果最好，AP值提升了0.9。（b）是对比不同的参数γ和a的实验结果，可以看出随着γ的增加，AP提升比较明显。（d）通过和OHEM的对比可以看出最好的Focal Loss比最好的OHEM提高了3.2AP。这里OHEM1:3表示在通过OHEM得到的minibatch上强制positive和negative样本的比例为1:3，通过对比可以看出这种强制的操作并没有提升AP。（e）加入了运算时间的对比，可以和前面的Figure2结合起来看，速度方面也有优势！注意这里RetinaNet-101-800的AP是37.8，当把训练时间扩大1.5倍同时采用scale jitter，AP可以提高到39.1，这就是全文和table2中的最高的39.1AP的由来。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-13.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-13.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-14.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RetinaNet-14.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="RFBNet"><a href="#RFBNet" class="headerlink" title="RFBNet"></a>RFBNet</h3><p><strong>RFBNet有哪些创新点？</strong></p>
<ol>
<li>提出RF block（RFB）模块</li>
</ol>
<p>RFBNet主要想利用一些技巧使得轻量级模型在速度和精度上达到很好的trade-off的检测器。灵感来自人类视觉的感受野结构Receptive Fields (RFs) ，提出了新奇的RF block（RFB）模块，来验证感受野尺寸和方向性的对提高有鉴别鲁棒特征的关系。RFBNet是以主干网络（backbone）为VGG16的SSD来构建的，主要是在Inception的基础上加入了dilated卷积层（dilated convolution），从而有效增大了感受野（receptive field）。整体上因为是基于SSD网络进行改进，所以检测速度还是比较快，同时精度也有一定的保证。</p>
<p><strong>RFB介绍</strong></p>
<p>RFB是一个类似Inception模块的多分支卷积模块，它的内部结构可分为两个组件：多分支卷积层和dilated卷积层。如下图：</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RFBNet-01.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RFBNet-01.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>1.多分支卷积层</strong><br>​      根据RF的定义，用多种尺寸的卷积核来实现比固定尺寸更好。具体设计：1.瓶颈结构，1x1-s2卷积减少通道特征，然后加上一个nxn卷积。2.替换5x5卷积为两个3x3卷积去减少参数，然后是更深的非线性层。有些例子，使用1xn和nx1代替nxn卷积；shortcut直连设计来自于ResNet和Inception ResNet V2。3.为了输出，卷积经常有stride=2或者是减少通道，所以直连层用一个不带非线性激活的1x1卷积层。</p>
<p><strong>2.Dilated 卷积层</strong></p>
<p>设计灵感来自Deeplab，在保持参数量和同样感受野的情况下，用来获取更高分辨率的特征。下图展示两种RFB结构：RFB和RFB-s。每个分支都是一个正常卷积后面加一个dilated卷积，主要是尺寸和dilated因子不同。（a）RFB。整体结构上借鉴了Inception的思想，主要不同点在于引入3个dilated卷积层（比如3x3conv，rate=1），这也是RFBNet增大感受野的主要方式之一；（b）RFB-s。RFB-s和RFB相比主要有两个改进，一方面用3x3卷积层代替5x5卷积层，另一方面用1x3和3x1卷积层代替3x3卷积层，主要目的应该是为了减少计算量，类似Inception后期版本对Inception结构的改进。 </p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RFBNet-02.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RFBNet-02.png" srcset="data:image/png;base64,666" alt=""></p>
<p>RFBNet300的整体结构如下图所示，基本上和SSD类似。RFBNet和SSD不同的是：1、主干网上用两个RFB结构替换原来新增的两层。2、conv4_3和conv7_fc在接预测层之前分别接RFB-s和RFB结构。 </p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RFBNet-03.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/RFBNet-03.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="M2Det"><a href="#M2Det" class="headerlink" title="M2Det"></a>M2Det</h3><p><strong>M2Det有哪些创新点？</strong></p>
<ol>
<li>提出了多层次特征金字塔网络（MLFPN）来构建更有效的特征金字塔，用于检测不同尺度的对象。</li>
</ol>
<p>M2Det的整体架构如下所示。M2Det使用backbone和多级特征金字塔网络（MLFPN）从输入图像中提取特征，然后类似于SSD，根据学习的特征生成密集的边界框和类别分数，最后是非最大抑制（NMS）操作以产生最终结果。 MLFPN由三个模块组成：特征融合模块（FFM），简化的U形模块（TUM）和按基于尺度的特征聚合模块（SFAM）。 FFMv1通过融合骨干网络的特征图，将语义信息丰富为基本特征。每个TUM生成一组多尺度特征，然后交替连接的TUM和FFMv2提取多级多尺度特征。此外，SFAM通过按比例缩放的特征连接操作和自适应注意机制将特征聚合到多级特征金字塔中。下面介绍有关M2Det中三个核心模块和网络配置的更多详细信息。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/M2Det-01.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/M2Det-01.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>FFMs</strong></p>
<p>FFM融合了M2Det中不同层次的特征，这对于构建最终的多级特征金字塔至关重要。它们使用1x1卷积层来压缩输入特征的通道，并使用连接操作来聚合这些特征图。特别是，由于FFMv1以backbone中不同比例的两个特征图作为输入，因此它采用一个上采样操作，在连接操作之前将深度特征重新缩放到相同的尺度。同时，FFMv2采用基本特征和前一个TUM的最大输出特征图 - 这两个具有相同的比例 - 作为输入，并产生下一个TUM的融合特征。 FFMv1和FFMv2的结构细节分别如下图（a）和（b）所示。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/M2Det-02.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/M2Det-02.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>TUMs</strong> </p>
<p>TUM不同于FPN和RetinaNet，TUM采用简化的U形结构，如上图（c）所示。编码器是一系列3x3，步长为2的卷积层.并且解码器将这些层的输出作为其参考特征集，而原始FPN选择ResNet主干网络中每个阶段的最后一层的输出。此外，在解码器分支的上采样层后添加1x1卷积层和按元素求和的操作，以增强学习能力并保持特征的平滑性。每个TUM的解码器中的所有输出形成当前级别的多尺度特征。整体而言，堆叠TUM的输出形成多层次多尺度特征，而前TUM主要提供浅层特征，中间TUM提供中等特征，后TUM提供深层特征。</p>
<p><strong>SFAM</strong></p>
<p>SFAM旨在将由TUM生成的多级多尺度特征聚合成多级特征金字塔，如下图所示。SFAM的第一阶段是沿着信道维度将等效尺度的特征连接在一起。聚合特征金字塔可以表示为$X = [X_1,X_2,…,X_i,…,X_L]$，其中</p>
<script type="math/tex; mode=display">X_i = Concat(X_{1i}, X_{2i}, ...., X_{Li}) \in R^{W_i \times H_i \times C}</script><p>指的是尺度第i个最大的特征。这里，聚合金字塔中的每个比例都包含来自多级深度的特征。但是，简单的连接操作不太适合。在第二阶段，引入了通道注意模块，以促使特征集中在最有益的通道。在SE区块之后，使用全局平均池化来在挤压步骤中生成通道统计z∈RC。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/M2Det-03.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/M2Det-03.png" srcset="data:image/png;base64,666" alt=""></p>
<h2 id="人脸检测"><a href="#人脸检测" class="headerlink" title="人脸检测"></a>人脸检测</h2><p>在目标检测领域可以划分为了人脸检测与通用目标检测，往往人脸这方面会有专门的算法（包括人脸检测、人脸识别、人脸其他属性的识别等等），并且和通用目标检测（识别）会有一定的差别，着主要来源于人脸的特殊性（有时候目标比较小、人脸之间特征不明显、遮挡问题等），下面将从人脸检测和通用目标检测两个方面来讲解目标检测。</p>
<h3 id="目前主要有人脸检测方法分类？"><a href="#目前主要有人脸检测方法分类？" class="headerlink" title="目前主要有人脸检测方法分类？"></a>目前主要有人脸检测方法分类？</h3><p>目前人脸检测方法主要包含两个区域：传统人脸检测算法和基于深度学习的人脸检测算法。传统人脸检测算法主要可以分为4类：</p>
<p>（1）基于知识的人脸检测方法；</p>
<p>（2）基于模型的人脸检测方法；</p>
<p>（3）基于特征的人脸检测方法；</p>
<p>（4）基于外观的人脸检测方法。</p>
<p>由于本书着重关注深度学习，下面会着重介绍基于深度学习的人脸检测方法。</p>
<p>2006年Hinton首次提出深度学习（Deep Learning）的概念，它是通过组合低层的特征形成更高层的抽象特征。随后研究者将深度学习应用在人脸检测领域，主要集中在基于卷积神经网络（CNN）的人脸检测研究，如基于级联卷积神经网络的人脸检测（cascade cnn）、 基于多任务卷积神经网络的人脸检测（MTCNN）、Facebox等，很大程度上提高了人脸检测的鲁棒性。当然通用目标检测算法像Faster-rcnn、yolo、ssd等也有用在人脸检测领域，也可以实现比较不错的结果，但是和专门人脸检测算法比还是有差别。下面部分主要介绍基于深度学习的的人脸检测算法，基于深度学习的通用目标检测算法将在第二大节介绍。</p>
<h3 id="如何检测图片中不同大小的人脸？"><a href="#如何检测图片中不同大小的人脸？" class="headerlink" title="如何检测图片中不同大小的人脸？"></a>如何检测图片中不同大小的人脸？</h3><p>传统人脸检测算法中针对不同大小人脸主要有两个策略：</p>
<p>（1）缩放图片的大小（图像金字塔如图8.4.1所示）；</p>
<p>（2）缩放滑动窗的大小（如图8.4.2所示）。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.1.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>图 8.1 图像金字塔           </p>
<p>​      <img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.2.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.2.png" srcset="data:image/png;base64,666" alt=""></p>
<p> 图 8.2 缩放滑动窗口</p>
<p>​    基于深度学习的人脸检测算法中针对不同大小人脸主要也有两个策略，但和传统人脸检测算法有点区别，主要包括:</p>
<p>（1）缩放图片大小。（不过也可以通过缩放滑动窗的方式，基于深度学习的滑动窗人脸检测方式效率会很慢存在多次重复卷积，所以要采用全卷积神经网络（FCN），用FCN将不能用滑动窗的方法。）</p>
<p>（2）通过anchor box的方法（如图8.3所示，不要和图8.2混淆，这里是通过特征图预测原图的anchor box区域，具体在facebox中有描述）。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.3.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.3.png" srcset="data:image/png;base64,666" alt=""></p>
<p>图 8.3 anchor box</p>
<h3 id="如何设定算法检测最小人脸尺寸"><a href="#如何设定算法检测最小人脸尺寸" class="headerlink" title="如何设定算法检测最小人脸尺寸?"></a>如何设定算法检测最小人脸尺寸?</h3><p>主要是看滑动窗的最小窗口和anchorbox的最小窗口。</p>
<p>（1）滑动窗的方法 </p>
<p>假设通过12×12的滑动窗，不对原图做缩放的话，就可以检测原图中12×12的最小人脸。但是往往通常给定最小人脸a=40、或者a=80，以这么大的输入训练CNN进行人脸检测不太现实，速度会很慢，并且下一次需求最小人脸a=30*30又要去重新训练，通常还会是12×12的输入，为满足最小人脸框a，只需要在检测的时候对原图进行缩放即可：w=w×12/a。</p>
<p>（2）anchorbox的方法</p>
<p>原理类似，这里主要看anchorbox的最小box，通过可以通过缩放输入图片实现最小人脸的设定。</p>
<h3 id="如何定位人脸的位置？"><a href="#如何定位人脸的位置？" class="headerlink" title="如何定位人脸的位置？"></a>如何定位人脸的位置？</h3><p>（1）滑动窗的方式：</p>
<p>滑动窗的方式是基于分类器识别为人脸的框的位置确定最终的人脸，</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.4.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.4.png" srcset="data:image/png;base64,666" alt=""></p>
<center>图 8.4 滑动窗</center>

<p>（2）FCN的方式：</p>
<p>​    FCN的方式通过特征图映射到原图的方式确定最终识别为人脸的位置，特征图映射到原图人脸框是要看特征图相比较于原图有多少次缩放（缩放主要查看卷积的步长和池化层），假设特征图上(2,3)的点，可粗略计算缩放比例为8倍，原图中的点应该是(16,24)；如果训练的FCN为12*12的输入，对于原图框位置应该是(16,24,12,12),当然这只是估计位置，具体的再构建网络时要加入回归框的预测，主要是相对于原图框的一个平移与缩放。</p>
<p>（3）通过anchor box的方式：</p>
<p>​    通过特征图映射到图的窗口，通过特征图映射到原图到多个框的方式确定最终识别为人脸的位置。</p>
<h3 id="如何通过一个人脸的多个框确定最终人脸框位置？"><a href="#如何通过一个人脸的多个框确定最终人脸框位置？" class="headerlink" title="如何通过一个人脸的多个框确定最终人脸框位置？"></a>如何通过一个人脸的多个框确定最终人脸框位置？</h3><p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.5.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.5.png" srcset="data:image/png;base64,666" alt=""></p>
<p>图 8.5 通过NMS得到最终的人脸位置</p>
<p>NMS改进版本有很多，最原始的NMS就是判断两个框的交集，如果交集大于设定的阈值，将删除其中一个框，那么两个框应该怎么选择删除哪一个呢？ 因为模型输出有概率值，一般会优选选择概率小的框删除。</p>
<h3 id="基于级联卷积神经网络的人脸检测（Cascade-CNN）"><a href="#基于级联卷积神经网络的人脸检测（Cascade-CNN）" class="headerlink" title="基于级联卷积神经网络的人脸检测（Cascade CNN）"></a>基于级联卷积神经网络的人脸检测（Cascade CNN）</h3><ol>
<li><p>cascade cnn的框架结构是什么？</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.6.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.6.png" srcset="data:image/png;base64,666" alt=""></p>
</li>
</ol>
<p>级联结构中有6个CNN，3个CNN用于人脸非人脸二分类，另外3个CNN用于人脸区域的边框校正。给定一幅图像，12-net密集扫描整幅图片，拒绝90%以上的窗口。剩余的窗口输入到12-calibration-net中调整大小和位置，以接近真实目标。接着输入到NMS中，消除高度重叠窗口。下面网络与上面类似。</p>
<ol>
<li>cascade cnn人脸校验模块原理是什么？ </li>
</ol>
<p>该网络用于窗口校正，使用三个偏移变量：Xn:水平平移量，Yn:垂直平移量，Sn:宽高比缩放。候选框口(x,y,w,h)中，(x,y)表示左上点坐标，(w,h)表示宽和高。</p>
<p>我们要将窗口的控制坐标调整为：</p>
<script type="math/tex; mode=display">
（x-{x_nw}/{s_n},y-{y_nh}/{s_n},{w}/{s_n},{h}/{s_n}）</script><p>这项工作中，我们有$N=5×3×3=45$种模式。偏移向量三个参数包括以下值：</p>
<script type="math/tex; mode=display">
Sn：(0.83,0.91,1.0,1.10,1.21)</script><script type="math/tex; mode=display">
Xn：(-0.17,0,0.17)</script><script type="math/tex; mode=display">
Yn：(-0.17,0,0.17)</script><p>同时对偏移向量三个参数进行校正。</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.8.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.8.png" srcset="data:image/png;base64,666" alt=""></p>
<p>3、训练样本应该如何准备？</p>
<p>人脸样本：</p>
<p>非人脸样本：</p>
<ol>
<li>级联的好处</li>
</ol>
<p>级联的工作原理和好处：</p>
<ul>
<li>最初阶段的网络可以比较简单，判别阈值可以设得宽松一点，这样就可以在保持较高召回率的同时排除掉大量的非人脸窗口；</li>
<li>最后阶段网络为了保证足够的性能，因此一般设计的比较复杂，但由于只需要处理前面剩下的窗口，因此可以保证足够的效率；</li>
<li>级联的思想可以帮助我们去组合利用性能较差的分类器，同时又可以获得一定的效率保证。</li>
</ul>
<h3 id="基于多任务卷积神经网络的人脸检测（MTCNN）"><a href="#基于多任务卷积神经网络的人脸检测（MTCNN）" class="headerlink" title="基于多任务卷积神经网络的人脸检测（MTCNN）"></a>基于多任务卷积神经网络的人脸检测（MTCNN）</h3><p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.9.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.9.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.10.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.10.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.11.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.11.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.12.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.12.png" srcset="data:image/png;base64,666" alt=""></p>
<p>1.MTCNN模型有三个子网络。分别是P-Net,R-Net,O-Net.我想问一下，1.模型中的三个input size是指的是同一张图resize到不同尺度下喂给不同模型，还是同一张图，依次经过三个模型，然后是不同的输入尺寸？（这部分能给我讲一下吗）2.每个模型它都有对应三个结果（face classification;bounding box;facial landmark）这三个在网络上是如何对应的呢？</p>
<p>为了检测不同大小的人脸，开始需要构建图像金字塔，先经过pNet模型，输出人脸类别和边界框（边界框的预测为了对特征图映射到原图的框平移和缩放得到更准确的框），将识别为人脸的框映射到原图框位置可以获取patch，之后每一个patch通过resize的方式输入到rNet，识别为人脸的框并且预测更准确的人脸框，最后rNet识别为人脸的的每一个patch通过resize的方式输入到oNet，跟rNet类似，关键点是为了在训练集有限情况下使模型更鲁棒。</p>
<p>还要注意一点构建图像金字塔的的缩放比例要保留，为了将边界框映射到最开始原图上的</p>
<p>还要注意一点：如何从featureMap映射回原图</p>
<h3 id="Facebox"><a href="#Facebox" class="headerlink" title="Facebox"></a>Facebox</h3><p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.13.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.13.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>（1）Rapidly Digested Convolutional Layers(RDCL)</strong></p>
<p>在网络前期，使用RDCL快速的缩小feature map的大小。 主要设计原则如下：</p>
<ul>
<li>Conv1, Pool1, Conv2 和 Pool2 的stride分别是4, 2, 2 和 2。这样整个RDCL的stride就是32，可以很快把feature map的尺寸变小。</li>
<li>卷积(或pooling)核太大速度就慢，太小覆盖信息又不足。文章权衡之后，将Conv1, Pool1, Conv2 和 Pool2 的核大小分别设为7x7,3x3,5x5,3x3</li>
<li>使用CReLU来保证输出维度不变的情况下，减少卷积核数量。</li>
</ul>
<p><strong>（2）Multiple Scale Convolutional Layers(MSCL)</strong></p>
<p>在网络后期，使用MSCL更好地检测不同尺度的人脸。 主要设计原则有：</p>
<ul>
<li>类似于SSD，在网络的不同层进行检测；</li>
<li>采用Inception模块。由于Inception包含多个不同的卷积分支，因此可以进一步使得感受野多样化。</li>
</ul>
<p><strong>（3）Anchor densification strategy</strong></p>
<p>为了anchor密度均衡，可以对密度不足的anchor以中心进行偏移加倍，如下图所示：</p>
<p><img src="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.14.png" class="lazyload" data-srcset="/zh-TW/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ch8/8.4.14.png" srcset="data:image/png;base64,666" alt=""></p>
<h2 id="目标检测的技巧汇总"><a href="#目标检测的技巧汇总" class="headerlink" title="目标检测的技巧汇总"></a>目标检测的技巧汇总</h2><ol>
<li>Data Augmentation</li>
<li>OHEM</li>
<li>NMS：Soft NMS/ Polygon NMS/ Inclined NMS/ ConvNMS/ Yes-Net NMS/ Softer NMS</li>
<li>Multi Scale Training/Testing</li>
<li>建立小物体与context的关系</li>
<li>参考relation network</li>
<li>结合GAN</li>
<li>结合attention</li>
</ol>
<h2 id="目标检测的常用数据集"><a href="#目标检测的常用数据集" class="headerlink" title="目标检测的常用数据集"></a>目标检测的常用数据集</h2><h3 id="PASCAL-VOC"><a href="#PASCAL-VOC" class="headerlink" title="PASCAL VOC"></a>PASCAL VOC</h3><p>​    VOC数据集是目标检测经常用的一个数据集，自2005年起每年举办一次比赛，最开始只有4类，到2007年扩充为20个类，共有两个常用的版本：2007和2012。学术界常用5k的train/val 2007和16k的train/val 2012作为训练集，test 2007作为测试集，用10k的train/val 2007+test 2007和16k的train/val 2012作为训练集，test2012作为测试集，分别汇报结果。</p>
<h3 id="MS-COCO"><a href="#MS-COCO" class="headerlink" title="MS COCO"></a>MS COCO</h3><p>​    COCO数据集是微软团队发布的一个可以用来图像recognition+segmentation+captioning 数据集，该数据集收集了大量包含常见物体的日常场景图片，并提供像素级的实例标注以更精确地评估检测和分割算法的效果，致力于推动场景理解的研究进展。依托这一数据集，每年举办一次比赛，现已涵盖检测、分割、关键点识别、注释等机器视觉的中心任务，是继ImageNet Chanllenge以来最有影响力的学术竞赛之一。</p>
<p>相比ImageNet，COCO更加偏好目标与其场景共同出现的图片，即non-iconic images。这样的图片能够反映视觉上的语义，更符合图像理解的任务要求。而相对的iconic images则更适合浅语义的图像分类等任务。</p>
<p>​    COCO的检测任务共含有80个类，在2014年发布的数据规模分train/val/test分别为80k/40k/40k，学术界较为通用的划分是使用train和35k的val子集作为训练集（trainval35k），使用剩余的val作为测试集（minival），同时向官方的evaluation server提交结果（test-dev）。除此之外，COCO官方也保留一部分test数据作为比赛的评测集。</p>
<h3 id="Google-Open-Image"><a href="#Google-Open-Image" class="headerlink" title="Google Open Image"></a>Google Open Image</h3><p>​    Open Image是谷歌团队发布的数据集。最新发布的Open Images V4包含190万图像、600个种类，1540万个bounding-box标注，是当前最大的带物体位置标注信息的数据集。这些边界框大部分都是由专业注释人员手动绘制的，确保了它们的准确性和一致性。另外，这些图像是非常多样化的，并且通常包含有多个对象的复杂场景（平均每个图像 8 个）。</p>
<h3 id="ImageNet"><a href="#ImageNet" class="headerlink" title="ImageNet"></a>ImageNet</h3><p>​    ImageNet是一个计算机视觉系统识别项目， 是目前世界上图像识别最大的数据库。ImageNet是美国斯坦福的计算机科学家，模拟人类的识别系统建立的。能够从图片识别物体。Imagenet数据集文档详细，有专门的团队维护，使用非常方便，在计算机视觉领域研究论文中应用非常广，几乎成为了目前深度学习图像领域算法性能检验的“标准”数据集。Imagenet数据集有1400多万幅图片，涵盖2万多个类别；其中有超过百万的图片有明确的类别标注和图像中物体位置的标注。</p>
<h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul>
<li>[ ] 目标检测基础知识：mAP、IoU和NMS等</li>
<li>[ ] 目标检测评测指标</li>
<li>[ ] 目标检测常见标注工具</li>
<li>[ ] 完善目标检测的技巧汇总</li>
<li>[ ] 目标检测的现在难点和未来发展</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>强化学习</title>
    <url>/zh-TW/ch10_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1><h2 id="强化学习的主要特点？"><a href="#强化学习的主要特点？" class="headerlink" title="强化学习的主要特点？"></a>强化学习的主要特点？</h2><p>其他许多机器学习算法中学习器都是学得怎样做，而RL是在尝试的过程中学习到在特定的情境下选择哪种行动可以得到最大的回报。在很多场景中，当前的行动不仅会影响当前的rewards，还会影响之后的状态和一系列的rewards。RL最重要的3个特定在于：<br>(1)    基本是以一种闭环的形式；<br>(2)    不会直接指示选择哪种行动（actions）；<br>(3)    一系列的actions和奖励信号（reward signals）都会影响之后较长的时间。 </p>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>强化学习是机器学习的一个重要分支，是多学科多领域交叉的一个产物，它的本质是解决 decision making 问题，即自动进行决策，并且可以做连续决策。<br>它主要包含四个元素，agent，环境状态，行动，奖励, 强化学习的目标就是获得最多的累计奖励。<br>我们列举几个形象的例子：<br>小孩想要走路，但在这之前，他需要先站起来，站起来之后还要保持平衡，接下来还要先迈出一条腿，是左腿还是右腿，迈出一步后还要迈出下一步。<br>小孩就是 agent，他试图通过采取行动（即行走）来操纵环境（行走的表面），并且从一个状态转变到另一个状态（即他走的每一步），当他完成任务的子任务（即走了几步）时，孩子得到奖励（给巧克力吃），并且当他不能走路时，就不会给巧克力。</p>
<p><img src="/zh-TW/ch10_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/ch10/10-1.png" class="lazyload" data-srcset="/zh-TW/ch10_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/ch10/10-1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>上图中agent代表自身，如果是自动驾驶，agent就是车；如果你玩游戏它就是你当前控制的游戏角色，如马里奥，马里奥往前走时环境就一直在发生变化，有小怪物或者障碍物出现，它需要通过跳跃来进行躲避，就是要做action（如向前走和跳起的动作）；无人驾驶的action就是车左转、右转或刹车等等，它无时无刻都在与环境产生交互，action会反馈给环境，进而改变环境，如果自动驾驶的车行驶目标是100米，它向前开了10米，那环境就发生了变化，所以每次产生action都会导致环境改变，环境的改变会反馈给自身（agent），就是这样的一个循环；反馈又两种方式：1、做的好（reward）即正反馈，2、做得不好（punishment惩罚）即负反馈。Agent可能做得好，也可能做的不好，环境始终都会给它反馈，agent会尽量去做对自身有利的决策，通过反反复复这样的一个循环，agent会越来越做的好，就像孩子在成长过程中会逐渐明辨是非，这就是强化学习。</p>
<h2 id="强化学习应用实例"><a href="#强化学习应用实例" class="headerlink" title="强化学习应用实例"></a>强化学习应用实例</h2><p>（1）Manufacturing</p>
<p>例如一家日本公司 Fanuc，工厂机器人在拿起一个物体时，会捕捉这个过程的视频，记住它每次操作的行动，操作成功还是失败了，积累经验，下一次可以更快更准地采取行动。</p>
<p><img src="/zh-TW/ch10_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/ch10/10-2.png" class="lazyload" data-srcset="/zh-TW/ch10_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/ch10/10-2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>（2）Inventory Management</p>
<p>在库存管理中，因为库存量大，库存需求波动较大，库存补货速度缓慢等阻碍使得管理是个比较难的问题，可以通过建立强化学习算法来减少库存周转时间，提高空间利用率。</p>
<p>（3）Dynamic pricing</p>
<p>强化学习中的 Q-learning 可以用来处理动态定价问题。</p>
<p>（4）Customer Delivery</p>
<p>制造商在向各个客户运输时，想要在满足客户的所有需求的同时降低车队总成本。通过 multi-agents 系统和 Q-learning，可以降低时间，减少车辆数量。</p>
<p>（5）ECommerce Personalization</p>
<p>在电商中，也可以用强化学习算法来学习和分析顾客行为，定制产品和服务以满足客户的个性化需求。</p>
<p>（6）Ad Serving</p>
<p>例如算法 LinUCB （属于强化学习算法 bandit 的一种算法），会尝试投放更广范围的广告，尽管过去还没有被浏览很多，能够更好地估计真实的点击率。<br>再如双 11 推荐场景中，阿里巴巴使用了深度强化学习与自适应在线学习，通过持续机器学习和模型优化建立决策引擎，对海量用户行为以及百亿级商品特征进行实时分析，帮助每一个用户迅速发现宝贝，提高人和商品的配对效率。还有，利用强化学习将手机用户点击率提升了 10-20%。</p>
<p>（7）Financial Investment Decisions</p>
<p>例如这家公司 Pit.ai，应用强化学习来评价交易策略，可以帮助用户建立交易策略，并帮助他们实现其投资目标。</p>
<p>（8）Medical Industry</p>
<p>动态治疗方案（DTR）是医学研究的一个主题，是为了给患者找到有效的治疗方法。 例如癌症这种需要长期施药的治疗，强化学习算法可以将患者的各种临床指标作为输入 来制定治疗策略。</p>
<h2 id="强化学习和监督式学习、非监督式学习的区别"><a href="#强化学习和监督式学习、非监督式学习的区别" class="headerlink" title="强化学习和监督式学习、非监督式学习的区别"></a>强化学习和监督式学习、非监督式学习的区别</h2><p>在机器学习中，我们比较熟知的是监督式学习，非监督学习，此外还有一个大类就是强化学习：<br>当前的机器学习算法可以分为3种：有监督的学习（Supervised Learning）、无监督的学习（Unsupervised Learning）和强化学习（Reinforcement Learning），结构图如下所示：</p>
<p> <img src="/zh-TW/ch10_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/ch10/10-3.png" class="lazyload" data-srcset="/zh-TW/ch10_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/ch10/10-3.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="强化学习和监督式学习的区别："><a href="#强化学习和监督式学习的区别：" class="headerlink" title="强化学习和监督式学习的区别："></a>强化学习和监督式学习的区别：</h3><p>监督式学习就好比你在学习的时候，有一个导师在旁边指点，他知道怎么是对的怎么是错的，但在很多实际问题中，例如 chess，go，这种有成千上万种组合方式的情况，不可能有一个导师知道所有可能的结果。</p>
<p>而这时，强化学习会在没有任何标签的情况下，通过先尝试做出一些行为得到一个结果，通过这个结果是对还是错的反馈，调整之前的行为，就这样不断的调整，算法能够学习到在什么样的情况下选择什么样的行为可以得到最好的结果。</p>
<p>就好比你有一只还没有训练好的小狗，每当它把屋子弄乱后，就减少美味食物的数量（惩罚），每次表现不错时，就加倍美味食物的数量（奖励），那么小狗最终会学到一个知识，就是把客厅弄乱是不好的行为。</p>
<p>两种学习方式都会学习出输入到输出的一个映射，监督式学习出的是之间的关系，可以告诉算法什么样的输入对应着什么样的输出，强化学习出的是给机器的反馈 reward function，即用来判断这个行为是好是坏。<br>另外强化学习的结果反馈有延时，有时候可能需要走了很多步以后才知道以前的某一步的选择是好还是坏，而监督学习做了比较坏的选择会立刻反馈给算法。</p>
<p>而且强化学习面对的输入总是在变化，每当算法做出一个行为，它影响下一次决策的输入，而监督学习的输入是独立同分布的。</p>
<p>通过强化学习，一个 agent 可以在探索和开发（exploration and exploitation）之间做权衡，并且选择一个最大的回报。 </p>
<p>exploration 会尝试很多不同的事情，看它们是否比以前尝试过的更好。 </p>
<p>exploitation 会尝试过去经验中最有效的行为。</p>
<p>一般的监督学习算法不考虑这种平衡，就只是是 exploitative。</p>
<h3 id="强化学习和非监督式学习的区别："><a href="#强化学习和非监督式学习的区别：" class="headerlink" title="强化学习和非监督式学习的区别："></a>强化学习和非监督式学习的区别：</h3><p>非监督式不是学习输入到输出的映射，而是模式。例如在向用户推荐新闻文章的任务中，非监督式会找到用户先前已经阅读过类似的文章并向他们推荐其一，而强化学习将通过向用户先推荐少量的新闻，并不断获得来自用户的反馈，最后构建用户可能会喜欢的文章的“知识图”。</p>
<p>对非监督学习来说，它通过对没有概念标记的训练例进行学习，以发现训练例中隐藏的结构性知识。这里的训练例的概念标记是不知道的，因此训练样本的歧义性最高。对强化学习来说，它通过对没有概念标记、但与一个延迟奖赏或效用（可视为延迟的概念标记）相关联的训练例进行学习，以获得某种从状态到行动的映射。这里本来没有概念标记的概<br>念，但延迟奖赏可被视为一种延迟概念标记，因此其训练样本的歧义性介于监督学习和非监督学习之间。</p>
<p>需要注意的是，监督学习和非监督学习从一开始就是相对的，而强化学习在提出时并没有从训练样本歧义性的角度考虑其与监督学习和非监督学习的区别，因此，一些早期的研究中把强化学习视为一种特殊的非监督学习。事实上，对强化学习的定位到目前仍然是有争议的，有的学者甚至认为它是与“从例子中学习”同一级别的概念。</p>
<p>从训练样本歧义性角度进行的分类体系，在近几年可望有一些扩展，例如多示例学习（multi-instancelearning）等从训练样本歧义性方面来看很特殊的新的学习框架有可能会进入该体系。但到目前为止，没有任何新的框架得到了公认的地位。另外，半监督学习（semi-supervisedlearning）也有一定希望，它的障碍是半监督学习中的歧义性并不是与生俱来的，而是人为的，即用户期望用未标记的样本来辅助对已标记样本的学习。这与监督学习、非监督学习、强化学习等天生的歧义性完全不同。半监督学习中人为的歧义性在解决工程问题上是需要的、有用的（对大量样本进行标记的代价可能是极为昂贵的），但可能不太会导致方法学或对学习问题视点的大的改变。</p>
<p><strong>强化学习和前二者的本质区别</strong>:没有前两者具有的明确数据概念，它不知道结果，只有目标。数据概念就是大量的数据，有监督学习、无监督学习需要大量数据去训练优化你建立的模型，就像猫狗识别，用n多张猫狗图片去训练模型，经过训练优化后，你用一张崭新的猫狗图片让模型作出判断，这个模型就知道是猫还是狗。</p>
<h2 id="强化学习主要有哪些算法？"><a href="#强化学习主要有哪些算法？" class="headerlink" title="强化学习主要有哪些算法？"></a>强化学习主要有哪些算法？</h2><p>强化学习不需要监督信号,可以在模型未知的环境中平衡探索和利用, 其主要算法有蒙特卡罗强化学习, 时间差分(temporal difference: TD)学习, 策略梯度等。典型的深度强化学习算法特点及性能比较如下图所示：</p>
<p><img src="/zh-TW/ch10_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/ch10/10-4.png" class="lazyload" data-srcset="/zh-TW/ch10_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/ch10/10-4.png" srcset="data:image/png;base64,666" alt=""></p>
<p>除了上述深度强化学习算法，还有深度迁移强化学习、分层深度强化学习、深度记忆强化学习以及多智能体强化学习等算法。</p>
<h2 id="深度迁移强化学习算法"><a href="#深度迁移强化学习算法" class="headerlink" title="深度迁移强化学习算法"></a>深度迁移强化学习算法</h2><p>传统深度强化学习算法每次只能解决一种游戏任务, 无法在一次训练中完成多种任务. 迁移学习和强化学习的结合也是深度强化学习的一种主要思路。</p>
<p>Parisotto等提出了一种基于行为模拟的深度迁移强化学习算法. 该算法通过监督信号的指导, 使得单一的策略网络学习各自的策略, 并将知识迁移到新任务中. Rusa等提出策略蒸馏(policy distillation)深度迁移强化学习算法. 策略蒸馏算法中分为学习网络和指导网络, 通过这两个网络Q值的偏差来确定目标函数,引导学习网络逼近指导网络的值函数空间. 此后,Rusa等又提出了一种基于渐进神经网络(progressive neural networks, PNN)的深度迁移强化学习算法.PNN是一种把神经网络和神经网络连起来的算法. 它在一系列序列任务中, 通过渐进的方式来存储知识和提取特征, 完成了对知识的迁移. PNN最终实现多个独立任务的训练, 通过迁移加速学习过程, 避免灾难性遗忘. Fernando 等提出了路径网络(PathNet)[45].PathNet可以说是PNN的进阶版. PathNet把网络中每一层都看作一个模块, 把构建一个网络看成搭积木,也就是复用积木. 它跟PNN非常类似, 只是这里不再有列, 而是不同的路径. PathNet将智能体嵌入到神经网络中, 其中智能体的任务是为新任务发现网络中可以复用的部分. 智能体是网络之中的路径, 其决定了反向传播过程中被使用和更新的参数范围. 在一系列的Atari强化学习任务上, PathNet都实现了正迁移, 这表明PathNet在训练神经网络上具有通用性应用能力.PathNet也可以显著提高A3C算法超参数选择的鲁棒性. Schaul等提出了一种通用值函数逼近器(universalvalue function approximators, UVFAs)来泛化状态和目标空间．UVFAs可以将学习到的知识迁移到环境动态特性相同但目标不同的新任务中.</p>
<h2 id="分层深度强化学习算法"><a href="#分层深度强化学习算法" class="headerlink" title="分层深度强化学习算法"></a>分层深度强化学习算法</h2><p>分层强化学习可以将最终目标分解为多个子任务来学习层次化的策略, 并通过组合多个子任务的策略形成有效的全局策略. Kulkarni等提出了分层DQN(hierarchical deep Q-network, h—DQN) 算法. h—DQN基于时空抽象和内在激励分层, 通过在不同的时空尺度上设置子目标对值函数进行层次化处理. 顶层的值函数用于确定宏观决策, 底层的值函数用于确定具体行动．Krishnamurthy等在h—DQN的基础上提出了基于内部选择的分层深度强化学习算法. 该模型结合时空抽象和深度神经网络, 自动地完成子目标的学习, 避免了特定的内在激励和人工设定中间目标,加速了智能体的学习进程, 同时也增强了模型的泛化能力. Kulkarni等基于后续状态表示法提出了深度后续强化学习(deep successor reinforcement learning,DSRL)．DSRL通过阶段性地分解子目标和学习子目标策略, 增强了对未知状态空间的探索, 使得智能体更加适应那些存在延迟反馈的任务．Vezhnevets等受封建(feudal)强化学习算法的启发, 提出一种分层深度强化学习的架构FeUdal网络(FuNs)[49]. FuNs框架使用一个管理员模块和一个工人模块. 管理员模块在较低的时间分辨率下工作, 设置抽象目标并传递给工人模块去执行. FuNs框架创造了一个稳定的自然层次结构, 并且允许两个模块以互补的方式学习. 实验证明, FuNs有助于处理长期信用分配和记忆任务,在Atari视频游戏和迷宫游戏中都取得了不错的效果。</p>
<h2 id="深度记忆强化学习算法"><a href="#深度记忆强化学习算法" class="headerlink" title="深度记忆强化学习算法"></a>深度记忆强化学习算法</h2><p>传统的深度强化学习模型不具备记忆、认知、推理等高层次的能力, 尤其是在面对状态部分可观察和延迟奖赏的情形时. Junhyuk等通过在传统的深度强化学习模型中加入外部的记忆网络部件和反馈控制机制, 提出反馈递归记忆Q网络(feedback recurrent memory Q-network, FRMQN)). FRMQN模型具备了一定的记忆与推理功能, 通过反馈控制机制,FRMQN整合过去存储的有价值的记忆和当前时刻的上下文状态, 评估动作值函数并做出决策. FRMQN初步模拟了人类的主动认知与推理能力, 并完成了一些高层次的认知任务. 在一些未经过训练的任务中,FRMQN模型表现出了很强的泛化能力．Blundell等设计出一种模型无关的情节控制算法(model-free episode control, MFEC). MFEC可以快速存储和回放状态转移序列, 并将回放的序列整合到结构化知识系统中, 使得智能体在面对一些复杂的决策任务时, 能快速达到人类玩家的水平．MFEC通过反向经验回放, 使智能体拥有初步的情节记忆. 实验表明, 基于MFEC算法的深度强化学习不仅可以在Atari游戏中学习到有效策略, 还可以处理一些三维场景的复杂任务. Pritzel等在MFEC的基础上进一步提出了神经情节控制(neural episodic control, NEC),有效提高了深度强化学习智能体的记忆能力和学习效率[53]. NEC能快速吸收新经验并依据新经验来采取行动. 价值函数包括价值函数渐变状态表示和价值函数快速更新估计两部分. 大量场景下的研究表明,NEC的学习速度明显快于目前最先进的通用深度强化学习智能体.</p>
<h2 id="多智能体深度强化学习算法"><a href="#多智能体深度强化学习算法" class="headerlink" title="多智能体深度强化学习算法"></a>多智能体深度强化学习算法</h2><p>在一些复杂场景中, 涉及到多智能体的感知决策问题, 这时需要将单一模型扩展为多个智能体之间相互合作、通信及竞争的多智能体深度强化学习系统.Foerster等提出了一种称为分布式深度递归Q网络(deep distributed recurrent Q-networks, DDRQN) 的模型, 解决了状态部分可观测状态下的多智能体通信与合作的挑战性难题[54]. 实验表明, 经过训练的DDRQN模型最终在多智能体之间达成了一致的通信协1536 控制理论与应用第34 卷议, 成功解决了经典的红蓝帽子问题.让智能体学会合作与竞争一直以来都是人工智能领域内的一项重要研究课题, 也是实现通用人工智能的必要条件. Lowe等提出了一种用于合作–竞争混合环境的多智能体actor-critic 算法(multi-agent deepdeterministic policy gradient, MADDPG)[55]. MADDPG对DDPG强化学习算法进行了延伸, 可实现多智能体的集中式学习和分布式执行, 让智能体学习彼此合作和竞争. 在多项测试任务中, MADDPG的表现都优于DDPG. </p>
<h2 id="强化学习开源框架"><a href="#强化学习开源框架" class="headerlink" title="强化学习开源框架"></a>强化学习开源框架</h2><p>谷歌TensorFlow Agents —-TensorFlow的加强版,它提供许多工具，通过强化学习可以实现各类智能应用程序的构建与训练。这个框架能够将OpoenAI Gym接口扩展至多个并行环境，并允许各代理立足TensorFlow之内实现以执行批量计算。其面向OpoenAI Gy环境的批量化接口可与TensorFlow实现全面集成，从而高效执行各类算法。该框架还结合有BatchPPO，一套经过优化的近端策略优化算法实现方案。其核心组件包括一个环境打包器，用于在外部过程中构建OpenAI Gym环境; 一套批量集成，用于实现TensorFlow图步并以强化学习运算的方式重置函数; 外加用于将TensorFlow图形批处理流程与强化学习算法纳入训练特内单一却步的组件。</p>
<p>Roboschool：Roboschool 提供开源软件以通过强化学习构建并训练机器人模拟。其有助于在同一环境当中对多个代理进行强化学习训练。通过多方训练机制，您可以训练同一代理分别作为两方玩家（因此能够自我对抗）、使用相同算法训练两套代理，或者设置两种算法进行彼此对抗。Roboschool由OpenAI开发完成，这一非营利性组织的背后赞助者包括Elon Musk、Sam Altman、Reid Hoffman以及Peter Thiel。其与OpenAI Gym相集成，后者是一套用于开发及评估强化学习算法的开源工具集。OpenAI Gym与TensorFlow、Theano以及其它多种深度学习库相兼容。OpenAI Gym当中包含用于数值计算、游戏以及物理引擎的相关代码。Roboschool基于Bullet物理引擎，这是一套开源许可物理库，并被其它多种仿真软件——例如Gazebo与Virtual Robot Experimentation Platform（简称V-REP）所广泛使用。其中包含多种强化学习算法，具体以怨报德 异步深度强化学习方法、Actor-Critic with Experience Replay、Actor- Critic using Kronecker-Factored Trust Region、深度确定性策略梯度、近端策略优化以及信任域策略优化等等。</p>
<p>Coach：英特尔公司的开源强化学习框架，可以对游戏、机器人以及其它基于代理的智能应用进行智能代理的建模、训练与评估。Coach 提供一套模块化沙箱、可复用组件以及用于组合新强化学习算法并在多种应用领域内训练新智能应用的Python API。该框架利用OpenAI Gym作为主工具，负责与不同强化学习环境进行交换。其还支持其它外部扩展，具体包括Roboschool、gym-extensions、PyBullet以及ViZDoom。Coach的环境打包器允许用户向其中添加自定义强化学习环境，从而解决其它学习问题。该框架能够在桌面计算机上高效训练强化学习代理，并利用多核CPU处理相关任务。其能够为一部分强化学习算法提供单线程与多线程实现能力，包括异步优势Actor-Critic、深度确定性策略梯度、近端策略优化、直接未来预测以及规范化优势函数。所有算法皆利用面向英特尔系统作出优化的TensorFLow完成，其中部分算法亦适用于英特尔的Neon深度学习框架。Coach 当中包含多种强化学习代理实现方案，具体包括从单线程实现到多线程实现的转换。其能够开发出支持单与多工作程序（同步或异步）强化学习实现方法的新代理。此外，其还支持连续与离散操作空间，以及视觉观察空间或仅包含原始测量指标的观察空间。</p>
<h2 id="深度强化学习算法小结"><a href="#深度强化学习算法小结" class="headerlink" title="深度强化学习算法小结"></a>深度强化学习算法小结</h2><p>基于值函数概念的DQN及其相应的扩展算法在离散状态、离散动作的控制任务中已经表现了卓越的性能, 但是受限于值函数离散型输出的影响, 在连续型控制任务上显得捉襟见肘. 基于策略梯度概念的,以DDPG, TRPO等为代表的策略型深度强化学习算法则更适用于处理基于连续状态空间的连续动作的控制输出任务, 并且算法在稳定性和可靠性上具有一定的理论保证, 理论完备性较强. 采用actor-critic架构的A3C算法及其扩展算法, 相比于传统DQN算法, 这类算法的数据利用效率更高, 学习速率更快, 通用性、可扩展应用性更强, 达到的表现性能更优, 但算法的稳定性无法得到保证. 而其他的如深度迁移强化学习、分层深度强化学习、深度记忆强化学习和多智能体深度强化学习等算法都是现在的研究热点, 通过这些算法能应对更为复杂的场景问题、系统环境及控制任务, 是目前深度强化学习算法研究的前沿领域.</p>
<p>展望未来，人工智能开发者们需要尽可能掌握上述框架以及其中所使用的各类强化学习算法。此外，还需要强化自身对于多代理强化学习架构的理解，因为其中多种框架都大量利用前沿博弈论研究成果。最后，还需要熟悉深度强化学习知识。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>循环神经网络</title>
    <url>/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="循环神经网络-RNN"><a href="#循环神经网络-RNN" class="headerlink" title="循环神经网络(RNN)"></a>循环神经网络(RNN)</h1><h2 id="为什么需要RNN？"><a href="#为什么需要RNN？" class="headerlink" title="为什么需要RNN？"></a>为什么需要RNN？</h2><p>​    时间序列数据是指在不同时间点上收集到的数据，这类数据反映了某一事物、现象等随时间的变化状态或程度。一般的神经网络，在训练数据足够、算法模型优越的情况下，给定特定的x，就能得到期望y。其一般处理单个的输入，前一个输入和后一个输入完全无关，但实际应用中，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如：</p>
<p>​    当我们在理解一句话意思时，孤立的理解这句话的每个词不足以理解整体意思，我们通常需要处理这些词连接起来的整个序列； 当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就由此诞生了。</p>
<h2 id="图解RNN基本结构"><a href="#图解RNN基本结构" class="headerlink" title="图解RNN基本结构"></a>图解RNN基本结构</h2><h3 id="基本的单层网络结构"><a href="#基本的单层网络结构" class="headerlink" title="基本的单层网络结构"></a>基本的单层网络结构</h3><p>​    在进一步了解RNN之前，先给出最基本的单层网络结构，输入是x，经过变换Wx+b和激活函数f得到输出y：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.1.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="图解经典RNN结构"><a href="#图解经典RNN结构" class="headerlink" title="图解经典RNN结构"></a>图解经典RNN结构</h3><p>​    在实际应用中，我们还会遇到很多序列形的数据，如：</p>
<ul>
<li><p>自然语言处理问题。x1可以看做是第一个单词，x2可以看做是第二个单词，依次类推。</p>
</li>
<li><p>语音处理。此时，x1、x2、x3……是每帧的声音信号。</p>
</li>
<li><p>时间序列问题。例如每天的股票价格等等。</p>
<p>其单个序列如下图所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.2.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.2.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>前面介绍了诸如此类的序列数据用原始的神经网络难以建模，基于此，RNN引入了隐状态$h$（hidden state），$h$可对序列数据提取特征，接着再转换为输出。</p>
<p>为了便于理解，先计算$h_1$：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.3.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.3.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>注：图中的圆圈表示向量，箭头表示对向量做变换。</p>
<p>RNN中，每个步骤使用的参数$U,W,b$​相同，$h_2$的计算方式和$h_1$类似，其计算结果如下：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.4.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.4.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>计算$h_3$,$h_4$也相似，可得：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.5.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.5.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>接下来，计算RNN的输出$y_1$，采用$Softmax$作为激活函数，根据$y_n=f(Wx+b)$，得$y_1$:</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.6.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.6.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>使用和$y_1$相同的参数$V,c$，得到$y_1,y_2,y_3,y_4$的输出结构：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.7.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.7.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>以上即为最经典的RNN结构，其输入为$x_1,x_2,x_3,x_4$，输出为$y_1,y_2,y_3,y_4$，当然实际中最大值为$y_n$，这里为了便于理解和展示，只计算4个输入和输出。从以上结构可看出，RNN结构的输入和输出等长。</p>
</li>
</ul>
<h3 id="vector-to-sequence结构"><a href="#vector-to-sequence结构" class="headerlink" title="vector-to-sequence结构"></a>vector-to-sequence结构</h3><p>​    有时我们要处理的问题输入是一个单独的值，输出是一个序列。此时，有两种主要建模方式：</p>
<p>​    方式一：可只在其中的某一个序列进行计算，比如序列第一个进行输入计算，其建模方式如下：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.9.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.9.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>​    方式二：把输入信息X作为每个阶段的输入，其建模方式如下：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.10.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.10.jpg" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="sequence-to-vector结构"><a href="#sequence-to-vector结构" class="headerlink" title="sequence-to-vector结构"></a>sequence-to-vector结构</h3><p>​    有时我们要处理的问题输入是一个序列，输出是一个单独的值，此时通常在最后的一个序列上进行输出变换，其建模如下所示：</p>
<p>  <img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.8.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.8.jpg" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="Encoder-Decoder结构"><a href="#Encoder-Decoder结构" class="headerlink" title="Encoder-Decoder结构"></a>Encoder-Decoder结构</h3><p>​    原始的sequence-to-sequence结构的RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。</p>
<p>​    其建模步骤如下：</p>
<p>​    <strong>步骤一</strong>：将输入数据编码成一个上下文向量$c$，这部分称为Encoder，得到$c$有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给$c$，还可以对最后的隐状态做一个变换得到$c$，也可以对所有的隐状态做变换。其示意如下所示：</p>
<p>  <img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.12.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.12.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>​    <strong>步骤二</strong>：用另一个RNN网络（我们将其称为Decoder）对其进行编码，方法一是将步骤一中的$c$作为初始状态输入到Decoder，示意图如下所示：</p>
<p>  <img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.13.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.13.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>方法二是将$c$作为Decoder的每一步输入，示意图如下所示：</p>
<p>  <img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.14.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.14.jpg" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="以上三种结构各有怎样的应用场景"><a href="#以上三种结构各有怎样的应用场景" class="headerlink" title="以上三种结构各有怎样的应用场景"></a>以上三种结构各有怎样的应用场景</h3><div class="table-container">
<table>
<thead>
<tr>
<th>网络结构</th>
<th style="text-align:center">结构图示</th>
<th>应用场景举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 vs N</td>
<td style="text-align:center"><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.9.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.9.jpg" srcset="data:image/png;base64,666" alt=""></td>
<td>1、从图像生成文字，输入为图像的特征，输出为一段句子<br>2、根据图像生成语音或音乐，输入为图像特征，输出为一段语音或音乐</td>
</tr>
<tr>
<td>N vs 1</td>
<td style="text-align:center"><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.8.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.8.jpg" srcset="data:image/png;base64,666" alt=""></td>
<td>1、输出一段文字，判断其所属类别<br>2、输入一个句子，判断其情感倾向<br>3、输入一段视频，判断其所属类别</td>
</tr>
<tr>
<td>N vs M</td>
<td style="text-align:center"><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.13.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.13.jpg" srcset="data:image/png;base64,666" alt=""></td>
<td>1、机器翻译，输入一种语言文本序列，输出另外一种语言的文本序列<br>2、文本摘要，输入文本序列，输出这段文本序列摘要<br>3、阅读理解，输入文章，输出问题答案<br>4、语音识别，输入语音序列信息，输出文字序列</td>
</tr>
</tbody>
</table>
</div>
<h3 id="图解RNN中的Attention机制"><a href="#图解RNN中的Attention机制" class="headerlink" title="图解RNN中的Attention机制"></a>图解RNN中的Attention机制</h3><p>​    在上述通用的Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征$c$再解码，因此，$c$中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。如机器翻译问题，当要翻译的句子较长时，一个$c$可能存不下那么多信息，就会造成翻译精度的下降。Attention机制通过在每个时间输入不同的$c$来解决此问题。</p>
<p>​    引入了Attention机制的Decoder中，有不同的$c$，每个$c$会自动选择与当前输出最匹配的上下文信息，其示意图如下所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.15.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.15.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>​    <strong>举例</strong>，比如输入序列是“我爱中国”，要将此输入翻译成英文：</p>
<p>​    假如用$a<em>{ij}$衡量Encoder中第$j$阶段的$h_j$和解码时第$i$阶段的相关性，$a</em>{ij}$从模型中学习得到，和Decoder的第$i-1$阶段的隐状态、Encoder 第$j$个阶段的隐状态有关，比如$a_{3j}$的计算示意如下所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.19.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.19.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>最终Decoder中第$i$阶段的输入的上下文信息 $c<em>i$来自于所有$h_j$对$a</em>{ij}$的加权和。</p>
<p>其示意图如下图所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.16.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/6.16.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>​    在Encoder中，$h<em>1,h_2,h_3,h_4$分别代表“我”，“爱”，“中”，“国”所代表信息。翻译的过程中，$c_1$会选择和“我”最相关的上下午信息，如上图所示，会优先选择$a</em>{11}$，以此类推，$c<em>2$会优先选择相关性较大的$a</em>{22}$，$c<em>3$会优先选择相关性较大的$a</em>{33}，a_{34}$，这就是attention机制。</p>
<h2 id="RNNs典型特点？"><a href="#RNNs典型特点？" class="headerlink" title="RNNs典型特点？"></a>RNNs典型特点？</h2><ol>
<li>RNNs主要用于处理序列数据。对于传统神经网络模型，从输入层到隐含层再到输出层，层与层之间一般为全连接，每层之间神经元是无连接的。但是传统神经网络无法处理数据间的前后关联问题。例如，为了预测句子的下一个单词，一般需要该词之前的语义信息。这是因为一个句子中前后单词是存在语义联系的。</li>
<li>RNNs中当前单元的输出与之前步骤输出也有关，因此称之为循环神经网络。具体的表现形式为当前单元会对之前步骤信息进行储存并应用于当前输出的计算中。隐藏层之间的节点连接起来，隐藏层当前输出由当前时刻输入向量和之前时刻隐藏层状态共同决定。</li>
<li>标准的RNNs结构图，图中每个箭头代表做一次变换，也就是说箭头连接带有权值。</li>
<li>在标准的RNN结构中，隐层的神经元之间也是带有权值的，且权值共享。</li>
<li>理论上，RNNs能够对任何长度序列数据进行处理。但是在实践中，为了降低复杂度往往假设当前的状态只与之前某几个时刻状态相关，<strong>下图便是一个典型的RNNs</strong>：</li>
</ol>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.2_1.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.2_1.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.2_2.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.2_2.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>输入单元(Input units)：输入集$\bigr{x<em>0,x_1,…,x_t,x</em>{t+1},…\bigr}$，</p>
<p>输出单元(Output units)：输出集$\bigr{y<em>0,y_1,…,y_t,y</em>{y+1},…\bigr}$，</p>
<p>隐藏单元(Hidden units)：输出集$\bigr{s<em>0,s_1,…,s_t,s</em>{t+1},…\bigr}$。</p>
<p><strong>图中信息传递特点：</strong></p>
<ol>
<li>一条单向流动的信息流是从输入单元到隐藏单元。</li>
<li>一条单向流动的信息流从隐藏单元到输出单元。</li>
<li>在某些情况下，RNNs会打破后者的限制，引导信息从输出单元返回隐藏单元，这些被称为“Back Projections”。</li>
<li>在某些情况下，隐藏层的输入还包括上一时刻隐藏层的状态，即隐藏层内的节点可以自连也可以互连。 </li>
<li>当前单元（cell）输出是由当前时刻输入和上一时刻隐藏层状态共同决定。</li>
</ol>
<h2 id="CNN和RNN的区别-？"><a href="#CNN和RNN的区别-？" class="headerlink" title="CNN和RNN的区别 ？"></a>CNN和RNN的区别 ？</h2><div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>特点描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>相同点</td>
<td>1、传统神经网络的扩展。<br>2、前向计算产生结果，反向计算模型更新。<br>3、每层神经网络横向可以多个神经元共存,纵向可以有多层神经网络连接。</td>
</tr>
<tr>
<td>不同点</td>
<td>1、CNN空间扩展，神经元与特征卷积；RNN时间扩展，神经元与多个时间输出计算<br>2、RNN可以用于描述时间上连续状态的输出，有记忆功能，CNN用于静态输出</td>
</tr>
</tbody>
</table>
</div>
<h2 id="RNNs和FNNs有什么区别？"><a href="#RNNs和FNNs有什么区别？" class="headerlink" title="RNNs和FNNs有什么区别？"></a>RNNs和FNNs有什么区别？</h2><ol>
<li>不同于传统的前馈神经网络(FNNs)，RNNs引入了定向循环，能够处理输入之间前后关联问题。</li>
<li>RNNs可以记忆之前步骤的训练信息。<br><strong>定向循环结构如下图所示</strong>：</li>
</ol>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.1_1.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.1_1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<h2 id="RNNs训练和传统ANN训练异同点？"><a href="#RNNs训练和传统ANN训练异同点？" class="headerlink" title="RNNs训练和传统ANN训练异同点？"></a>RNNs训练和传统ANN训练异同点？</h2><p><strong>相同点</strong>：</p>
<ol>
<li>RNNs与传统ANN都使用BP（Back Propagation）误差反向传播算法。</li>
</ol>
<p><strong>不同点</strong>：</p>
<ol>
<li>RNNs网络参数W,U,V是共享的(具体在本章6.2节中已介绍)，而传统神经网络各层参数间没有直接联系。</li>
<li>对于RNNs，在使用梯度下降算法中，每一步的输出不仅依赖当前步的网络，还依赖于之前若干步的网络状态。</li>
</ol>
<h2 id="为什么RNN-训练的时候Loss波动很大"><a href="#为什么RNN-训练的时候Loss波动很大" class="headerlink" title="为什么RNN 训练的时候Loss波动很大"></a>为什么RNN 训练的时候Loss波动很大</h2><p>​    由于RNN特有的memory会影响后期其他的RNN的特点，梯度时大时小，learning rate没法个性化的调整，导致RNN在train的过程中，Loss会震荡起伏，为了解决RNN的这个问题，在训练的时候，可以设置临界值，当梯度大于某个临界值，直接截断，用这个临界值作为梯度的大小，防止大幅震荡。</p>
<h2 id="标准RNN前向输出流程"><a href="#标准RNN前向输出流程" class="headerlink" title="标准RNN前向输出流程"></a>标准RNN前向输出流程</h2><p>​    以$x$表示输入，$h$是隐层单元，$o$是输出，$L$为损失函数，$y$为训练集标签。$t$表示$t$时刻的状态，$V,U,W$是权值，同一类型的连接权值相同。以下图为例进行说明标准RNN的前向传播算法：</p>
<p>​    <img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/rnnbp.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/rnnbp.png" srcset="data:image/png;base64,666" alt=""></p>
<p>对于$t$时刻：</p>
<script type="math/tex; mode=display">
h^{(t)}=\phi(Ux^{(t)}+Wh^{(t-1)}+b)</script><p>其中$\phi()$为激活函数，一般会选择tanh函数，$b$为偏置。</p>
<p>$t$时刻的输出为：</p>
<script type="math/tex; mode=display">
o^{(t)}=Vh^{(t)}+c</script><p>模型的预测输出为：</p>
<script type="math/tex; mode=display">
\widehat{y}^{(t)}=\sigma(o^{(t)})</script><p>其中$\sigma$为激活函数，通常RNN用于分类，故这里一般用softmax函数。</p>
<h2 id="BPTT算法推导"><a href="#BPTT算法推导" class="headerlink" title="BPTT算法推导"></a>BPTT算法推导</h2><p>​    BPTT（back-propagation through time）算法是常用的训练RNN的方法，其本质还是BP算法，只不过RNN处理时间序列数据，所以要基于时间反向传播，故叫随时间反向传播。BPTT的中心思想和BP算法相同，沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛。需要寻优的参数有三个，分别是U、V、W。与BP算法不同的是，其中W和U两个参数的寻优过程需要追溯之前的历史数据，参数V相对简单只需关注目前，那么我们就来先求解参数V的偏导数。</p>
<script type="math/tex; mode=display">
\frac{\partial L^{(t)}}{\partial V}=\frac{\partial L^{(t)}}{\partial o^{(t)}}\cdot \frac{\partial o^{(t)}}{\partial V}</script><p>RNN的损失也是会随着时间累加的，所以不能只求t时刻的偏导。</p>
<script type="math/tex; mode=display">
L=\sum_{t=1}^{n}L^{(t)}</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial V}=\sum_{t=1}^{n}\frac{\partial L^{(t)}}{\partial o^{(t)}}\cdot \frac{\partial o^{(t)}}{\partial V}</script><p>​    W和U的偏导的求解由于需要涉及到历史数据，其偏导求起来相对复杂。为了简化推导过程，我们假设只有三个时刻，那么在第三个时刻 L对W，L对U的偏导数分别为：</p>
<script type="math/tex; mode=display">
\frac{\partial L^{(3)}}{\partial W}=\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial W}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial W}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial h^{(1)}}\frac{\partial h^{(1)}}{\partial W}</script><script type="math/tex; mode=display">
\frac{\partial L^{(3)}}{\partial U}=\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial U}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial U}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial h^{(1)}}\frac{\partial h^{(1)}}{\partial U}</script><p>可以观察到，在某个时刻的对W或是U的偏导数，需要追溯这个时刻之前所有时刻的信息。根据上面两个式子得出L在t时刻对W和U偏导数的通式： </p>
<script type="math/tex; mode=display">
\frac{\partial L^{(t)}}{\partial W}=\sum_{k=0}^{t}\frac{\partial L^{(t)}}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial h^{(t)}}(\prod_{j=k+1}^{t}\frac{\partial h^{(j)}}{\partial h^{(j-1)}})\frac{\partial h^{(k)}}{\partial W}</script><script type="math/tex; mode=display">
\frac{\partial L^{(t)}}{\partial U}=\sum_{k=0}^{t}\frac{\partial L^{(t)}}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial h^{(t)}}(\prod_{j=k+1}^{t}\frac{\partial h^{(j)}}{\partial h^{(j-1)}})\frac{\partial h^{(k)}}{\partial U}</script><p>整体的偏导公式就是将其按时刻再一一加起来。</p>
<h2 id="RNN中为什么会出现梯度消失？"><a href="#RNN中为什么会出现梯度消失？" class="headerlink" title="RNN中为什么会出现梯度消失？"></a>RNN中为什么会出现梯度消失？</h2><p>首先来看tanh函数的函数及导数图如下所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/tanh.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/tanh.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>sigmoid函数的函数及导数图如下所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/sigmoid.jpg" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/sigmoid.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>从上图观察可知，sigmoid函数的导数范围是(0,0.25]，tach函数的导数范围是(0,1]，他们的导数最大都不大于1。</p>
<p>​    基于6.8中式（9-10）中的推导，RNN的激活函数是嵌套在里面的，如果选择激活函数为$tanh$或$sigmoid$，把激活函数放进去，拿出中间累乘的那部分可得：</p>
<script type="math/tex; mode=display">
\prod_{j=k+1}^{t}{\frac{\partial{h^{j}}}{\partial{h^{j-1}}}} = \prod_{j=k+1}^{t}{tanh^{'}}\cdot W_{s}</script><script type="math/tex; mode=display">
\prod_{j=k+1}^{t}{\frac{\partial{h^{j}}}{\partial{h^{j-1}}}} = \prod_{j=k+1}^{t}{sigmoid^{'}}\cdot W_{s}</script><p>​    <strong>梯度消失现象</strong>：基于上式，会发现累乘会导致激活函数导数的累乘，如果取tanh或sigmoid函数作为激活函数的话，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于0，这就是“梯度消失“现象。</p>
<p>​    实际使用中，会优先选择tanh函数，原因是tanh函数相对于sigmoid函数来说梯度较大，收敛速度更快且引起梯度消失更慢。</p>
<h2 id="如何解决RNN中的梯度消失问题？"><a href="#如何解决RNN中的梯度消失问题？" class="headerlink" title="如何解决RNN中的梯度消失问题？"></a>如何解决RNN中的梯度消失问题？</h2><p>​    上节描述的梯度消失是在无限的利用历史数据而造成，但是RNN的特点本来就是能利用历史数据获取更多的可利用信息，解决RNN中的梯度消失方法主要有：</p>
<p>​    1、选取更好的激活函数，如Relu激活函数。ReLU函数的左侧导数为0，右侧导数恒为1，这就避免了“梯度消失“的发生。但恒为1的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。</p>
<p>​    2、加入BN层，其优点包括可加速收敛、控制过拟合，可以少用或不用Dropout和正则、降低网络对初始化权重不敏感，且能允许使用较大的学习率等。</p>
<p>​    2、改变传播结构，LSTM结构可以有效解决这个问题。下面将介绍LSTM相关内容。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h3 id="LSTM的产生原因"><a href="#LSTM的产生原因" class="headerlink" title="LSTM的产生原因"></a>LSTM的产生原因</h3><p>​    RNN在处理长期依赖（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会涉及雅可比矩阵的多次相乘，会造成梯度消失或者梯度膨胀的现象。为了解决该问题，研究人员提出了许多解决办法，例如ESN（Echo State Network），增加有漏单元（Leaky Units）等等。其中最成功应用最广泛的就是门限RNN（Gated RNN），而LSTM就是门限RNN中最著名的一种。有漏单元通过设计连接间的权重系数，从而允许RNN累积距离较远节点间的长期联系；而门限RNN则泛化了这样的思想，允许在不同时刻改变该系数，且允许网络忘记当前已经累积的信息。</p>
<h3 id="图解标准RNN和LSTM的区别"><a href="#图解标准RNN和LSTM的区别" class="headerlink" title="图解标准RNN和LSTM的区别"></a>图解标准RNN和LSTM的区别</h3><p>​    所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层，如下图所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM1.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM2.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>注：上图图标具体含义如下所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM3.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM3.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    上图中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。</p>
<h3 id="LSTM核心思想图解"><a href="#LSTM核心思想图解" class="headerlink" title="LSTM核心思想图解"></a>LSTM核心思想图解</h3><p>​    LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。示意图如下所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM4.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM4.png" srcset="data:image/png;base64,666" alt=""></p>
<p>LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。示意图如下：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM5.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM5.png" srcset="data:image/png;base64,666" alt=""></p>
<p>LSTM 拥有三个门，分别是忘记层门，输入层门和输出层门，来保护和控制细胞状态。</p>
<p><strong>忘记层门</strong></p>
<p>​    作用对象：细胞状态 。</p>
<p>​    作用：将细胞状态中的信息选择性的遗忘。</p>
<p>​    操作步骤：该门会读取$h<em>{t-1}$和$x_t$，输出一个在 0 到 1 之间的数值给每个在细胞状态$C</em>{t-1}$中的数字。1 表示“完全保留”，0 表示“完全舍弃”。示意图如下：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM6.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM6.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>输入层门</strong></p>
<p>​    作用对象：细胞状态 </p>
<p>​    作用：将新的信息选择性的记录到细胞状态中。</p>
<p>​    操作步骤：</p>
<p>​    步骤一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。</p>
<p>​    步骤二，tanh 层创建一个新的候选值向量$\tilde{C}_t$加入到状态中。其示意图如下：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM7.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM7.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    步骤三：将$c<em>{t-1}$更新为$c</em>{t}$。将旧状态与$f_t$相乘，丢弃掉我们确定需要丢弃的信息。接着加上$i_t * \tilde{C}_t$得到新的候选值，根据我们决定更新每个状态的程度进行变化。其示意图如下：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM8.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM8.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>输出层门</strong><br>    作用对象：隐层$h_t$ </p>
<p>​    作用：确定输出什么值。</p>
<p>​    操作步骤：</p>
<p>​    步骤一：通过sigmoid 层来确定细胞状态的哪个部分将输出。</p>
<p>​    步骤二：把细胞状态通过 tanh 进行处理，并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p>
<p>其示意图如下所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM9.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM9.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="LSTM流行的变体"><a href="#LSTM流行的变体" class="headerlink" title="LSTM流行的变体"></a>LSTM流行的变体</h3><p><strong>增加peephole 连接</strong></p>
<p>​    在正常的LSTM结构中，Gers F A 等人提出增加peephole 连接，可以门层接受细胞状态的输入。示意图如下所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM10.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM10.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>对忘记门和输入门进行同时确定</strong></p>
<p>​    不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。示意图如下所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM11.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM11.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>Gated Recurrent Unit</strong></p>
<p>​     由Kyunghyun Cho等人提出的Gated Recurrent Unit (GRU)，其将忘记门和输入门合成了一个单一的更新门，同样还混合了细胞状态和隐藏状态，和其他一些改动。其示意图如下：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM12.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/LSTM12.png" srcset="data:image/png;base64,666" alt=""></p>
<p>最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。</p>
<h2 id="LSTMs与GRUs的区别"><a href="#LSTMs与GRUs的区别" class="headerlink" title="LSTMs与GRUs的区别"></a>LSTMs与GRUs的区别</h2><p>LSTMs与GRUs的区别如图所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.6_2.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.6_2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>从上图可以看出，二者结构十分相似，<strong>不同在于</strong>：</p>
<ol>
<li>new memory都是根据之前state及input进行计算，但是GRUs中有一个reset gate控制之前state的进入量，而在LSTMs里没有类似gate；</li>
<li>产生新的state的方式不同，LSTMs有两个不同的gate，分别是forget gate (f gate)和input gate(i gate)，而GRUs只有一种update gate(z gate)；</li>
<li>LSTMs对新产生的state可以通过output gate(o gate)进行调节，而GRUs对输出无任何调节。</li>
</ol>
<h2 id="RNNs在NLP中典型应用？"><a href="#RNNs在NLP中典型应用？" class="headerlink" title="RNNs在NLP中典型应用？"></a>RNNs在NLP中典型应用？</h2><p><strong>（1）语言模型与文本生成(Language Modeling and Generating Text)</strong></p>
<p>​    给定一组单词序列，需要根据前面单词预测每个单词出现的可能性。语言模型能够评估某个语句正确的可能性，可能性越大，语句越正确。另一种应用便是使用生成模型预测下一个单词的出现概率，从而利用输出概率的采样生成新的文本。</p>
<p><strong>（2）机器翻译(Machine Translation)</strong></p>
<p>​    机器翻译是将一种源语言语句变成意思相同的另一种源语言语句，如将英语语句变成同样意思的中文语句。与语言模型关键的区别在于，需要将源语言语句序列输入后，才进行输出，即输出第一个单词时，便需要从完整的输入序列中进行获取。</p>
<p><strong>（3）语音识别(Speech Recognition)</strong></p>
<p>​    语音识别是指给定一段声波的声音信号，预测该声波对应的某种指定源语言语句以及计算该语句的概率值。 </p>
<p><strong>（4）图像描述生成 (Generating Image Descriptions)</strong></p>
<p>​    同卷积神经网络一样，RNNs已经在对无标图像描述自动生成中得到应用。CNNs与RNNs结合也被应用于图像描述自动生成。<br><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.4_1.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.4_1.png" srcset="data:image/png;base64,666" alt=""></p>
<h2 id="常见的RNNs扩展和改进模型"><a href="#常见的RNNs扩展和改进模型" class="headerlink" title="常见的RNNs扩展和改进模型"></a>常见的RNNs扩展和改进模型</h2><h3 id="Simple-RNNs-SRNs"><a href="#Simple-RNNs-SRNs" class="headerlink" title="Simple RNNs(SRNs)"></a>Simple RNNs(SRNs)</h3><ol>
<li>SRNs是一个三层网络，其在隐藏层增加了上下文单元。下图中的y是隐藏层，u是上下文单元。上下文单元节点与隐藏层中节点的连接是固定的，并且权值也是固定的。上下文节点与隐藏层节点一一对应，并且值是确定的。</li>
<li>在每一步中，使用标准的前向反馈进行传播，然后使用学习算法进行学习。上下文每一个节点保存其连接隐藏层节点上一步输出，即保存上文，并作用于当前步对应的隐藏层节点状态，即隐藏层的输入由输入层的输出与上一步的自身状态所决定。因此SRNs能够解决标准多层感知机(MLP)无法解决的对序列数据进行预测的问题。<br>SRNs网络结构如下图所示：</li>
</ol>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.1_1.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.1_1.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="Bidirectional-RNNs"><a href="#Bidirectional-RNNs" class="headerlink" title="Bidirectional RNNs"></a>Bidirectional RNNs</h3><p>​    Bidirectional RNNs(双向网络)将两层RNNs叠加在一起，当前时刻输出(第t步的输出)不仅仅与之前序列有关，还与之后序列有关。例如：为了预测一个语句中的缺失词语，就需要该词汇的上下文信息。Bidirectional RNNs是一个相对较简单的RNNs，是由两个RNNs上下叠加在一起组成的。输出由前向RNNs和后向RNNs共同决定。如下图所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.2_1.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.2_1.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="Deep-RNNs"><a href="#Deep-RNNs" class="headerlink" title="Deep RNNs"></a>Deep RNNs</h3><p>​    Deep RNNs与Bidirectional RNNs相似，其也是又多层RNNs叠加，因此每一步的输入有了多层网络。该网络具有更强大的表达与学习能力，但是复杂性也随之提高，同时需要更多的训练数据。Deep RNNs的结构如下图所示：<br><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.3_1.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.3_1.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="Echo-State-Networks（ESNs）"><a href="#Echo-State-Networks（ESNs）" class="headerlink" title="Echo State Networks（ESNs）"></a>Echo State Networks（ESNs）</h3><p><strong>ESNs特点</strong>：</p>
<ol>
<li>它的核心结构为一个随机生成、且保持不变的储备池(Reservoir)。储备池是大规模随机生成稀疏连接(SD通常保持1%～5%，SD表示储备池中互相连接的神经元占总神经元个数N的比例)的循环结构；</li>
<li>从储备池到输出层的权值矩阵是唯一需要调整的部分；</li>
<li>简单的线性回归便能够完成网络训练；</li>
</ol>
<p><strong>ESNs基本思想</strong>：</p>
<p>​    使用大规模随机连接的循环网络取代经典神经网络中的中间层，从而简化网络的训练过程。<br>网络中的参数包括：<br>（1）W - 储备池中节点间连接权值矩阵；<br>（2）Win - 输入层到储备池之间连接权值矩阵，表明储备池中的神经元之间是相互连接；<br>（3）Wback - 输出层到储备池之间的反馈连接权值矩阵，表明储备池会有输出层来的反馈；<br>（4）Wout - 输入层、储备池、输出层到输出层的连接权值矩阵，表明输出层不仅与储备池连接，还与输入层和自己连接。<br>（5）Woutbias - 输出层的偏置项。 </p>
<p>​    ESNs的结构如下图所示：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.4_2.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.4_2.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="Gated-Recurrent-Unit-Recurrent-Neural-Networks"><a href="#Gated-Recurrent-Unit-Recurrent-Neural-Networks" class="headerlink" title="Gated Recurrent Unit Recurrent Neural Networks"></a>Gated Recurrent Unit Recurrent Neural Networks</h3><p>GRUs是一般的RNNs的变型版本，其主要是从以下两个方面进行改进。</p>
<ol>
<li><p>以语句为例，序列中不同单词处的数据对当前隐藏层状态的影响不同，越前面的影响越小，即每个之前状态对当前的影响进行了距离加权，距离越远，权值越小。</p>
</li>
<li><p>在产生误差error时，其可能是由之前某一个或者几个单词共同造成，所以应当对对应的单词weight进行更新。GRUs的结构如下图所示。GRUs首先根据当前输入单词向量word vector以及前一个隐藏层状态hidden state计算出update gate和reset gate。再根据reset gate、当前word vector以及前一个hidden state计算新的记忆单元内容(new memory content)。当reset gate为1的时候，new memory content忽略之前所有memory content，最终的memory是由之前的hidden state与new memory content一起决定。</p>
</li>
</ol>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.5_1.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.5_1.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="Bidirectional-LSTMs"><a href="#Bidirectional-LSTMs" class="headerlink" title="Bidirectional LSTMs"></a>Bidirectional LSTMs</h3><ol>
<li>与bidirectional RNNs 类似，bidirectional LSTMs有两层LSTMs。一层处理过去的训练信息，另一层处理将来的训练信息。</li>
<li>在bidirectional LSTMs中，通过前向LSTMs获得前向隐藏状态，后向LSTMs获得后向隐藏状态，当前隐藏状态是前向隐藏状态与后向隐藏状态的组合。</li>
</ol>
<h3 id="Stacked-LSTMs"><a href="#Stacked-LSTMs" class="headerlink" title="Stacked LSTMs"></a>Stacked LSTMs</h3><ol>
<li>与deep rnns 类似，stacked LSTMs 通过将多层LSTMs叠加起来得到一个更加复杂的模型。</li>
<li>不同于bidirectional LSTMs，stacked LSTMs只利用之前步骤的训练信息。 </li>
</ol>
<h3 id="Clockwork-RNNs-CW-RNNs"><a href="#Clockwork-RNNs-CW-RNNs" class="headerlink" title="Clockwork RNNs(CW-RNNs)"></a>Clockwork RNNs(CW-RNNs)</h3><p>​    CW-RNNs是RNNs的改良版本，其使用时钟频率来驱动。它将隐藏层分为几个块(组，Group/Module)，每一组按照自己规定的时钟频率对输入进行处理。为了降低RNNs的复杂度，CW-RNNs减少了参数数量，并且提高了网络性能，加速网络训练。CW-RNNs通过不同隐藏层模块在不同时钟频率下工作来解决长时依赖问题。将时钟时间进行离散化，不同的隐藏层组将在不同时刻进行工作。因此，所有的隐藏层组在每一步不会全部同时工作，这样便会加快网络的训练。并且，时钟周期小组的神经元不会连接到时钟周期大组的神经元，只允许周期大的神经元连接到周期小的(组与组之间的连接以及信息传递是有向的)。周期大的速度慢，周期小的速度快，因此是速度慢的神经元连速度快的神经元，反之则不成立。</p>
<p>​    CW-RNNs与SRNs网络结构类似，也包括输入层(Input)、隐藏层(Hidden)、输出层(Output)，它们之间存在前向连接，输入层到隐藏层连接，隐藏层到输出层连接。但是与SRN不同的是，隐藏层中的神经元会被划分为若干个组，设为$g$，每一组中的神经元个数相同，设为$k$，并为每一个组分配一个时钟周期$T_i\epsilon{T_1,T_2,…,T_g}$，每一组中的所有神经元都是全连接，但是组$j$到组$i$的循环连接则需要满足$T_j$大于$T_i$。如下图所示，将这些组按照时钟周期递增从左到右进行排序，即$T_1&lt;T_2&lt;…&lt;T_g$，那么连接便是从右到左。例如：隐藏层共有256个节点，分为四组，周期分别是[1,2,4,8]，那么每个隐藏层组256/4=64个节点，第一组隐藏层与隐藏层的连接矩阵为64$\times$64的矩阵，第二层的矩阵则为64$\times$128矩阵，第三组为64$\times$(3$\times$64)=64$\times$192矩阵，第四组为64$\times$(4$\times$64)=64$\times$256矩阵。这就解释了上一段中速度慢的组连接到速度快的组，反之则不成立。</p>
<p><strong>CW-RNNs的网络结构如下图所示</strong>：</p>
<p><img src="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.7_1.png" class="lazyload" data-srcset="/zh-TW/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ch6/figure_6.6.7_1.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="CNN-LSTMs"><a href="#CNN-LSTMs" class="headerlink" title="CNN-LSTMs"></a>CNN-LSTMs</h3><ol>
<li>为了同时利用CNN以及LSTMs的优点，CNN-LSTMs被提出。在该模型中，CNN用于提取对象特征，LSTMs用于预测。CNN由于卷积特性，其能够快速而且准确地捕捉对象特征。LSTMs的优点在于能够捕捉数据间的长时依赖性。</li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>生成对抗网络</title>
    <url>/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="生成对抗网络"><a href="#生成对抗网络" class="headerlink" title="生成对抗网络"></a>生成对抗网络</h1><h2 id="GAN基本概念"><a href="#GAN基本概念" class="headerlink" title="GAN基本概念"></a>GAN基本概念</h2><h3 id="如何通俗理解GAN？"><a href="#如何通俗理解GAN？" class="headerlink" title="如何通俗理解GAN？"></a>如何通俗理解GAN？</h3><p>​    生成对抗网络(GAN, Generative adversarial network)自从2014年被Ian Goodfellow提出以来，掀起来了一股研究热潮。GAN由生成器和判别器组成，生成器负责生成样本，判别器负责判断生成器生成的样本是否为真。生成器要尽可能迷惑判别器，而判别器要尽可能区分生成器生成的样本和真实样本。</p>
<p>​    在GAN的原作[1]中，作者将生成器比喻为印假钞票的犯罪分子，判别器则类比为警察。犯罪分子努力让钞票看起来逼真，警察则不断提升对于假钞的辨识能力。二者互相博弈，随着时间的进行，都会越来越强。那么类比于图像生成任务，生成器不断生成尽可能逼真的假图像。判别器则判断图像是否是真实的图像，还是生成的图像，二者不断博弈优化。最终生成器生成的图像使得判别器完全无法判别真假。</p>
<h3 id="GAN的形式化表达"><a href="#GAN的形式化表达" class="headerlink" title="GAN的形式化表达"></a>GAN的形式化表达</h3><p>​    上述例子只是简要介绍了一下GAN的思想，下面对于GAN做一个形式化的，更加具体的定义。通常情况下，无论是生成器还是判别器，我们都可以用神经网络来实现。那么，我们可以把通俗化的定义用下面这个模型来表示：<br><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/7.1.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/7.1.png" srcset="data:image/png;base64,666" alt="GAN网络结构"></p>
<p>​    上述模型左边是生成器G，其输入是$z$，对于原始的GAN，$z$是由高斯分布随机采样得到的噪声。噪声$z$通过生成器得到了生成的假样本。</p>
<p>​    生成的假样本与真实样本放到一起，被随机抽取送入到判别器D，由判别器去区分输入的样本是生成的假样本还是真实的样本。整个过程简单明了，生成对抗网络中的“生成对抗”主要体现在生成器和判别器之间的对抗。</p>
<h3 id="GAN的目标函数是什么？"><a href="#GAN的目标函数是什么？" class="headerlink" title="GAN的目标函数是什么？"></a>GAN的目标函数是什么？</h3><p>​    对于上述神经网络模型，如果想要学习其参数，首先需要一个目标函数。GAN的目标函数定义如下：</p>
<script type="math/tex; mode=display">
\mathop {\min }\limits_G \mathop {\max }\limits_D V(D,G) = {\rm E}_{x\sim{p_{data}(x)}}[\log D(x)] + {\rm E}_{z\sim{p_z}(z)}[\log (1 - D(G(z)))]</script><p>​    这个目标函数可以分为两个部分来理解：</p>
<p>​    第一部分：判别器的优化通过$\mathop {\max}\limits<em>D V(D,G)$实现，$V(D,G)$为判别器的目标函数，其第一项${\rm E}</em>{x\sim{p<em>{data}(x)}}[\log D(x)]$表示对于从真实数据分布 中采用的样本 ,其被判别器判定为真实样本概率的数学期望。对于真实数据分布 中采样的样本，其预测为正样本的概率当然是越接近1越好。因此希望最大化这一项。第二项${\rm E}</em>{z\sim{p_z}(z)}[\log (1 - D(G(z)))]$表示：对于从噪声$P_z(z)$分布当中采样得到的样本，经过生成器生成之后得到的生成图片，然后送入判别器，其预测概率的负对数的期望，这个值自然是越大越好，这个值越大， 越接近0，也就代表判别器越好。</p>
<p>​    第二部分：生成器的优化通过$\mathop {\min }\limits_G({\mathop {\max }\limits_D V(D,G)})$来实现。注意，生成器的目标不是$\mathop {\min }\limits_GV(D,G)$，即生成器不是最小化判别器的目标函数，二是最小化判别器目标函数的最大值，判别器目标函数的最大值代表的是真实数据分布与生成数据分布的JS散度(详情可以参阅附录的推导)，JS散度可以度量分布的相似性，两个分布越接近，JS散度越小。</p>
<h3 id="GAN的目标函数和交叉熵有什么区别？"><a href="#GAN的目标函数和交叉熵有什么区别？" class="headerlink" title="GAN的目标函数和交叉熵有什么区别？"></a>GAN的目标函数和交叉熵有什么区别？</h3><p>​    判别器目标函数写成离散形式即为:</p>
<script type="math/tex; mode=display">
V(D,G)=-\frac{1}{m}\sum_{i=1}^{i=m}logD(x^i)-\frac{1}{m}\sum_{i=1}^{i=m}log(1-D(\tilde{x}^i))</script><p>​    可以看出，这个目标函数和交叉熵是一致的，即<strong>判别器的目标是最小化交叉熵损失，生成器的目标是最小化生成数据分布和真实数据分布的JS散度</strong>。</p>
<hr>
<p>[1]: Goodfellow, Ian, et al. “Generative adversarial nets.” Advances in neural information processing systems. 2014.</p>
<h3 id="GAN的Loss为什么降不下去？"><a href="#GAN的Loss为什么降不下去？" class="headerlink" title="GAN的Loss为什么降不下去？"></a>GAN的Loss为什么降不下去？</h3><p>​    对于很多GAN的初学者在实践过程中可能会纳闷，为什么GAN的Loss一直降不下去。GAN到底什么时候才算收敛？其实，作为一个训练良好的GAN，其Loss就是降不下去的。衡量GAN是否训练好了，只能由人肉眼去看生成的图片质量是否好。不过，对于没有一个很好的评价是否收敛指标的问题，也有许多学者做了一些研究，后文提及的WGAN就提出了一种新的Loss设计方式，较好的解决了难以判断收敛性的问题。下面我们分析一下GAN的Loss为什么降不下去？<br>​    对于判别器而言，GAN的Loss如下：</p>
<script type="math/tex; mode=display">
\mathop {\min }\limits_G \mathop {\max }\limits_D V(D,G) = {\rm E}_{x\sim{p_{data}(x)}}[\log D(x)] + {\rm E}_{z\sim{p_z}(z)}[\log (1 - D(G(z)))]</script><p>​    从$\mathop {\min }\limits_G \mathop {\max }\limits_D V(D,G)$可以看出，生成器和判别器的目的相反，也就是说两个生成器网络和判别器网络互为对抗，此消彼长。不可能Loss一直降到一个收敛的状态。</p>
<ul>
<li>对于生成器，其Loss下降快，很有可能是判别器太弱，导致生成器很轻易的就”愚弄”了判别器。</li>
<li>对于判别器，其Loss下降快，意味着判别器很强，判别器很强则说明生成器生成的图像不够逼真，才使得判别器轻易判别，导致Loss下降很快。</li>
</ul>
<p>​    也就是说，无论是判别器，还是生成器。loss的高低不能代表生成器的好坏。一个好的GAN网络，其GAN Loss往往是不断波动的。</p>
<p>​    看到这里可能有点让人绝望，似乎判断模型是否收敛就只能看生成的图像质量了。实际上，后文探讨的WGAN，提出了一种新的loss度量方式，让我们可以通过一定的手段来判断模型是否收敛。</p>
<h3 id="生成式模型、判别式模型的区别？"><a href="#生成式模型、判别式模型的区别？" class="headerlink" title="生成式模型、判别式模型的区别？"></a>生成式模型、判别式模型的区别？</h3><p>​    对于机器学习模型，我们可以根据模型对数据的建模方式将模型分为两大类，生成式模型和判别式模型。如果我们要训练一个关于猫狗分类的模型，对于判别式模型，只需要学习二者差异即可。比如说猫的体型会比狗小一点。而生成式模型则不一样，需要学习猫张什么样，狗张什么样。有了二者的长相以后，再根据长相去区分。具体而言：</p>
<ul>
<li><p>生成式模型：由数据学习联合概率分布P(X,Y), 然后由P(Y|X)=P(X,Y)/P(X)求出概率分布P(Y|X)作为预测的模型。该方法表示了给定输入X与产生输出Y的生成关系</p>
</li>
<li><p>判别式模型：由数据直接学习决策函数Y=f(X)或条件概率分布P(Y|X)作为预测模型，即判别模型。判别方法关心的是对于给定的输入X，应该预测什么样的输出Y。</p>
</li>
</ul>
<p>​    对于上述两种模型，从文字上理解起来似乎不太直观。我们举个例子来阐述一下，对于性别分类问题，分别用不同的模型来做：</p>
<p>​    1）如果用生成式模型：可以训练一个模型，学习输入人的特征X和性别Y的关系。比如现在有下面一批数据：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Y（性别）</th>
<th></th>
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr>
<td>X（特征）</td>
<td>0</td>
<td>1/4</td>
<td>3/4</td>
</tr>
<tr>
<td></td>
<td>1</td>
<td>3/4</td>
<td>1/4</td>
</tr>
</tbody>
</table>
</div>
<p>​    这个数据可以统计得到，即统计人的特征X=0,1….的时候，其类别为Y=0,1的概率。统计得到上述联合概率分布P(X, Y)后，可以学习一个模型，比如让二维高斯分布去拟合上述数据，这样就学习到了X，Y的联合分布。在预测时，如果我们希望给一个输入特征X，预测其类别，则需要通过贝叶斯公式得到条件概率分布才能进行推断：</p>
<script type="math/tex; mode=display">
P(Y|X)={\frac{P(X,Y)}{P(X)}}={\frac{P(X,Y)}{P(X|Y)P(Y)}}</script><p>​    2）如果用判别式模型：可以训练一个模型，输入人的特征X，这些特征包括人的五官，穿衣风格，发型等。输出则是对于性别的判断概率，这个概率服从一个分布，分布的取值只有两个，要么男，要么女，记这个分布为Y。这个过程学习了一个条件概率分布P(Y|X)，即输入特征X的分布已知条件下，Y的概率分布。</p>
<p>​    显然，从上面的分析可以看出。判别式模型似乎要方便很多，因为生成式模型要学习一个X，Y的联合分布往往需要很多数据，而判别式模型需要的数据则相对少，因为判别式模型更关注输入特征的差异性。不过生成式既然使用了更多数据来生成联合分布，自然也能够提供更多的信息，现在有一个样本（X,Y）,其联合概率P（X,Y）经过计算特别小，那么可以认为这个样本是异常样本。这种模型可以用来做outlier detection。</p>
<h3 id="什么是mode-collapsing"><a href="#什么是mode-collapsing" class="headerlink" title="什么是mode collapsing?"></a>什么是mode collapsing?</h3><p>​    某个模式(mode)出现大量重复样本，例如：<br><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/model_collpsing.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/model_collpsing.png" srcset="data:image/png;base64,666" alt="model collapsing"><br>​    上图左侧的蓝色五角星表示真实样本空间，黄色的是生成的。生成样本缺乏多样性，存在大量重复。比如上图右侧中，红框里面人物反复出现。</p>
<h3 id="如何解决mode-collapsing？"><a href="#如何解决mode-collapsing？" class="headerlink" title="如何解决mode collapsing？"></a>如何解决mode collapsing？</h3><p>方法一：<strong>针对目标函数的改进方法</strong></p>
<p>​    为了避免前面提到的由于优化maxmin导致mode跳来跳去的问题，UnrolledGAN采用修改生成器loss来解决。具体而言，UnrolledGAN在更新生成器时更新k次生成器，参考的Loss不是某一次的loss，是判别器后面k次迭代的loss。注意，判别器后面k次迭代不更新自己的参数，只计算loss用于更新生成器。这种方式使得生成器考虑到了后面k次判别器的变化情况，避免在不同mode之间切换导致的模式崩溃问题。此处务必和迭代k次生成器，然后迭代1次判别器区分开[8]。DRAGAN则引入博弈论中的无后悔算法，改造其loss以解决mode collapse问题[9]。前文所述的EBGAN则是加入VAE的重构误差以解决mode collapse。</p>
<p>方法二：<strong>针对网络结构的改进方法</strong></p>
<p>​    Multi agent diverse GAN(MAD-GAN)采用多个生成器，一个判别器以保障样本生成的多样性。具体结构如下：</p>
<p><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/MAD_GAN.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/MAD_GAN.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    相比于普通GAN，多了几个生成器，且在loss设计的时候，加入一个正则项。正则项使用余弦距离惩罚三个生成器生成样本的一致性。</p>
<p>​    MRGAN则添加了一个判别器来惩罚生成样本的mode collapse问题。具体结构如下：</p>
<p><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/MRGAN.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/MRGAN.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    输入样本$x$通过一个Encoder编码为隐变量$E(x)$，然后隐变量被Generator重构，训练时，Loss有三个。$D_M$和$R$（重构误差）用于指导生成real-like的样本。而$D_D$则对$E(x)$和$z$生成的样本进行判别，显然二者生成样本都是fake samples，所以这个判别器主要用于判断生成的样本是否具有多样性，即是否出现mode collapse。</p>
<p>方法三：<strong>Mini-batch Discrimination</strong></p>
<p>​    Mini-batch discrimination在判别器的中间层建立一个mini-batch layer用于计算基于L1距离的样本统计量，通过建立该统计量，实现了一个batch内某个样本与其他样本有多接近。这个信息可以被判别器利用到，从而甄别出哪些缺乏多样性的样本。对生成器而言，则要试图生成具有多样性的样本。</p>
<h2 id="GAN的生成能力评价"><a href="#GAN的生成能力评价" class="headerlink" title="GAN的生成能力评价"></a>GAN的生成能力评价</h2><h3 id="如何客观评价GAN的生成能力？"><a href="#如何客观评价GAN的生成能力？" class="headerlink" title="如何客观评价GAN的生成能力？"></a>如何客观评价GAN的生成能力？</h3><p>​    最常见评价GAN的方法就是主观评价。主观评价需要花费大量人力物力，且存在以下问题：</p>
<ul>
<li><p>评价带有主管色彩，有些bad case没看到很容易造成误判</p>
</li>
<li><p>如果一个GAN过拟合了，那么生成的样本会非常真实，人类主观评价得分会非常高，可是这并不是一个好的GAN。</p>
</li>
</ul>
<p>因此，就有许多学者提出了GAN的客观评价方法。</p>
<h3 id="Inception-Score"><a href="#Inception-Score" class="headerlink" title="Inception Score"></a>Inception Score</h3><p>​    对于一个在ImageNet训练良好的GAN，其生成的样本丢给Inception网络进行测试的时候，得到的判别概率应该具有如下特性：</p>
<ul>
<li>对于同一个类别的图片，其输出的概率分布应该趋向于一个脉冲分布。可以保证生成样本的准确性。</li>
<li>对于所有类别，其输出的概率分布应该趋向于一个均匀分布，这样才不会出现mode dropping等，可以保证生成样本的多样性。</li>
</ul>
<p>​    因此，可以设计如下指标：</p>
<script type="math/tex; mode=display">
IS(P_g)=e^{E_{x\sim P_g}[KL(p_M(y|x)\Vert{p_M(y)})]}
根据前面分析，如果是一个训练良好的GAN，$p_M(y|x)​$趋近于脉冲分布，$p_M(y)​$趋近于均匀分布。二者KL散度会很大。Inception Score自然就高。实际实验表明，Inception Score和人的主观判别趋向一致。IS的计算没有用到真实数据，具体值取决于模型M的选择</script><p>​    根据前面分析，如果是一个训练良好的GAN，$p_M(y|x)$趋近于脉冲分布，$p_M(y)$趋近于均匀分布。二者KL散度会很大。Inception Score自然就高。实际实验表明，Inception Score和人的主观判别趋向一致。IS的计算没有用到真实数据，具体值取决于模型M的选择。</p>
<p>​    <strong>特点：可以一定程度上衡量生成样本的多样性和准确性，但是无法检测过拟合。Mode Score也是如此。不推荐在和ImageNet数据集差别比较大的数据上使用。</strong></p>
<h3 id="Mode-Score"><a href="#Mode-Score" class="headerlink" title="Mode Score"></a>Mode Score</h3><p>​    Mode Score作为Inception Score的改进版本，添加了关于生成样本和真实样本预测的概率分布相似性度量一项。具体公式如下：</p>
<script type="math/tex; mode=display">
MS(P_g)=e^{E_{x\sim P_g}[KL(p_M(y|x)\Vert{p_M(y)})-KL(p_M(y)\Vert p_M(y^*))]}</script><h3 id="Kernel-MMD-Maximum-Mean-Discrepancy"><a href="#Kernel-MMD-Maximum-Mean-Discrepancy" class="headerlink" title="Kernel MMD (Maximum Mean Discrepancy)"></a>Kernel MMD (Maximum Mean Discrepancy)</h3><p>计算公式如下：</p>
<script type="math/tex; mode=display">
MMD^2(P_r,P_g)=E_{x_r\sim{P_r},x_g\sim{P_g}}[\lVert\Sigma_{i=1}^{n1}k(x_r)-\Sigma_{i=1}^{n2}k(x_g)\rVert]</script><p>​    对于Kernel MMD值的计算，首先需要选择一个核函数$k$，这个核函数把样本映射到再生希尔伯特空间(Reproducing Kernel Hilbert Space, RKHS) ，RKHS相比于欧几里得空间有许多优点，对于函数内积的计算是完备的。将上述公式展开即可得到下面的计算公式：</p>
<script type="math/tex; mode=display">
MMD^2(P_r,P_g)=E_{x_r,x_r{'}\sim{P_r},x_g,x_g{'}\sim{P_g}}[k(x_r,x_r{'})-2k(x_r,x_g)+k(x_g,x_g{'})]</script><p>MMD值越小，两个分布越接近。</p>
<p><strong>特点：可以一定程度上衡量模型生成图像的优劣性，计算代价小。推荐使用。</strong></p>
<h3 id="Wasserstein-distance"><a href="#Wasserstein-distance" class="headerlink" title="Wasserstein distance"></a>Wasserstein distance</h3><p>​    Wasserstein distance在最优传输问题中通常也叫做推土机距离。这个距离的介绍在WGAN中有详细讨论。公式如下：</p>
<script type="math/tex; mode=display">
WD(P_r,P_g)=min_{\omega\in\mathbb{R}^{m\times n}}\Sigma_{i=1}^n\Sigma_{i=1}^m\omega_{ij}d(x_i^r,x_j^g)</script><script type="math/tex; mode=display">
s.t. \Sigma_{i=1}^mw_{i,j}=p_r(x_i^r),  \forall i;\Sigma_{j=1}^nw_{i,j}=p_g(x_j^g),  \forall j</script><p>​    Wasserstein distance可以衡量两个分布之间的相似性。距离越小，分布越相似。</p>
<p><strong>特点：如果特征空间选择合适，会有一定的效果。但是计算复杂度为$O(n^3)$太高</strong></p>
<h3 id="Frechet-Inception-Distance-FID"><a href="#Frechet-Inception-Distance-FID" class="headerlink" title="Fréchet Inception Distance (FID)"></a>Fréchet Inception Distance (FID)</h3><p>​    FID距离计算真实样本，生成样本在特征空间之间的距离。首先利用Inception网络来提取特征，然后使用高斯模型对特征空间进行建模。根据高斯模型的均值和协方差来进行距离计算。具体公式如下：</p>
<script type="math/tex; mode=display">
FID(\mathbb P_r,\mathbb P_g)=\lVert\mu_r-\mu_g\rVert+Tr(C_r+C_g-2(C_rC_g)^{1/2})
$\mu,C$分别代表协方差和均值。</script><p>$\mu,C$分别代表协方差和均值。</p>
<p>​    <strong>特点：尽管只计算了特征空间的前两阶矩，但是鲁棒，且计算高效。</strong></p>
<h3 id="1-Nearest-Neighbor-classifier"><a href="#1-Nearest-Neighbor-classifier" class="headerlink" title="1-Nearest Neighbor classifier"></a>1-Nearest Neighbor classifier</h3><p>​    使用留一法，结合1-NN分类器（别的也行）计算真实图片，生成图像的精度。如果二者接近，则精度接近50%，否则接近0%。对于GAN的评价问题，作者分别用正样本的分类精度，生成样本的分类精度去衡量生成样本的真实性，多样性。</p>
<ul>
<li>对于真实样本$x_r$，进行1-NN分类的时候，如果生成的样本越真实。则真实样本空间$\mathbb R$将被生成的样本$x_g$包围。那么$x_r$的精度会很低。</li>
<li>对于生成的样本$x_g$，进行1-NN分类的时候，如果生成的样本多样性不足。由于生成的样本聚在几个mode，则$x_g$很容易就和$x_r$区分，导致精度会很高。</li>
</ul>
<p><strong>特点：理想的度量指标，且可以检测过拟合。</strong></p>
<h3 id="其他评价方法"><a href="#其他评价方法" class="headerlink" title="其他评价方法"></a>其他评价方法</h3><p>​    AIS，KDE方法也可以用于评价GAN，但这些方法不是model agnostic metrics。也就是说，这些评价指标的计算无法只利用：生成的样本，真实样本来计算。</p>
<h2 id="其他常见的生成式模型有哪些？"><a href="#其他常见的生成式模型有哪些？" class="headerlink" title="其他常见的生成式模型有哪些？"></a>其他常见的生成式模型有哪些？</h2><h3 id="什么是自回归模型：pixelRNN与pixelCNN？"><a href="#什么是自回归模型：pixelRNN与pixelCNN？" class="headerlink" title="什么是自回归模型：pixelRNN与pixelCNN？"></a>什么是自回归模型：pixelRNN与pixelCNN？</h3><p>​    自回归模型通过对图像数据的概率分布$p_{data}(x)$进行显式建模，并利用极大似然估计优化模型。具体如下：</p>
<script type="math/tex; mode=display">
p_{data}(x)=\prod_{i=1}^np(x_i|x_1,x_2,...,x_{i-1})</script><p>​    上述公式很好理解，给定$x<em>1,x_2,…,x</em>{i-1}$条件下，所有$p(x_i)$的概率乘起来就是图像数据的分布。如果使用RNN对上述依然关系建模，就是pixelRNN。如果使用CNN，则是pixelCNN。具体如下[5]：</p>
<p><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/pixRNN.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/pixRNN.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/pixCNN.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/pixCNN.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    显然，不论是对于pixelCNN还是pixelRNN，由于其像素值是一个个生成的，速度会很慢。语音领域大火的WaveNet就是一个典型的自回归模型。</p>
<h3 id="什么是VAE？"><a href="#什么是VAE？" class="headerlink" title="什么是VAE？"></a>什么是VAE？</h3><p>​    PixelCNN/RNN定义了一个易于处理的密度函数，我们可以直接优化训练数据的似然；对于变分自编码器我们将定义一个不易处理的密度函数，通过附加的隐变量$z$对密度函数进行建模。 VAE原理图如下[6]：</p>
<p><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/VAE.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/VAE.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    在VAE中，真实样本$X$通过神经网络计算出均值方差（假设隐变量服从正太分布），然后通过采样得到采样变量$Z$并进行重构。VAE和GAN均是学习了隐变量$z$到真实数据分布的映射。但是和GAN不同的是：</p>
<ul>
<li>GAN的思路比较粗暴，使用一个判别器去度量分布转换模块（即生成器）生成分布与真实数据分布的距离。</li>
<li>VAE则没有那么直观，VAE通过约束隐变量$z$服从标准正太分布以及重构数据实现了分布转换映射$X=G(z)$</li>
</ul>
<p><strong>生成式模型对比</strong></p>
<ul>
<li>自回归模型通过对概率分布显式建模来生成数据</li>
<li>VAE和GAN均是：假设隐变量$z$服从某种分布，并学习一个映射$X=G(z)$，实现隐变量分布$z$与真实数据分布$p_{data}(x)$的转换。</li>
<li>GAN使用判别器去度量映射$X=G(z)$的优劣，而VAE通过隐变量$z$与标准正太分布的KL散度和重构误差去度量。</li>
</ul>
<h2 id="GAN的改进与优化"><a href="#GAN的改进与优化" class="headerlink" title="GAN的改进与优化"></a>GAN的改进与优化</h2><h3 id="如何生成指定类型的图像——条件GAN"><a href="#如何生成指定类型的图像——条件GAN" class="headerlink" title="如何生成指定类型的图像——条件GAN"></a>如何生成指定类型的图像——条件GAN</h3><p>​    条件生成对抗网络（CGAN, Conditional Generative Adversarial Networks）作为一个GAN的改进，其一定程度上解决了GAN生成结果的不确定性。如果在Mnist数据集上训练原始GAN，GAN生成的图像是完全不确定的，具体生成的是数字1，还是2，还是几，根本不可控。为了让生成的数字可控，我们可以把数据集做一个切分，把数字0~9的数据集分别拆分开训练9个模型，不过这样太麻烦了，也不现实。因为数据集拆分不仅仅是分类麻烦，更主要在于，每一个类别的样本少，拿去训练GAN很有可能导致欠拟合。因此，CGAN就应运而生了。我们先看一下CGAN的网络结构：<br><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/CGAN网络结构.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/CGAN网络结构.png" srcset="data:image/png;base64,666" alt="CGAN网络结构"><br>​    从网络结构图可以看到，对于生成器Generator，其输入不仅仅是随机噪声的采样z，还有欲生成图像的标签信息。比如对于mnist数据生成，就是一个one-hot向量，某一维度为1则表示生成某个数字的图片。同样地，判别器的输入也包括样本的标签。这样就使得判别器和生成器可以学习到样本和标签之间的联系。Loss如下：</p>
<script type="math/tex; mode=display">
\mathop {\min }\limits_G \mathop {\max }\limits_D V(D,G) = {\rm E}_{x\sim{p_{data}(x)}}[\log D(x|y)] + {\rm E}_{z\sim{p_z}(z)}[\log (1 - D(G(z|y)))]</script><p>​    Loss设计和原始GAN基本一致，只不过生成器，判别器的输入数据是一个条件分布。在具体编程实现时只需要对随机噪声采样z和输入条件y做一个级联即可。</p>
<h3 id="CNN与GAN——DCGAN"><a href="#CNN与GAN——DCGAN" class="headerlink" title="CNN与GAN——DCGAN"></a>CNN与GAN——DCGAN</h3><p>​    前面我们聊的GAN都是基于简单的神经网络构建的。可是对于视觉问题，如果使用原始的基于DNN的GAN，则会出现许多问题。如果输入GAN的随机噪声为100维的随机噪声，输出图像为256x256大小。也就是说，要将100维的信息映射为65536维。如果单纯用DNN来实现，那么整个模型参数会非常巨大，而且学习难度很大（低维度映射到高维度需要添加许多信息）。因此，DCGAN就出现了。具体而言，DCGAN将传统GAN的生成器，判别器均采用GAN实现，且使用了一下tricks：</p>
<ul>
<li>将pooling层convolutions替代，其中，在discriminator上用strided convolutions替代，在generator上用fractional-strided convolutions替代。</li>
<li>在generator和discriminator上都使用batchnorm。 </li>
<li>移除全连接层，global pooling增加了模型的稳定性，但伤害了收敛速度。</li>
<li>在generator的除了输出层外的所有层使用ReLU，输出层采用tanh。</li>
<li>在discriminator的所有层上使用LeakyReLU。</li>
</ul>
<p>网络结构图如下：<br><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/DCGAN%E7%BB%93%E6%9E%84%E5%9B%BE.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/DCGAN%E7%BB%93%E6%9E%84%E5%9B%BE.png" srcset="data:image/png;base64,666" alt="CGAN网络结构图"></p>
<h3 id="如何理解GAN中的输入随机噪声？"><a href="#如何理解GAN中的输入随机噪声？" class="headerlink" title="如何理解GAN中的输入随机噪声？"></a>如何理解GAN中的输入随机噪声？</h3><p>​    为了了解输入随机噪声每一个维度代表的含义，作者做了一个非常有趣的工作。即在隐空间上，假设知道哪几个变量控制着某个物体，那么僵这几个变量挡住是不是就可以将生成图片中的某个物体消失？论文中的实验是这样的：首先，生成150张图片，包括有窗户的和没有窗户的，然后使用一个逻辑斯底回归函数来进行分类，对于权重不为0的特征，认为它和窗户有关。将其挡住，得到新的生成图片，结果如下：<br><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/7.4.3.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/7.4.3.png" srcset="data:image/png;base64,666" alt="DCGAN输入噪声理解"><br>此外，将几个输入噪声进行算数运算，可以得到语义上进行算数运算的非常有趣的结果。类似于word2vec。<br><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/7.4.3-2.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/7.4.3-2.png" srcset="data:image/png;base64,666" alt="DCGAN输入噪声算术运算"></p>
<h3 id="GAN为什么容易训练崩溃？"><a href="#GAN为什么容易训练崩溃？" class="headerlink" title="GAN为什么容易训练崩溃？"></a>GAN为什么容易训练崩溃？</h3><p>​    所谓GAN的训练崩溃，指的是训练过程中，生成器和判别器存在一方压倒另一方的情况。<br>GAN原始判别器的Loss在判别器达到最优的时候，等价于最小化生成分布与真实分布之间的JS散度，由于随机生成分布很难与真实分布有不可忽略的重叠以及JS散度的突变特性，使得生成器面临梯度消失的问题；可是如果不把判别器训练到最优，那么生成器优化的目标就失去了意义。因此需要我们小心的平衡二者，要把判别器训练的不好也不坏才行。否则就会出现训练崩溃，得不到想要的结果</p>
<h3 id="WGAN如何解决训练崩溃问题？"><a href="#WGAN如何解决训练崩溃问题？" class="headerlink" title="WGAN如何解决训练崩溃问题？"></a>WGAN如何解决训练崩溃问题？</h3><p>​    WGAN作者提出了使用Wasserstein距离，以解决GAN网络训练过程难以判断收敛性的问题。Wasserstein距离定义如下:</p>
<script type="math/tex; mode=display">
L={\rm E}_{x\sim{p_{data}}(x)}[f_w(x)] - {\rm E}_{x\sim{p_g}(x)}[f_w(x)]</script><p>通过最小化Wasserstein距离，得到了WGAN的Loss：</p>
<ul>
<li>WGAN生成器Loss：$- {\rm E}_{x\sim{p_g}(x)}[f_w(x)]$</li>
<li>WGAN判别器Loss：$L=-{\rm E}<em>{x\sim{p</em>{data}}(x)}[f<em>w(x)] + {\rm E}</em>{x\sim{p_g}(x)}[f_w(x)]$</li>
</ul>
<p>从公式上GAN似乎总是让人摸不着头脑，在代码实现上来说，其实就以下几点：</p>
<ul>
<li>判别器最后一层去掉sigmoid</li>
<li>生成器和判别器的loss不取log</li>
<li>每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c</li>
</ul>
<h3 id="WGAN-GP：带有梯度正则的WGAN"><a href="#WGAN-GP：带有梯度正则的WGAN" class="headerlink" title="WGAN-GP：带有梯度正则的WGAN"></a>WGAN-GP：带有梯度正则的WGAN</h3><p>​    实际实验过程发现，WGAN没有那么好用，主要原因在于WAGN进行梯度截断。梯度截断将导致判别网络趋向于一个二值网络，造成模型容量的下降。<br>于是作者提出使用梯度惩罚来替代梯度裁剪。公式如下：</p>
<script type="math/tex; mode=display">
L=-{\rm E}_{x\sim{p_{data}}(x)}[f_w(x)] + {\rm E}_{x\sim{p_g}(x)}[f_w(x)]+\lambda{\rm E}_{x\sim{p_x}(x)}[\lVert\nabla_x(D(x))\rVert_p-1]^2
由于上式是对每一个梯度进行惩罚，所以不适合使用BN，因为它会引入同个batch中不同样本的相互依赖关系。如果需要的话，可以选择Layer Normalization。实际训练过程中，就可以通过Wasserstein距离来度量模型收敛程度了：
![Wass距离随迭代次数变化](./img/ch7/Wass%E8%B7%9D%E7%A6%BB%E9%9A%8F%E8%BF%AD%E4%BB%A3%E6%AC%A1%E6%95%B0%E5%8F%98%E5%8C%96.png)
上图纵坐标是Wasserstein距离，横坐标是迭代次数。可以看出，随着迭代的进行，Wasserstein距离趋于收敛，生成图像也趋于稳定。</script><p>​    由于上式是对每一个梯度进行惩罚，所以不适合使用BN，因为它会引入同个batch中不同样本的相互依赖关系。如果需要的话，可以选择Layer Normalization。实际训练过程中，就可以通过Wasserstein距离来度量模型收敛程度了：<br><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/Wass%E8%B7%9D%E7%A6%BB%E9%9A%8F%E8%BF%AD%E4%BB%A3%E6%AC%A1%E6%95%B0%E5%8F%98%E5%8C%96.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/Wass%E8%B7%9D%E7%A6%BB%E9%9A%8F%E8%BF%AD%E4%BB%A3%E6%AC%A1%E6%95%B0%E5%8F%98%E5%8C%96.png" srcset="data:image/png;base64,666" alt="Wass距离随迭代次数变化"><br>​    上图纵坐标是Wasserstein距离，横坐标是迭代次数。可以看出，随着迭代的进行，Wasserstein距离趋于收敛，生成图像也趋于稳定。</p>
<h3 id="LSGAN"><a href="#LSGAN" class="headerlink" title="LSGAN"></a>LSGAN</h3><p>​    LSGAN（Least Squares GAN）这篇文章主要针对标准GAN的稳定性和图片生成质量不高做了一个改进。作者将原始GAN的交叉熵损失采用最小二乘损失替代。LSGAN的Loss：</p>
<script type="math/tex; mode=display">
\mathop{\min }\limits_DJ(D)=\mathop{\min}\limits_D[{\frac{1}{2}}{\rm E}_{x\sim{p_{data}}(x)}[D(x)-a]^2 + {\frac{1}{2}}{\rm E}_{z\sim{p_z}(z)}[D(G(z))-b]^2]</script><script type="math/tex; mode=display">
\mathop{\min }\limits_GJ(G)=\mathop{\min}\limits_G{\frac{1}{2}}{\rm E}_{z\sim{p_z}(z)}[D(G(z))-c]^2</script><p>​    实际实现的时候非常简单，最后一层去掉sigmoid，并且计算Loss的时候用平方误差即可。之所以这么做，作者在原文给出了一张图:<br><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/lsgan loss compare.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/lsgan loss compare.png" srcset="data:image/png;base64,666" alt="LSGAN交叉熵与最小二乘损失对比图"><br>​    上面是作者给出的基于交叉熵损失以及最小二乘损失的Loss函数。横坐标代表Loss函数的输入，纵坐标代表输出的Loss值。可以看出，随着输入的增大，sigmoid交叉熵损失很快趋于0，容易导致梯度饱和问题。如果使用右边的Loss设计，则只在x=0点处饱和。因此使用LSGAN可以很好的解决交叉熵损失的问题。</p>
<h3 id="如何尽量避免GAN的训练崩溃问题？"><a href="#如何尽量避免GAN的训练崩溃问题？" class="headerlink" title="如何尽量避免GAN的训练崩溃问题？"></a>如何尽量避免GAN的训练崩溃问题？</h3><ul>
<li>归一化图像输入到（-1，1）之间；Generator最后一层使用tanh激活函数</li>
<li>生成器的Loss采用：min (log 1-D)。因为原始的生成器Loss存在梯度消失问题；训练生成器的时候，考虑反转标签，real=fake, fake=real</li>
<li>不要在均匀分布上采样，应该在高斯分布上采样</li>
<li>一个Mini-batch里面必须只有正样本，或者负样本。不要混在一起；如果用不了Batch Norm，可以用Instance Norm</li>
<li>避免稀疏梯度，即少用ReLU，MaxPool。可以用LeakyReLU替代ReLU，下采样可以用Average Pooling或者Convolution + stride替代。上采样可以用PixelShuffle, ConvTranspose2d + stride</li>
<li>平滑标签或者给标签加噪声；平滑标签，即对于正样本，可以使用0.7-1.2的随机数替代；对于负样本，可以使用0-0.3的随机数替代。  给标签加噪声：即训练判别器的时候，随机翻转部分样本的标签。</li>
<li>如果可以，请用DCGAN或者混合模型：KL+GAN，VAE+GAN。</li>
<li>使用LSGAN，WGAN-GP</li>
<li>Generator使用Adam，Discriminator使用SGD</li>
<li>尽快发现错误；比如：判别器Loss为0，说明训练失败了；如果生成器Loss稳步下降，说明判别器没发挥作用</li>
<li>不要试着通过比较生成器，判别器Loss的大小来解决训练过程中的模型坍塌问题。比如：<br>While Loss D &gt; Loss A:<br>Train D<br>While Loss A &gt; Loss D:<br>Train A</li>
<li>如果有标签，请尽量利用标签信息来训练</li>
<li>给判别器的输入加一些噪声，给G的每一层加一些人工噪声。</li>
<li>多训练判别器，尤其是加了噪声的时候</li>
<li>对于生成器，在训练，测试的时候使用Dropout</li>
</ul>
<h2 id="GAN的应用（图像翻译）"><a href="#GAN的应用（图像翻译）" class="headerlink" title="GAN的应用（图像翻译）"></a>GAN的应用（图像翻译）</h2><h3 id="什么是图像翻译？"><a href="#什么是图像翻译？" class="headerlink" title="什么是图像翻译？"></a>什么是图像翻译？</h3><p>​    GAN作为一种强有力的生成模型，其应用十分广泛。最为常见的应用就是图像翻译。所谓图像翻译，指从一副图像到另一副图像的转换。可以类比机器翻译，一种语言转换为另一种语言。常见的图像翻译任务有：</p>
<ul>
<li>图像去噪</li>
<li>图像超分辨</li>
<li>图像补全</li>
<li>风格迁移</li>
<li><p>…</p>
<p>本节将介绍一个经典的图像翻译网络及其改进。图像翻译可以分为有监督图像翻译和无监督图像翻译：</p>
</li>
<li><p>有监督图像翻译：原始域与目标域存在一一对应数据</p>
</li>
<li>无监督图像翻译：原始域与目标域不存在一一对应数据</li>
</ul>
<h3 id="有监督图像翻译：pix2pix"><a href="#有监督图像翻译：pix2pix" class="headerlink" title="有监督图像翻译：pix2pix"></a>有监督图像翻译：pix2pix</h3><p>​    在这篇paper里面，作者提出的框架十分简洁优雅（好用的算法总是简洁优雅的）。相比以往算法的大量专家知识，手工复杂的loss。这篇paper非常粗暴，使用CGAN处理了一系列的转换问题。下面是一些转换示例：<br><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/pix2pix结果示例.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/pix2pix结果示例.png" srcset="data:image/png;base64,666" alt="pix2pix结果示例"></p>
<p>​    上面展示了许多有趣的结果，比如分割图$\longrightarrow$街景图，边缘图$\longrightarrow$真实图。对于第一次看到的时候还是很惊艳的，那么这个是怎么做到的呢？我们可以设想一下，如果是我们，我们自己会如何设计这个网络？</p>
<p><strong>直观的想法</strong>？</p>
<p>​    最直接的想法就是，设计一个CNN网络，直接建立输入-输出的映射，就像图像去噪问题一样。可是对于上面的问题，这样做会带来一个问题。<strong>生成图像质量不清晰。</strong></p>
<p>​    拿左上角的分割图$\longrightarrow$街景图为例，语义分割图的每个标签比如“汽车”可能对应不同样式，颜色的汽车。那么模型学习到的会是所有不同汽车的评均，这样会造成模糊。<img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/pix2pix语义地图L1loss结果.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/pix2pix语义地图L1loss结果.png" srcset="data:image/png;base64,666" alt="pix2pix语义地图L1loss结果"></p>
<p><strong>如何解决生成图像的模糊问题</strong>？</p>
<p>​    这里作者想了一个办法，即加入GAN的Loss去惩罚模型。GAN相比于传统生成式模型可以较好的生成高分辨率图片。思路也很简单，在上述直观想法的基础上加入一个判别器，判断输入图片是否是真实样本。模型示意图如下：<br><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/pix2pix模型示意图.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/pix2pix模型示意图.png" srcset="data:image/png;base64,666" alt="pix2pix模型示意图"></p>
<p>​    上图模型和CGAN有所不同，但它是一个CGAN，只不过输入只有一个，这个输入就是条件信息。原始的CGAN需要输入随机噪声，以及条件。这里之所有没有输入噪声信息，是因为在实际实验中，如果输入噪声和条件，噪声往往被淹没在条件C当中，所以这里直接省去了。</p>
<h3 id="其他图像翻译的tricks"><a href="#其他图像翻译的tricks" class="headerlink" title="其他图像翻译的tricks"></a>其他图像翻译的tricks</h3><p>从上面两点可以得到最终的Loss由两部分构成：</p>
<ul>
<li><p>输出和标签信息的L1 Loss。</p>
</li>
<li><p>GAN Loss</p>
</li>
<li><p>测试也使用Dropout，以使输出多样化</p>
<script type="math/tex; mode=display">
G^*=arg\mathop {\min }\limits_G \mathop {\max }\limits_D \Gamma_{cGAN}(G,D)+\lambda\Gamma_{L1}(G)</script></li>
</ul>
<p>​    采用L1 Loss而不是L2 Loss的理由很简单，L1 Loss相比于L2 Loss保边缘（L2 Loss基于高斯先验，L1 Loss基于拉普拉斯先验）。</p>
<p>​    GAN Loss为LSGAN的最小二乘Loss，并使用PatchGAN(进一步保证生成图像的清晰度)。PatchGAN将图像换分成很多个Patch，并对每一个Patch使用判别器进行判别（实际代码实现有更取巧的办法），将所有Patch的Loss求平均作为最终的Loss。</p>
<h3 id="如何生成高分辨率图像和高分辨率视频？"><a href="#如何生成高分辨率图像和高分辨率视频？" class="headerlink" title="如何生成高分辨率图像和高分辨率视频？"></a>如何生成高分辨率图像和高分辨率视频？</h3><p>​    pix2pix提出了一个通用的图像翻译框架。对于高分辨率的图像生成以及高分辨率的视频生成，则需要利用更好的网络结构以及更多的先验只是。pix2pixHD提出了一种多尺度的生成器以及判别器等方式从而生成高分辨率图像。Vid2Vid则在pix2pixHD的基础上利用光流，时序约束生成了高分辨率视频。</p>
<h3 id="有监督的图像翻译的缺点？"><a href="#有监督的图像翻译的缺点？" class="headerlink" title="有监督的图像翻译的缺点？"></a>有监督的图像翻译的缺点？</h3><p>​    许多图像翻译算法如前面提及的pix2pix系列，需要一一对应的图像。可是在许多应用场景下，往往没有这种一一对应的强监督信息。比如说以下一些应用场景：<br><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/CycleGAN结果例子.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/CycleGAN结果例子.png" srcset="data:image/png;base64,666" alt="CycleGAN结果例子"><br>以第一排第一幅图为例，要找到这种一一配对的数据是不现实的。因此，无监督图像翻译算法就被引入了。</p>
<h3 id="无监督图像翻译：CycleGAN"><a href="#无监督图像翻译：CycleGAN" class="headerlink" title="无监督图像翻译：CycleGAN"></a>无监督图像翻译：CycleGAN</h3><p><strong>模型结构</strong></p>
<p>​    总体思路如下，假设有两个域的数据，记为A，B。对于上图第一排第一幅图A域就是普通的马，B域就是斑马。由于A-&gt;B的转换缺乏监督信息，于是，作者提出采用如下方法进行转换：</p>
<blockquote>
<p>a.    A-&gt;fake_B-&gt;rec_A<br>b.    B-&gt;fake_A-&gt;rec_B</p>
</blockquote>
<p>​    对于A域的所有图像，学习一个网络G_B，该网络可以生成B。对于B域的所有图像，也学习一个网络G_A，该网络可以生成G_B。</p>
<p>​    训练过程分成两步，首先对于A域的某张图像，送入G_B生成fake_B，然后对fake_B送入G_A，得到重构后的A图像rec_A。对于B域的某一张图像也是类似。重构后的图像rec_A/rec_B可以和原图A/B做均方误差，实现了有监督的训练。此处值得注意的是A-&gt;fake_B(B-&gt;fake_A)和fake_A-&gt;rec_B(fake_B-&gt;rec_A)的网络是一模一样的。下图是形象化的网络结构图：<br><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/CycleGAN模型示意图.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/CycleGAN模型示意图.png" srcset="data:image/png;base64,666" alt="CycleGAN模型示意图"><br>​    cycleGAN的生成器采用U-Net，判别器采用LS-GAN。</p>
<p><strong>Loss设计</strong></p>
<p>​    总的Loss就是X域和Y域的GAN Loss，以及Cycle consistency loss：</p>
<script type="math/tex; mode=display">
L(G,F,D_X,D_Y)=L_{GAN}(G,D_Y,X,Y)+L_{GAN}(F,D_X,Y,X)+\lambda L_{cycle}(G,F)</script><p>整个过程End to end训练，效果非常惊艳，利用这一框架可以完成非常多有趣的任务</p>
<h3 id="多领域的无监督图像翻译：StarGAN"><a href="#多领域的无监督图像翻译：StarGAN" class="headerlink" title="多领域的无监督图像翻译：StarGAN"></a>多领域的无监督图像翻译：StarGAN</h3><p>cycleGAN模型较好的解决了无监督图像转换问题，可是这种单一域的图像转换还存在一些问题：</p>
<ul>
<li>要针对每一个域训练一个模型，效率太低。举例来说，我希望可以将橘子转换为红苹果和青苹果。对于cycleGAN而言，需要针对红苹果，青苹果分别训练一个模型。</li>
<li><p>对于每一个域都需要搜集大量数据，太麻烦。还是以橘子转换为红苹果和青苹果为例。不管是红苹果还是青苹果，都是苹果，只是颜色不一样而已。这两个任务信息是可以共享的，没必要分别训练两个模型。而且针对红苹果，青苹果分别取搜集大量数据太费事。</p>
<p>starGAN则提出了一个多领域的无监督图像翻译框架，实现了多个领域的图像转换，且对于不同领域的数据可以混合在一起训练，提高了数据利用率</p>
</li>
</ul>
<h2 id="GAN的应用（文本生成）"><a href="#GAN的应用（文本生成）" class="headerlink" title="GAN的应用（文本生成）"></a>GAN的应用（文本生成）</h2><h3 id="GAN为什么不适合文本任务？"><a href="#GAN为什么不适合文本任务？" class="headerlink" title="GAN为什么不适合文本任务？"></a>GAN为什么不适合文本任务？</h3><p>​    GAN在2014年被提出之后，在图像生成领域取得了广泛的研究应用。然后在文本领域却一直没有很惊艳的效果。主要在于文本数据是离散数据，而GAN在应用于离散数据时存在以下几个问题：</p>
<ul>
<li>GAN的生成器梯度来源于判别器对于正负样本的判别。然而，对于文本生成问题，RNN输出的是一个概率序列，然后取argmax。这会导致生成器Loss不可导。还可以站在另一个角度理解，由于是argmax，所以参数更新一点点并不会改变argmax的结果，这也使得GAN不适合离散数据。</li>
<li>GAN只能评估整个序列的loss，但是无法评估半句话，或者是当前生成单词对后续结果好坏的影响。</li>
<li>如果不加argmax，那么由于生成器生成的都是浮点数值，而ground truth都是one-hot encoding，那么判别器只要判别生成的结果是不是0/1序列组成的就可以了。这容易导致训练崩溃。</li>
</ul>
<h3 id="seqGAN用于文本生成"><a href="#seqGAN用于文本生成" class="headerlink" title="seqGAN用于文本生成"></a>seqGAN用于文本生成</h3><p>​    seqGAN在GAN的框架下，结合强化学习来做文本生成。 模型示意图如下：</p>
<p><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/seqGAN模型.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/seqGAN模型.png" srcset="data:image/png;base64,666" alt="seqGAN模型"><br>在文本生成任务，seqGAN相比较于普通GAN区别在以下几点：</p>
<ul>
<li>生成器不取argmax。</li>
<li>每生成一个单词，则根据当前的词语序列进行蒙特卡洛采样生成完成的句子。然后将句子送入判别器计算reward。</li>
<li>根据得到的reward进行策略梯度下降优化模型。</li>
</ul>
<h2 id="GAN在其他领域的应用"><a href="#GAN在其他领域的应用" class="headerlink" title="GAN在其他领域的应用"></a>GAN在其他领域的应用</h2><h3 id="数据增广"><a href="#数据增广" class="headerlink" title="数据增广"></a>数据增广</h3><p>​    GAN的良好生成特性近年来也开始被用于数据增广。以行人重识别为例，有许多GAN用于数据增广的工作[1-4]。行人重识别问题一个难点在于不同摄像头下拍摄的人物环境，角度差别非常大，导致存在较大的Domain gap。因此，可以考虑使用GAN来产生不同摄像头下的数据进行数据增广。以论文[1]为例，本篇paper提出了一个cycleGAN用于数据增广的方法。具体模型结构如下：</p>
<p><img src="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/cycleGAN数据增广.png" class="lazyload" data-srcset="/zh-TW/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/ch7/cycleGAN数据增广.png" srcset="data:image/png;base64,666" alt="cycleGAN数据增广"></p>
<p>​    对于每一对摄像头都训练一个cycleGAN，这样就可以实现将一个摄像头下的数据转换成另一个摄像头下的数据，但是内容（人物）保持不变。</p>
<h3 id="图像超分辨与图像补全"><a href="#图像超分辨与图像补全" class="headerlink" title="图像超分辨与图像补全"></a>图像超分辨与图像补全</h3><p>​    图像超分辨与补全均可以作为图像翻译问题，该类问题的处理办法也大都是训练一个端到端的网络，输入是原始图片，输出是超分辨率后的图片，或者是补全后的图片。文献[5]利用GAN作为判别器，使得超分辨率模型输出的图片更加清晰，更符合人眼主管感受。日本早稻田大学研究人员[6]提出一种全局+局部一致性的GAN实现图像补全，使得修复后的图像不仅细节清晰，且具有整体一致性。</p>
<h3 id="语音领域"><a href="#语音领域" class="headerlink" title="语音领域"></a>语音领域</h3><p>​    相比于图像领域遍地开花，GAN在语音领域则应用相对少了很多。这里零碎的找一些GAN在语音领域进行应用的例子作为介绍。文献[7]提出了一种音频去噪的SEGAN，缓解了传统方法支持噪声种类稀少，泛化能力不强的问题。Donahue利用GAN进行语音增强，提升了ASR系统的识别率。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>优化算法</title>
    <url>/zh-TW/ch13_%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h2 id="如何解决训练样本少的问题"><a href="#如何解决训练样本少的问题" class="headerlink" title="如何解决训练样本少的问题"></a>如何解决训练样本少的问题</h2><p>目前大部分的深度学习模型仍然需要海量的数据支持。例如 ImageNet 数据就拥有1400多万的图片。而现实生产环境中，数据集通常较小，只有几万甚至几百个样本。这时候，如何在这种情况下应用深度学习呢?<br>（1）利用预训练模型进行迁移微调（fine-tuning），预训练模型通常在特征上拥有很好的语义表达。此时，只需将模型在小数据集上进行微调就能取得不错的效果。这也是目前大部分小数据集常用的训练方式。视觉领域内，通常会ImageNet上训练完成的模型。自然语言处理领域，也有BERT模型等预训练模型可以使用。<br>  <br>（2）单样本或者少样本学习（one-shot，few-shot learning），这种方式适用于样本类别远远大于样本数量的情况等极端数据集。例如有1000个类别，每个类别只提供1-5个样本。少样本学习同样也需要借助预训练模型，但有别于微调的在于，微调通常仍然在学习不同类别的语义，而少样本学习通常需要学习样本之间的距离度量。例如孪生网络（Siamese Neural Networks）就是通过训练两个同种结构的网络来判别输入的两张图片是否属于同一类。<br>​    上述两种是常用训练小样本数据集的方式。此外，也有些常用的手段，例如数据集增强、正则或者半监督学习等方式来解决小样本数据集的训练问题。</p>
<h2 id="深度学习是否能胜任所有数据集"><a href="#深度学习是否能胜任所有数据集" class="headerlink" title="深度学习是否能胜任所有数据集?"></a>深度学习是否能胜任所有数据集?</h2><p>深度学习并不能胜任目前所有的数据环境，以下列举两种情况：</p>
<p>（1）深度学习能取得目前的成果，很大一部分原因依赖于海量的数据集以及高性能密集计算硬件。因此，当数据集过小时，需要考虑与传统机器学习相比，是否在性能和硬件资源效率更具有优势。<br>（2）深度学习目前在视觉，自然语言处理等领域都有取得不错的成果。这些领域最大的特点就是具有局部相关性。例如图像中，人的耳朵位于两侧，鼻子位于两眼之间，文本中单词组成句子。这些都是具有局部相关性的，一旦被打乱则会破坏语义或者有不同的语义。所以当数据不具备这种相关性的时候，深度学习就很难取得效果。</p>
<h2 id="有没有可能找到比已知算法更好的算法"><a href="#有没有可能找到比已知算法更好的算法" class="headerlink" title="有没有可能找到比已知算法更好的算法?"></a>有没有可能找到比已知算法更好的算法?</h2><p>在最优化理论发展中，有个没有免费午餐的定律，其主要含义在于，在不考虑具体背景和细节的情况下，任何算法和随机猜的效果期望是一样的。即，没有任何一种算法能优于其他一切算法，甚至不比随机猜好。深度学习作为机器学习领域的一个分支同样符合这个定律。所以，虽然目前深度学习取得了非常不错的成果，但是我们同样不能盲目崇拜。</p>
<p>优化算法本质上是在寻找和探索更符合数据集和问题的算法，这里数据集是算法的驱动力，而需要通过数据集解决的问题就是算法的核心，任何算法脱离了数据都会没有实际价值，任何算法的假设都不能脱离实际问题。因此，实际应用中，面对不同的场景和不同的问题，可以从多个角度针对问题进行分析，寻找更优的算法。</p>
<h2 id="什么是共线性，如何判断和解决共线性问题"><a href="#什么是共线性，如何判断和解决共线性问题" class="headerlink" title="什么是共线性，如何判断和解决共线性问题?"></a>什么是共线性，如何判断和解决共线性问题?</h2><p>对于回归算法，无论是一般回归还是逻辑回归，在使用多个变量进行预测分析时，都可能存在多变量相关的情况，这就是多重共线性。共线性的存在，使得特征之间存在冗余，导致过拟合。</p>
<p>常用判断是否存在共线性的方法有：</p>
<p>（1）相关性分析。当相关性系数高于0.8，表明存在多重共线性；但相关系数低，并不能表示不存在多重共线性；</p>
<p>（2）方差膨胀因子VIF。当VIF大于5或10时，代表模型存在严重的共线性问题；</p>
<p>（3）条件系数检验。 当条件数大于100、1000时，代表模型存在严重的共线性问题。</p>
<p>通常可通过PCA降维、逐步回归法和LASSO回归等方法消除共线性。</p>
<h2 id="权值初始化方法有哪些？"><a href="#权值初始化方法有哪些？" class="headerlink" title="权值初始化方法有哪些？"></a>权值初始化方法有哪些？</h2><p>在深度学习的模型中，从零开始训练时，权重的初始化有时候会对模型训练产生较大的影响。良好的初始化能让模型快速、有效的收敛，而糟糕的初始化会使得模型无法训练。</p>
<p>目前，大部分深度学习框架都提供了各类初始化方式，其中一般常用的会有如下几种：<br><strong>1. 常数初始化(constant)</strong> </p>
<p>​    把权值或者偏置初始化为一个常数。例如设置为0，偏置初始化为0较为常见，权重很少会初始化为0。TensorFlow中也有zeros_initializer、ones_initializer等特殊常数初始化函数。  </p>
<p><strong>2. 高斯初始化(gaussian)</strong> </p>
<p>​     给定一组均值和标准差，随机初始化的参数会满足给定均值和标准差的高斯分布。高斯初始化是很常用的初始化方式。特殊地，在TensorFlow中还有一种截断高斯分布初始化（truncated_normal_initializer），其主要为了将超过两个标准差的随机数重新随机，使得随机数更稳定。</p>
<p><strong>3. 均匀分布初始化(uniform)</strong> </p>
<p>​    给定最大最小的上下限，参数会在该范围内以均匀分布方式进行初始化，常用上下限为（0，1）。</p>
<p><strong>4. xavier 初始化(uniform)</strong>  </p>
<p>​    在batchnorm还未出现之前，要训练较深的网络，防止梯度弥散，需要依赖非常好的初始化方式。xavier 就是一种比较优秀的初始化方式，也是目前最常用的初始化方式之一。其目的是为了使得模型各层的激活值和梯度在传播过程中的方差保持一致。本质上xavier 还是属于均匀分布初始化，但与上述的均匀分布初始化有所不同，xavier 的上下限将在如下范围内进行均匀分布采样：</p>
<script type="math/tex; mode=display">
[-\sqrt{\frac{6}{n+m}},\sqrt{\frac{6}{n+m}}]</script><p>​    其中，n为所在层的输入维度，m为所在层的输出维度。</p>
<p><strong>6. kaiming初始化（msra 初始化）</strong> </p>
<p>​    kaiming初始化，在caffe中也叫msra 初始化。kaiming初始化和xavier 一样都是为了防止梯度弥散而使用的初始化方式。kaiming初始化的出现是因为xavier存在一个不成立的假设。xavier在推导中假设激活函数都是线性的，而在深度学习中常用的ReLu等都是非线性的激活函数。而kaiming初始化本质上是高斯分布初始化，与上述高斯分布初始化有所不同，其是个满足均值为0，方差为2/n的高斯分布：</p>
<script type="math/tex; mode=display">
[0,\sqrt{\frac{2}{n}}]</script><p>​    其中，n为所在层的输入维度。</p>
<p>除上述常见的初始化方式以外，不同深度学习框架下也会有不同的初始化方式，读者可自行查阅官方文档。</p>
<h2 id="如何防止梯度下降陷入局部最优解"><a href="#如何防止梯度下降陷入局部最优解" class="headerlink" title="如何防止梯度下降陷入局部最优解?"></a>如何防止梯度下降陷入局部最优解?</h2><p>梯度下降法(GD)及其一些变种算法是目前深度学习里最常用于求解凸优化问题的优化算法。神经网络很可能存在很多局部最优解，而非全局最优解。 为了防止陷入局部最优，通常会采用如下一些方法，当然，这并不能保证一定能找到全局最优解，或许能得到一个比目前更优的局部最优解也是不错的：</p>
<p><strong>（1）stochastic GD</strong> /<strong>Mini-Batch GD</strong> </p>
<p>​    在GD算法中，每次的梯度都是从所有样本中累计获取的，这种情况最容易导致梯度方向过于稳定一致，且更新次数过少，容易陷入局部最优。而stochastic GD是GD的另一种极端更新方式，其每次都只使用一个样本进行参数更新，这样更新次数大大增加也就不容易陷入局部最优。但引出的一个问题的在于其更新方向过多，导致不易于进一步优化。Mini-Batch GD便是两种极端的折中，即每次更新使用一小批样本进行参数更新。Mini-Batch GD是目前最常用的优化算法，严格意义上Mini-Batch GD也叫做stochastic GD，所以很多深度学习框架上都叫做SGD。<br><strong>（2）动量 </strong><br>​    动量也是GD中常用的方式之一，SGD的更新方式虽然有效，但每次只依赖于当前批样本的梯度方向，这样的梯度方向依然很可能很随机。动量就是用来减少随机，增加稳定性。其思想是模仿物理学的动量方式，每次更新前加入部分上一次的梯度量，这样整个梯度方向就不容易过于随机。一些常见情况时，如上次梯度过大，导致进入局部最小点时，下一次更新能很容易借助上次的大梯度跳出局部最小点。</p>
<p><strong>（3）自适应学习率 </strong> </p>
<p>​    无论是GD还是动量重点优化角度是梯度方向。而学习率则是用来直接控制梯度更新幅度的超参数。自适应学习率的优化方法有很多，例如Adagrad和RMSprop。两种自适应学习率的方式稍有差异，但主要思想都是基于历史的累计梯度去计算一个当前较优的学习率。</p>
<h2 id="为什么需要激活函数？"><a href="#为什么需要激活函数？" class="headerlink" title="为什么需要激活函数？"></a>为什么需要激活函数？</h2><p>（1）非线性：即导数不是常数。这个条件是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。</p>
<p>（2）几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响[1]。</p>
<p>（3）计算简单：非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。</p>
<p>（4）非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x&gt;0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x&lt;0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU[3]和PReLU[4]的提出正是为了解决这一问题。</p>
<p>（5）单调性（monotonic）：即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。</p>
<p>（6）输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响[1]）、LSTM里的gate函数。</p>
<p>（7）接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x&gt;0时为线性。这个性质也让初始化参数范围的推导更为简单[5][4]。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。</p>
<p>（8）参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。</p>
<p>（9）归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。</p>
<h2 id="常见的损失函数有哪些"><a href="#常见的损失函数有哪些" class="headerlink" title="常见的损失函数有哪些?"></a>常见的损失函数有哪些?</h2><p>机器学习通过对算法中的目标函数进行不断求解优化，得到最终想要的结果。分类和回归问题中，通常使用损失函数或代价函数作为目标函数。</p>
<p>损失函数用来评价预测值和真实值不一样的程度。通常损失函数越好，模型的性能也越好。</p>
<p>损失函数可分为<strong>经验风险损失</strong>和<strong>结构风险损失</strong>。经验风险损失是根据已知数据得到的损失。结构风险损失是为了防止模型被过度拟合已知数据而加入的惩罚项。</p>
<p>下面介绍常用的损失函数:<br><strong>（1）0-1 损失函数</strong><br>  <br>如果预测值和目标值相等，值为 0，如果不相等，值为 1：</p>
<script type="math/tex; mode=display">
L(Y,f(x))=
\left\{
\begin{array}{}
1\;\;\;,\;\;Y\ne f(x), \\
0\;\;\;,\;\;Y=f(x).
\end{array}
\right.</script><p>  <br>一般的在实际使用中，相等的条件过于严格，可适当放宽条件：</p>
<script type="math/tex; mode=display">
L(Y,f(x))=
\left\{
\begin{array}{}
1\;\;\;,\;\;|Y - f(x)| \ge T, \\
0\;\;\;,\;\;|Y-f(x)| < T.
\end{array}
\right.</script><p>  <br><strong>（2）绝对值损失函数</strong><br>  <br>和 0-1 损失函数相似，绝对值损失函数表示为：</p>
<script type="math/tex; mode=display">
L(Y,f(x))=|Y-f(x)|.</script><p>  <br><strong>（3）平方损失函数</strong>  </p>
<script type="math/tex; mode=display">
L(Y|f(x))=\sum_{N}(Y-f(x))^2.</script><p>  <br>这点可从最小二乘法和欧几里得距离角度理解。最小二乘法的原理是，最优拟合曲线应该 使所有点到回归直线的距离和最小。</p>
<p>  <br><strong>（4）log 对数损失函数</strong>  </p>
<script type="math/tex; mode=display">
L(Y,P(Y|X))=-logP(Y|X).</script><p>  <br>常见的逻辑回归使用的就是对数损失函数，有很多人认为逻辑回归的损失函数式平方损失， 其实不然。逻辑回归它假设样本服从伯努利分布，进而求得满足该分布的似然函数，接着取对 数求极值等。逻辑回归推导出的经验风险函数是最小化负的似然函数，从损失函数的角度看， 就是 log 损失函数。</p>
<p>  <br><strong>（5）指数损失函数</strong><br>  <br>指数损失函数的标准形式为：</p>
<script type="math/tex; mode=display">
L(Y|f(x))=exp[-yf(x)].</script><p>  <br>例如 AdaBoost 就是以指数损失函数为损失函数。</p>
<p>  <br><strong>（6）Hinge 损失函数</strong><br>  <br>Hinge 损失函数的标准形式如下：</p>
<script type="math/tex; mode=display">
L(y)=max(0, 1-ty).</script><p>  <br>其中 y 是预测值，范围为(-1,1), t 为目标值，其为-1 或 1。<br>  <br>在线性支持向量机中，最优化问题可等价于：</p>
<script type="math/tex; mode=display">
\underset{w,b}{min}\sum_{i=1}^{N}(1-y_i(wx_i+b))+\lambda \lVert w^2 \rVert</script><p>  </p>
<script type="math/tex; mode=display">
\frac{1}{m}\sum_{i=1}^{N}l(wx_i+by_i))+\lVert w^2 \rVert</script><p>  <br>其中$l(wx_i+by_i))$是Hinge损失函数，$\lVert w^2 \rVert$可看做为正则化项。</p>
<h2 id="如何进行特征选择-feature-selection"><a href="#如何进行特征选择-feature-selection" class="headerlink" title="如何进行特征选择(feature selection)?"></a>如何进行特征选择(feature selection)?</h2><h3 id="特征类型有哪些？"><a href="#特征类型有哪些？" class="headerlink" title="特征类型有哪些？"></a>特征类型有哪些？</h3><p>对象本身会有许多属性。所谓特征，即能在某方面最能表征对象的一个或者一组属性。一般地，我们可以把特征分为如下三个类型：</p>
<p>（1）相关特征：对于特定的任务和场景具有一定帮助的属性，这些属性通常能有效提升算法性能；</p>
<p>（2）无关特征：在特定的任务和场景下完全无用的属性，这些属性对对象在本目标环境下完全无用；</p>
<p>（3）冗余特征：同样是在特定的任务和场景下具有一定帮助的属性，但这类属性已过多的存在，不具有产生任何新的信息的能力。</p>
<h3 id="如何考虑特征选择"><a href="#如何考虑特征选择" class="headerlink" title="如何考虑特征选择"></a>如何考虑特征选择</h3><p>当完成数据预处理之后，对特定的场景和目标而言很多维度上的特征都是不具有任何判别或者表征能力的，所以需要对数据在维度上进行筛选。一般地，可以从以下两个方面考虑来选择特征:</p>
<p>（1）特征是否具有发散性：某个特征若在所有样本上的都是一样的或者接近一致，即方差非常小。 也就是说所有样本的都具有一致的表现，那这些就不具有任何信息。</p>
<p>（2）特征与目标的相关性：与目标相关性高的特征，应当优选选择。</p>
<h3 id="特征选择方法分类"><a href="#特征选择方法分类" class="headerlink" title="特征选择方法分类"></a>特征选择方法分类</h3><p>根据特征选择的形式又可以将特征选择方法分为 3 种:<br>（1）过滤法：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。  </p>
<p>（2）包装法：根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。  </p>
<p>（3）嵌入法：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。</p>
<h3 id="特征选择目的"><a href="#特征选择目的" class="headerlink" title="特征选择目的"></a>特征选择目的</h3><p>（1）减少特征维度，使模型泛化能力更强，减少过拟合;   </p>
<p>（2）降低任务目标的学习难度；</p>
<p>（3）一组优秀的特征通常能有效的降低模型复杂度，提升模型效率 </p>
<h2 id="梯度消失-梯度爆炸原因，以及解决方法"><a href="#梯度消失-梯度爆炸原因，以及解决方法" class="headerlink" title="梯度消失/梯度爆炸原因，以及解决方法"></a>梯度消失/梯度爆炸原因，以及解决方法</h2><h3 id="为什么要使用梯度更新规则"><a href="#为什么要使用梯度更新规则" class="headerlink" title="为什么要使用梯度更新规则?"></a>为什么要使用梯度更新规则?</h3><p>目前深度学习的火热，其最大的功臣之一就是反向传播。反向传播，即根据损失评价函数计算的误差，计算得到梯度，通过梯度反向传播的方式，指导深度网络权值的更新优化。这样做的原因在于，深层网络由许多非线性层堆叠而来，每一层非线性层都可以视为是一个非线性函数，因此整个深度网络可以视为是一个复合的非线性多元函数：</p>
<script type="math/tex; mode=display">
F(x)=f_n(\cdots f_3(f_2(f_1(x)*\theta_1+b)*\theta_2+b)\cdots)</script><p>我们最终的目的是希望这个多元函数可以很好的完成输入到输出之间的映射，假设不同的输入，输出的最优解是g(x) ，那么，优化深度网络就是为了寻找到合适的权值，满足 Loss=L(g(x),F(x))取得极小值点，比如最简单的损失函数：</p>
<script type="math/tex; mode=display">
Loss = \lVert g(x)-f(x) \rVert^2_2.</script><p>假设损失函数的数据空间是下图这样的，我们最优的权值就是为了寻找下图中的最小值点， 对于这种数学寻找最小值问题，采用梯度下降的方法再适合不过了。</p>
<p><img src="/zh-TW/ch13_%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/ch13/figure_13_15_1.png" class="lazyload" data-srcset="/zh-TW/ch13_%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/ch13/figure_13_15_1.png" srcset="data:image/png;base64,666" alt=""></p>
<center>图 13.8.1 </center>

<h3 id="梯度消失-爆炸产生的原因"><a href="#梯度消失-爆炸产生的原因" class="headerlink" title="梯度消失/爆炸产生的原因?"></a>梯度消失/爆炸产生的原因?</h3><p>本质上，梯度消失和爆炸是一种情况。在深层网络中，由于网络过深，如果初始得到的梯度过小，或者传播途中在某一层上过小，则在之后的层上得到的梯度会越来越小，即产生了梯度消失。梯度爆炸也是同样的。一般地，不合理的初始化以及激活函数，如sigmoid等，都会导致梯度过大或者过小，从而引起消失/爆炸。</p>
<p>下面分别从网络深度角度以及激活函数角度进行解释：</p>
<p>（1）网络深度 </p>
<p>若在网络很深时，若权重初始化较小，各层上的相乘得到的数值都会0-1之间的小数，而激活函数梯度也是0-1之间的数。那么连乘后，结果数值就会变得非常小，导致<strong>梯度消失</strong>。若权重初始化较大，大到乘以激活函数的导数都大于1，那么连乘后，可能会导致求导的结果很大，形成<strong>梯度爆炸</strong>。</p>
<p>（2）激活函数<br>如果激活函数选择不合适，比如使用 sigmoid，梯度消失就会很明显了，原因看下图，左图是sigmoid的函数图，右边是其导数的图像，如果使用sigmoid作为损失函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失。<br><img src="/zh-TW/ch13_%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/ch13/figure_13_15_2.png" class="lazyload" data-srcset="/zh-TW/ch13_%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/ch13/figure_13_15_2.png" srcset="data:image/png;base64,666" alt=""></p>
<center>图 13.8.2 sigmod函数与其导数</center>

<h3 id="梯度消失、爆炸的解决方案"><a href="#梯度消失、爆炸的解决方案" class="headerlink" title="梯度消失、爆炸的解决方案"></a>梯度消失、爆炸的解决方案</h3><p><strong>1、预训练加微调</strong><br>此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。</p>
<p><strong>2、梯度剪切、正则</strong><br>梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。<br>另外一种解决梯度爆炸的手段是采用权重正则化（weithts regularization）比较常见的是L1和L2正则。</p>
<p><strong>3、ReLu、leakReLu等激活函数</strong><br>（1）ReLu：其函数的导数在正数部分是恒等于1，这样在深层网络中，在激活函数部分就不存在导致梯度过大或者过小的问题，缓解了梯度消失或者爆炸。同时也方便计算。当然，其也存在存在一些缺点，例如过滤到了负数部分，导致部分信息的丢失，输出的数据分布不在以0为中心，改变了数据分布。<br>（2）leakrelu：就是为了解决relu的0区间带来的影响，其数学表达为：leakrelu=max(k*x,0)其中k是leak系数，一般选择0.01或者0.02，或者通过学习而来。</p>
<p><strong>4、batchnorm</strong><br>Batchnorm是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。Batchnorm全名是Batch Normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。  </p>
<p><strong>5、残差结构</strong><br>残差的方式，能使得深层的网络梯度通过跳级连接路径直接返回到浅层部分，使得网络无论多深都能将梯度进行有效的回传。</p>
<p><strong>6、LSTM</strong><br>LSTM全称是长短期记忆网络（long-short term memory networks），是不那么容易发生梯度消失的，主要原因在于LSTM内部复杂的“门”(gates)。在计算时，将过程中的梯度进行了抵消。</p>
<h2 id="深度学习为什么不用二阶优化？"><a href="#深度学习为什么不用二阶优化？" class="headerlink" title="深度学习为什么不用二阶优化？"></a>深度学习为什么不用二阶优化？</h2><p>目前深度学习中，反向传播主要是依靠一阶梯度。二阶梯度在理论和实际上都是可以应用都网络中的，但相比于一阶梯度，二阶优化会存在以下一些主要问题：<br>（1）计算量大，训练非常慢。<br>（2）二阶方法能够更快地求得更高精度的解，这在浅层模型是有益的。而在神经网络这类深层模型中对参数的精度要求不高，甚至不高的精度对模型还有益处，能够提高模型的泛化能力。<br>（3）稳定性。二阶方法能更快求高精度的解，同样对数据本身要的精度也会相应的变高，这就会导致稳定性上的问题。</p>
<h2 id="为什么要设置单一数字评估指标，设置指标的意义？"><a href="#为什么要设置单一数字评估指标，设置指标的意义？" class="headerlink" title="为什么要设置单一数字评估指标，设置指标的意义？"></a>为什么要设置单一数字评估指标，设置指标的意义？</h2><p>在训练模型时，无论是调整超参数，还是调整不同的模型算法，我们都需要一个有效的评价指标，这个评价标准能帮助我们快速了解新的尝试后模型的性能是否更优。例如在分类时，我们通常会选择选择准确率，当样本不平衡时，查准率和查全率又会是更好的评价指标。所以在训练模型时，如果设置了单一数字的评估指标通常能很快的反应出我们模型的改进是否直接产生了收益，从而加速我们的算法改进过程。若在训练过程中，发现优化目标进一步深入，现有指标无法完全反应进一步的目标时，就需要重新选择评估指标了。</p>
<h2 id="训练-验证-测试集的定义及划分"><a href="#训练-验证-测试集的定义及划分" class="headerlink" title="训练/验证/测试集的定义及划分"></a>训练/验证/测试集的定义及划分</h2><p>训练、验证、测试集在机器学习领域是非常重要的三个内容。三者共同组成了整个项目的性能的上限和走向。</p>
<p>训练集：用于模型训练的样本集合，样本占用量是最大的；</p>
<p>验证集：用于训练过程中的模型性能评价，跟着性能评价才能更好的调参；</p>
<p>测试集：用于最终模型的一次最终评价，直接反应了模型的性能。</p>
<p>在划分上，可以分两种情况：</p>
<p>1、在样本量有限的情况下，有时候会把验证集和测试集合并。实际中，若划分为三类，那么训练集：验证集：测试集=6:2:2；若是两类，则训练集：验证集=7:3。这里需要主要在数据量不够多的情况，验证集和测试集需要占的数据比例比较多，以充分了解模型的泛化性。</p>
<p>2、在海量样本的情况下，这种情况在目前深度学习中会比较常见。此时由于数据量巨大，我们不需要将过多的数据用于验证和测试集。例如拥有1百万样本时，我们按训练集：验证集：测试集=98:1:1的比例划分，1%的验证和1%的测试集都已经拥有了1万个样本。这已足够验证模型性能了。</p>
<p>此外，三个数据集的划分不是一次就可以的，若调试过程中发现，三者得到的性能评价差异很大时，可以重新划分以确定是数据集划分的问题导致还是由模型本身导致的。其次，若评价指标发生变化，而导致模型性能差异在三者上很大时，同样可重新划分确认排除数据问题，以方便进一步的优化。</p>
<h2 id="什么是TOP5错误率？"><a href="#什么是TOP5错误率？" class="headerlink" title="什么是TOP5错误率？"></a>什么是TOP5错误率？</h2><p>通常对于分类系统而言，系统会对某个未知样本进行所有已知样本的匹配，并给出该未知样本在每个已知类别上的概率。其中最大的概率就是系统系统判定最可能的一个类别。TOP5则就是在前五个最大概率的类别。TOP5错误率，即预测最可能的五类都不是该样本类别的错误率。  </p>
<p>TOP5错误率通常会用于在类别数量很多或者细粒度类别的模型系统。典型地，例如著名的ImageNet ，其包含了1000个类别。通常就会采用TOP5错误率。</p>
<h2 id="什么是泛化误差，如何理解方差和偏差？"><a href="#什么是泛化误差，如何理解方差和偏差？" class="headerlink" title="什么是泛化误差，如何理解方差和偏差？"></a>什么是泛化误差，如何理解方差和偏差？</h2><p>一般情况下，我们评价模型性能时都会使用泛化误差。泛化误差越低，模型性能越好。泛化误差可分解为方差、偏差和噪声三部分。这三部分中，噪声是个不可控因素，它的存在是算法一直无法解决的问题，很难约减，所以我们更多考虑的是方差和偏差。</p>
<p> 方差和偏差在泛化误差上可做如下分解，假设我们的预测值为g(x)，真实值为f(x)，则均方误差为</p>
<script type="math/tex; mode=display">
E((g(x)−f(x))2)</script><p>这里假设不考虑噪声，g来代表预测值，f代表真实值，g¯=E(g)代表算法的期望预测，则有如下表达：</p>
<script type="math/tex; mode=display">
\begin{align}
E(g-f)^2&=E(g^2-2gf+f^2)
\\&=E(g^2)-\bar g^2+(\bar g-f)^2
\\&=E(g^2)-2\bar g^2+\bar g^2+(\bar g-f)^2
\\&=E(g^2-2g\bar g^2+\bar g^2)+(\bar g-f)^2
\\&=\underbrace{E(g-\bar g)^2}_{var(x)}+\underbrace{(\bar g-f)^2}_{bias^2(x)}
\end{align}</script><p>有上述公式可知，方差描述是理论期望和预测值之间的关系，这里的理论期望通常是指所有适用于模型的各种不同分布类型的数据集；偏差描述为真实值和预测值之间的关系，这里的真实值通常指某一个特定分布的数据集合。</p>
<p>所以综上方差表现为模型在各类分布数据的适应能力，方差越大，说明数据分布越分散，而偏差则表现为在特定分布上的适应能力，偏差越大越偏离真实值。</p>
<h2 id="如何提升模型的稳定性？"><a href="#如何提升模型的稳定性？" class="headerlink" title="如何提升模型的稳定性？"></a>如何提升模型的稳定性？</h2><p>评价模型不仅要从模型的主要指标上的性能，也要注重模型的稳定性。模型的稳定性体现在对不同样本之间的体现的差异。如模型的方差很大，那可以从如下几个方面进行考虑：</p>
<p>（1）正则化（L2, L1, dropout）：模型方差大，很可能来自于过拟合。正则化能有效的降低模型的复杂度，增加对更多分布的适应性。</p>
<p>（2）提前停止训练：提前停止是指模型在验证集上取得不错的性能时停止训练。这种方式本质和正则化是一个道理，能减少方差的同时增加的偏差。目的为了平衡训练集和未知数据之间在模型的表现差异。</p>
<p>（3）扩充训练集：正则化通过控制模型复杂度，来增加更多样本的适应性。那增加训练集让模型适应不同类型的数据本身就是一种最简单直接的方式提升模型稳定的方法，也是最可靠的一种方式。  与正则有所不同的是，扩充数据集既可以减小偏差又能减小方差。</p>
<p>（4）特征选择：过高的特征维度会使模型过拟合，减少特征维度和正则一样可能会处理好方差问题，但是同时会增大偏差。但需要注意的是若过度删减特征，很可能会删除很多有用的特征，降低模型的性能。所以需要多注意删减的特征对模型的性能的影响。</p>
<h2 id="有哪些改善模型的思路"><a href="#有哪些改善模型的思路" class="headerlink" title="有哪些改善模型的思路"></a>有哪些改善模型的思路</h2><p>改善模型本质是如何优化模型，这本身是个很宽泛的问题。也是目前学界一直探索的目的，而从目前常规的手段上来说，一般可取如下几点。</p>
<h3 id="数据角度"><a href="#数据角度" class="headerlink" title="数据角度"></a>数据角度</h3><p>增强数据集。无论是有监督还是无监督学习，数据永远是最重要的驱动力。更多的类型数据对良好的模型能带来更好的稳定性和对未知数据的可预见性。对模型来说，“看到过的总比没看到的更具有判别的信心”。但增大数据并不是盲目的，模型容限能力不高的情况下即使增大数据也对模型毫无意义。而从数据获取的成本角度，对现有数据进行有效的扩充也是个非常有效且实际的方式。良好的数据处理，常见的处理方式如数据缩放、归一化和标准化等。</p>
<h3 id="模型角度"><a href="#模型角度" class="headerlink" title="模型角度"></a>模型角度</h3><p>模型的容限能力决定着模型可优化的空间。在数据量充足的前提下，对同类型的模型，增大模型规模来提升容限无疑是最直接和有效的手段。但越大的参数模型优化也会越难，所以需要在合理的范围内对模型进行参数规模的修改。而不同类型的模型，在不同数据上的优化成本都可能不一样，所以在探索模型时需要尽可能挑选优化简单，训练效率更高的模型进行训练。</p>
<h3 id="调参优化角度"><a href="#调参优化角度" class="headerlink" title="调参优化角度"></a>调参优化角度</h3><p>如果你知道模型的性能为什么不再提高了，那已经向提升性能跨出了一大步。 超参数调整本身是一个比较大的问题。一般可以包含模型初始化的配置，优化算法的选取、学习率的策略以及如何配置正则和损失函数等等。这里需要提出的是对于同一优化算法，相近参数规模的前提下，不同类型的模型总能表现出不同的性能。这实际上就是模型优化成本。从这个角度的反方向来考虑，同一模型也总能找到一种比较适合的优化算法。所以确定了模型后选择一个适合模型的优化算法也是非常重要的手段。</p>
<h3 id="训练角度"><a href="#训练角度" class="headerlink" title="训练角度"></a>训练角度</h3><p>很多时候我们会把优化和训练放一起。但这里我们分开来讲，主要是为了强调充分的训练。在越大规模的数据集或者模型上，诚然一个好的优化算法总能加速收敛。但你在未探索到模型的上限之前，永远不知道训练多久算训练完成。所以在改善模型上充分训练永远是最必要的过程。充分训练的含义不仅仅只是增大训练轮数。有效的学习率衰减和正则同样是充分训练中非常必要的手段。</p>
<h2 id="如何快速构建有效初始模型？"><a href="#如何快速构建有效初始模型？" class="headerlink" title="如何快速构建有效初始模型？"></a>如何快速构建有效初始模型？</h2><p>​    构建一个有效的初始模型能帮助我们快速了解数据的质量和确定模型构建的方向。构建一个良好的初始模型，一般需要注意如下几点：</p>
<p>​    1、了解”对手”。这里的“对手”通常是指数据，我们在得到数据时，第一步是需要了解数据特点和使用场合。了解数据特点能帮助我们快速定位如何进行建模。确定使用场合能帮助我们进一步确定模型需要优化的方向。数据特点一般需要了解例如数据集规模、训练集和验证集是否匹配、样本的分布是否均匀、数据是否存在缺失值等等。</p>
<p>​    2、站在巨人肩膀上。根据数据特点，我们通常能匹配到一个现有比较优秀的模型。这类模型都通常能在类似数据上表现出一个比较不错的性能。</p>
<p>​    3、一切从简。初始模型的作用在于迅速了解数据质量和特点，所以模型的性能通常不需要达到很高，模型复杂度也不需要很高。例如，做图像分类时，我们在使用预训练模型时，不需要一开始就使用例如ResNet152这类模型巨大，复杂度过高的模型。这在数据量较小时，很容易造成过拟合而导致出现我们对数据产生一些误导性的判断，此外也增加了额外训练构建时间。所以使用更小更简单的模型以及损失函数来试探数据是相比更明智的选择。</p>
<p>​    4、总比瞎猜强。构建模型的意义在于建立一个高效的模型，虽然初始模型我们不对性能做过高的要求。但前提在于必须要比随机猜测好，不然构建模型的意义就不存在了。</p>
<p>​    5、解剖模型。一旦确定了一个初始模型时，无论你对该模型多熟悉，当其面对一批新数据时，你永远需要重新去认识这个模型，因为你永远不确定模型内部到底发生了些什么。解剖模型一般需要在训练时注意误差变化、注意训练和验证集的差异；出现一些NAN或者INf等情况时，需要打印观察内部输出，确定问题出现的时间和位置；在完成训练后，需要测试模型的输出是否正确合理，以确认评价指标是否符合该数据场景。无论使用任何一种模型，我们都不能把它当做黑盒去看待。</p>
<h2 id="如何通过模型重新观察数据？"><a href="#如何通过模型重新观察数据？" class="headerlink" title="如何通过模型重新观察数据？"></a>如何通过模型重新观察数据？</h2><p>​    对于这个问题，与其说如何做，倒不如说这个问题是用来强调这样做的重要性。如何重新观察数据其实不难，而是很多读者，会忽略这一项过程的重要性。</p>
<p>​    通过模型重新观察数据，不仅能让我们了解模型情况，也能让我们对数据质量产生进一步的理解。目前深度学习在监督学习领域成就是非常显著的。监督学习需要依赖大量的人为标注，人为标注很难确定是否使用的数据中是否存在错误标注或者漏标注等问题。这无论是哪种情况都会影响我们对模型的判断。所以通过模型重新验证数据质量是非常重要的一步。很多初学者，通常会忽略这一点，而导致出现对模型的一些误判，严重时甚至会影响整个建模方向。此外，对于若出现一些过拟合的情况，我们也可以通过观察来了解模型。例如分类任务，样本严重不平衡时，模型全预测到了一边时，其正确率仍然很高，但显然模型已经出现了问题。</p>
<h2 id="如何解决数据不匹配问题？"><a href="#如何解决数据不匹配问题？" class="headerlink" title="如何解决数据不匹配问题？"></a>如何解决数据不匹配问题？</h2><h3 id="如何定位数据不匹配"><a href="#如何定位数据不匹配" class="headerlink" title="如何定位数据不匹配?"></a>如何定位数据不匹配?</h3><p>​    数据不匹配问题是个不容易定位和解决的问题。这个问题出现总会和模型过拟合表现很相似,即在训练集上能体现非常不错的性能,但在测试集上表现总是差强人意但区别在于如果遇到是数据不匹配的问题,通常在用一批和训<br>练集有看相同或者相似分布的数据上仍然能取得不错的结果。但很多时候,当测试集上结果表现很差时,很多初学<br>者可能会直接将问题定位在模型过拟合上,最后对模型尝试各种方法后,性能却始终不能得到有效提升。当遇到这<br>种情况时,建议先定位出是否存在数据不匹配的问题。最简单的验证方式就是可以从训练集中挑选出一部分数据作<br>为验证集,重新划分后训练和验证模型表现。</p>
<h3 id="举例常见几个数据不匹配的场景"><a href="#举例常见几个数据不匹配的场景" class="headerlink" title="举例常见几个数据不匹配的场景?"></a>举例常见几个数据不匹配的场景?</h3><p>​    例如设计款识别物体的app时,实际场景的图片均来自于手机拍摄,而训练集确是来自于网上各类抓取下来的图<br>片。例如在图像去噪、去模糊、去雾、超分辨率等图像处理场景时,由于大量数据的难以获取,因此都会采用人为<br>假设合成的图像进行训练,这时候应用到实际场景中也容易出现不匹配的问题</p>
<h3 id="如何解决数据不匹配问题"><a href="#如何解决数据不匹配问题" class="headerlink" title="如何解决数据不匹配问题?"></a>如何解决数据不匹配问题?</h3><p>​    数据不匹配是个很难有固定方法来解决的问题。这里提供几条供参考的途径：<br>​    1、收集更多符合实际场最需要的数据。这似乎是最简单但也最难方式<br>​    2、对结果做错误分析。找出数据集中出错的数据和正确数据之间的特点和区别,这对你无论是进行后续模型的分析或者是数据的处理提供非常有效的思路。注意,这里的数据集包括训练集和测试集<br>​    3、数据集增强。数据集增强并不意味看数据集越大越好,其目的是丰富数据的分布以适应更多的变化当遇到数<br>据不匹配时,对数据处理般可以有两种方式。其一,合成或处理更多接近需要的数据特点。其二,对所有数据包<br>括实际场景数据都进行处理,将所有数据都统一到另一个分布上,统一出一种新的特点。</p>
<h3 id="如何提高深度学习系统的性能"><a href="#如何提高深度学习系统的性能" class="headerlink" title="如何提高深度学习系统的性能"></a>如何提高深度学习系统的性能</h3><p>​    当我们要试图提高深度学习系统的性能时，目前我们大致可以从三方面考虑：</p>
<p>​    1、提高模型的结构，比如增加神经网络的层数，或者将简单的神经元单位换成复杂的 LSTM 神经元，比如在自然语言处理领域内，利用 LSTM 模型挖掘语法分析的优势。</p>
<p>​    2、改进模型的初始化方式，保证早期梯度具有某些有益的性质，或者具备大量的稀疏性，或者利用线性代数原理的优势。  </p>
<p>​    3、选择更强大的学习算法，比如对度梯度更新的方式，也可以是采用除以先前梯度 L2 范数来更新所有参数，甚至还可以选用计算代价较大的二阶算法。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>图像分割</title>
    <url>/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h1><h2 id="图像分割算法分类？"><a href="#图像分割算法分类？" class="headerlink" title="图像分割算法分类？"></a>图像分割算法分类？</h2><p>图像分割是预测图像中每一个像素所属的类别或者物体。基于深度学习的图像分割算法主要分为两类：</p>
<p><strong>1.语义分割</strong></p>
<p>为图像中的每个像素分配一个类别，如把画面中的所有物体都指出它们各自的类别。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/Semantic-01.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/Semantic-01.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>2.实例分割</strong></p>
<p>与语义分割不同，实例分割只对特定物体进行类别分配，这一点与目标检测有点相似，但目标检测输出的是边界框和类别，而实例分割输出的是掩膜（mask）和类别。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/Instance-01.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/Instance-01.png" srcset="data:image/png;base64,666" alt=""></p>
<h2 id="传统的基于CNN的分割方法缺点？"><a href="#传统的基于CNN的分割方法缺点？" class="headerlink" title="传统的基于CNN的分割方法缺点？"></a>传统的基于CNN的分割方法缺点？</h2><p>传统的基于CNN的分割方法：为了对一个像素分类，使用该像素周围的一个图像块作为CNN的输入，用于训练与预测，这种方法主要有几个缺点：<br>1）存储开销大，例如，对每个像素使用15 * 15的图像块，然后不断滑动窗口，将图像块输入到CNN中进行类别判断，因此，需要的存储空间随滑动窗口的次数和大小急剧上升；<br>2）效率低下，相邻像素块基本上是重复的，针对每个像素块逐个计算卷积，这种计算有很大程度上的重复；<br>3）像素块的大小限制了感受区域的大小，通常像素块的大小比整幅图像的大小小很多，只能提取一些局部特征，从而导致分类性能受到限制。<br>而全卷积网络(FCN)则是从抽象的特征中恢复出每个像素所属的类别。即从图像级别的分类进一步延伸到像素级别的分类。</p>
<h2 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h2><h3 id="FCN改变了什么"><a href="#FCN改变了什么" class="headerlink" title="FCN改变了什么?"></a>FCN改变了什么?</h3><p>​    对于一般的分类CNN网络，如VGG和Resnet，都会在网络的最后加入一些全连接层，经过softmax后就可以获得类别概率信息。但是这个概率信息是1维的，即只能标识整个图片的类别，不能标识每个像素点的类别，所以这种全连接方法不适用于图像分割。<br>​    而FCN提出可以把后面几个全连接都换成卷积，这样就可以获得一张2维的feature map，后接softmax层获得每个像素点的分类信息，从而解决了分割问题，如图4。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.1_2.jpg" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.1_2.jpg" srcset="data:image/png;base64,666" alt=""></p>
<center>图 4</center>

<h3 id="FCN网络结构？"><a href="#FCN网络结构？" class="headerlink" title="FCN网络结构？"></a>FCN网络结构？</h3><p>​    FCN对图像进行像素级的分类，从而解决了语义级别的图像分割（semantic segmentation）问题。与经典的CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类（全联接层＋softmax输出）不同，FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。<br>​    下图是语义分割所采用的全卷积网络(FCN)的结构示意图：</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.2_1.jpg" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.2_1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="全卷积网络举例？"><a href="#全卷积网络举例？" class="headerlink" title="全卷积网络举例？"></a>全卷积网络举例？</h3><p>​    通常CNN网络在卷积层之后会接上若干个全连接层, 将卷积层产生的特征图(feature map)映射成一个固定长度的特征向量。以AlexNet为代表的经典CNN结构适合于图像级的分类和回归任务，因为它们最后都得到整个输入图像的一个概率向量。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.3_1.jpg" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.3_1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>  <br>如上图所示：<br>  <br>（1）在CNN中, 猫的图片输入到AlexNet, 得到一个长为1000的输出向量, 表示输入图像属于每一类的概率, 其中在“tabby cat”这一类统计概率最高, 用来做分类任务。<br>  <br>（2）FCN与CNN的区别在于把CNN最后的全连接层转换成卷积层，输出的是一张已经带有标签的图片, 而这个图片就可以做语义分割。<br>  <br>（3）CNN的强大之处在于它的多层结构能自动学习特征，并且可以学习到多个层次的特征: 较浅的卷积层感知域较小，学习到一些局部区域的特征；较深的卷积层具有较大的感知域，能够学习到更加抽象一些的特征。高层的抽象特征对物体的大小、位置和方向等敏感性更低，从而有助于识别性能的提高, 所以我们常常可以将卷积层看作是特征提取器。</p>
<h3 id="全连接层和卷积层如何相互转化？"><a href="#全连接层和卷积层如何相互转化？" class="headerlink" title="全连接层和卷积层如何相互转化？"></a>全连接层和卷积层如何相互转化？</h3><p>  <br><strong>两者相互转换的可能性：</strong><br>  <br>全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：<br>  <br>（1）对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块，其余部分都是零。而在其中大部分块中，元素都是相等的。<br>  <br>（2）任何全连接层都可以被转化为卷积层。比如VGG16中第一个全连接层是25088 <em> 4096的数据尺寸，将它转化为512 </em> 7 <em> 7 </em> 4096的数据尺寸，即一个K=4096的全连接层，输入数据体的尺寸是7 <em> 7 </em> 512，这个全连接层可以被等效地看做一个F=7, P=0, S=1, K=4096 的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致7 <em> 7, 这样输出就变为1 </em> 1 <em> 4096, 本质上和全连接层的输出是一样的。<br>  <br><em>*输出激活数据体深度是由卷积核的数目决定的(K=4096)。</em></em><br>  <br>在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是227x227x3的图像，一系列的卷积层和下采样层将图像数据变为尺寸为7x7x512的激活数据体, AlexNet的处理方式为使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：<br>  <br>（1）第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7,K=4096，这样输出数据体就为[1x1x4096]。<br>  <br>（2）第二个全连接层，令其滤波器尺寸为F=1,K=4096，这样输出数据体为[1x1x4096]。<br>  <br>（3）最后一个全连接层也做类似的，令其F=1,K=1000，最终输出为[1x1x1000]。</p>
<h3 id="为什么传统CNN的输入图片是固定大小？"><a href="#为什么传统CNN的输入图片是固定大小？" class="headerlink" title="为什么传统CNN的输入图片是固定大小？"></a>为什么传统CNN的输入图片是固定大小？</h3><p>  <br>对于CNN，一幅输入图片在经过卷积和pooling层时，这些层是不关心图片大小的。比如对于一个卷积层，outputsize = (inputsize - kernelsize) / stride + 1，它并不关心inputsize多大，对于一个inputsize大小的输入feature map，滑窗卷积，输出outputsize大小的feature map即可。pooling层同理。但是在进入全连接层时，feature map（假设大小为n×n）要拉成一条向量，而向量中每个元素（共n×n个）作为一个结点都要与下一个层的所有结点（假设4096个）全连接，这里的权值个数是4096×n×n，而我们知道神经网络结构一旦确定，它的权值个数都是固定的，所以这个n不能变化，n是conv5的outputsize，所以层层向回看，每个outputsize都要固定，那每个inputsize都要固定，因此输入图片大小要固定。</p>
<h3 id="把全连接层的权重W重塑成卷积层的滤波器有什么好处？"><a href="#把全连接层的权重W重塑成卷积层的滤波器有什么好处？" class="headerlink" title="把全连接层的权重W重塑成卷积层的滤波器有什么好处？"></a>把全连接层的权重W重塑成卷积层的滤波器有什么好处？</h3><p>  <br>这样的转化可以在单个向前传播的过程中, 使得卷积网络在一张更大的输入图片上滑动，从而得到多个输出(可以理解为一个label map)。<br>  <br>比如: 我们想让224×224尺寸的浮窗，以步长为32在384×384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6×6个位置的类别得分, 那么通过将全连接层转化为卷积层之后的运算过程为:<br>  <br>如果224×224的输入图片经过卷积层和下采样层之后得到了[7x7x512]的数组，那么，384×384的大图片直接经过同样的卷积层和下采样层之后会得到[12x12x512]的数组, 然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出((12 – 7)/1 + 1 = 6), 这个结果正是浮窗在原图经停的6×6个位置的得分。<br>  <br>一个确定的CNN网络结构之所以要固定输入图片大小，是因为全连接层权值数固定，而该权值数和feature map大小有关, 但是FCN在CNN的基础上把1000个结点的全连接层改为含有1000个1×1卷积核的卷积层，经过这一层，还是得到二维的feature map，同样我们也不关心这个feature map大小, 所以对于输入图片的size并没有限制。<br>  <br>如下图所示，FCN将传统CNN中的全连接层转化成卷积层，对应CNN网络FCN把最后三层全连接层转换成为三层卷积层:</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.7_1.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.7_1.png" srcset="data:image/png;base64,666" alt=""></p>
<center>一个分类网络</center>
![](./img/ch9/figure_9.1.7_2.png)

<center>变为全卷积网络</center>
![](./img/ch9/figure_9.1.7_3.png)

<center>End-to-end, pixels-to pixels网络</center>
![](./img/ch9/figure_9.1.7_4.jpg)



（1）全连接层转化为全卷积层 : 在传统的CNN结构中，前5层是卷积层，第6层和第7层分别是一个长度为4096的一维向量，第8层是长度为1000的一维向量，分别对应1000个不同类别的概率。FCN将这3层表示为卷积层，卷积核的大小 (通道数，宽，高) 分别为 (4096,1,1)、(4096,1,1)、(1000,1,1)。看上去数字上并没有什么差别，但是卷积跟全连接是不一样的概念和计算过程，使用的是之前CNN已经训练好的权值和偏置，但是不一样的在于权值和偏置是有自己的范围，属于自己的一个卷积核。  
  
（2）CNN中输入的图像大小是统一固定成227x227大小的图像，第一层pooling后为55x55，第二层pooling后图像大小为27x27，第五层pooling后的图像大小为13x13, 而FCN输入的图像是H * W大小，第一层pooling后变为原图大小的1/2，第二层变为原图大小的1/4，第五层变为原图大小的1/8，第八层变为原图大小的1/16。  
  
（3）经过多次卷积和pooling以后，得到的图像越来越小，分辨率越来越低。其中图像到H/32 * W/32的时候图片是最小的一层时，所产生图叫做heatmap热图，热图就是我们最重要的高维特征图，得到高维特征的heatmap之后就是最重要的一步也是最后的一步对原图像进行upsampling，把图像进行放大几次到原图像的大小。  
  
相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算优化模型，然后再对36个位置做预测，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。

### 反卷积层理解

  
Upsampling的操作可以看成是反卷积(deconvolutional)，卷积运算的参数和CNN的参数一样是在训练FCN模型的过程中通过bp算法学习得到。反卷积层也是卷积层，不关心input大小，滑窗卷积后输出output。deconv并不是真正的deconvolution（卷积的逆变换），最近比较公认的叫法应该是transposed convolution，deconv的前向传播就是conv的反向传播。  
  
反卷积参数: 利用卷积过程filter的转置（实际上就是水平和竖直方向上翻转filter）作为计算卷积前的特征图。  
  
反卷积的运算如下所示:  
  
蓝色是反卷积层的input，绿色是反卷积层的outputFull padding, transposed Full padding, transposed。

![](./img/ch9/figure_9.1.8_1.png)

<center>上图中的反卷积，input是2×2, output是4×4。     Zero padding, non-unit strides, transposed。</center>
![](./img/ch9/figure_9.1.8_2.png)
<center>上图中的反卷积，input feature map是3×3, 转化后是5×5, output是5×5</center>

<h3 id="跳级-skip-结构"><a href="#跳级-skip-结构" class="headerlink" title="跳级(skip)结构"></a>跳级(skip)结构</h3><p>  <br>对CNN的结果做处理，得到了dense prediction，而作者在试验中发现，得到的分割结果比较粗糙，所以考虑加入更多前层的细节信息，也就是把倒数第几层的输出和最后的输出做一个fusion，实际上也就是加和：</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.9_1.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.9_1.png" srcset="data:image/png;base64,666" alt=""><br>  <br>实验表明，这样的分割结果更细致更准确。在逐层fusion的过程中，做到第三行再往下，结果又会变差，所以作者做到这里就停了。</p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>  <br>（1）用AlexNet，VGG16或者GoogleNet训练好的模型做初始化，在这个基础上做fine-tuning，全部都fine-tuning，只需在末尾加上upsampling，参数的学习还是利用CNN本身的反向传播原理。<br>  <br>（2）采用whole image做训练，不进行patchwise sampling。实验证明直接用全图已经很effective and efficient。<br>  <br>（3）对class score的卷积层做全零初始化。随机初始化在性能和收敛上没有优势。<br><em>举例：</em><br>  <br><em>FCN例子: 输入可为任意尺寸图像彩色图像；输出与输入尺寸相同，深度为：20类目标+背景=21，模型基于AlexNet。</em><br>  <br><em>蓝色：卷积层。</em><br>  <br><em>绿色：Max Pooling层。</em><br>  <br><em>黄色: 求和运算, 使用逐数据相加，把三个不同深度的预测结果进行融合：较浅的结果更为精细，较深的结果更为鲁棒。</em><br>  <br><em>灰色: 裁剪, 在融合之前，使用裁剪层统一两者大小, 最后裁剪成和输入相同尺寸输出。</em><br>  <br><em>对于不同尺寸的输入图像，各层数据的尺寸（height，width）相应变化，深度（channel）不变。</em></p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.10_1.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.10_1.png" srcset="data:image/png;base64,666" alt=""><br>  <br>（1）全卷积层部分进行特征提取, 提取卷积层（3个蓝色层）的输出来作为预测21个类别的特征。</p>
<p>  <br>（2）图中虚线内是反卷积层的运算, 反卷积层（3个橙色层）可以把输入数据尺寸放大。和卷积层一样，升采样的具体参数经过训练确定。    </p>
<p>    <br>1) 以经典的AlexNet分类网络为初始化。最后两级是全连接（红色），参数弃去不用。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.10_2.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.10_2.png" srcset="data:image/png;base64,666" alt=""><br>    <br>2) 从特征小图（）预测分割小图（），之后直接升采样为大图。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.10_3.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.10_3.png" srcset="data:image/png;base64,666" alt=""></p>
<center>反卷积（橙色）的步长为32，这个网络称为FCN-32s</center>  
    
3) 升采样分为两次完成（橙色×2）, 在第二次升采样前，把第4个pooling层（绿色）的预测结果（蓝色）融合进来。使用跳级结构提升精确性。

![](./img/ch9/figure_9.1.10_4.png)

<center>第二次反卷积步长为16，这个网络称为FCN-16s</center>
    
4) 升采样分为三次完成（橙色×3）, 进一步融合了第3个pooling层的预测结果。

![](./img/ch9/figure_9.1.10_5.png)

<center>第三次反卷积步长为8，记为FCN-8s</center>

<p>其他参数:<br>  <br>minibatch：20张图片。<br>  <br>learning rate：0.001。<br>  <br>初始化：分类网络之外的卷积层参数初始化为0。<br>  <br>反卷积参数初始化为bilinear插值。<br>  <br>最后一层反卷积固定位bilinear插值不做学习。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.10_6.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.1.10_6.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="FCN缺点"><a href="#FCN缺点" class="headerlink" title="FCN缺点"></a>FCN缺点</h3><p>  <br>（1）得到的结果还是不够精细。进行8倍上采样虽然比32倍的效果好了很多，但是上采样的结果还是比较模糊和平滑，对图像中的细节不敏感。<br>  <br>（2）对各个像素进行分类，没有充分考虑像素与像素之间的关系。忽略了在通常的基于像素分类的分割方法中使用的空间规整（spatial regularization）步骤，缺乏空间一致性。</p>
<h2 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h2><p>  <br>卷积网络被大规模应用在分类任务中，输出的结果是整个图像的类标签。然而，在许多视觉任务，尤其是生物医学图像处理领域，目标输出应该包括目标类别的位置，并且每个像素都应该有类标签。另外，在生物医学图像往往缺少训练图片。所以，Ciresan等人训练了一个卷积神经网络，用滑动窗口提供像素的周围区域（patch）作为输入来预测每个像素的类标签。这个网络有两个优点：<br>第一，输出结果可以定位出目标类别的位置；<br>第二，由于输入的训练数据是patches，这样就相当于进行了数据增广，解决了生物医学图像数量少的问题。<br>  <br>但是，这个方法也有两个很明显缺点。<br>  <br>第一，它很慢，因为这个网络必须训练每个patch，并且因为patch间的重叠有很多的冗余(冗余会造成什么影响呢？卷积核里面的W，就是提取特征的权重，两个块如果重叠的部分太多，这个权重会被同一些特征训练两次，造成资源的浪费，减慢训练时间和效率，虽然说会有一些冗余，训练集大了，准确率不就高了吗？可是你这个是相同的图片啊，重叠的东西都是相同的，举个例子，我用一张相同的图片训练20次，按照这个意思也是增大了训练集啊，可是会出现什么结果呢，很显然，会导致过拟合，也就是对你这个图片识别很准，别的图片就不一定了)。<br>  <br>第二，定位准确性和获取上下文信息不可兼得。大的patches需要更多的max-pooling层这样减小了定位准确性(为什么？因为你是对以这个像素为中心的点进行分类，如果patch太大，最后经过全连接层的前一层大小肯定是不变的，如果你patch大就需要更多的pooling达到这个大小，而pooling层越多，丢失信息的信息也越多；小的patches只能看到很小的局部信息，包含的背景信息不够。<br>  <br>这篇论文建立了一个更好全卷积方法。我们定义和扩展了这个方法它使用更少的训练图片但产生更精确的分割。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.2_1.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.2_1.png" srcset="data:image/png;base64,666" alt="">   </p>
<p>  <br>(1)    使用全卷积神经网络。(全卷积神经网络就是卷积取代了全连接层，全连接层必须固定图像大小而卷积不用，所以这个策略使得，你可以输入任意尺寸的图片，而且输出也是图片，所以这是一个端到端的网络。)<br>  <br>(2)    左边的网络是收缩路径：使用卷积和maxpooling。<br>  <br>(3)    右边的网络是扩张路径:使用上采样产生的特征图与左侧收缩路径对应层产生的特征图进行concatenate操作。（pooling层会丢失图像信息和降低图像分辨率且是不可逆的操作，对图像分割任务有一些影响，对图像分类任务的影响不大，为什么要做上采样？因为上采样可以补足一些图片的信息，但是信息补充的肯定不完全，所以还需要与左边的分辨率比较高的图片相连接起来（直接复制过来再裁剪到与上采样图片一样大小），这就相当于在高分辨率和更抽象特征当中做一个折衷，因为随着卷积次数增多，提取的特征也更加有效，更加抽象，上采样的图片是经历多次卷积后的图片，肯定是比较高效和抽象的图片，然后把它与左边不怎么抽象但更高分辨率的特征图片进行连接）。<br>  <br>(4)    最后再经过两次反卷积操作，生成特征图，再用两个1X1的卷积做分类得到最后的两张heatmap,例如第一张表示的是第一类的得分，第二张表示第二类的得分heatmap,然后作为softmax函数的输入，算出概率比较大的softmax类，选择它作为输入给交叉熵进行反向传播训练。</p>
<p>下面是U-Net模型的代码实现：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">def get_unet():</span><br><span class="line">    inputs = Input((img_rows, img_cols, 1))</span><br><span class="line">    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)</span><br><span class="line">    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)</span><br><span class="line">    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)</span><br><span class="line">    # pool1 = Dropout(0.25)(pool1)</span><br><span class="line">    # pool1 = BatchNormalization()(pool1)</span><br><span class="line"></span><br><span class="line">    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)</span><br><span class="line">    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)</span><br><span class="line">    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)</span><br><span class="line">    # pool2 = Dropout(0.5)(pool2)</span><br><span class="line">    # pool2 = BatchNormalization()(pool2)</span><br><span class="line"></span><br><span class="line">    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)</span><br><span class="line">    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)</span><br><span class="line">    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)</span><br><span class="line">    # pool3 = Dropout(0.5)(pool3)</span><br><span class="line">    # pool3 = BatchNormalization()(pool3)</span><br><span class="line"></span><br><span class="line">    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)</span><br><span class="line">    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)</span><br><span class="line">    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)</span><br><span class="line">    # pool4 = Dropout(0.5)(pool4)</span><br><span class="line">    # pool4 = BatchNormalization()(pool4)</span><br><span class="line"></span><br><span class="line">    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)</span><br><span class="line">    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)</span><br><span class="line"></span><br><span class="line">    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(</span><br><span class="line">        2, 2), padding='same')(conv5), conv4], axis=3)</span><br><span class="line">    # up6 = Dropout(0.5)(up6)</span><br><span class="line">    # up6 = BatchNormalization()(up6)</span><br><span class="line">    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)</span><br><span class="line">    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)</span><br><span class="line"></span><br><span class="line">    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(</span><br><span class="line">        2, 2), padding='same')(conv6), conv3], axis=3)</span><br><span class="line">    # up7 = Dropout(0.5)(up7)</span><br><span class="line">    # up7 = BatchNormalization()(up7)</span><br><span class="line">    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)</span><br><span class="line">    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)</span><br><span class="line"></span><br><span class="line">    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(</span><br><span class="line">        2, 2), padding='same')(conv7), conv2], axis=3)</span><br><span class="line">    # up8 = Dropout(0.5)(up8)</span><br><span class="line">    # up8 = BatchNormalization()(up8)</span><br><span class="line">    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)</span><br><span class="line">    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)</span><br><span class="line"></span><br><span class="line">    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(</span><br><span class="line">        2, 2), padding='same')(conv8), conv1], axis=3)</span><br><span class="line">    # up9 = Dropout(0.5)(up9)</span><br><span class="line">    # up9 = BatchNormalization()(up9)</span><br><span class="line">    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)</span><br><span class="line">    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)</span><br><span class="line"></span><br><span class="line">    # conv9 = Dropout(0.5)(conv9)</span><br><span class="line"></span><br><span class="line">    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=[inputs], outputs=[conv10])</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=Adam(lr=1e-5),</span><br><span class="line">                  loss=dice_coef_loss, metrics=[dice_coef])</span><br><span class="line"></span><br><span class="line">    return model</span><br></pre></td></tr></tbody></table></figure>
<h2 id="SegNet"><a href="#SegNet" class="headerlink" title="SegNet"></a>SegNet</h2><p>  <br>可训练的图像分割引擎，包含一个encoder网络，一个对应的decoder网络，衔接像素级分类层，解码网络与VGG16的13层卷积层相同。解码网络是将低分辨率的编码特征图映射到全分辨率的特征图。解码网络使用最大池化层的池化索引进行非线性上采样，上采样过程就不需要学习。上采样得到的稀疏图与可训练的滤波器卷积得到致密的特征图。<br>  <br>使用池化层索引进行上采样的优势：<br>  <br>1）提升边缘刻画度；<br>  <br>2）减少训练的参数；<br>  <br>3）这种上采样模式可以包含到任何编码-解码网络中。<br>  <br>SegNet网络的结构如下图所示：</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.3_1.jpg" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.3_1.jpg" srcset="data:image/png;base64,666" alt="">  </p>
<p>  <br>SegNet网络结构如图1所示，Input为输入图片，Output为输出分割的图像，不同颜色代表不同的分类。语义分割的重要性就在于不仅告诉你图片中某个东西是什么，而且告知你他在图片的位置。我们可以看到是一个对称网络，由中间绿色pooling层与红色upsampling层作为分割，左边是卷积提取高维特征，并通过pooling使图片变小，SegNet作者称为Encoder，右边是反卷积（在这里反卷积与卷积没有区别）与upsampling，通过反卷积使得图像分类后特征得以重现，upsampling使图像变大，SegNet作者称为Decoder，最后通过Softmax，输出不同分类的最大值。这就是大致的SegNet过程，下面对这个过程里面使用到的方法进行介绍。<br>  <br>编码网络与滤波器族卷积得到特征图，进行BN，ReLU，最大池化。最大池化是为了获得空间小位移的平移不变。最大池化和下采样损失了边缘细节，因此，在编码过程中保存边缘信息很重要。考虑到内存原因，只保存最大池化索引，如最大特征值的位置。<br>  <br>SegNet解码技术如下图所示：</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.3_2.jpg" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.3_2.jpg" srcset="data:image/png;base64,666" alt="">  </p>
<p>  <br>解码网络使用保存的最大池化索引上采样，得到稀疏的特征图，将特征图与可训练的解码滤波器族卷积得到致密的特征图。之后进行BN。高维的特征图输入soft-max层，对每个像素进行分类，得到每个像素属于K类的概率。  图3中右边是FCN的解码技术，FCN对编码的特征图进行降维，降维后输入到解码网络，解码网络中，上采样使用反卷积实现，上采样的特征图与降维的编码图进行element-wise add得到最终的解码特征图。FCN解码模型需要存储编码特征图，在嵌入式设备中内存紧张。<br>  <br>SegNet的Encoder过程中，卷积的作用是提取特征，SegNet使用的卷积为same卷积（详见卷积神经网络CNN（1）)，即卷积后不改变图片大小；在Decoder过程中，同样使用same卷积，不过卷积的作用是为upsampling变大的图像丰富信息，使得在Pooling过程丢失的信息可以通过学习在Decoder得到。SegNet中的卷积与传统CNN的卷积并没有区别。</p>
<h2 id="空洞卷积-Dilated-Convolutions"><a href="#空洞卷积-Dilated-Convolutions" class="headerlink" title="空洞卷积(Dilated Convolutions)"></a>空洞卷积(Dilated Convolutions)</h2><p>  <br>在图像分割领域，图像输入到CNN（典型的网络比如FCN[3]）中，FCN先像传统的CNN那样对图像做卷积再pooling，降低图像尺寸的同时增大感受野，但是由于图像分割预测是pixel-wise的输出，所以要将pooling后较小的图像尺寸upsampling到原始的图像尺寸进行预测（upsampling一般采用deconv反卷积操作，deconv可参见知乎答案如何理解深度学习中的deconvolution networks？），之前的pooling操作使得每个pixel预测都能看到较大感受野信息。因此图像分割FCN中有两个关键，一个是pooling减小图像尺寸增大感受野，另一个是upsampling扩大图像尺寸。在先减小再增大尺寸的过程中，肯定有一些信息损失掉了，那么能不能设计一种新的操作，不通过pooling也能有较大的感受野看到更多的信息呢？答案就是dilated conv。<br>  <br>以前的CNN主要问题总结：<br>  <br>（1）Up-sampling / pooling layer<br>  <br>（2）内部数据结构丢失；空间层级化信息丢失。<br>  <br>（3）小物体信息无法重建 (假设有四个pooling layer 则 任何小于 2^4 = 16 pixel 的物体信息将理论上无法重建。)<br>  <br>举例如下：</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.3_3.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.3_3.png" srcset="data:image/png;base64,666" alt=""></p>
<p></p><center>Dilated Convolution with a 3 x 3 kernel and dilation rate 2</center><br>  <br>下面看一下dilated conv原始论文[4]中的示意图<p></p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.3_4.jpg" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.3_4.jpg" srcset="data:image/png;base64,666" alt="">  </p>
<p>  <br>(a)    图对应3x3的1-dilated conv，和普通的卷积操作一样，(b)图对应3x3的2-dilated conv，实际的卷积kernel size还是3x3，但是空洞为1，也就是对于一个7x7的图像patch，只有9个红色的点和3x3的kernel发生卷积操作，其余的点略过。也可以理解为kernel的size为7x7，但是只有图中的9个点的权重不为0，其余都为0。 可以看到虽然kernel size只有3x3，但是这个卷积的感受野已经增大到了7x7（如果考虑到这个2-dilated conv的前一层是一个1-dilated conv的话，那么每个红点就是1-dilated的卷积输出，所以感受野为3x3，所以1-dilated和2-dilated合起来就能达到7x7的conv）,(c)图是4-dilated conv操作，同理跟在两个1-dilated和2-dilated conv的后面，能达到15x15的感受野。对比传统的conv操作，3层3x3的卷积加起来，stride为1的话，只能达到(kernel-1) * layer+1=7的感受野，也就是和层数layer成线性关系，而dilated conv的感受野是指数级的增长。<br>  <br>dilated的好处是不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。在图像需要全局信息或者语音文本需要较长的sequence信息依赖的问题中，都能很好的应用dilated conv，比如图像分割、语音合成WaveNet、机器翻译ByteNet中。</p>
<h2 id="RefineNet"><a href="#RefineNet" class="headerlink" title="RefineNet"></a>RefineNet</h2><p>  <br>网络结构：<br>  <br>RefineNet block的作用就是把不同resolution level的feature map进行融合。网络结构如下：</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.4_1.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.4_1.png" srcset="data:image/png;base64,666" alt=""><br>  <br>最左边一栏就是FCN的encoder部分(文中是用的ResNet)，先把pretrained ResNet按feature map的分辨率分成四个ResNet blocks，然后向右把四个blocks分别作为4个path通过RefineNet block进行融合refine，最后得到一个refined feature map(接softmax再双线性插值输出)。<br>注意除了RefineNet-4，所有的RefineNet block都是二输入的，用于融合不同level做refine，而单输入的RefineNet-4可以看作是先对ResNet的一个task adaptation。  </p>
<p>  <br><strong>RefineNet Block</strong><br>  <br>接下来仔细看一下RefineNet block，可以看到主要组成部分是Residual convolution unit, Multi-resolution fusion, Chained residual pooling, Output convolutions. 切记这个block作用是融合多个level的feature map输出单个level的feature map，但具体的实现应该是和输入个数、shape无关的。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.4_2.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.4_2.png" srcset="data:image/png;base64,666" alt=""> </p>
<p>  <br>Residual convolution unit就是普通的去除了BN的residual unit；  </p>
<p>  <br>Multi-resolution fusion是先对多输入的feature map都用一个卷积层进行adaptation(都化到最小的feature map的shape)，再上采样再做element-wise的相加。注意如果是像RefineNet-4那样的单输入block这一部分就直接pass了；</p>
<p>  <br>Chained residual pooling中的ReLU对接下来池化的有效性很重要，还可以使模型对学习率的变化没这么敏感。这个链式结构能从很大范围区域上获取背景context。另外，这个结构中大量使用了identity mapping这样的连接，无论长距离或者短距离的，这样的结构允许梯度从一个block直接向其他任一block传播。</p>
<p>  <br>Output convolutions就是输出前再加一个RCU。</p>
<h2 id="PSPNet"><a href="#PSPNet" class="headerlink" title="PSPNet"></a>PSPNet</h2><p>  <br>场景解析对于无限制的开放词汇和不同场景来说是具有挑战性的.本文使用文中的pyramid pooling module实现基于不同区域的上下文集成，提出了PSPNet，实现利用上下文信息的能力进行场景解析。<br>  <br>作者认为，FCN存在的主要问题是没有采取合适的策略来用全局的信息，本文的做法就是借鉴SPPNet来设计了PSPNet解决这个问题。<br>  <br>很多State-of-the-art的场景解析框架都是基于FCN的.基于CNN的方法能够增强动态物体的理解，但是在无限制词汇和不同场景中仍然面临挑战.举个例子，如下图.</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_1.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_1.png" srcset="data:image/png;base64,666" alt=""><br>  <br>FCN认为右侧框中是汽车，但是实际上是船，如果参考上下文的先验知识，就会发现左边是一个船屋，进而推断是框中是船.FCN存在的主要问题就是不能利用好全局的场景线索。  </p>
<p>  <br>对于尤其复杂的场景理解，之前都是采用空间金字塔池化来做的，和之前方法不同（为什么不同，需要参考一下经典的金字塔算法），本文提出了pyramid scene parsing network(PSPNet)。<br>  <br>本文的主要贡献如下:<br>  <br>(1)    提出了PSPNet在基于FCN的框架中集成困难的上下文特征<br>  <br>(2)    通过基于深度监督误差开发了针对ResNet的高效优化策略<br>  <br>(3)    构建了一个用于state-of-the-art的场景解析和语义分割的实践系统（具体是什么？）<br>  <br>通过观察FCN的结果，发现了如下问题：<br>  <br>(1)    关系不匹配（Mismatched Relationship）<br>  <br>(2)    易混淆的类别（Confusion Categories）<br>  <br>(3)    不显眼的类别（Inconspicuous Classes）<br>  <br>总结以上结果发现，以上问题部分或者全部与上下文关系和全局信息有关系，因此本文提出了PSPNet.框架如下:</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_2.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_2.png" srcset="data:image/png;base64,666" alt=""><br>  <br>并且加入额外的深度监督 Loss</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_3.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_3.png" srcset="data:image/png;base64,666" alt=""></p>
<h2 id="DeepLab系列"><a href="#DeepLab系列" class="headerlink" title="DeepLab系列"></a>DeepLab系列</h2><h3 id="DeepLabv1"><a href="#DeepLabv1" class="headerlink" title="DeepLabv1"></a>DeepLabv1</h3><p>  <br>DeepLab 是结合了深度卷积神经网络（DCNNs）和概率图模型（DenseCRFs）的方法。<br>  <br>在实验中发现 DCNNs 做语义分割时精准度不够的问题，根本原因是 DCNNs 的高级特征的平移不变性，即高层次特征映射，根源于重复的池化和下采样。<br>  <br>针对信号下采样或池化降低分辨率，DeepLab 是采用的 atrous（带孔）算法扩展感受野，获取更多的上下文信息。<br>  <br>分类器获取以对象中心的决策是需要空间变换的不变性，这天然地限制了 DCNN 的定位精度，DeepLab 采用完全连接的条件随机场（CRF）提高模型捕获细节的能力。<br>  <br>除空洞卷积和 CRFs 之外，论文使用的 tricks 还有 Multi-Scale features。其实就是 U-Net 和 FPN 的思想，在输入图像和前四个最大池化层的输出上附加了两层的 MLP，第一层是 128 个 3×3 卷积，第二层是 128 个 1×1 卷积。最终输出的特征与主干网的最后一层特征图融合，特征图增加 5×128=640 个通道。<br>  <br>实验表示多尺度有助于提升预测结果，但是效果不如 CRF 明显。<br>  <br>论文模型基于 VGG16，在 Titan GPU 上运行速度达到了 8FPS，全连接 CRF 平均推断需要 0.5s ，在 PASCAL VOC-2012 达到 71.6% IOU accuracy。</p>
<h3 id="DeepLabv2"><a href="#DeepLabv2" class="headerlink" title="DeepLabv2"></a>DeepLabv2</h3><p>  <br>DeepLabv2 是相对于 DeepLabv1 基础上的优化。DeepLabv1 在三个方向努力解决，但是问题依然存在：特征分辨率的降低、物体存在多尺度，DCNN 的平移不变性。<br>  <br>因 DCNN 连续池化和下采样造成分辨率降低，DeepLabv2 在最后几个最大池化层中去除下采样，取而代之的是使用空洞卷积，以更高的采样密度计算特征映射。<br>  <br>物体存在多尺度的问题，DeepLabv1 中是用多个 MLP 结合多尺度特征解决，虽然可以提供系统的性能，但是增加特征计算量和存储空间。<br>  <br>论文受到 Spatial Pyramid Pooling (SPP) 的启发，提出了一个类似的结构，在给定的输入上以不同采样率的空洞卷积并行采样，相当于以多个比例捕捉图像的上下文，称为 ASPP (atrous spatial pyramid pooling) 模块。<br>  <br>DCNN 的分类不变形影响空间精度。DeepLabv2 是采样全连接的 CRF 在增强模型捕捉细节的能力。<br>  <br>论文模型基于 ResNet，在 NVidia Titan X GPU 上运行速度达到了 8FPS，全连接 CRF 平均推断需要 0.5s ，在耗时方面和 DeepLabv1 无差异，但在 PASCAL VOC-2012 达到 79.7 mIOU。</p>
<h3 id="DeepLabv3"><a href="#DeepLabv3" class="headerlink" title="DeepLabv3"></a>DeepLabv3</h3><p>  <br>好的论文不止说明怎么做，还告诉为什么。DeepLab 延续到 DeepLabv3 系列，依然是在空洞卷积做文章，但是探讨不同结构的方向。<br>  <br>DeepLabv3 论文比较了多种捕获多尺度信息的方式：</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_4.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_4.png" srcset="data:image/png;base64,666" alt=""> </p>
<p>  <br>1.Image Pyramid：将输入图片放缩成不同比例，分别应用在 DCNN 上，将预测结果融合得到最终输出。<br>  <br>2.Encoder-Decoder：利用 Encoder 阶段的多尺度特征，运用到 Decoder 阶段上恢复空间分辨率，代表工作有 FCN、SegNet、PSPNet 等工。<br>  <br>3.Deeper w. Atrous Convolution：在原始模型的顶端增加额外的模块，例如 DenseCRF，捕捉像素间长距离信息。<br>  <br>4.Spatial Pyramid Pooling：空间金字塔池化具有不同采样率和多种视野的卷积核，能够以多尺度捕捉对象。<br>  <br>DeepLabv1-v2 都是使用带孔卷积提取密集特征来进行语义分割。但是为了解决分割对象的多尺度问题，DeepLabv3 设计采用多比例的带孔卷积级联或并行来捕获多尺度背景。<br>  <br>此外，DeepLabv3 将修改之前提出的带孔空间金字塔池化模块，该模块用于探索多尺度卷积特征，将全局背景基于图像层次进行编码获得特征，取得 state-of-art 性能，在 PASCAL VOC-2012 达到 86.9 mIOU。</p>
<h3 id="DeepLabv3-1"><a href="#DeepLabv3-1" class="headerlink" title="DeepLabv3+"></a>DeepLabv3+</h3><p>  <br>语义分割关注的问题:<br>  <br>1、 实例对象多尺度问题。<br>  <br>2、 因为深度网络存在stride=2的层，会导致feature分辨率下降，从而导致预测精度降低，而造成的边界信息丢失问题。<br>  <br>deeplab V3新设计的aspp结构解决了问题1，deeplab v3+主要目的在于解决问题2。<br>  <br>问题2 可以使用空洞卷积替代更多的pooling层来获取分辨率更高的feature。但是feature分辨率更高会极大增加运算量。以deeplab v3使用的resnet101为例，stride=16将造成后面9层feature变大，后面9层的计算量变为原来的2*2=4倍大。stride=8则更为恐怖，后面78层的计算量都会变大很多。<br>  <br>解决方案：1、编解码器结构；2 Modified Aligned Xception</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_5.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_5.png" srcset="data:image/png;base64,666" alt="">   </p>
<p>  <br>在deeplabv3基础上加入解码器。A是aspp结构，其中8x的上采样可以看做是一个解码器。B是编解码结构，它集合了高层和底层的特征。C就是本文采取的结构。<br>  <br>方法：<br>  <br>（1）Encoder-Decoder with Atrous Convolution</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_6.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_6.png" srcset="data:image/png;base64,666" alt="">  </p>
<p>  <br>编码器采用deeplabv3。<br>  <br>解码器部分：先从低层级选一个feature，将低层级的feature用1 <em> 1的卷积进行通道压缩（原本为256通道，或者512通道），目的在于减少低层级的比重。作者认为编码器得到的feature具有更丰富的信息，所以编码器的feature应该有更高的比重。 这样做有利于训练。<br>  <br>再将编码器的输出上采样，使其分辨率与低层级feature一致。举个例子，如果采用resnet conv2 输出的feature，则这里要</em> 4上采样。将两种feature连接后，再进行一次3 * 3的卷积（细化作用），然后再次上采样就得到了像素级的预测。后面的实验结果表明这种结构在 stride=16 时既有很高的精度速度又很快。stride=8相对来说只获得了一点点精度的提升，但增加了很多的计算量。<br>  <br>（2）Modified Aligned Xception<br>  <br>Xception主要采用了deepwish seperable convolution来替换原来的卷积层。简单的说就是这种结构能在更少参数更少计算量的情况下学到同样的信息。这边则是考虑将原来的resnet-101骨架网换成xception。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_7.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.6_7.png" srcset="data:image/png;base64,666" alt="">  </p>
<p>  <br><strong>红色部分为修改</strong><br>  <br>更多层：重复8次改为16次（基于MSRA目标检测的工作）。<br>  <br>将原来简单的pool层改成了stride为2的deepwish seperable convolution。<br>  <br>额外的RELU层和归一化操作添加在每个 3 × 3 depthwise convolution之后（原来只在1 * 1卷积之后）</p>
<h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask-R-CNN"></a>Mask-R-CNN</h2><h3 id="Mask-RCNN-的网络结构示意图"><a href="#Mask-RCNN-的网络结构示意图" class="headerlink" title="Mask-RCNN 的网络结构示意图"></a>Mask-RCNN 的网络结构示意图</h3><p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.8_1.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.8_1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>  <br>其中黑色部分为原来的Faster-RCNN，红色部分为在Faster网络上的修改：<br>  <br>1）将ROI Pooling层替换成了ROIAlign；<br>  <br>2）添加并列的FCN层（Mask层）；<br>  <br>先来概述一下Mask-RCNN的几个特点（来自于Paper<a href="https://arxiv.org/pdf/1703.06870.pdf">Mask R-CNN</a>的Abstract）：<br>  <br>1）在边框识别的基础上添加分支网络，用于语义Mask识别；<br>  <br>2）训练简单，相对于Faster仅增加一个小的Overhead，可以跑到5FPS；<br>  <br>3）可以方便的扩展到其他任务，比如人的姿态估计等；<br>  <br>4）不借助Trick，在每个任务上，效果优于目前所有的 single-model entries；包括 COCO 2016 的Winners。</p>
<h3 id="RCNN行人检测框架"><a href="#RCNN行人检测框架" class="headerlink" title="RCNN行人检测框架"></a>RCNN行人检测框架</h3><p>  <br>来看下后面两种RCNN方法与Mask结合的示意图:</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.8_2.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.8_2.png" srcset="data:image/png;base64,666" alt=""><br>  <br>图中灰色部分是原来的RCNN结合ResNet or FPN的网络，下面黑色部分为新添加的并联Mask层，这个图本身与上面的图也没有什么区别，旨在说明作者所提出的Mask RCNN方法的泛化适应能力：可以和多种RCNN框架结合，表现都不错。</p>
<h3 id="Mask-RCNN-技术要点"><a href="#Mask-RCNN-技术要点" class="headerlink" title="Mask-RCNN 技术要点"></a>Mask-RCNN 技术要点</h3><p>  <br><strong>1.技术要点1 - 强化的基础网络</strong><br>  <br>通过ResNeXt-101+FPN用作特征提取网络，达到state-of-the-art的效果。<br>  <br><strong>2.技术要点2 - ROIAlign</strong><br>  <br>采用ROIAlign替代RoiPooling（改进池化操作）。引入了一个插值过程，先通过双线性插值到14<em>14，再pooling到7</em>7，很大程度上解决了仅通过Pooling直接采样带来的Misalignment对齐问题。<br>  <br>PS： 虽然 Misalignment 在分类问题上影响并不大，但在 Pixel 级别的 Mask 上会存在较大误差。<br>  <br>后面我们把结果对比贴出来（Table2 c &amp; d），能够看到 ROIAlign 带来较大的改进，可以看到，Stride 越大改进越明显。<br>  <br><strong>3.技术要点3 - Loss Function</strong><br>  <br>每个ROIAlign对应K <em> m^2维度的输出。K对应类别个数，即输出K个mask，m对应池化分辨率（7 </em> 7）。Loss函数定义：</p>
<script type="math/tex; mode=display">
Lmask(Cls_k)=Sigmoid(Cls_k)</script><p>  <br>$Lmask(Cls_k) = Sigmoid (Cls_k)$，平均二值交叉熵 （average binary cross-entropy）Loss，通过逐像素的 Sigmoid 计算得到。<br>  <br>Why K个mask？通过对每个 Class 对应一个Mask可以有效避免类间竞争（其他Class不贡献Loss）。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.8_3.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.8_3.png" srcset="data:image/png;base64,666" alt=""><br>  <br>通过结果对比来看（Table2 b），也就是作者所说的 Decouple 解耦，要比多分类的Softmax效果好很多。<br>  <br>另外，作者给出了很多实验分割效果，就不都列了，只贴一张和FCIS的对比图（FCIS出现了Overlap的问题）</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.8_4.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.8_4.png" srcset="data:image/png;base64,666" alt=""></p>
<h2 id="CNN在基于弱监督学习的图像分割中的应用"><a href="#CNN在基于弱监督学习的图像分割中的应用" class="headerlink" title="CNN在基于弱监督学习的图像分割中的应用"></a>CNN在基于弱监督学习的图像分割中的应用</h2><p>  <br>答案来源：<a href="https://zhuanlan.zhihu.com/p/23811946">CNN在基于弱监督学习的图像分割中的应用</a>  </p>
<p>  <br>最近基于深度学习的图像分割技术一般依赖于卷积神经网络CNN的训练，训练过程中需要非常大量的标记图像，即一般要求训练图像中都要有精确的分割结果。<br>  <br>对于图像分割而言，要得到大量的完整标记过的图像非常困难，比如在ImageNet数据集上，有1400万张图有类别标记，有50万张图给出了bounding box,但是只有4460张图像有像素级别的分割结果。对训练图像中的每个像素做标记非常耗时，特别是对医学图像而言，完成对一个三维的CT或者MRI图像中各组织的标记过程需要数小时。<br>  <br>如果学习算法能通过对一些初略标记过的数据集的学习就能完成好的分割结果，那么对训练数据的标记过程就很简单，这可以大大降低花在训练数据标记上的时间。这些初略标记可以是：<br>  <br>1、只给出一张图像里面包含哪些物体，<br>  <br>2、给出某个物体的边界框，<br>  <br>3、对图像中的物体区域做部分像素的标记，例如画一些线条、涂鸦等（scribbles)。</p>
<h3 id="Scribble标记"><a href="#Scribble标记" class="headerlink" title="Scribble标记"></a>Scribble标记</h3><p>  <br>论文地址：<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_ScribbleSup_Scribble-Supervised_Convolutional_CVPR_2016_paper.pdf">ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation (CVPR 2016)</a><br>  <br>香港中文大学的Di Lin提出了一个基于Scribble标记的弱监督学习方法。Scribble是一个很方便使用的标记方法，因此被用得比较广泛。如下图，只需要画五条线就能完成对一副图像的标记工作。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_1.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_1.png" srcset="data:image/png;base64,666" alt=""><br>  <br>ScribbleSup分为两步，第一步将像素的类别信息从scribbles传播到其他未标记的像素，自动完成所有的训练图像的标记工作； 第二步使用这些标记图像训练CNN。在第一步中，该方法先生成super-pxels, 然后基于graph cut的方法对所有的super-pixel进行标记。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_2.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_2.png" srcset="data:image/png;base64,666" alt="">  </p>
<p>  <br>Graph Cut的能量函数为：</p>
<script type="math/tex; mode=display">
\sum_{i}\psi _i\left(y_i|X,S\right)+\sum_{i,j}\psi_{ij}\left(y_i,y_j,X\right)</script><p>  <br>在这个graph中，每个super-pixel是graph中的一个节点，相接壤的super-pixel之间有一条连接的边。这个能量函数中的一元项包括两种情况，一个是来自于scribble的，一个是来自CNN对该super-pixel预测的概率。整个最优化过程实际上是求graph cut能量函数和CNN参数联合最优值的过程：</p>
<script type="math/tex; mode=display">
\sum_{i}\psi _i^{scr}\left(y_i|X,S\right)+\sum _i-logP\left(y_i| X,\theta\right)+\sum_{i,j}\psi _{ij}\left(y_i,y_j|X\right)</script><p>  <br>上式的最优化是通过交替求 $Y$ 和 $\theta$ 的最优值来实现的。文章中发现通过三次迭代就能得到比较好的结果。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_3.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_3.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="图像级别标记"><a href="#图像级别标记" class="headerlink" title="图像级别标记"></a>图像级别标记</h3><p>  <br>论文地址：<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Pathak_Constrained_Convolutional_Neural_ICCV_2015_paper.pdf">Constrained Convolutional Neural Networks for Weakly Supervised Segmentation （ICCV 2015）</a><br>  <br>UC Berkeley的Deepak Pathak使用了一个具有图像级别标记的训练数据来做弱监督学习。训练数据中只给出图像中包含某种物体，但是没有其位置信息和所包含的像素信息。该文章的方法将image tags转化为对CNN输出的label分布的限制条件，因此称为 Constrained convolutional neural network (CCNN).</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_4.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_4.png" srcset="data:image/png;base64,666" alt=""><br>  </p>
<p>该方法把训练过程看作是有线性限制条件的最优化过程：</p>
<script type="math/tex; mode=display">
\underset{\theta ,P}{minimize}\qquad D(P(X)||Q(X|\theta ))\\
subject\to\qquad A\overrightarrow{P} \geqslant \overrightarrow{b},\sum_{X}^{ }P(X)=1</script><p>  </p>
<p>其中的线性限制条件来自于训练数据上的标记，例如一幅图像中前景类别像素个数期望值的上界或者下界（物体大小）、某个类别的像素个数在某图像中为0，或者至少为1等。该目标函数可以转化为为一个loss function，然后通过SGD进行训练。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_5.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_5.png" srcset="data:image/png;base64,666" alt=""><br>  <br>实验中发现单纯使用Image tags作为限制条件得到的分割结果还比较差，在PASCAL VOC 2012 test数据集上得到的mIoU为35.6%，加上物体大小的限制条件后能达到45.1%，如果再使用bounding box做限制，可以达到54%。FCN-8s可以达到62.2%，可见弱监督学习要取得好的结果还是比较难。     </p>
<h3 id="DeepLab-bounding-box-image-level-labels"><a href="#DeepLab-bounding-box-image-level-labels" class="headerlink" title="DeepLab+bounding box+image-level labels**"></a>DeepLab+bounding box+image-level labels**</h3><p>  <br>论文地址：<a href="https://arxiv.org/pdf/1502.02734.pdf">Weakly-and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation</a><br>  <br>Google的George Papandreou 和UCLA的Liang-Chieh Chen等在DeepLab的基础上进一步研究了使用bounding box和image-level labels作为标记的训练数据。使用了期望值最大化算法（EM）来估计未标记的像素的类别和CNN的参数。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_6.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_6.png" srcset="data:image/png;base64,666" alt=""><br>  <br>对于image-level标记的数据，我们可以观测到图像的像素值和图像级别的标记 ,但是不知道每个像素的标号,因此把$y$当做隐变量。使用如下的概率图模式：</p>
<script type="math/tex; mode=display">
P\left ( x,y,z;\theta \right ) = P\left ( x \right )\left (\prod_{m=1}^{M} P\left ( y_m|x;\theta \right )\right )P\left ( z|y \right )</script><p>  <br>这篇论文是通过EM算法来学习模型的参数$\theta$，具体推导过程可参考原论文。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_7.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_7.png" srcset="data:image/png;base64,666" alt=""><br>  <br>对于给出bounding box标记的训练图像，该方法先使用CRF对该训练图像做自动分割，然后在分割的基础上做全监督学习。通过实验发现，单纯使用图像级别的标记得到的分割效果较差，但是使用bounding box的训练数据可以得到较好的结果，在VOC2012 test数据集上得到mIoU 62.2%。另外如果使用少量的全标记图像和大量的弱标记图像进行结合，可以得到与全监督学习(70.3%)接近的分割结果(69.0%)。     </p>
<h3 id="统一的框架"><a href="#统一的框架" class="headerlink" title="统一的框架"></a>统一的框架</h3><p>  <br>论文地址：<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Xu_Learning_to_Segment_2015_CVPR_paper.pdf">Learning to Segment Under Various Forms of Weak Supervision (CVPR 2015)</a> </p>
<p>  <br>Wisconsin-Madison大学的Jia Xu提出了一个统一的框架来处理各种不同类型的弱标记：图像级别的标记、bounding box和部分像素标记如scribbles。该方法把所有的训练图像分成共计$n$个super-pixel，对每个super-pixel提取一个$d$维特征向量。因为不知道每个super-pixel所属的类别，相当于无监督学习，因此该方法对所有的super-pixel做聚类，使用的是最大间隔聚类方法(max-margin clustering, MMC),该过程的最优化目标函数是：</p>
<script type="math/tex; mode=display">
\underset{W,H}{min} \qquad  \frac{1}{2}tr\left ( W^TW \right ) + \lambda\sum_{p=1}^{n}\sum_{c=1}^{C}\xi \left ( w_c;x_p;h_p^c \right)</script><p>  <br>在这个目标函数的基础上，根据不同的弱标记方式，可以给出不同的限制条件，因此该方法就是在相应的限制条件下求最大间隔聚类。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_8.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/figure_9.9_8.png" srcset="data:image/png;base64,666" alt="">  </p>
<p>  <br>该方法在Siftflow数据集上得到了比较好的结果，比state-of-the-art的结果提高了10%以上。</p>
<p>  <br>小结：在弱标记的数据集上训练图像分割算法可以减少对大量全标记数据的依赖，在大多数应用中会更加贴合实际情况。弱标记可以是图像级别的标记、边框和部分像素的标记等。训练的方法一般看做是限制条件下的最优化方法。另外EM算法可以用于CNN参数和像素类别的联合求优。</p>
<h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p>  <br>这篇论文是CVPR2017年的最佳论文。</p>
<p>  <br>卷积神经网络结构的设计主要朝着两个方向发展，一个是更宽的网络（代表：GoogleNet、VGG），一个是更深的网络（代表：ResNet）。但是随着层数的加深会出现一个问题——梯度消失，这将会导致网络停止训练。到目前为止解决这个问题的思路基本都是在前后层之间加一个identity connections(short path)。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/9-10-3.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/9-10-3.png" srcset="data:image/png;base64,666" alt=""></p>
<p>  <br>由上图中可知Resnet是做值的相加（也就是add操作），通道数是不变的。而DenseNet是做通道的合并（也就是Concatenation操作），就像Inception那样。从这两个公式就可以看出这两个网络的本质不同。此外DensetNet的前面一层输出也是后面所有层的输入，这也不同于ResNet残差网络。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/9-10-1.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/9-10-1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>  <br>DenseNet的Block结构如上图所示。</p>
<p>  <br>1*1卷积核的目的：减少输入的特征图数量，这样既能降维减少计算量，又能融合各个通道的特征。我们将使用BottleNeck Layers的DenseNet表示为DenseNet-B。(在论文的实验里，将1×1×n小卷积里的n设置为4k，k为每个H产生的特征图数量)</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/9-10-2.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/9-10-2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>  <br>上图是DenseNet网络的整体网络结构示意图。其中1*1卷积核的目的是进一步压缩参数，并且在Transition Layer层有个参数Reduction（范围是0到1），表示将这些输出缩小到原来的多少倍，默认是0.5，这样传给下一个Dense Block的时候channel数量就会减少一半。当Reduction的值小于1的时候，我们就把带有这种层的网络称为DenseNet-C。</p>
<p>  <br>DenseNet网络的优点包括：</p>
<ul>
<li>减轻了梯度消失</li>
<li>加强了feature的传递</li>
<li>更有效地利用了feature </li>
<li>一定程度上较少了参数数量</li>
<li>一定程度上减轻了过拟合</li>
</ul>
<h2 id="图像分割的常用数据集"><a href="#图像分割的常用数据集" class="headerlink" title="图像分割的常用数据集"></a>图像分割的常用数据集</h2><h3 id="PASCAL-VOC"><a href="#PASCAL-VOC" class="headerlink" title="PASCAL VOC"></a>PASCAL VOC</h3><p>VOC 数据集分为20类，包括背景为21类，分别如下： </p>
<ul>
<li>Person: person </li>
<li>Animal: bird, cat, cow, dog, horse, sheep </li>
<li>Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train </li>
<li>Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor</li>
</ul>
<p>VOC 数据集中用于分割比赛的图片实例如下，包含原图以及图像分类分割和图像物体分割两种图（PNG格式）。图像分类分割是在20种物体中，ground-turth图片上每个物体的轮廓填充都有一个特定的颜色，一共20种颜色。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/VOC-01.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/VOC-01.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="MS-COCO"><a href="#MS-COCO" class="headerlink" title="MS COCO"></a>MS COCO</h3><p>MS COCO 是最大图像分割数据集，提供的类别有 80 类，有超过 33 万张图片，其中 20 万张有标注，整个数据集中个体的数目超过 150 万个。MS COCO是目前难度最大，挑战最高的图像分割数据集。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/COCO-01.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/COCO-01.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="Cityscapes"><a href="#Cityscapes" class="headerlink" title="Cityscapes"></a>Cityscapes</h3><p>Cityscapes 是驾驶领域进行效果和性能测试的图像分割数据集，它包含了5000张精细标注的图像和20000张粗略标注的图像，这些图像包含50个城市的不同场景、不同背景、不同街景，以及30类涵盖地面、建筑、交通标志、自然、天空、人和车辆等的物体标注。Cityscapes评测集有两项任务：像素级（Pixel-level）图像场景分割（以下简称语义分割）与实例级（Instance-level）图像场景分割（以下简称实例分割）。</p>
<p><img src="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/Cityscapes-01.png" class="lazyload" data-srcset="/zh-TW/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ch9/Cityscapes-01.png" srcset="data:image/png;base64,666" alt=""></p>
<p>TODO</p>
<ul>
<li>[ ] 图像分割数据集标注工具</li>
<li>[ ] 图像分割评价标准</li>
<li>[ ] 全景分割</li>
<li>[ ] UNet++</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>迁移学习</title>
    <url>/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h1><p>​    本章主要简明地介绍了迁移学习的基本概念、迁移学习的必要性、研究领域和基本方法。重点介绍了几大类常用的迁移学习方法：数据分布自适应方法、特征选择方法、子空间学习方法、以及目前最热门的深度迁移学习方法。除此之外，我们也结合最近的一些研究成果对未来迁移学习进行了一些展望。并提供了一些迁移学习领域的常用学习资源，以方便感兴趣的读者快速开始学习。</p>
<h2 id="迁移学习基础知识"><a href="#迁移学习基础知识" class="headerlink" title="迁移学习基础知识"></a>迁移学习基础知识</h2><h3 id="什么是迁移学习？"><a href="#什么是迁移学习？" class="headerlink" title="什么是迁移学习？"></a>什么是迁移学习？</h3><p>找到目标问题的相似性，迁移学习任务就是从相似性出发，将旧领域(domain)学习过的模型应用在新领域上。</p>
<h3 id="为什么需要迁移学习？"><a href="#为什么需要迁移学习？" class="headerlink" title="为什么需要迁移学习？"></a>为什么需要迁移学习？</h3><ol>
<li><strong>大数据与少标注的矛盾</strong>：虽然有大量的数据，但往往都是没有标注的，无法训练机器学习模型。人工进行数据标定太耗时。</li>
<li><strong>大数据与弱计算的矛盾</strong>：普通人无法拥有庞大的数据量与计算资源。因此需要借助于模型的迁移。</li>
<li><strong>普适化模型与个性化需求的矛盾</strong>：即使是在同一个任务上，一个模型也往往难以满足每个人的个性化需求，比如特定的隐私设置。这就需要在不同人之间做模型的适配。</li>
<li><strong>特定应用（如冷启动）的需求</strong>。</li>
</ol>
<h3 id="迁移学习的基本问题有哪些？"><a href="#迁移学习的基本问题有哪些？" class="headerlink" title="迁移学习的基本问题有哪些？"></a>迁移学习的基本问题有哪些？</h3><p>基本问题主要有3个：</p>
<ul>
<li><strong>How to transfer</strong>： 如何进行迁移学习？（设计迁移方法）</li>
<li><strong>What to transfer</strong>： 给定一个目标领域，如何找到相对应的源领域，然后进行迁移？（源领域选择）</li>
<li><strong>When to transfer</strong>： 什么时候可以进行迁移，什么时候不可以？（避免负迁移）</li>
</ul>
<h3 id="迁移学习有哪些常用概念？"><a href="#迁移学习有哪些常用概念？" class="headerlink" title="迁移学习有哪些常用概念？"></a>迁移学习有哪些常用概念？</h3><ul>
<li>基本定义<ul>
<li><strong>域(Domain)</strong>：数据特征和特征分布组成，是学习的主体<ul>
<li><strong>源域 (Source domain)</strong>：已有知识的域</li>
<li><strong>目标域 (Target domain)</strong>：要进行学习的域</li>
</ul>
</li>
<li><strong>任务 (Task)</strong>：由目标函数和学习结果组成，是学习的结果</li>
</ul>
</li>
<li>按特征空间分类<ul>
<li><strong>同构迁移学习（Homogeneous TL）</strong>： 源域和目标域的特征空间相同，$D_s=D_t$</li>
<li><strong>异构迁移学习（Heterogeneous TL）</strong>：源域和目标域的特征空间不同，$D_s\ne D_t$</li>
</ul>
</li>
<li>按迁移情景分类<ul>
<li><strong>归纳式迁移学习（Inductive TL）</strong>：源域和目标域的学习任务不同</li>
<li><strong>直推式迁移学习（Transductive TL)</strong>：源域和目标域不同，学习任务相同</li>
<li><strong>无监督迁移学习（Unsupervised TL)</strong>：源域和目标域均没有标签</li>
</ul>
</li>
<li>按迁移方法分类<ul>
<li><strong>基于实例的迁移 (Instance based TL)</strong>：通过权重重用源域和目标域的样例进行迁移</li>
<li><strong>基于特征的迁移 (Feature based TL)</strong>：将源域和目标域的特征变换到相同空间</li>
<li><strong>基于模型的迁移 (Parameter based TL)</strong>：利用源域和目标域的参数共享模型</li>
<li><strong>基于关系的迁移 (Relation based TL)</strong>：利用源域中的逻辑网络关系进行迁移</li>
</ul>
</li>
</ul>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/ch11/1542972502781.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/ch11/1542972502781.png" srcset="data:image/png;base64,666" alt="1542972502781"></p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/ch11/1542974131814.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/ch11/1542974131814.png" srcset="data:image/png;base64,666" alt="1542974131814"></p>
<h3 id="迁移学习与传统机器学习有什么区别？"><a href="#迁移学习与传统机器学习有什么区别？" class="headerlink" title="迁移学习与传统机器学习有什么区别？"></a>迁移学习与传统机器学习有什么区别？</h3><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>迁移学习</th>
<th>传统机器学习</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据分布</td>
<td>训练和测试数据不需要同分布</td>
<td>训练和测试数据同分布</td>
</tr>
<tr>
<td>数据标签</td>
<td>不需要足够的数据标注</td>
<td>足够的数据标注</td>
</tr>
<tr>
<td>建模</td>
<td>可以重用之前的模型</td>
<td>每个任务分别建模</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/ch11/1542973960796.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/ch11/1542973960796.png" srcset="data:image/png;base64,666" alt="1542973960796"></p>
<h3 id="迁移学习的核心及度量准则？"><a href="#迁移学习的核心及度量准则？" class="headerlink" title="迁移学习的核心及度量准则？"></a>迁移学习的核心及度量准则？</h3><p><strong>迁移学习的总体思路可以概括为</strong>：开发算法来最大限度地利用有标注的领域的知识，来辅助目标领域的知识获取和学习。</p>
<p><strong>迁移学习的核心是</strong>：找到源领域和目标领域之间的相似性，并加以合理利用。这种相似性非常普遍。比如，不同人的身体构造是相似的；自行车和摩托车的骑行方式是相似的；国际象棋和中国象棋是相似的；羽毛球和网球的打球方式是相似的。这种相似性也可以理解为不变量。以不变应万变，才能立于不败之地。</p>
<p><strong>有了这种相似性后，下一步工作就是， 如何度量和利用这种相似性。</strong>度量工作的目标有两点：一是很好地度量两个领域的相似性，不仅定性地告诉我们它们是否相似，更定量地给出相似程度。二是以度量为准则，通过我们所要采用的学习手段，增大两个领域之间的相似性，从而完成迁移学习。</p>
<p><strong>一句话总结： 相似性是核心，度量准则是重要手段。</strong></p>
<h3 id="迁移学习与其他概念的区别？"><a href="#迁移学习与其他概念的区别？" class="headerlink" title="迁移学习与其他概念的区别？"></a>迁移学习与其他概念的区别？</h3><ol>
<li>迁移学习与多任务学习关系：<ul>
<li><strong>多任务学习</strong>：多个相关任务一起协同学习；</li>
<li><strong>迁移学习</strong>：强调信息复用，从一个领域(domain)迁移到另一个领域。</li>
</ul>
</li>
<li>迁移学习与领域自适应：<strong>领域自适应</strong>：使两个特征分布不一致的domain一致。</li>
<li>迁移学习与协方差漂移：<strong>协方差漂移</strong>：数据的条件概率分布发生变化。</li>
</ol>
<p>Reference： </p>
<ol>
<li><a href="https：//github.com/jindongwang/transferlearning-tutorial">王晋东，迁移学习简明手册</a></li>
<li>Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., &amp; Vaughan, J. W. (2010). A theory of learning from different domains. Machine learning, 79(1-2), 151-175.</li>
<li>Tan, B., Song, Y., Zhong, E. and Yang, Q., 2015, August. Transitive transfer learning. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1155-1164). ACM.</li>
</ol>
<h3 id="什么是负迁移？产生负迁移的原因有哪些？"><a href="#什么是负迁移？产生负迁移的原因有哪些？" class="headerlink" title="什么是负迁移？产生负迁移的原因有哪些？"></a>什么是负迁移？产生负迁移的原因有哪些？</h3><p>负迁移(Negative Transfer)指的是，在源域上学习到的知识，对于目标域上的学习产生负面作用。</p>
<p>产生负迁移的原因主要有：</p>
<ul>
<li>数据问题：源域和目标域压根不相似，谈何迁移？</li>
<li>方法问题：源域和目标域是相似的，但是，迁移学习方法不够好，没找到可迁移的成分。</li>
</ul>
<p>负迁移给迁移学习的研究和应用带来了负面影响。在实际应用中，找到合理的相似性，并且选择或开发合理的迁移学习方法，能够避免负迁移现象。</p>
<h3 id="迁移学习的基本思路？"><a href="#迁移学习的基本思路？" class="headerlink" title="迁移学习的基本思路？"></a>迁移学习的基本思路？</h3><p>迁移学习的总体思路可以概括为：开发算法来最大限度地利用有标注的领域的知识，来辅助目标领域的知识获取和学习。</p>
<ol>
<li>找到目标问题的相似性，迁移学习任务就是从相似性出发，将旧领域(domain)学习过的模型应用在新领域上。</li>
<li>迁移学习，是指利用数据、任务、或模型之间的相似性，将在旧领域学习过的模型，应用于新领域的一种学习过程。</li>
<li>迁移学习<strong>最有用的场合</strong>是，如果你尝试优化任务B的性能，通常这个任务数据相对较少。<br>例如，在放射科中你知道很难收集很多射线扫描图来搭建一个性能良好的放射科诊断系统，所以在这种情况下，你可能会找一个相关但不同的任务，如图像识别，其中你可能用 1 百万张图片训练过了，并从中学到很多低层次特征，所以那也许能帮助网络在任务在放射科任务上做得更好，尽管任务没有这么多数据。</li>
<li>迁移学习什么时候是有意义的？它确实可以<strong>显著提高</strong>你的<strong>学习任务的性能</strong>，但我有时候也见过有些场合使用迁移学习时，任务实际上数据量比任务要少， 这种情况下增益可能不多。<blockquote>
<p>什么情况下可以使用迁移学习？</p>
<p>假如两个领域之间的区别特别的大，<strong>不可以直接采用迁移学习</strong>，因为在这种情况下效果不是很好。在这种情况下，推荐使用[3]的工作，在两个相似度很低的domain之间一步步迁移过去（踩着石头过河）。</p>
</blockquote>
</li>
</ol>
<blockquote>
<ol>
<li>迁移学习主要解决方案有哪些？</li>
<li>除直接看infer的结果的Accurancy以外，如何衡量迁移学习学习效果？</li>
<li>对抗网络是如何进行迁移的？</li>
</ol>
</blockquote>
<p>Reference： </p>
<ol>
<li><a href="https：//github.com/jindongwang/transferlearning-tutorial">王晋东，迁移学习简明手册</a></li>
<li>Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., &amp; Vaughan, J. W. (2010). A theory of learning from different domains. Machine learning, 79(1-2), 151-175.</li>
<li>Tan, B., Song, Y., Zhong, E. and Yang, Q., 2015, August. Transitive transfer learning. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1155-1164). ACM.</li>
</ol>
<h2 id="迁移学习的基本思路有哪些？"><a href="#迁移学习的基本思路有哪些？" class="headerlink" title="迁移学习的基本思路有哪些？"></a>迁移学习的基本思路有哪些？</h2><p>​    迁移学习的基本方法可以分为四种。这四种基本的方法分别是：基于样本的迁移， 基于模型 的迁移， 基于特征的迁移，及基于关系的迁移。</p>
<h3 id="基于样本迁移"><a href="#基于样本迁移" class="headerlink" title="基于样本迁移"></a>基于样本迁移</h3><p>​    基于样本的迁移学习方法 (Instance based Transfer Learning) 根据一定的权重生成规则，对数据样本进行重用，来进行迁移学习。图<a href="#bookmark90">14</a>形象地表示了基于样本迁移方法的思想源域中存在不同种类的动物，如狗、鸟、猫等，目标域只有狗这一种类别。在迁移时，为了最大限度地和目标域相似，我们可以人为地提高源域中属于狗这个类别的样本权重。</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/631e5aab4e0680c374793804817bfbb6.jpg" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/631e5aab4e0680c374793804817bfbb6.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p></p><center>图 14: 基于样本的迁移学习方法示意图</center><p></p>
<p>​    在迁移学习中，对于源域D~s~和目标域D~t~，通常假定产生它们的概率分布是不同且未知的(P(X~s~) =P(X~t~))。另外，由于实例的维度和数量通常都非常大，因此，直接对 P(X~s~) 和P(X~t~) 进行估计是不可行的。因而，大量的研究工作 [<a href="#bookmark267">Khan and Heisterkamp,2016</a>, <a href="#bookmark319">Zadrozny, 2004</a>, <a href="#bookmark242">Cortes et al.,2008</a>, <a href="#bookmark243">Dai et al., 2007</a>, <a href="#bookmark302">Tan et al.,2015</a>, <a href="#bookmark303">Tan et al., 2017</a>] 着眼于对源域和目标域的分布比值进行估计(P(<strong>X</strong>t)/P(<strong>X</strong>s))。所估计得到的比值即为样本的权重。这些方法通常都假设P(<strong>x</strong>s) \&lt;并且源域和目标域的条件概率分布相同(P(y|x~s~)=<em>P</em>(y|x~t~))。特别地，上海交通大学Dai等人[<a href="#bookmark243">Dai et al.,2007</a>]提出了 TrAdaboost方法，将AdaBoost的思想应用于迁移学习中，提高有利于目标分类任务的实例权重、降低不利于目标分类任务的实例权重，并基于PAC理论推导了模型的泛化误差上界。TrAdaBoost方法是此方面的经典研究之一。文献 [<a href="#bookmark264">Huang et al., 2007</a>]提出核均值匹配方法 (Kernel Mean atching, KMM)对于概率分布进行估计，目标是使得加权后的源域和目标域的概率分布尽可能相近。在最新的研究成果中，香港科技大学的Tan等人扩展了实例迁移学习方法的应用场景，提出 了传递迁移学习方法(Transitive Transfer Learning, TTL) [<a href="#bookmark302">Tan etal., 2015</a>] 和远域迁移学习 (Distant Domain Transfer Learning,DDTL) [<a href="#bookmark303">Tan et al., 2017</a>]，利用联合矩阵分解和深度神经网络，将迁移学习应用于多个不相似的领域之间的知识共享，取得了良好的效果。</p>
<p>​    虽然实例权重法具有较好的理论支撑、容易推导泛化误差上界，但这类方法通常只在领域间分布差异较小时有效，因此对自然语言处理、计算机视觉等任务效果并不理想。而基于特征表示的迁移学习方法效果更好,是我们研究的重点。</p>
<h3 id="基于特征迁移"><a href="#基于特征迁移" class="headerlink" title="基于特征迁移"></a>基于特征迁移</h3><p>​    基于特征的迁移方法 (Feature based Transfer Learning) 是指将通过特征变换的方式互相迁移 [<a href="#bookmark272">Liu et al., 2011</a>, <a href="#bookmark327">Zheng et al.,2008</a>, <a href="#bookmark263">Hu and Yang, 2011</a>],来减少源域和目标域之间的差距；或者将源域和目标域的数据特征变换到统一特征空间中 [<a href="#bookmark288">Pan et al.,2011</a>, <a href="#bookmark278">Long et al., 2014b</a>, <a href="#bookmark248">Duan et al.,2012</a>],然后利用传统的机器学习方法进行分类识别。根据特征的同构和异构性,又可以分为同构和异构迁移学习。图<a href="#bookmark93">15</a>很形象地表示了两种基于特 征的迁移学习方法。</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/fa08900e89bfd53cc28345d21bc6aca0.jpg" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/fa08900e89bfd53cc28345d21bc6aca0.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p></p><center>图 15: 基于特征的迁移学习方法示意图</center><p></p>
<p>​    基于特征的迁移学习方法是迁移学习领域中最热门的研究方法,这类方法通常假设源域和目标域间有一些交叉的特征。香港科技大学的 Pan 等人 [<a href="#bookmark288">Pan et al.,2011</a>] 提出的迁移 成分分析方法 (Transfer Component Analysis, TCA)是其中较为典型的一个方法。该方法的 核心内容是以最大均值差异 (Maximum MeanDiscrepancy, MMD) [<a href="#bookmark236">Borgwardt et al., 2006</a>]作为度量准则,将不同数据领域中的分布差异最小化。加州大学伯克利分校的 Blitzer 等人 [<a href="#bookmark235">Blitzer et al., 2006</a>] 提出了一种基于结构对应的学习方法(Structural Corresponding Learning,SCL),该算法可以通过映射将一个空间中独有的一些特征变换到其他所有空间中的轴特征上,然后在该特征上使用机器学习的算法进行分类预测。清华大学龙明盛等人[<a href="#bookmark278">Long et al.,2014b</a>]提出在最小化分布距离的同时，加入实例选择的迁移联合匹配(Tran-fer Joint Matching, TJM) 方法,将实例和特征迁移学习方法进行了有机的结合。澳大利亚卧龙岗大学的 Jing Zhang 等人 [<a href="#bookmark321">Zhang et al., 2017a</a>]提出对于源域和目标域各自训练不同 的变换矩阵,从而达到迁移学习的目标。</p>
<h3 id="基于模型迁移"><a href="#基于模型迁移" class="headerlink" title="基于模型迁移"></a>基于模型迁移</h3><p>​    基于模型的迁移方法 (Parameter/Model based Transfer Learning) 是指从源域和目标域中找到他们之间共享的参数信息,以实现迁移的方法。这种迁移方式要求的假设条件是： 源域中的数据与目标域中的数据可以共享一些模型的参数。其中的代表性工作主要有［<a href="#bookmark324">Zhao et al., 2010</a>, <a href="#bookmark325">Zhao et al., 2011</a>, <a href="#bookmark287">Panet al., 2008b</a>, <a href="#bookmark286">Pan et al., 2008a</a>］。图<a href="#bookmark96">16</a>形象地 表示了基于模型的迁移学习方法的基本思想。</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/602723a1d3ce0f3abe7c591a8e4bb6ec.jpg" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/602723a1d3ce0f3abe7c591a8e4bb6ec.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p></p><center>图 16: 基于模型的迁移学习方法示意图</center><p></p>
<p>​    其中，中科院计算所的Zhao等人[<a href="#bookmark325">Zhao et al., 2011</a>]提出了TransEMDT方法。该方法首先针对已有标记的数据，利用决策树构建鲁棒性的行为识别模型，然后针对无标定数据，利用K-Means聚类方法寻找最优化的标定参数。西安邮电大学的Deng等人[<a href="#bookmark245">Deng et al.,2014</a>] 也用超限学习机做了类似的工作。香港科技大学的Pan等人[<a href="#bookmark286">Pan etal., 2008a</a>]利用HMM，针对Wifi室内定位在不同设备、不同时间和不同空间下动态变化的特点，进行不同分布下的室内定位研究。另一部分研究人员对支持向量机 SVM 进行了改进研究 [<a href="#bookmark285">Nater et al.,2011</a>, <a href="#bookmark269">Li et al., 2012</a>]。这些方法假定 SVM中的权重向量 <strong>w</strong> 可以分成两个部分： <strong>w</strong> = <strong>wo</strong>+<strong>v</strong>， 其中 <strong>w</strong>0代表源域和目标域的共享部分， <strong>v</strong> 代表了对于不同领域的特定处理。在最新的研究成果中，香港科技大学的 Wei 等人 [<a href="#bookmark313">Wei et al., 2016b</a>]将社交信息加入迁移学习方法的 正则项中，对方法进行了改进。清华大学龙明盛等人[<a href="#bookmark275">Long et al., 2015a</a>, <a href="#bookmark276">Long et al., 2016</a>, <a href="#bookmark280">Long etal., 2017</a>]改进了深度网络结构，通过在网络中加入概率分布适配层，进一步提高了深度迁移学习网络对于大数据的泛化能力。</p>
<h3 id="基于关系迁移"><a href="#基于关系迁移" class="headerlink" title="基于关系迁移"></a>基于关系迁移</h3><p>​    基于关系的迁移学习方法 (Relation Based Transfer Learning) 与上述三种方法具有截然不同的思路。这种方法比较关注源域和目标域的样本之间的关系。图<a href="#bookmark82">17</a>形象地表示了不 同领域之间相似的关系。</p>
<p>​    就目前来说，基于关系的迁移学习方法的相关研究工作非常少，仅有几篇连贯式的文章讨论： [<a href="#bookmark283">Mihalkova et al., 2007</a>, <a href="#bookmark284">Mihalkova and Mooney,2008</a>, <a href="#bookmark244">Davis and Domingos, 2009</a>]。这些文章都借助于马尔科夫逻辑网络(Markov Logic Net)来挖掘不同领域之间的关系相似性。</p>
<p>​    我们将重点讨论基于特征和基于模型的迁移学习方法，这也是目前绝大多数研究工作的热点。</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/aa10d36f758430dd4ff72d2bf6a76a6c.jpg" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/aa10d36f758430dd4ff72d2bf6a76a6c.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p></p><center>图 17: 基于关系的迁移学习方法示意图</center><p></p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542812440636.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542812440636.png" srcset="data:image/png;base64,666" alt="1542812440636"></p>
<p></p><center>图 18: 基于马尔科夫逻辑网的关系迁移</center><p></p>
<h2 id="迁移学习的常用方法"><a href="#迁移学习的常用方法" class="headerlink" title="迁移学习的常用方法"></a>迁移学习的常用方法</h2><h3 id="数据分布自适应"><a href="#数据分布自适应" class="headerlink" title="数据分布自适应"></a>数据分布自适应</h3><p>​    数据分布自适应 (Distribution Adaptation) 是一类最常用的迁移学习方法。这种方法的基本思想是,由于源域和目标域的数据概率分布不同,那么最直接的方式就是通过一些变换,将不同的数据分布的距离拉近。</p>
<p>​    图 <a href="#bookmark84">19</a>形象地表示了几种数据分布的情况。简单来说，数据的边缘分布不同，就是数据整体不相似。数据的条件分布不同，就是数据整体相似，但是具体到每个类里，都不太相似。</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542812748062.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542812748062.png" srcset="data:image/png;base64,666" alt="1542812748062"></p>
<p></p><center>图 19: 不同数据分布的目标域数据</center><p></p>
<p>​    根据数据分布的性质,这类方法又可以分为边缘分布自适应、条件分布自适应、以及联合分布自适应。下面我们分别介绍每类方法的基本原理和代表性研究工作。介绍每类研究工作时,我们首先给出基本思路,然后介绍该类方法的核心,最后结合最近的相关工作介绍该类方法的扩展。</p>
<h3 id="边缘分布自适应"><a href="#边缘分布自适应" class="headerlink" title="边缘分布自适应"></a>边缘分布自适应</h3><p>​    边缘分布自适应方法 (Marginal Distribution Adaptation) 的目标是减小源域和目标域的边缘概率分布的距离,从而完成迁移学习。从形式上来说,边缘分布自适应方法是用P(X~s~)和 P(X~t~)之间的距离来近似两个领域之间的差异。即：</p>
<p>​    $DISTANCE(D~s~,D~t~)\approx\lVert P(X_s)-P(X_t)\Vert$ (6.1)</p>
<p>​    边缘分布自适应对应于图<a href="#bookmark84">19</a>中由图<a href="#bookmark101">19(a)</a>迁移到图<a href="#bookmark83">19(b)</a>的情形。</p>
<h3 id="条件分布自适应"><a href="#条件分布自适应" class="headerlink" title="条件分布自适应"></a>条件分布自适应</h3><p>​    条件分布自适应方法 (Conditional Distribution Adaptation) 的目标是减小源域和目标域的条件概率分布的距离，从而完成迁移学习。从形式上来说，条件分布自适应方法是用  P(y~s~|X~s~) 和 P (y~t~|X~t~) 之间的距离来近似两个领域之间的差异。即：</p>
<p>​    $DISTANCE(D~s~,D~t~)\approx\lVert P(y_s|X_s)-P(y_t|X_t)\Vert$(6.8)</p>
<p>​    条件分布自适应对应于图<a href="#bookmark84">19</a>中由图<a href="#bookmark101">19(a)</a>迁移到图<a href="#bookmark85">19(c)</a>的情形。</p>
<p>​    目前单独利用条件分布自适应的工作较少，这些工作主要可以在 [<a href="#bookmark292">Saito et al.,2017</a>] 中找到。最近，中科院计算所的 Wang 等人提出了 STL 方法(Stratified Transfer Learn­ing) [<a href="#bookmark309">Wang tal.,2018</a>]。作者提出了类内迁移 (Intra-class Transfer)的思想。指出现有的 绝大多数方法都只是学习一个全局的特征变换(Global DomainShift)，而忽略了类内的相 似性。类内迁移可以利用类内特征，实现更好的迁移效果。</p>
<p>​    STL 方法的基本思路如图所示。首先利用大多数投票的思想，对无标定的位置行为生成伪标；然后在再生核希尔伯特空间中，利用类内相关性进行自适应地空间降维，使得不同情境中的行为数据之间的相关性增大；最后，通过二次标定，实现对未知标定数据的精准标定。</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542817481582.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542817481582.png" srcset="data:image/png;base64,666" alt="1542817481582"></p>
<p></p><center>图 21: STL 方法的示意图</center><p></p>
<h3 id="11-3-4-联合分布自适应"><a href="#11-3-4-联合分布自适应" class="headerlink" title="11.3.4 联合分布自适应"></a>11.3.4 联合分布自适应</h3><p>​    联合分布自适应方法 (Joint Distribution Adaptation) 的目标是减小源域和目标域的联合概率分布的距离，从而完成迁移学习。从形式上来说，联合分布自适应方法是用<em>P</em>(<strong>x</strong>s) 和P(<strong>x</strong>t)之间的距离、以及P(ys|<strong>x</strong>s)和P(yt|<strong>x</strong>t)之间的距离来近似两个领域之间的差异。即:</p>
<p>​    $DISTANCE(D~s~,D~t~)\approx\lVert P(X_s)-P(X_t)\Vert-\lVert P(y_s|X_s)-P(y_t|X_t)\Vert$(6.10)</p>
<p>​    联合分布自适应对应于图<a href="#bookmark84">19</a>中由图<a href="#bookmark101">19(a)</a>迁移到图<a href="#bookmark83">19(b)</a>的情形、以及图<a href="#bookmark101">19(a)</a>迁移到<br>图<a href="#bookmark85">19(c)</a>的情形。</p>
<h3 id="概率分布自适应方法优劣性比较"><a href="#概率分布自适应方法优劣性比较" class="headerlink" title="概率分布自适应方法优劣性比较"></a>概率分布自适应方法优劣性比较</h3><p>综合上述三种概率分布自适应方法，我们可以得出如下的结论：</p>
<ol>
<li>精度比较： BDA &gt;JDA &gt;TCA &gt;条件分布自适应。</li>
<li>将不同的概率分布自适应方法用于神经网络，是一个发展趋势。图<a href="#bookmark119">23</a>展示的结果表明将概率分布适配加入深度网络中，往往会取得比非深度方法更好的结果。</li>
</ol>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542823019007.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542823019007.png" srcset="data:image/png;base64,666" alt="1542823019007"></p>
<p></p><center>图 22: BDA 方法的效果第二类方法：特征选择</center><p></p>
<h3 id="11-3-6-特征选择"><a href="#11-3-6-特征选择" class="headerlink" title="11.3.6 特征选择"></a>11.3.6 特征选择</h3><p>​    特征选择法的基本假设是：源域和目标域中均含有一部分公共的特征，在这部分公共的特征，源领域和目标领域的数据分布是一致的。因此，此类方法的目标就是，通过机器学习方法，选择出这部分共享的特征，即可依据这些特征构建模型。</p>
<p>​    图 <a href="#bookmark122">24</a>形象地表示了特征选择法的主要思路。</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542823210556.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542823210556.png" srcset="data:image/png;base64,666" alt="1542823210556"></p>
<p></p><center>图 23: 不同分布自适应方法的精度比较</center><p></p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/a3db84158d9b6454adff88dbe4fa5d28.jpg" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/a3db84158d9b6454adff88dbe4fa5d28.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p></p><center>图 24: 特征选择法示意图</center><p></p>
<p>​    这这个领域比较经典的一个方法是发表在 2006 年的 ECML-PKDD 会议上,作者提出了一个叫做 SCL 的方法 (Structural Correspondence Learning) [<a href="#bookmark235">Blitzer et al.,2006</a>]。这个方法的目标就是我们说的,找到两个领域公共的那些特征。作者将这些公共的特征叫做Pivot feature。找出来这些Pivot feature,就完成了迁移学习的任务。</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/4abacd82901988c3e0a98bdb07b2abc6.jpg" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/4abacd82901988c3e0a98bdb07b2abc6.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p></p><center>图 25: 特征选择法中的 Pivot feature 示意图</center><p></p>
<p>​    图 <a href="#bookmark124">25</a>形象地展示了 Pivot feature 的含义。 Pivot feature指的是在文本分类中,在不同领域中出现频次较高的那些词。总结起来：</p>
<ul>
<li>特征选择法从源域和目标域中选择提取共享的特征,建立统一模型</li>
<li>通常与分布自适应方法进行结合</li>
<li>通常采用稀疏表示 ||<strong>A</strong>||2,1 实现特征选择</li>
</ul>
<h3 id="统计特征对齐方法"><a href="#统计特征对齐方法" class="headerlink" title="统计特征对齐方法"></a>统计特征对齐方法</h3><p>​    统计特征对齐方法主要将数据的统计特征进行变换对齐。对齐后的数据，可以利用传统机器学习方法构建分类器进行学习。SA方法(Subspace Alignment，子空间对齐)[<a href="#bookmark249">Fernando et al.,2013</a>]是其中的代表性成果。SA方法直接寻求一个线性变换<strong>M</strong>，将不同的数据实现变换对齐。SA方法的优化目标如下：</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542823438846.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542823438846.png" srcset="data:image/png;base64,666" alt="1542823438846"></p>
<p>则变换 <strong>M</strong> 的值为：</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542823455820.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542823455820.png" srcset="data:image/png;base64,666" alt="1542823455820"></p>
<p>可以直接获得上述优化问题的闭式解：</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542823474720.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542823474720.png" srcset="data:image/png;base64,666" alt="1542823474720"></p>
<p>​    SA 方法实现简单，计算过程高效，是子空间学习的代表性方法。</p>
<h3 id="流形学习方法"><a href="#流形学习方法" class="headerlink" title="流形学习方法"></a>流形学习方法</h3><p><strong>什么是流形学习</strong></p>
<p>​    流形学习自从 2000 年在 Science 上被提出来以后,就成为了机器学习和数据挖掘领域的热门问题。它的基本假设是,现有的数据是从一个高维空间中采样出来的,所以,它具有高维空间中的低维流形结构。流形就是是一种几何对象（就是我们能想像能观测到的）。通俗点说就是,我们无法从原始的数据表达形式明显看出数据所具有的结构特征,那我把它想像成是处在一个高维空间,在这个高维空间里它是有个形状的。一个很好的例子就是星座。满天星星怎么描述？我们想像它们在一个更高维的宇宙空间里是有形状的,这就有了各自星座,比如织女座、猎户座。流形学习的经典方法有Isomap、locally linear embedding、 laplacian eigenmap 等。</p>
<p>​    流形空间中的距离度量：两点之间什么最短？在二维上是直线（线段）,可在三维呢？地球上的两个点的最短距离可不是直线,它是把地球展开成二维平面后画的那条直线。那条线在三维的地球上就是一条曲线。这条曲线就表示了两个点之间的最短距离,我们叫它测地线。更通俗一点, 两点之间，测地线最短。在流形学习中,我们遇到测量距离的时候更多的时候用的就是这个测地线。在我们要介绍的 GFK 方法中,也是利用了这个测地线距离。比如在下面的图中,从 A 到 C 最短的距离在就是展开后的线段,但是在三维球体上看它却是一条曲线。</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/fcbe02803e45f6455a4602b645b472c5.jpg" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/fcbe02803e45f6455a4602b645b472c5.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p></p><center>图 28: 三维空间中两点之间的距离示意图</center><p></p>
<p>​    由于在流形空间中的特征通常都有着很好的几何性质,可以避免特征扭曲,因此我们首先将原始空间下的特征变换到流形空间中。在众多已知的流形中, Grassmann 流形G（d） 可以通过将原始的 d 维子空间 （特征向量）看作它基础的元素,从而可以帮助学习分类 器。在 Grassmann流形中,特征变换和分布适配通常都有着有效的数值形式,因此在迁移学习问题中可以被很高效地表示和求解 [<a href="#bookmark260">Hamm and Lee,2008</a>]。因此,利用 Grassmann流形空间中来进行迁移学习是可行的。现存有很多方法可以将原始特征变换到流形空间 中[<a href="#bookmark257">Gopalan et al., 2011</a>, <a href="#bookmark230">Baktashmotlagh et al.,2014</a>]。</p>
<p>​    在众多的基于流形变换的迁移学习方法中，GFK(Geodesic Flow Kernel)方法[<a href="#bookmark255">Gong et<br>al., 2012</a>]是最为代表性的一个。GFK是在2011年发表在ICCV上的SGF方法[<a href="#bookmark257">Gopalan et al.,<br>2011</a>]发展起来的。我们首先介绍SGF方法。</p>
<p>​    SGF 方法从增量学习中得到启发：人类从一个点想到达另一个点，需要从这个点一步一步走到那一个点。那么，如果我们把源域和目标域都分别看成是高维空间中的两个点，由源域变换到目标域的过程不就完成了迁移学习吗？也就是说， 路是一步一步走出来的。</p>
<p>​    于是 SGF 就做了这个事情。它是怎么做的呢？把源域和目标域分别看成高维空间 (即Grassmann流形)中的两个点，在这两个点的测地线距离上取d个中间点，然后依次连接起来。这样，源域和目标域就构成了一条测地线的路径。我们只需要找到合适的每一步的变换，就能从源域变换到目标域了。图 <a href="#bookmark133">29</a>是 SGF 方法的示意图。</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/103de3658cbb97ad4c24bafe28f9d957.jpg" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/103de3658cbb97ad4c24bafe28f9d957.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p></p><center>图 29: SGF 流形迁移学习方法示意图</center><p></p>
<p>​    SGF 方法的主要贡献在于：提出了这种变换的计算及实现了相应的算法。但是它有很明显的缺点：到底需要找几个中间点？ SGF也没能给出答案，就是说这个参数d是没法估计的，没有一个好的方法。这个问题在 GFK 中被回答了。</p>
<p>​    GFK方法首先解决SGF的问题：如何确定中间点的个数d。它通过提出一种核学习的方法，利用路径上的无穷个点的积分，把这个问题解决了。这是第一个贡献。然后，它又解决了第二个问题：当有多个源域的时候，我们如何决定使用哪个源域跟目标域进行迁移？ GFK通过提出Rank of Domain度量，度量出跟目标域最近的源域，来解决这个问题。图 <a href="#bookmark134">30</a>是 GFK 方法的示意图。</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/e654d14df0b44ee4e8a0e505c654044b.jpg" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/e654d14df0b44ee4e8a0e505c654044b.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p></p><center>图 30: GFK 流形迁移学习方法示意图</center><p></p>
<p>​    用Ss和St分别表示源域和目标域经过主成分分析(PCA)之后的子空间，则G可以视为所有的d维子空间的集合。每一个d维的原始子空间都可以被看作G上的一个点。因此，在两点之间的测地线｛$(t) :0 \&lt; t \&lt;1｝可以在两个子空间之间构成一条路径。如果我 们令Ss = $(0)，St =$(1)，则寻找一条从$(0)到$(1)的测地线就等同于将原始的特征变换到一个无穷维度的空间中，最终减小域之间的漂移现象。这种方法可以被看作是一种从$(0)到$(1)的増量式“行走”方法。</p>
<p>​    特别地，流形空间中的特征可以被表示为<strong>z</strong> =$(t)T<strong>x</strong>。变换后的特征<strong>Z</strong>i和<strong>Z</strong>j的内积定义了一个半正定 (positive semidefinite) 的测地线流式核</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542823895008.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542823895008.png" srcset="data:image/png;base64,666" alt="1542823895008"></p>
<p>​    GFK 方法详细的计算过程可以参考原始的文章，我们在这里不再赘述。</p>
<h3 id="什么是finetune？"><a href="#什么是finetune？" class="headerlink" title="什么是finetune？"></a>什么是finetune？</h3><p>​    深度网络的finetune也许是最简单的深度网络迁移方法。<strong>Finetune</strong>,也叫微调、fine-tuning, 是深度学习中的一个重要概念。简而言之，finetune就是利用别人己经训练好的网络，针对自己的任务再进行调整。从这个意思上看，我们不难理解finetune是迁移学习的一部分。</p>
<p><strong>为什么需要已经训练好的网络？</strong></p>
<p>​    在实际的应用中,我们通常不会针对一个新任务,就去从头开始训练一个神经网络。这样的操作显然是非常耗时的。尤其是，我们的训练数据不可能像ImageNet那么大，可以训练出泛化能力足够强的深度神经网络。即使有如此之多的训练数据,我们从头开始训练,其代价也是不可承受的。</p>
<p>​    那么怎么办呢？迁移学习告诉我们,利用之前己经训练好的模型,将它很好地迁移到自己的任务上即可。</p>
<p><strong>为什么需要 finetune？</strong></p>
<p>​    因为别人训练好的模型,可能并不是完全适用于我们自己的任务。可能别人的训练数据和我们的数据之间不服从同一个分布；可能别人的网络能做比我们的任务更多的事情；可能别人的网络比较复杂,我们的任务比较简单。</p>
<p>​    举一个例子来说,假如我们想训练一个猫狗图像二分类的神经网络,那么很有参考价值的就是在 CIFAR-100 上训练好的神经网络。但是 CIFAR-100 有 100 个类别,我们只需要 2个类别。此时,就需要针对我们自己的任务,固定原始网络的相关层,修改网络的输出层以使结果更符合我们的需要。</p>
<p>​    图<a href="#bookmark148">36</a>展示了一个简单的finetune过程。从图中我们可以看到，我们采用的预训练好的网络非常复杂,如果直接拿来从头开始训练,则时间成本会非常高昂。我们可以将此网络进行改造,固定前面若干层的参数,只针对我们的任务,微调后面若干层。这样,网络训练速度会极大地加快,而且对提高我们任务的表现也具有很大的促进作用。</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/b1630ca5d004d4b430672c8b8ce7fb90.jpg" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/b1630ca5d004d4b430672c8b8ce7fb90.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p></p><center>图 36: 一个简单的 finetune 示意图<br><strong>Finetune 的优势</strong></center><p></p>
<p>​    Finetune 的优势是显然的，包括:</p>
<ul>
<li>不需要针对新任务从头开始训练网络，节省了时间成本；</li>
<li>预训练好的模型通常都是在大数据集上进行的，无形中扩充了我们的训练数据，使得模型更鲁棒、泛化能力更好；</li>
<li>Finetune 实现简单，使得我们只关注自己的任务即可。</li>
</ul>
<p><strong>Finetune 的扩展</strong></p>
<p>​    在实际应用中，通常几乎没有人会针对自己的新任务从头开始训练一个神经网络。Fine-tune 是一个理想的选择。</p>
<p>​    Finetune 并不只是针对深度神经网络有促进作用，对传统的非深度学习也有很好的效果。例如， finetune对传统的人工提取特征方法就进行了很好的替代。我们可以使用深度网络对原始数据进行训练，依赖网络提取出更丰富更有表现力的特征。然后，将这些特征作为传统机器学习方法的输入。这样的好处是显然的: 既避免了繁复的手工特征提取，又能自动地提取出更有表现力的特征。</p>
<p>​    比如，图像领域的研究，一直是以 SIFT、SURF 等传统特征为依据的，直到 2014 年，伯克利的研究人员提出了 DeCAF特征提取方法［<a href="#bookmark246">Donahue et al.,2014</a>］，直接使用深度卷积神经网络进行特征提取。实验结果表明，该特征提取方法对比传统的图像特征，在精度上有着无可匹敌的优势。另外，也有研究人员用卷积神经网络提取的特征作为SVM分类器的输 入［<a href="#bookmark291">Razavian et al.,014</a>］，显著提升了图像分类的精度。</p>
<h3 id="finetune为什么有效？"><a href="#finetune为什么有效？" class="headerlink" title="finetune为什么有效？"></a>finetune为什么有效？</h3><p>​    随着 AlexNet [<a href="#bookmark268">Krizhevsky et al., 2012</a>] 在 2012 年的 ImageNet大赛上获得冠军，深度学习开始在机器学习的研究和应用领域大放异彩。尽管取得了很好的结果，但是神经网络本身就像一个黑箱子，看得见，摸不着，解释性不好。由于神经网络具有良好的层次结构很自然地就有人开始关注，能否通过这些层次结构来很好地解释网络？于是，有了我们熟知的例子：假设一个网络要识别一只猫，那么一开始它只能检测到一些边边角角的东西，和猫根本没有关系；然后可能会检测到一些线条和圆形；慢慢地，可以检测到有猫的区域；接着是猫腿、猫脸等等。图 <a href="#bookmark137">32</a>是一个简单的示例。</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542824195602.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542824195602.png" srcset="data:image/png;base64,666" alt="1542824195602"></p>
<p></p><center>图 32: 深度神经网络进行特征提取到分类的简单示例</center><p></p>
<p>​    这表达了一个什么事实呢？概括来说就是：前面几层都学习到的是通用的特征（general feature）；随着网络层次的加深，后面的网络更偏重于学习任务特定的特征（specific feature）。<br>这非常好理解，我们也都很好接受。那么问题来了：如何得知哪些层能够学习到 general feature，哪些层能够学习到specific feature。更进一步：如果应用于迁移学习，如何决定该迁移哪些层、固定哪些层？</p>
<p>​    这个问题对于理解神经网络以及深度迁移学习都有着非常重要的意义。</p>
<p>​    来自康奈尔大学的 Jason Yosinski 等人 [<a href="#bookmark318">Yosinski et al., 2014</a>]率先进行了深度神经网络可迁移性的研究，将成果发表在2014年机器学习领域顶级会议NIPS上并做了口头汇报。该论文是一篇实验性质的文章（通篇没有一个公式）。其目的就是要探究上面我们提到的几个关键性问题。因此，文章的全部贡献都来自于实验及其结果。（别说为啥做实验也能发文章：都是高考，我只上了个普通一本，我高中同学就上了清华）</p>
<p>​    在ImageNet的1000类上，作者把1000类分成两份（A和B），每份500个类别。然后，分别对A和B基于Caffe训练了一个AlexNet网络。一个AlexNet网络一共有8层， 除去第8层是类别相关的网络无法迁移以外，作者在 1 到 7这 7层上逐层进行 finetune 实验，探索网络的可迁移性。</p>
<p>​    为了更好地说明 finetune 的结果，作者提出了有趣的概念： AnB 和 BnB。</p>
<p>​    迁移A网络的前n层到B （AnB） vs固定B网络的前n层（BnB）</p>
<p>​    简单说一下什么叫AnB:（所有实验都是针对数据B来说的）将A网络的前n层拿来并将它frozen，剩下的8 - n层随机初始化，然后对B进行分类。</p>
<p>​    相应地，有BnB:把训练好的B网络的前n层拿来并将它frozen，剩下的8 - n层随机初始化，然后对 B 进行分类。</p>
<p>​    <strong>实验结果</strong></p>
<p>​    实验结果如下图（图<a href="#bookmark145">33</a>） 所示:</p>
<p>​    这个图说明了什么呢？我们先看蓝色的BnB和BnB+（就是BnB加上finetune）。对 BnB而言，原训练好的 B 模型的前 3 层直接拿来就可以用而不会对模型精度有什么损失到了第4 和第5 层，精度略有下降，不过还是可以接受。然而到了第6 第第7层，精度居然奇迹般地回升了！这是为什么？原因如下:对于一开始精度下降的第4 第 5 层来说，确</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542824318155.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542824318155.png" srcset="data:image/png;base64,666" alt="1542824318155"></p>
<p></p><center>图 33: 深度网络迁移实验结果 1</center><p></p>
<p>实是到了这一步，feature变得越来越specific,所以下降了。那对于第6第7层为什么精度又不变了？那是因为，整个网络就8层，我们固定了第6第7层，这个网络还能学什么呢？所以很自然地，精度和原来的 B 网络几乎一致！</p>
<p>​    对 BnB+ 来说，结果基本上都保持不变。说明 finetune 对模型结果有着很好的促进作用！</p>
<p>​    我们重点关注AnB和AnB+。对AnB来说，直接将A网络的前3层迁移到B,貌似不会有什么影响，再一次说明，网络的前3层学到的几乎都是general feature!往后，到了第4第5层的时候，精度开始下降，我们直接说：一定是feature不general 了！然而，到了第6第7层，精度出现了小小的提升后又下降，这又是为什么？作者在这里提出两点co-adaptation和feature representation。就是说，第4第5层精度下降的时候，主要是由于A和B两个数据集的差异比较大，所以会下降；至I」了第6第7层，由于网络几乎不迭代了，学习能力太差，此时 feature 学不到，所以精度下降得更厉害。</p>
<p>​    再看AnB+。加入了 finetune以后，AnB+的表现对于所有的n几乎都非常好，甚至 比baseB<br>（最初的B）还要好一些！这说明：finetune对于深度迁移有着非常好的促进作用!</p>
<p>​    把上面的结果合并就得到了下面一张图 （图<a href="#bookmark138">34</a>）：</p>
<p>​    至此， AnB 和 BnB 基本完成。作者又想，是不是我分 A 和 B 数据的时候，里面存在一些比较相似的类使结果好了？比如说A里有猫，B里有狮子，所以结果会好？为了排除这些影响，作者又分了一下数据集，这次使得A和B里几乎没有相似的类别。在这个条件下再做AnB,与原来精度比较（0%为基准）得到了下图（图<a href="#bookmark139">35</a>）:</p>
<p>​    这个图说明了什么呢？简单：随着可迁移层数的增加，模型性能下降。但是，前3层仍然还是可以迁移的！同时,与随机初始化所有权重比较,迁移学习的精度是很高的!总之：</p>
<ul>
<li>深度迁移网络要比随机初始化权重效果好；</li>
</ul>
<ul>
<li>网络层数的迁移可以加速网络的学习和优化。</li>
</ul>
<h3 id="什么是深度网络自适应？"><a href="#什么是深度网络自适应？" class="headerlink" title="什么是深度网络自适应？"></a>什么是深度网络自适应？</h3><p><strong>基本思路</strong></p>
<p>​    深度网络的 finetune 可以帮助我们节省训练时间，提高学习精度。但是 finetune 有它的先天不足:它无法处理训练数据和测试数据分布不同的情况。而这一现象在实际应用中比比皆是。因为 finetune 的基本假设也是训练数据和测试数据服从相同的数据分布。这在迁移学习中也是不成立的。因此，我们需要更进一步，针对深度网络开发出更好的方法使之更好地完成迁移学习任务。</p>
<p>​    以我们之前介绍过的数据分布自适应方法为参考，许多深度学习方法［<a href="#bookmark307">Tzeng et al.,2014</a>, <a href="#bookmark275">Long et al.,2015a</a>］都开发出了自适应层(AdaptationLayer)来完成源域和目标域数据的自适应。自适应能够使得源域和目标域的数据分布更加接近，从而使得网络的效果更好。</p>
<p>​    从上述的分析我们可以得出，深度网络的自适应主要完成两部分的工作:</p>
<p>​    一是哪些层可以自适应，这决定了网络的学习程度；</p>
<p>​    二是采用什么样的自适应方法 (度量准则)，这决定了网络的泛化能力。</p>
<p>​    深度网络中最重要的是网络损失的定义。绝大多数深度迁移学习方法都采用了以下的损失定义方式:</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542824918145.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542824918145.png" srcset="data:image/png;base64,666" alt="1542824918145"></p>
<p>​    其中，I表示网络的最终损失，lc(Ds,<strong>y</strong>s)表示网络在有标注的数据(大部分是源域)上的常规分类损失(这与普通的深度网络完全一致)，Ia(Ds,Dt)表示网络的自适应损失。最后一部分是传统的深度网络所不具有的、迁移学习所独有的。此部分的表达与我们先前讨论过的源域和目标域的分布差异，在道理上是相同的。式中的A是权衡两部分的权重参数。</p>
<p>​    上述的分析指导我们设计深度迁移网络的基本准则：决定自适应层，然后在这些层加入自适应度量，最后对网络进行 finetune。</p>
<h3 id="GAN在迁移学习中的应用"><a href="#GAN在迁移学习中的应用" class="headerlink" title="GAN在迁移学习中的应用"></a>GAN在迁移学习中的应用</h3><p>生成对抗网络 GAN(Generative Adversarial Nets) [<a href="#bookmark256">Goodfellow et al.,2014</a>] 是目前人工智能领域最炙手可热的概念之一。其也被深度学习领军人物 Yann Lecun 评为近年来最令人欣喜的成就。由此发展而来的对抗网络，也成为了提升网络性能的利器。本小节介绍深度对抗网络用于解决迁移学习问题方面的基本思路以及代表性研究成果。</p>
<p><strong>基本思路</strong></p>
<p>​    GAN 受到自博弈论中的二人零和博弈 (two-player game) 思想的启发而提出。它一共包括两个部分：一部分为生成网络(Generative Network)，此部分负责生成尽可能地以假乱真的样本，这部分被成为生成器(Generator)；另一部分为判别网络(Discriminative Network), 此部分负责判断样本是真实的，还是由生成器生成的，这部分被成为判别器(Discriminator) 生成器和判别器的互相博弈，就完成了对抗训练。</p>
<p>​    GAN 的目标很明确：生成训练样本。这似乎与迁移学习的大目标有些许出入。然而，由于在迁移学习中，天然地存在一个源领域，一个目标领域，因此，我们可以免去生成样本的过程，而直接将其中一个领域的数据 (通常是目标域) 当作是生成的样本。此时，生成器的职能发生变化，不再生成新样本，而是扮演了特征提取的功能：不断学习领域数据的特征使得判别器无法对两个领域进行分辨。这样，原来的生成器也可以称为特征提取器<br>(Feature Extractor)。</p>
<p>​    通常用 Gf 来表示特征提取器，用 Gd 来表示判别器。正是基于这样的领域对抗的思想，深度对抗网络可以被很好地运用于迁移学习问题中。与深度网络自适应迁移方法类似，深度对抗网络的损失也由两部分构成：网络训练的损失lc*和领域判别损失Id：</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542826334834.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542826334834.png" srcset="data:image/png;base64,666" alt="1542826334834"></p>
<p><strong>DANN</strong></p>
<p>Yaroslav Ganin 等人 [<a href="#bookmark251">Ganin et al., 2016</a>]首先在神经网络的训练中加入了对抗机制，作者将他们的网络称之为DANN(Domain-Adversarial Neural Network)。在此研宄中，网络的学习目标是：生成的特征尽可能帮助区分两个领域的特征，同时使得判别器无法对两个领域的差异进行判别。该方法的领域对抗损失函数表示为：</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542826461988.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542826461988.png" srcset="data:image/png;base64,666" alt="1542826461988"></p>
<p>Id = max 其中的 Ld 表示为</p>
<p><img src="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542826475517.png" class="lazyload" data-srcset="/zh-TW/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/1542826475517.png" srcset="data:image/png;base64,666" alt="1542826475517"></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>网络搭建及训练</title>
    <url>/zh-TW/ch12_%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="网络搭建及训练"><a href="#网络搭建及训练" class="headerlink" title="网络搭建及训练"></a>网络搭建及训练</h1><h1 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h1><h2 id="TensorFlow是什么？"><a href="#TensorFlow是什么？" class="headerlink" title="TensorFlow是什么？"></a>TensorFlow是什么？</h2><p>  TensorFlow支持各种异构平台，支持多CPU/GPU、服务器、移动设备，具有良好的跨平台的特性；TensorFlow架构灵活，能够支持各种网络模型，具有良好的通用性；此外，TensorFlow架构具有良好的可扩展性，对OP的扩展支持，Kernel特化方面表现出众。</p>
<p>  TensorFlow最初由Google大脑的研究员和工程师开发出来，用于机器学习和神经网络方面的研究，于2015.10宣布开源，在众多深度学习框架中脱颖而出，在Github上获得了最多的Star量。</p>
<h2 id="TensorFlow的设计理念是什么？"><a href="#TensorFlow的设计理念是什么？" class="headerlink" title="TensorFlow的设计理念是什么？"></a>TensorFlow的设计理念是什么？</h2><p>TensorFlow的设计理念主要体现在两个方面：</p>
<p>（1）将图定义和图运算完全分开。<br>  TensorFlow 被认为是一个“符号主义”的库。我们知道，编程模式通常分为命令式编程（imperative style programming）和符号式编程（symbolic style programming）。命令式编程就是编写我们理解的通常意义上的程序，很容易理解和调试，按照原有逻辑执行。符号式编程涉及很多的嵌入和优化，不容易理解和调试，但运行速度相对有所提升。现有的深度学习框架中，Torch 是典型的命令式的，Caffe、MXNet 采用了两种编程模式混合的方法，而 TensorFlow 完全采用符号式编程。</p>
<p>  符号式计算一般是先定义各种变量，然后建立一个数据流图，在数据流图中规定各个变量间的计算关系，最后需要对据流图进行编译，但此时的数据流图还是一个空壳儿，里面没有任何实际数据，只有把需要运算的输入放进去后，才能在整个模型中形成数据流，从而形成输出值。</p>
<p>　　例如：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">t = 8 + 9</span><br><span class="line">print(t)</span><br></pre></td></tr></tbody></table></figure>
<p>  在传统的程序操作中，定义了 t 的运算，在运行时就执行了，并输出 17。而在 TensorFlow中，数据流图中的节点，实际上对应的是 TensorFlow API 中的一个操作，并没有真正去运行：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">t = tf.add(8,9)</span><br><span class="line">print(t)</span><br><span class="line"></span><br><span class="line">#输出  Tensor{"Add_1:0",shape={},dtype=int32}</span><br></pre></td></tr></tbody></table></figure>
<p>  （2）TensorFlow 中涉及的运算都要放在图中，而图的运行只发生在会话（session）中。开启会话后，就可以用数据去填充节点，进行运算；关闭会话后，就不能进行计算了。因此，会话提供了操作运行和 Tensor 求值的环境。</p>
<p>　　例如：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">#创建图</span><br><span class="line">a = tf.constant([4.0,5.0])</span><br><span class="line">b = tf.constant([6.0,7.0])</span><br><span class="line">c = a * b</span><br><span class="line">#创建会话</span><br><span class="line">sess  = tf.Session()</span><br><span class="line">#计算c</span><br><span class="line">print(sess.run(c))   #进行矩阵乘法，输出[24.,35.]</span><br><span class="line">sess.close()</span><br></pre></td></tr></tbody></table></figure>
<h2 id="TensorFlow特点有哪些？"><a href="#TensorFlow特点有哪些？" class="headerlink" title="TensorFlow特点有哪些？"></a>TensorFlow特点有哪些？</h2><h3 id="1-高度的灵活性"><a href="#1-高度的灵活性" class="headerlink" title="1.高度的灵活性"></a>1.高度的灵活性</h3><p>  TensorFlow 并不仅仅是一个深度学习库，只要可以把你的计算过程表示称一个数据流图的过程，我们就可以使用 TensorFlow 来进行计算。TensorFlow 允许我们用计算图的方式建立计算网络，同时又可以很方便的对网络进行操作。用户可以基于 TensorFlow 的基础上用 python 编写自己的上层结构和库，如果TensorFlow没有提供我们需要的API的，我们也可以自己编写底层的 C++ 代码，通过自定义操作将新编写的功能添加到 TensorFlow 中。</p>
<h3 id="2-真正的可移植性"><a href="#2-真正的可移植性" class="headerlink" title="2.真正的可移植性"></a>2.真正的可移植性</h3><p>  TensorFlow 可以在 CPU 和 GPU 上运行，可以在台式机、服务器、移动设备上运行。你想在你的笔记本上跑一下深度学习的训练，或者又不想修改代码，想把你的模型在多个CPU上运行， 亦或想将训练好的模型放到移动设备上跑一下，这些TensorFlow都可以帮你做到。</p>
<h3 id="3-多语言支持"><a href="#3-多语言支持" class="headerlink" title="3.多语言支持"></a>3.多语言支持</h3><p>  TensorFlow采用非常易用的python来构建和执行我们的计算图，同时也支持 C++ 的语言。我们可以直接写python和C++的程序来执行TensorFlow，也可以采用交互式的ipython来方便的尝试我们的想法。当然，这只是一个开始，后续会支持更多流行的语言，比如Lua，JavaScript 或者R语言。</p>
<h3 id="4-丰富的算法库"><a href="#4-丰富的算法库" class="headerlink" title="4.丰富的算法库"></a>4.丰富的算法库</h3><p>  TensorFlow提供了所有开源的深度学习框架里，最全的算法库，并且在不断的添加新的算法库。这些算法库基本上已经满足了大部分的需求，对于普通的应用，基本上不用自己再去自定义实现基本的算法库了。</p>
<h3 id="5-完善的文档"><a href="#5-完善的文档" class="headerlink" title="5.完善的文档"></a>5.完善的文档</h3><p>  TensorFlow的官方网站，提供了非常详细的文档介绍，内容包括各种API的使用介绍和各种基础应用的使用例子，也包括一部分深度学习的基础理论。</p>
<p>  自从宣布开源以来，大量人员对TensorFlow做出贡献，其中包括Google员工，外部研究人员和独立程序员，全球各地的工程师对TensorFlow的完善，已经让TensorFlow社区变成了Github上最活跃的深度学习框架。</p>
<h2 id="TensorFlow的系统架构是怎样的？"><a href="#TensorFlow的系统架构是怎样的？" class="headerlink" title="TensorFlow的系统架构是怎样的？"></a>TensorFlow的系统架构是怎样的？</h2><h3 id="emsp-emsp-整个系统从底层到上层可分为七层："><a href="#emsp-emsp-整个系统从底层到上层可分为七层：" class="headerlink" title="  整个系统从底层到上层可分为七层："></a>  整个系统从底层到上层可分为七层：</h3><p><img src="/zh-TW/ch12_%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/img\ch12\1.bmp" class="lazyload" data-srcset="/zh-TW/ch12_%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/img\ch12\1.bmp" srcset="data:image/png;base64,666" alt=""></p>
<p>  设备层：硬件计算资源，支持CPU、GPU</p>
<p>  网络层：支持两种通信协议</p>
<p>  数值计算层：提供最基础的计算，有线性计算、卷积计算</p>
<p>  高维计算层：数据的计算都是以数组的形式参与计算</p>
<p>  计算图层：用来设计神经网络的结构</p>
<p>  工作流层：提供轻量级的框架调用</p>
<p>  构造层：最后构造的深度学习网络可以通过TensorBoard服务端可视化</p>
<h2 id="TensorFlow编程模型是怎样的？"><a href="#TensorFlow编程模型是怎样的？" class="headerlink" title="TensorFlow编程模型是怎样的？"></a>TensorFlow编程模型是怎样的？</h2><p>TensorFlow的编程模型：让向量数据在计算图里流动。那么在编程时至少有这几个过程：1.构建图，2.启动图，3.给图输入数据并获取结果。</p>
<h3 id="1-构建图"><a href="#1-构建图" class="headerlink" title="1.构建图"></a>1.构建图</h3><p>TensorFlow的图的类型是tf.Graph，它包含着计算节点和tensor的集合。</p>
<p>  这里引用了两个新概念：tensor和计算节点。<br>  我们先介绍tensor，一开始我们就介绍了，我们需要把数据输入给启动的图才能获取计算结果。那么问题来了，在构建图时用什么表示中间计算结果？这个时候tensor的概念就需要引入了。<br>  类型是tf.Tensor，代表某个计算节点的输出，一定要看清楚是“代表”。它主要有两个作用：</p>
<p>1.构建不同计算节点之间的数据流</p>
<p>2.在启动图时，可以设置某些tensor的值，然后获取指定tensor的值。这样就完成了计算的输入输出功能。</p>
<p>如下代码所示：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">inImage = tf.placeholder(tf.float32,[32,32,3],"inputImage")</span><br><span class="line">processedImage = tf.image.per_image_standardization(inImage,"processedImage")</span><br></pre></td></tr></tbody></table></figure>
<p>  这里inImage和processedImage都是tensor类型。它们代表着计算节点输出的数据，数据的值具体是多少在启动图的时候才知道。上面两个方法调用都传递了一个字符串，它是计算节点的名字，最好给节点命名，这样我们可以在图上调用get_tensor_by_name(name)获取对应的tensor对象，十分方便。（tensor名字为“&lt;计算节点名字&gt;:<tensor索引>”）</tensor索引></p>
<p>  创建tensor时，需要指定类型和shape。对不同tensor进行计算时要求类型相同，可以使用 tf.cast 进行类型转换。同时也要求 shape (向量维度)满足运算的条件，我们可以使用 tf.reshape 改变shape。</p>
<p>  现在了解计算节点的概念，其功能是对tensor进行计算、创建tensor或进行其他操作，类型是tf.Operation。获取节点对象的方法为get_operation_by_name(name)。</p>
<p>构建图，如下代码：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">g=tf.Graph()</span><br><span class="line"></span><br><span class="line">with g.as_default():</span><br><span class="line">    input_data=tf.placeholder(tf.float32,[None,2],"input_data")</span><br><span class="line">    input_label=tf.placeholder(tf.float32,[None,2],"input_label")</span><br><span class="line"></span><br><span class="line">    W1=tf.Variable(tf.truncated_normal([2,2]),name="W1")</span><br><span class="line">    B1=tf.Variable(tf.zeros([2]),name="B1")</span><br><span class="line"></span><br><span class="line">    output=tf.add(tf.matmul(input_data,W1),B1,name="output")</span><br><span class="line">    cross_entropy=tf.nn.softmax_cross_entropy_with_logits(logits=output,labels=input_label)</span><br><span class="line"></span><br><span class="line">    train_step=tf.train.AdamOptimizer().minimize(cross_entropy,name="train_step")</span><br><span class="line"></span><br><span class="line">    initer=tf.global_variables_initializer()</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>  上面的代码中我们创建了一个图，并在上面添加了很多节点。我们可以通过调用get_default_graph()获取默认的图。</p>
<p>  Input_data，input_label，W1，B1，output，cross_entropy都是tensor类型，train_step，initer，是节点类型。</p>
<p>有几类tensor或节点比较重要，下面介绍一下：</p>
<h4 id="1-placeholder"><a href="#1-placeholder" class="headerlink" title="1.placeholder"></a>1.placeholder</h4><p>  Tensorflow，顾名思义， tensor代表张量数据，flow代表流，其最初的设计理念就是构建一张静态的数据流图。图是有各个计算节点连接而成，计算节点之间流动的便是中间的张量数据。要想让张量数据在我们构建的静态计算图中流动起来，就必须有最初的输入数据流。而placeholder，翻译过来叫做占位符，顾名思义，是给我们的输入数据提供一个接口，也就是说我们的一切输入数据，例如训练样本数据，超参数数据等都可以通过占位符接口输送到数据流图之中。使用实例如下代码：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">x = tf.placeholder(dtype=tf.float32,shape=[],name='x')</span><br><span class="line">y = tf.placeholder(dtpe=tf.float32,shape=[],nmae='y')</span><br><span class="line">z = x*y</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">	prod = sess.run(z,feed_dict={x:1.,y:5.2})</span><br><span class="line">	print(prod)</span><br><span class="line">[out]:5.2</span><br></pre></td></tr></tbody></table></figure>
<h4 id="2-variable"><a href="#2-variable" class="headerlink" title="2. variable"></a>2. variable</h4><p>  无论是传统的机器学习算法，例如线性支持向量机（Support Vector Machine, SVM)，其数学模型为y = <w,x> + b，还是更先进的深度学习算法，例如卷积神经网络（Convolutional Neural Network， CNN）单个神经元输出的模型y = w*x + b。可以看到，w和b就是我们要求的模型，模型的求解是通过优化算法（对于SVM，使用<br>SMO[1]算法，对于CNN，一般基于梯度下降法）来一步一步更新w和b的值直到满足停止条件。因此，大多数机器学习的模型中的w和b实际上是以变量的形式出现在代码中的，这就要求我们在代码中定义模型变量。</w,x></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a = tf.Variable(2.)</span><br><span class="line">b = tf.Variable(3.)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">	sess.run(tf.global_variables_initializer()) #变量初始化</span><br><span class="line">    print(sess.run(a*b))</span><br><span class="line">[out]:6.</span><br></pre></td></tr></tbody></table></figure>
<p>[1] Platt, John. “Sequential minimal optimization: A fast algorithm for training support vector machines.” (1998).</p>
<h4 id="3-initializer"><a href="#3-initializer" class="headerlink" title="3. initializer"></a>3. initializer</h4><p>  由于tensorflow构建的是静态的计算流图，在开启会话之前，所有的操作都不会被执行。因此为了执行在计算图中所构建的赋值初始化计算节点，需要在开启会话之后，在会话环境下运行初始化。如果计算图中定义了变量，而会话环境下为执行初始化命令，则程序报错，代码如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a = tf.Variable(2.)</span><br><span class="line">b = tf.Variable(3.)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">	#sess.run(tf.global_variables_initializer()) #注释掉初始化命令</span><br><span class="line">    print(sess.run(a*b))</span><br><span class="line">[Error]: Attempting to use uninitialized value Variable</span><br></pre></td></tr></tbody></table></figure>
<h3 id="2-启动图"><a href="#2-启动图" class="headerlink" title="2.启动图"></a>2.启动图</h3><p>  先了解session的概念，然后才能更好的理解图的启动。<br>  图的每个运行实例都必须在一个session里，session为图的运行提供环境。Session的类型是tf.Session，在实例化session对象时我们需要给它传递一个图对象，如果不显示给出将使用默认的图。Session有一个graph属性，我们可以通过它获取session对应的图。</p>
<p>代码如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">numOfBatch=5</span><br><span class="line">datas=np.zeros([numOfBatch,2],np.float32)</span><br><span class="line">labels=np.zeros([numOfBatch,2],np.float32)</span><br><span class="line"></span><br><span class="line">sess=tf.Session(graph=g)</span><br><span class="line">graph=sess.graph</span><br><span class="line">sess.run([graph.get_operation_by_name("initer")])</span><br><span class="line"></span><br><span class="line">dataHolder=graph.get_tensor_by_name("input_data:0")</span><br><span class="line">labelHolder=graph.get_tensor_by_name("input_label:0")</span><br><span class="line">train=graph.get_operation_by_name("train_step")</span><br><span class="line">out=graph.get_tensor_by_name("output:0")</span><br><span class="line"></span><br><span class="line">for i inrange(200):</span><br><span class="line">   result=sess.run([out,train],feed_dict={dataHolder:datas,labelHolder:labels})</span><br><span class="line">   if i%100==0:</span><br><span class="line">       saver.save(sess,"./moules")</span><br><span class="line"></span><br><span class="line">sess.close()</span><br></pre></td></tr></tbody></table></figure>
<p>代码都比较简单，就不介绍了。不过要注意2点：1.别忘记运行初始化节点，2.别忘记close掉session对象以释放资源。</p>
<h4 id="3-给图输入数据并获取结果"><a href="#3-给图输入数据并获取结果" class="headerlink" title="3.给图输入数据并获取结果"></a>3.给图输入数据并获取结果</h4><p>代码：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">for i inrange(200):</span><br><span class="line">    result=sess.run([out,train],feed_dict={dataHolder:datas,labelHolder:labels})</span><br></pre></td></tr></tbody></table></figure>
<p>  这里主要用到了session对象的run方法，它用来运行某个节点或tensor并获取对应的值。我们一般会一次传递一小部分数据进行mini-batch梯度下降来优化模型。</p>
<p>  我们需要把我们需要运行的节点或tensor放入一个列表，然后作为第一个参数(不考虑self)传递给run方法，run方法会返回一个计算结果的列表，与我们传递的参数一一对应。</p>
<p>  如果我们运行的节点依赖某个placeholder，那我们必须给这个placeholder指定值，怎么指定代码里面很清楚，给关键字参数feed_dict传递一个字典即可，字典里的元素的key是placeholder对象，value是我们指定的值。值的数据的类型必须和placeholder一致，包括shape。值本身的类型是numpy数组。</p>
<p>这里再解释一个细节，在定义placeholder时代码如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">input_data=tf.placeholder(tf.float32,[None,2],"input_data")</span><br><span class="line">input_label=tf.placeholder(tf.float32,[None,2],"input_label")</span><br></pre></td></tr></tbody></table></figure>
<p>  shape为[None,2]，说明数据第一个维度是不确定的，然后TensorFlow会根据我们传递的数据动态推断第一个维度，这样我们就可以在运行时改变batch的大小。比如一个数据是2维，一次传递10个数据对应的tensor的shape就是[10,2]。可不可以把多个维度指定为None？理论上不可以！</p>
<h2 id="如何基于tensorflow搭建VGG16"><a href="#如何基于tensorflow搭建VGG16" class="headerlink" title="如何基于tensorflow搭建VGG16"></a>如何基于tensorflow搭建VGG16</h2><p>​    介绍完关于tensorflow的基础知识，是时候来一波网络搭建实战了。虽然网上有很多相关教程，但我想从最标准的tensorflow代码和语法出发（而不是调用更高级的API，失去了原来的味道），向大家展示如何搭建其标准的VGG16网络架构。话不多说，上代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_weight_variable</span>(<span class="params">shape</span>):</span><br><span class="line">    <span class="keyword">return</span> tf.get_variable(<span class="string">'weight'</span>, shape=shape, initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_bias_variable</span>(<span class="params">shape</span>):</span><br><span class="line">    <span class="keyword">return</span> tf.get_variable(<span class="string">'bias'</span>, shape=shape, initializer=tf.constant_initializer(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d</span>(<span class="params">x, w, padding = <span class="string">'SAME'</span>, s=<span class="number">1</span></span>):</span><br><span class="line">    x = tf.nn.conv2d(x, w, strides=[<span class="number">1</span>, s, s, <span class="number">1</span>], padding = padding)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">maxPoolLayer</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                          strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d_layer</span>(<span class="params">x,in_chs, out_chs, ksize, layer_name</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(layer_name):</span><br><span class="line">        w = get_weight_variable([ksize, ksize, in_chs, out_chs])</span><br><span class="line">        b = get_bias_variable([out_chs])</span><br><span class="line">        y = tf.nn.relu(tf.bias_add(conv2d(x,w,padding = <span class="string">'SAME'</span>, s=<span class="number">1</span>), b))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fc_layer</span>(<span class="params">x,in_kernels, out_kernels, layer_name</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(layer_name):</span><br><span class="line">        w = get_weight_variable([in_kernels,out_kernels])</span><br><span class="line">        b = get_bias_variable([out_kernels])</span><br><span class="line">        y = tf.nn.relu(tf.bias_add(tf.matmul(x,w),b))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">VGG16</span>(<span class="params">x</span>):</span><br><span class="line">    conv1_1 = conv2d_layer(x,tf.get_shape(x).as_list()[-<span class="number">1</span>], <span class="number">64</span>, <span class="number">3</span>, <span class="string">'conv1_1'</span>)</span><br><span class="line">    conv1_2 = conv2d_layer(conv1_1,<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="string">'conv1_2'</span>)</span><br><span class="line">    pool_1 = maxPoolLayer(conv1_2)</span><br><span class="line">    </span><br><span class="line">    conv2_1 = conv2d_layer(pool1,<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="string">'conv2_1'</span>)</span><br><span class="line">    conv2_2 = conv2d_layer(conv2_1,<span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="string">'conv2_2'</span>)</span><br><span class="line">    pool2 = maxPoolLayer(conv2_2)</span><br><span class="line">    </span><br><span class="line">	conv3_1 = conv2d_layer(pool2,<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="string">'conv3_1'</span>)</span><br><span class="line">    conv3_2 = conv2d_layer(conv3_1,<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="string">'conv3_2'</span>)</span><br><span class="line">	conv3_3 = conv2d_layer(conv3_2,<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="string">'conv3_3'</span>)</span><br><span class="line">    pool3 = maxPoolLayer(conv3_3)</span><br><span class="line">    </span><br><span class="line">	conv4_1 = conv2d_layer(pool3,<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv4_1'</span>)</span><br><span class="line">    conv4_2 = conv2d_layer(conv4_1,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv4_2'</span>)</span><br><span class="line">	conv4_3 = conv2d_layer(conv4_2,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv4_3'</span>)</span><br><span class="line">    pool4 = maxPoolLayer(conv4_3)</span><br><span class="line">    </span><br><span class="line">	conv5_1 = conv2d_layer(pool4,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv5_1'</span>)</span><br><span class="line">    conv5_2 = conv2d_layer(conv5_1,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv5_2'</span>)</span><br><span class="line">	conv5_3 = conv2d_layer(conv5_1,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv5_3'</span>)</span><br><span class="line">    pool5 = maxPoolLayer(conv5_3)</span><br><span class="line">    </span><br><span class="line">	pool5_flatten_dims = <span class="built_in">int</span>(np.prod(pool5.get_shape().as_list()[<span class="number">1</span>:]))</span><br><span class="line">    pool5_flatten = tf.reshape(pool5,[-<span class="number">1</span>,pool5_flatten_dims])</span><br><span class="line">    </span><br><span class="line">    fc_6 = fc_layer(pool5_flatten, pool5_flatten_dims, <span class="number">4096</span>, <span class="string">'fc6'</span>)</span><br><span class="line">	fc_7 = fc_layer(fc_6, <span class="number">4096</span>, <span class="number">4096</span>, <span class="string">'fc7'</span>)</span><br><span class="line">	fc_8 = fc_layer(fc_7, <span class="number">4096</span>, <span class="number">10</span>, <span class="string">'fc8'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> fc_8</span><br><span class="line">    </span><br></pre></td></tr></tbody></table></figure>
<h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><h2 id="Pytorch是什么？"><a href="#Pytorch是什么？" class="headerlink" title="Pytorch是什么？"></a>Pytorch是什么？</h2><p>  Pytorch是torch的python版本，是由Facebook开源的神经网络框架，专门针对 GPU 加速的深度神经网络（DNN）编程。Torch 是一个经典的对多维矩阵数据进行操作的张量（tensor ）库，在机器学习和其他数学密集型应用有广泛应用。与Tensorflow的静态计算图不同，pytorch的计算图是动态的，可以根据计算需要实时改变计算图。但由于Torch语言采用 Lua，导致在国内一直很小众，并逐渐被支持 Python 的 Tensorflow 抢走用户。作为经典机器学习库 Torch 的端口，PyTorch 为 Python 语言使用者提供了舒适的写代码选择。</p>
<h2 id="为什么选择-Pytorch？"><a href="#为什么选择-Pytorch？" class="headerlink" title="为什么选择 Pytorch？"></a>为什么选择 Pytorch？</h2><h3 id="1-简洁："><a href="#1-简洁：" class="headerlink" title="1.简洁："></a>1.简洁：</h3><p>  PyTorch的设计追求最少的封装，尽量避免重复造轮子。不像 TensorFlow 中充斥着session、graph、operation、name_scope、variable、tensor、layer等全新的概念，PyTorch 的设计遵循tensor→variable(autograd)→nn.Module 三个由低到高的抽象层次，分别代表高维数组（张量）、自动求导（变量）和神经网络（层/模块），而且这三个抽象之间联系紧密，可以同时进行修改和操作。<br>简洁的设计带来的另外一个好处就是代码易于理解。PyTorch的源码只有TensorFlow的十分之一左右，更少的抽象、更直观的设计使得PyTorch的源码十分易于阅读。</p>
<h3 id="2-速度："><a href="#2-速度：" class="headerlink" title="2.速度："></a>2.速度：</h3><p>  PyTorch 的灵活性不以速度为代价，在许多评测中，PyTorch 的速度表现胜过 TensorFlow和Keras 等框架。框架的运行速度和程序员的编码水平有极大关系，但同样的算法，使用PyTorch实现的那个更有可能快过用其他框架实现的。</p>
<h3 id="3-易用："><a href="#3-易用：" class="headerlink" title="3.易用："></a>3.易用：</h3><p>  PyTorch 是所有的框架中面向对象设计的最优雅的一个。PyTorch的面向对象的接口设计来源于Torch，而Torch的接口设计以灵活易用而著称，Keras作者最初就是受Torch的启发才开发了Keras。PyTorch继承了Torch的衣钵，尤其是API的设计和模块的接口都与Torch高度一致。PyTorch的设计最符合人们的思维，它让用户尽可能地专注于实现自己的想法，即所思即所得，不需要考虑太多关于框架本身的束缚。</p>
<h3 id="4-活跃的社区："><a href="#4-活跃的社区：" class="headerlink" title="4.活跃的社区："></a>4.活跃的社区：</h3><p>  PyTorch 提供了完整的文档，循序渐进的指南，作者亲自维护的论坛 供用户交流和求教问题。Facebook 人工智能研究院对 PyTorch 提供了强力支持，作为当今排名前三的深度学习研究机构，FAIR的支持足以确保PyTorch获得持续的开发更新，不至于像许多由个人开发的框架那样昙花一现。</p>
<h2 id="PyTorch-的架构是怎样的？"><a href="#PyTorch-的架构是怎样的？" class="headerlink" title="PyTorch 的架构是怎样的？"></a>PyTorch 的架构是怎样的？</h2><p>  PyTorch(Caffe2) 通过混合前端，分布式训练以及工具和库生态系统实现快速，灵活的实验和高效生产。PyTorch 和 TensorFlow 具有不同计算图实现形式，TensorFlow 采用静态图机制(预定义后再使用)，PyTorch采用动态图机制(运行时动态定义)。PyTorch 具有以下高级特征：</p>
<p>  混合前端:新的混合前端在急切模式下提供易用性和灵活性，同时无缝转换到图形模式，以便在C ++运行时环境中实现速度，优化和功能。<br>  分布式训练:通过利用本地支持集合操作的异步执行和可从Python和C ++访问的对等通信，优化了性能。<br>  Python优先: PyTorch为了深入集成到Python中而构建的，因此它可以与流行的库和Cython和Numba等软件包一起使用。<br>  丰富的工具和库:活跃的研究人员和开发人员社区建立了丰富的工具和库生态系统，用于扩展PyTorch并支持从计算机视觉到强化学习等领域的开发。<br>  本机ONNX支持:以标准ONNX（开放式神经网络交换）格式导出模型，以便直接访问与ONNX兼容的平台，运行时，可视化工具等。<br>  C++前端：C++前端是PyTorch的纯C++接口，它遵循已建立的Python前端的设计和体系结构。它旨在实现高性能，低延迟和裸机C++应用程序的研究。<br>使用GPU和CPU优化的深度学习张量库。</p>
<h2 id="Pytorch-与-tensorflow-之间的差异在哪里？"><a href="#Pytorch-与-tensorflow-之间的差异在哪里？" class="headerlink" title="Pytorch 与 tensorflow 之间的差异在哪里？"></a>Pytorch 与 tensorflow 之间的差异在哪里？</h2><p>  上面也将了PyTorch 最大优势是建立的神经网络是动态的, 对比静态的 Tensorflow, 它能更有效地处理一些问题, 比如说 RNN 变化时间长度的输出。各有各的优势和劣势。两者都是大公司发布的, Tensorflow（Google）宣称在分布式训练上下了很大的功夫, 那就默认 Tensorflow 在分布式训练上要超出 Pytorch（Facebook），还有tensorboard可视化工具, 但是 Tensorflow 的静态计算图使得在 RNN 上有一点点被动 (虽然它用其他途径解决了), 不过用 PyTorch 的时候, 会对这种动态的 RNN 有更好的理解。而且 Tensorflow 的高度工业化, 它的底层代码很难看懂， Pytorch 好那么一点点, 如果深入 PytorchAPI, 至少能比看 Tensorflow 多看懂一点点 Pytorch 的底层在干啥。</p>
<h2 id="Pytorch有哪些常用工具包？"><a href="#Pytorch有哪些常用工具包？" class="headerlink" title="Pytorch有哪些常用工具包？"></a>Pytorch有哪些常用工具包？</h2><p>  torch ：类似 NumPy 的张量库，强 GPU 支持 ；<br>  torch.autograd ：基于 tape 的自动区别库，支持 torch 之中的所有可区分张量运行；<br>  torch.nn ：为最大化灵活性未涉及、与 autograd 深度整合的神经网络库；<br>  torch.optim：与 torch.nn 一起使用的优化包，包含 SGD、RMSProp、LBFGS、Adam 等标准优化方式；<br>  torch.multiprocessing： python 多进程并发，进程之间 torch Tensors 的内存共享；<br>  torch.utils：数据载入器。具有训练器和其他便利功能；<br>  torch.legacy(.nn/.optim) ：处于向后兼容性考虑，从 Torch 移植来的 legacy 代码；</p>
<h1 id="Caffe"><a href="#Caffe" class="headerlink" title="Caffe"></a>Caffe</h1><h2 id="什么是-Caffe？"><a href="#什么是-Caffe？" class="headerlink" title="什么是 Caffe？"></a>什么是 Caffe？</h2><p>  Caffe的全称应该是Convolutional Architecture for Fast Feature Embedding，它是一个清晰、高效的深度学习框架，它是开源的，核心语言是C++，它支持命令行、Python和Matlab接口，它既可以在CPU上运行也可以在GPU上运行。它的license是BSD 2-Clause。</p>
<h2 id="Caffe的特点是什么？"><a href="#Caffe的特点是什么？" class="headerlink" title="Caffe的特点是什么？"></a>Caffe的特点是什么？</h2><p>(1)、模块化：Caffe从一开始就设计得尽可能模块化，允许对新数据格式、网络层和损失函数进行扩展。</p>
<p>(2)、表示和实现分离：Caffe的模型(model)定义是用Protocol Buffer语言写进配置文件的。以任意有向无环图的形式，Caffe支持网络架构。Caffe会根据网络的需要来正确占用内存。通过一个函数调用，实现CPU和GPU之间的切换。</p>
<p>(3)、测试覆盖：在Caffe中，每一个单一的模块都对应一个测试。</p>
<p>(4)、python和Matlab接口：同时提供Python和Matlab接口。</p>
<p>(5)、预训练参考模型：针对视觉项目，Caffe提供了一些参考模型，这些模型仅应用在学术和非商业领域，它们的license不是BSD。</p>
<h2 id="Caffe的设计思想是怎样的？"><a href="#Caffe的设计思想是怎样的？" class="headerlink" title="Caffe的设计思想是怎样的？"></a>Caffe的设计思想是怎样的？</h2><p>  基本上，Caffe 沿用了神经网络的一个简单假设——所有的计算都是以layer的形式表示的，layer做的事情就是take一些数据，然后输出一些计算以后的结果，比如说卷积，就是输入一个图像，然后和这一层的参数（filter）做卷积，然后输出卷积的结果。每一个layer需要做两个计算：forward是从输入计算输出，然后backward是从上面给的gradient来计算相对于输入的gradient，只要这两个函数实现了以后，我们就可以把很多层连接成一个网络，这个网络做的事情就是输入我们的数据（图像或者语音或者whatever），然后来计算我们需要的输出（比如说识别的label），在training的时候，我们可以根据已有的label来计算loss和gradient，然后用gradient来update网络的参数，这个就是Caffe的一个基本流程。</p>
<p>  基本上，最简单地用Caffe上手的方法就是先把数据写成Caffe的格式，然后设计一个网络，然后用Caffe提供的solver来做优化看效果如何，如果你的数据是图像的话，可以从现有的网络，比如说alexnet或者googlenet开始，然后做fine tuning，如果你的数据稍有不同，比如说是直接的float vector，你可能需要做一些custom的configuration，Caffe的logistic regression example兴许会很有帮助。</p>
<p>  Fine tune方法：fine tuning的想法就是说，在imagenet那么大的数据集上train好一个很牛的网络了，那别的task上肯定也不错，所以我们可以把pretrain的网络拿过来，然后只重新train最后几层，重新train的意思是说，比如我以前需要classify imagenet的一千类，现在我只想识别是狗还是猫，或者是不是车牌，于是我就可以把最后一层softmax从一个4096<em>1000的分类器变成一个4096</em>2的分类器，这个strategy在应用中非常好使，所以我们经常会先在imagenet上pretrain一个网络，因为我们知道imagenet上training的大概过程会怎么样。</p>
<h2 id="Caffe架构是怎样的？"><a href="#Caffe架构是怎样的？" class="headerlink" title="Caffe架构是怎样的？"></a>Caffe架构是怎样的？</h2><p>  Caffe的架构与其它的深度学习框架稍微不同，它没有根据算法实现过程的方式来进行编码，而是以系统级的抽象作为整体架构，逐层的封装实现细节，使得上层的架构变得很清晰。Caffe的整体架构如下：</p>
<h3 id="1-SyncedMem"><a href="#1-SyncedMem" class="headerlink" title="1. SyncedMem"></a>1. SyncedMem</h3><p>  这个类的主要功能是封装CPU和GPU的数据交互操作。一般来说，数据的流动形式都是：硬盘-&gt;CPU内存-&gt;GPU内存-&gt;CPU内存-&gt;（硬盘），所以在写代码的过程中经常会写CPU/GPU之间数据传输的代码，同时还要维护CPU和GPU两个处理端的内存指针。这些事情处理起来不会很难，但是会很繁琐。因此SyncedMem的出现就是把CPU/GPU的数据传输操作封装起来，只需要调用简单的接口就可以获得两个处理端同步后的数据。</p>
<h3 id="2-Blob"><a href="#2-Blob" class="headerlink" title="2. Blob"></a>2. Blob</h3><p>  Blob是用于存储数据的对象，在Caffe中各种数据(图像输入、模型参数)都是以Blob的形式在网络中传输的，Blob提供统一的存储操作接口，可用来保存训练数据、模型参数等，同时Blob还能在CPU和GPU之间进行同步以支持CPU/GPU的混合运算。<br>  这个类做了两个封装：一个是操作数据的封装，使用Blob可以操纵高维的数据，快速访问其中的数据，变换数据的维度等；另一个是对原始数据和更新量的封装，每一个Blob中都有data和diff两个数据指针，data用于存储原始数据，diff 用于存储反向传播（Backpropagation）的梯度更新值。Blob使用了SyncedMem，这样便于访问不同的处理端。Blob基本实现了整个Caffe数据结构部分的封装，在Net类中可以看到所有的前后向数据和参数都用Blob来表示就足够了。数据的抽象到这个就可以了，接下来作层级的抽象。神经网络的前后向计算可以做到层与层之间完全独立，只要每个层按照一定的接口规则实现，就可以确保整个网络的正确性。</p>
<h3 id="3-Layer"><a href="#3-Layer" class="headerlink" title="3. Layer"></a>3. Layer</h3><p>  Layer是网络Net的基本单元，也是Caffe中能在外部进行调整的最小网络结构单元，每个Layer都有输入Blob和输出Blob。Layer（层）是Caffe中最庞大最繁杂的模块，它是神经网络的基本计算单元。由于Caffe强调模块化设计，因此只允许每个layer完成一类特定的计算，例如convolution操作、pooling、非线性变换、内积运算，以及数据加载、归一化和损失计算等。Caffe中layer的种类有很多，具体的种类及功能请看官方文档。在创建一个Caffe模型的时候，也是以Layer为基础进行的。Layer是一个父类，它的下面还有各种实现特定功能的子类，例如data_layer，conv_layer，loss_layer等。Layer是通过LayFactory来创建的。</p>
<h3 id="4-Net"><a href="#4-Net" class="headerlink" title="4. Net"></a>4. Net</h3><p>  Net是一个完整的深度网络，包含输入层、隐藏层、输出层，在Caffe中一般是一个卷积神经网络(Convolution Neural Networ，CNN)。通过定义不同类型的Layer，并用Blob将不同的Layer连接起来，就能产生一个Net。Net将数据Blob和层Layer组合起来做进一步的封装，对外提供了初始化和前后传播的接口，使得整体看上去和一个层的功能类似，但内部的组合可以是多种多样的。值得一提的是，每一层的输入输出数据统一保存在Net中，同时每个层内的参数指针也保存在Net中，不同的层可以通过WeightShare共享相同的参数，因此可以通过配置来实现多个神经网络层之间共享参数的功能。一个Net由多个Layer组成。一个典型的网络从data layer（从磁盘中载入数据）出发到loss layer结束。</p>
<h3 id="5-Solver"><a href="#5-Solver" class="headerlink" title="5. Solver"></a>5. Solver</h3><p>  有了Net就可以进行神经网络的前后向传播计算了，但是还缺少神经网络的训练和预测功能，Solver类进一步封装了训练和预测相关的一些功能。它还提供了两个接口：一个是更新参数的接口，继承Solver可以实现不同的参数更新方法，如Momentum，Nesterov，Adagrad等，因此可以使用不同的优化算法。另一个接口是训练过程中每一轮特定状态下的可注入的一些回调函数，在代码中这个回调点的直接使用者就是多GPU训练算法。Solver定义了针对Net网络模型的求解方法，记录网络的训练过程，保存网络模型参数，中断并恢复网络的训练过程。自定义Solver能够实现不同的神经网络求解方式。阅读Solver的代码可以了解网络的求解优化过程。Solver是一个父类，它下面还有实现不同优化方法的子类，例如sgd_solver，adagrad_sovler等，Solver是通过SolverFactory来创建的。</p>
<h3 id="6-Proto"><a href="#6-Proto" class="headerlink" title="6. Proto"></a>6. Proto</h3><p>  caffe.proto位于…/src/caffe/proto目录下，在这个文件夹下还有一个.pb.cc和一个.pb.h文件，这两个文件都是由caffe.proto编译而来的。 在caffe.proto中定义了很多结构化数据，包括：<br>BlobProto、Datum、FillerParameter、NetParameter、SolverParameter、SolverState、LayerParameter、ConcatParameter、ConvolutionParameter、DataParameter、DropoutParameter、HDF5DataParameter、HDF5OutputParameter、ImageDataParameter、InfogainLossParameter、InnerProductParameter、LRNParameter、MemoryDataParameter、PoolingParameter、PowerParameter、WindowDataParameter、V0LayerParameter。</p>
<h3 id="7-IO"><a href="#7-IO" class="headerlink" title="7. IO"></a>7. IO</h3><p>  除了上面的东西之外，还需要输入数据和参数。DataReader和DataTransformer帮助准备输入数据，Filler对参数进行初始化，一些Snapshot方法可以对模型进行持久化。</p>
<h2 id="Caffe的有哪些接口？"><a href="#Caffe的有哪些接口？" class="headerlink" title="Caffe的有哪些接口？"></a>Caffe的有哪些接口？</h2><p>  Caffe深度学习框架支持多种编程接口，包括命令行、Python和Matlab,下面将介绍如何使用这些接口。</p>
<h3 id="1-Caffe-Python接口"><a href="#1-Caffe-Python接口" class="headerlink" title="1. Caffe Python接口"></a>1. Caffe Python接口</h3><p>  Caffe提供 Python 接口，即Pycaffe，具体实现在caffe、python文件夹内。在Python代码中import caffe，可以load models（导入模型）、forward and backward （前向、反向迭代）、handle IO（数据输入输出）、visualize networks（绘制net）和instrument model solving（自定义优化方法)。所有的模型数据、计算参数都是暴露在外、可供读写的。<br>  (1)caffe.Net 是主要接口，负责导入数据、校验数据、计算模型。<br>  (2)caffe.Classsifier 用于图像分类。<br>  (3)caffe.Detector 用于图像检测。<br>  (4)caffe.SGDSolver 是露在外的 solver 的接口。<br>  (5)caffe.io 处理输入输出，数据预处理。<br>  (6)caffe.draw 可视化 net 的结构。<br>  (7)caffe blobs 以 numpy ndarrys 的形式表示，方便而且高效。</p>
<h3 id="2-Caffe-MATLAB接口"><a href="#2-Caffe-MATLAB接口" class="headerlink" title="2. Caffe MATLAB接口"></a>2. Caffe MATLAB接口</h3><p>  MATLAB接口（Matcaffe）在 caffe/matlab 目录的 caffe 软件包。在 matcaffe 的基础上，可将Caffe整合到MATLAB代码中。<br>  MATLAB接口包括：<br>  (1)MATLAB 中创建多个网络结构。<br>  (2)网络的前向传播（Forward）与反向传播（Backward）计算。<br>  (3)网络中的任意一层以及参数的存取。<br>  (4)网络参数保存至文件或从文件夹加载。<br>  (5)blob 和 network 形状调整。<br>  (6)网络参数编辑和调整。<br>  (7)创建多个 solvers 进行训练。<br>  (8)从solver 快照（Snapshots）恢复并继续训练。<br>  (9)访问训练网络（Train nets）和测试网络(Test nets)。<br>  (10)迭代后网络交由 MATLAB 控制。<br>  (11)MATLAB代码融合梯度算法。</p>
<h3 id="3-Caffe-命令行接口"><a href="#3-Caffe-命令行接口" class="headerlink" title="3. Caffe 命令行接口"></a>3. Caffe 命令行接口</h3><p>  命令行接口 Cmdcaffe 是 Caffe 中用来训练模型、计算得分以及方法判断的工具。Cmdcaffe 存放在 caffe/build/tools 目录下。</p>
<h4 id="1-caffe-train"><a href="#1-caffe-train" class="headerlink" title="1. caffe train"></a>1. caffe train</h4><p>  caffe train 命令用于模型学习，具体包括：<br>  (1)caffe train 带 solver.prototxt 参数完成配置。<br>  (2)caffe train 带 snapshot mode_iter_1000.solverstate 参数加载 solver snapshot。<br>  (3)caffe train 带 weights 参数 model.caffemodel 完成 Fine-tuning 模型初始化。</p>
<h4 id="2-caffe-test"><a href="#2-caffe-test" class="headerlink" title="2. caffe test"></a>2. caffe test</h4><p>  caffe test 命令用于测试运行模型的得分，并且用百分比表示网络输出的最终结果，比如 accuracyhuoloss 作为其结果。测试过程中，显示每个 batch 的得分，最后输出全部 batch 的平均得分值。</p>
<h4 id="3-caffe-time"><a href="#3-caffe-time" class="headerlink" title="3. caffe time"></a>3. caffe time</h4><p>  caffe time 命令用来检测系统性能和测量模型相对执行时间，此命令通过逐层计时与同步，执行模型检测。</p>
<p>参考文献：<br>1.深度学习：Caffe之经典模型讲解与实战/ 乐毅，王斌</p>
<h2 id="网络搭建有什么原则？"><a href="#网络搭建有什么原则？" class="headerlink" title="网络搭建有什么原则？"></a>网络搭建有什么原则？</h2><h3 id="新手原则"><a href="#新手原则" class="headerlink" title="新手原则"></a>新手原则</h3><p>刚入门的新手不建议直接上来就开始搭建网络模型。比较建议的学习顺序如下：</p>
<ul>
<li>1.了解神经网络工作原理，熟悉基本概念及术语。</li>
<li>2.阅读经典网络模型论文+实现源码(深度学习框架视自己情况而定)。</li>
<li>3.找数据集动手跑一个网络，可以尝试更改已有的网络模型结构。</li>
<li>4.根据自己的项目需要设计网络。</li>
</ul>
<h3 id="深度优先原则"><a href="#深度优先原则" class="headerlink" title="深度优先原则"></a>深度优先原则</h3><p>通常增加网络深度可以提高准确率，但同时会牺牲一些速度和内存。但深度不是盲目堆起来的，一定要在浅层网络有一定效果的基础上，增加深度。深度增加是为了增加模型的准确率，如果浅层都学不到东西，深了也没效果。</p>
<h3 id="卷积核size一般为奇数"><a href="#卷积核size一般为奇数" class="headerlink" title="卷积核size一般为奇数"></a>卷积核size一般为奇数</h3><p>卷积核为奇数有以下好处：</p>
<ul>
<li>1 保证锚点刚好在中间，方便以 central pixel为标准进行滑动卷积，避免了位置信息发生偏移 。</li>
<li>2 保证在填充（Padding）时，在图像之间添加额外的零层，图像的两边仍然对称。</li>
</ul>
<h3 id="卷积核不是越大越好"><a href="#卷积核不是越大越好" class="headerlink" title="卷积核不是越大越好"></a>卷积核不是越大越好</h3><p>AlexNet中用到了一些非常大的卷积核，比如11×11、5×5卷积核，之前人们的观念是，卷积核越大，感受野越大，看到的图片信息越多，因此获得的特征越好。但是大的卷积核会导致计算量的暴增，不利于模型深度的增加，计算性能也会降低。于是在VGG、Inception网络中，利用2个3×3卷积核的组合比1个5×5卷积核的效果更佳，同时参数量（3×3×2+1=19&lt;26=5×5×1+1）被降低，因此后来3×3卷积核被广泛应用在各种模型中。</p>
<h2 id="有哪些经典的网络模型值得我们去学习的？"><a href="#有哪些经典的网络模型值得我们去学习的？" class="headerlink" title="有哪些经典的网络模型值得我们去学习的？"></a>有哪些经典的网络模型值得我们去学习的？</h2><p>提起经典的网络模型就不得不提起计算机视觉领域的经典比赛：ILSVRC .其全称是 ImageNet Large Scale Visual Recognition Challenge.正是因为ILSVRC 2012挑战赛上的AlexNet横空出世，使得全球范围内掀起了一波深度学习热潮。这一年也被称作“深度学习元年”。而在历年ILSVRC比赛中每次刷新比赛记录的那些神经网络也成为了人们心中的经典，成为学术界与工业届竞相学习与复现的对象，并在此基础上展开新的研究。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>序号</th>
<th>年份</th>
<th>网络名称</th>
<th>获得荣誉</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2012</td>
<td>AlexNet</td>
<td>ILSVRC图像分类冠军</td>
</tr>
<tr>
<td>2</td>
<td>2014</td>
<td>VGGNet</td>
<td>ILSVRC图像分类亚军</td>
</tr>
<tr>
<td>3</td>
<td>2014</td>
<td>GoogLeNet</td>
<td>ILSVRC图像分类冠军</td>
</tr>
<tr>
<td>4</td>
<td>2015</td>
<td>ResNet</td>
<td>ILSVRC图像分类冠军</td>
</tr>
<tr>
<td>5</td>
<td>2017</td>
<td>SeNet</td>
<td>ILSVRC图像分类冠军</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<ul>
<li><p>1 AlexNet<br>论文:<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a><br>代码实现:<a href="https://github.com/tensorflow/tensorflow/blob/361a82d73a50a800510674b3aaa20e4845e56434/tensorflow/contrib/slim/python/slim/nets/alexnet.py">tensorflow</a><br>主要特点：</p>
<blockquote>
<ul>
<li>1.第一次使用非线性激活函数ReLU。</li>
<li>2.增加防加过拟合方法：Droupout层,提升了模型鲁棒性。</li>
<li>3.首次使用数据增强。  </li>
<li>4.首次使用GPU加速运算。</li>
</ul>
</blockquote>
</li>
<li><p>2 VGGNet<br>论文:<a href="https://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a><br>代码实现:<a href="https://github.com/tensorflow/tensorflow/blob/361a82d73a50a800510674b3aaa20e4845e56434/tensorflow/contrib/slim/python/slim/nets/vgg.py">tensorflow</a><br>主要特点：</p>
<blockquote>
<ul>
<li>1.网络结构更深。</li>
<li>2.普遍使用小卷积核。</li>
</ul>
</blockquote>
</li>
<li><p>3 GoogLeNet<br>论文:<a href="https://arxiv.org/abs/1409.4842">Going Deeper with Convolutions</a><br>代码实现:<a href="https://github.com/tensorflow/tensorflow/blob/361a82d73a50a800510674b3aaa20e4845e56434/tensorflow/contrib/slim/python/slim/nets/inception_v1.py">tensorflow</a><br>主要特点：</p>
<blockquote>
<ul>
<li>1.增强卷积模块功能。<br>主要的创新在于他的Inception，这是一种网中网（Network In Network）的结构，即原来的结点也是一个网络。Inception一直在不断发展，目前已经V2、V3、V4。其中1*1卷积主要用来降维，用了Inception之后整个网络结构的宽度和深度都可扩大，能够带来2-3倍的性能提升。</li>
<li>2.连续小卷积代替大卷积，保证感受野不变的同时，减少了参数数目。</li>
</ul>
</blockquote>
</li>
<li>4 ResNet<br>论文:<a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a><br>代码实现:<a href="https://github.com/tensorflow/tensorflow/blob/361a82d73a50a800510674b3aaa20e4845e56434/tensorflow/contrib/slim/python/slim/nets/inception_v1.py">tensorflow</a><br>主要特点:<blockquote>
<p>解决了“退化”问题，即当模型的层次加深时，错误率却提高了。</p>
</blockquote>
</li>
<li>5 SeNet<br>论文:<a href="https://arxiv.org/abs/1709.01507">Squeeze-and-Excitation Networks</a><br>代码实现:<a href="https://github.com/ry/tensorflow-resnet">tensorflow</a><br>主要特点:<blockquote>
<p>提出了feature recalibration，通过引入 attention 重新加权，可以得到抑制无效特征，提升有效特征的权重，并很容易地和现有网络结合，提升现有网络性能，而计算量不会增加太多。</p>
</blockquote>
</li>
</ul>
</blockquote>
<p><strong>CV领域网络结构演进历程：</strong><br><img src="/zh-TW/ch12_%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/ch12/网络结构演进.png" class="lazyload" data-srcset="/zh-TW/ch12_%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/ch12/网络结构演进.png" srcset="data:image/png;base64,666" alt="CV领域网络结构演进历程"></p>
<p><strong>ILSVRC挑战赛历年冠军:</strong><br><img src="/zh-TW/ch12_%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/ch12/历年冠军.png" class="lazyload" data-srcset="/zh-TW/ch12_%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/ch12/历年冠军.png" srcset="data:image/png;base64,666" alt="ILSVRC挑战赛历年冠军"></p>
<p>此后，ILSVRC挑战赛的名次一直是衡量一个研究机构或企业技术水平的重要标尺。<br>ILSVRC 2017 已是最后一届举办.2018年起，将由WebVision竞赛（Challenge on Visual Understanding by Learning from Web Data）来接棒。因此，即使ILSVRC挑战赛停办了，但其对深度学习的深远影响和巨大贡献，将永载史册。</p>
<h2 id="网络训练有哪些技巧吗？"><a href="#网络训练有哪些技巧吗？" class="headerlink" title="网络训练有哪些技巧吗？"></a>网络训练有哪些技巧吗？</h2><h3 id="合适的数据集。"><a href="#合适的数据集。" class="headerlink" title="合适的数据集。"></a>合适的数据集。</h3><ul>
<li>1 没有明显脏数据(可以极大避免Loss输出为NaN)。</li>
<li>2 样本数据分布均匀。</li>
</ul>
<h3 id="合适的预处理方法。"><a href="#合适的预处理方法。" class="headerlink" title="合适的预处理方法。"></a>合适的预处理方法。</h3><p>关于数据预处理，在Batch Normalization未出现之前预处理的主要做法是减去均值，然后除去方差。在Batch Normalization出现之后，减均值除方差的做法已经没有必要了。对应的预处理方法主要是数据筛查、数据增强等。</p>
<h3 id="网络的初始化。"><a href="#网络的初始化。" class="headerlink" title="网络的初始化。"></a>网络的初始化。</h3><p>网络初始化最粗暴的做法是参数赋值为全0，这是绝对不可取的。因为如果所有的参数都是0，那么所有神经元的输出都将是相同的，那在back propagation的时候同一层内所有神经元的行为也是相同的，这可能会直接导致模型失效，无法收敛。吴恩达视频中介绍的方法是将网络权重初始化均值为0、方差为1符合的正态分布的随机数据。</p>
<h3 id="小规模数据试练。"><a href="#小规模数据试练。" class="headerlink" title="小规模数据试练。"></a>小规模数据试练。</h3><p>在正式开始训练之前，可以先用小规模数据进行试练。原因如下：</p>
<ul>
<li>1 可以验证自己的训练流程对否。</li>
<li>2 可以观察收敛速度，帮助调整学习速率。</li>
<li>3 查看GPU显存占用情况，最大化batch_size(前提是进行了batch normalization，只要显卡不爆，尽量挑大的)。</li>
</ul>
<h3 id="设置合理Learning-Rate。"><a href="#设置合理Learning-Rate。" class="headerlink" title="设置合理Learning Rate。"></a>设置合理Learning Rate。</h3><ul>
<li>1 太大。Loss爆炸、输出NaN等。</li>
<li>2 太小。收敛速度过慢，训练时长大大延长。</li>
<li>3 可变的学习速率。比如当输出准确率到达某个阈值后，可以让Learning Rate减半继续训练。</li>
</ul>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数主要分为两大类:分类损失和回归损失</p>
<blockquote>
<p>1.回归损失：</p>
<blockquote>
<ul>
<li>1 均方误差(MSE 二次损失 L2损失)<br>它是我们的目标变量与预测值变量差值平方。</li>
<li>2 平均绝对误差(MAE L1损失)<br>它是我们的目标变量与预测值变量差值绝对值。<br>关于MSE与MAE的比较。MSE更容易解决问题，但是MAE对于异常值更加鲁棒。更多关于MAE和MSE的性能，可以参考<a href="https://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/">L1vs.L2 Loss Function</a></li>
</ul>
</blockquote>
<p>2.分类损失：</p>
<blockquote>
<ul>
<li>1 交叉熵损失函数。<br>是目前神经网络中最常用的分类目标损失函数。</li>
<li>2 合页损失函数<br>合页损失函数广泛在支持向量机中使用，有时也会在损失函数中使用。缺点:合页损失函数是对错误越大的样本施以更严重的惩罚，但是这样会导致损失函数对噪声敏感。</li>
</ul>
</blockquote>
</blockquote>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>超参数调整</title>
    <url>/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="超参数调整"><a href="#超参数调整" class="headerlink" title="超参数调整"></a>超参数调整</h1><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>​    关于训练深度学习模型最难的事情之一是你要处理的参数的数量。无论是从网络本身的层宽（宽度）、层数（深度）、连接方式，还是损失函数的超参数设计和调试，亦或者是学习率、批样本数量、优化器参数等等。这些大量的参数都会有网络模型最终的有效容限直接或者间接的影响。面对如此众多的参数，如果我们要一一对其优化调整，所需的无论是时间、资源都是不切实际。结果证实一些超参数比其它的更为重要，因此认识各个超参数的作用和其可能会造成的影响是深度学习训练中必不可少的一项重要技能。</p>
<p>​    超参数调整可以说是深度学习中理论和实际联系最重要的一个环节。目前，深度学习仍存在很多不可解释的部分，如何设计优化出好的网络可以为深度学习理论的探索提供重要的支持。超参数调整一般分为手动调整和自动优化超参数两种。读者可先浏览思维导图，本章节不会过多阐述所有超参数的详细原理，如果需要了解这部分，您可以翻阅前面的基础章节或者查阅相关文献资料。当然，下面会讲到的一些超参数优化的建议是根据笔者们的实践以及部分文献资料得到认知建议，并不是非常严格且一定有效的，很多研究者可能会很不同意某些的观点或有着不同的直觉，这都是可保留讨论的，因为这很依赖于数据本身情况。</p>
<p><img src="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\思维导图.png" class="lazyload" data-srcset="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\思维导图.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    </p>
<h2 id="超参数概念"><a href="#超参数概念" class="headerlink" title="超参数概念"></a>超参数概念</h2><h3 id="什么是超参数，参数和超参数的区别？"><a href="#什么是超参数，参数和超参数的区别？" class="headerlink" title="什么是超参数，参数和超参数的区别？"></a>什么是超参数，参数和超参数的区别？</h3><p>​    区分两者最大的一点就是是否通过数据来进行调整，模型参数通常是有数据来驱动调整，超参数则不需要数据来驱动，而是在训练前或者训练中人为的进行调整的参数。例如卷积核的具体核参数就是指模型参数，这是有数据驱动的。而学习率则是人为来进行调整的超参数。这里需要注意的是，通常情况下卷积核数量、卷积核尺寸这些也是超参数，注意与卷积核的核参数区分。</p>
<h3 id="神经网络中包含哪些超参数？"><a href="#神经网络中包含哪些超参数？" class="headerlink" title="神经网络中包含哪些超参数？"></a>神经网络中包含哪些超参数？</h3><p>　　 通常可以将超参数分为三类：网络参数、优化参数、正则化参数。</p>
<p>​    网络参数：可指网络层与层之间的交互方式（相加、相乘或者串接等）、卷积核数量和卷积核尺寸、网络层数（也称深度）和激活函数等。</p>
<p>​    优化参数：一般指学习率（learning rate）、批样本数量（batch size）、不同优化器的参数以及部分损失函数的可调参数。</p>
<p>​    正则化：权重衰减系数，丢弃法比率（dropout）</p>
<h3 id="为什么要进行超参数调优？"><a href="#为什么要进行超参数调优？" class="headerlink" title="为什么要进行超参数调优？"></a>为什么要进行超参数调优？</h3><p>​    本质上，这是模型优化寻找最优解和正则项之间的关系。网络模型优化调整的目的是为了寻找到全局最优解（或者相比更好的局部最优解），而正则项又希望模型尽量拟合到最优。两者通常情况下，存在一定的对立，但两者的目标是一致的，即最小化期望风险。模型优化希望最小化经验风险，而容易陷入过拟合，正则项用来约束模型复杂度。所以如何平衡两者之间的关系，得到最优或者较优的解就是超参数调整优化的目的。</p>
<h3 id="超参数的重要性顺序"><a href="#超参数的重要性顺序" class="headerlink" title="超参数的重要性顺序"></a>超参数的重要性顺序</h3><ul>
<li><p>首先， <strong>学习率，损失函数上的可调参数</strong>。在网络参数、优化参数、正则化参数中最重要的超参数可能就是学习率了。学习率直接控制着训练中网络梯度更新的量级，直接影响着模型的<strong>有效容限能力</strong>；损失函数上的可调参数，这些参数通常情况下需要结合实际的损失函数来调整，大部分情况下这些参数也能很直接的影响到模型的的有效容限能力。这些损失一般可分成三类，第一类辅助损失结合常见的损失函数，起到辅助优化特征表达的作用。例如度量学习中的Center loss，通常结合交叉熵损失伴随一个权重完成一些特定的任务。这种情况下一般建议辅助损失值不高于或者不低于交叉熵损失值的两个数量级；第二类，多任务模型的多个损失函数，每个损失函数之间或独立或相关，用于各自任务，这种情况取决于任务之间本身的相关性，目前笔者并没有一个普适的经验由于提供参考；第三类，独立损失函数，这类损失通常会在特定的任务有显著性的效果。例如RetinaNet中的focal loss，其中的参数γ，α，对最终的效果会产生较大的影响。这类损失通常论文中会给出特定的建议值。</p>
</li>
<li><p>其次，<strong>批样本数量，动量优化器（Gradient Descent with Momentum）的动量参数<em>β</em></strong>。批样本决定了数量梯度下降的方向。过小的批数量，极端情况下，例如batch size为1，即每个样本都去修正一次梯度方向，样本之间的差异越大越难以收敛。若网络中存在批归一化（batchnorm），batch size过小则更难以收敛，甚至垮掉。这是因为数据样本越少，统计量越不具有代表性，噪声也相应的增加。而过大的batch size，会使得梯度方向基本稳定，容易陷入局部最优解，降低精度。一般参考范围会取在[1:1024]之间，当然这个不是绝对的，需要结合具体场景和样本情况；动量衰减参数<em>β</em>是计算梯度的指数加权平均数，并利用该值来更新参数，设置为 0.9 是一个常见且效果不错的选择；</p>
</li>
</ul>
<ul>
<li>最后，<strong>Adam优化器的超参数、权重衰减系数、丢弃法比率（dropout）和网络参数</strong>。在这里说明下，这些参数重要性放在最后<strong>并不等价于这些参数不重要</strong>。而是表示这些参数在大部分实践中<strong>不建议过多尝试</strong>，例如Adam优化器中的<em>β1，β2，ϵ</em>，常设为 0.9、0.999、10−8就会有不错的表现。权重衰减系数通常会有个建议值，例如0.0005 ，使用建议值即可，不必过多尝试。dropout通常会在全连接层之间使用防止过拟合，建议比率控制在[0.2,0.5]之间。使用dropout时需要特别注意两点：一、在RNN中，如果直接放在memory cell中,循环会放大噪声，扰乱学习。一般会建议放在输入和输出层；二、不建议dropout后直接跟上batchnorm，dropout很可能影响batchnorm计算统计量，导致方差偏移，这种情况下会使得推理阶段出现模型完全垮掉的极端情况；网络参数通常也属于超参数的范围内，通常情况下增加网络层数能增加模型的容限能力，但模型真正有效的容限能力还和样本数量和质量、层之间的关系等有关，所以一般情况下会选择先固定网络层数，调优到一定阶段或者有大量的硬件资源支持可以在网络深度上进行进一步调整。</li>
</ul>
<h3 id="部分超参数如何影响模型性能？"><a href="#部分超参数如何影响模型性能？" class="headerlink" title="部分超参数如何影响模型性能？"></a>部分超参数如何影响模型性能？</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">超参数</th>
<th style="text-align:center">如何影响模型容量</th>
<th style="text-align:center">原因</th>
<th style="text-align:center">注意事项</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">学习率</td>
<td style="text-align:center">调至最优，提升有效容量</td>
<td style="text-align:center">过高或者过低的学习率，都会由于优化失败而导致降低模型有效容限</td>
<td style="text-align:center">学习率最优点，在训练的不同时间点都可能变化，所以需要一套有效的学习率衰减策略</td>
</tr>
<tr>
<td style="text-align:center">损失函数部分超参数</td>
<td style="text-align:center">调至最优，提升有效容量</td>
<td style="text-align:center">损失函数超参数大部分情况都会可能影响优化，不合适的超参数会使即便是对目标优化非常合适的损失函数同样难以优化模型，降低模型有效容限。</td>
<td style="text-align:center">对于部分损失函数超参数其变化会对结果十分敏感，而有些则并不会太影响。在调整时，建议参考论文的推荐值，并在该推荐值数量级上进行最大最小值调试该参数对结果的影响。</td>
</tr>
<tr>
<td style="text-align:center">批样本数量</td>
<td style="text-align:center">过大过小，容易降低有效容量</td>
<td style="text-align:center">大部分情况下，选择适合自身硬件容量的批样本数量，并不会对模型容限造成。</td>
<td style="text-align:center">在一些特殊的目标函数的设计中，如何选择样本是很可能影响到模型的有效容限的，例如度量学习（metric learning）中的N-pair loss。这类损失因为需要样本的多样性，可能会依赖于批样本数量。</td>
</tr>
<tr>
<td style="text-align:center">丢弃法</td>
<td style="text-align:center">比率降低会提升模型的容量</td>
<td style="text-align:center">较少的丢弃参数意味着模型参数量的提升，参数间适应性提升，模型容量提升，但不一定能提升模型有效容限</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">权重衰减系数</td>
<td style="text-align:center">调至最优，提升有效容量</td>
<td style="text-align:center">权重衰减可以有效的起到限制参数变化的幅度，起到一定的正则作用</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">优化器动量</td>
<td style="text-align:center">调至最优，可能提升有效容量</td>
<td style="text-align:center">动量参数通常用来加快训练，同时更容易跳出极值点，避免陷入局部最优解。</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">模型深度</td>
<td style="text-align:center">同条件下，深度增加，模型容量提升</td>
<td style="text-align:center">同条件，下增加深度意味着模型具有更多的参数，更强的拟合能力。</td>
<td style="text-align:center">同条件下，深度越深意味着参数越多，需要的时间和硬件资源也越高。</td>
</tr>
<tr>
<td style="text-align:center">卷积核尺寸</td>
<td style="text-align:center">尺寸增加，模型容量提升</td>
<td style="text-align:center">增加卷积核尺寸意味着参数量的增加，同条件下，模型参数也相应的增加。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="部分超参数合适的范围"><a href="#部分超参数合适的范围" class="headerlink" title="部分超参数合适的范围"></a>部分超参数合适的范围</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">超参数</th>
<th style="text-align:center">建议范围</th>
<th style="text-align:center">注意事项</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">初始学习率</td>
<td style="text-align:center">SGD: [1e-2, 1e-1]<br>momentum: [1e-3, 1e-2]<br>Adagrad: [1e-3, 1e-2]<br>Adadelta: [1e-2, 1e-1]<br>RMSprop: [1e-3, 1e-2]<br>Adam: [1e-3, 1e-2]<br>Adamax: [1e-3, 1e-2]<br>Nadam: [1e-3, 1e-2]</td>
<td style="text-align:center">这些范围通常是指从头开始训练的情况。若是微调，初始学习率可在降低一到两个数量级。</td>
</tr>
<tr>
<td style="text-align:center">损失函数部分超参数</td>
<td style="text-align:center">多个损失函数之间，损失值之间尽量相近，不建议超过或者低于两个数量级</td>
<td style="text-align:center">这是指多个损失组合的情况，不一定完全正确。单个损失超参数需结合实际情况。</td>
</tr>
<tr>
<td style="text-align:center">批样本数量</td>
<td style="text-align:center">[1:1024]</td>
<td style="text-align:center">当批样本数量过大(大于6000)或者等于1时，需要注意学习策略或者内部归一化方式的调整。</td>
</tr>
<tr>
<td style="text-align:center">丢弃法比率</td>
<td style="text-align:center">[0, 0.5]</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">权重衰减系数</td>
<td style="text-align:center">[0, 1e-4]</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">卷积核尺寸</td>
<td style="text-align:center">[7x7],[5x5],[3x3],[1x1], [7x1,1x7]</td>
</tr>
</tbody>
</table>
</div>
<h2 id="网络训练中的超参调整策略"><a href="#网络训练中的超参调整策略" class="headerlink" title="网络训练中的超参调整策略"></a>网络训练中的超参调整策略</h2><h3 id="如何调试模型？"><a href="#如何调试模型？" class="headerlink" title="如何调试模型？"></a>如何调试模型？</h3><p>在讨论如何调试模型之前，我们先来纠正一个误区。通常理解如何调试模型的时候，我们想到一系列优秀的神经网络模型以及调试技巧。但这里需要指出的是数据才是模型的根本，如果有一批质量优秀的数据，或者说你能将数据质量处理的很好的时候，往往比挑选或者设计模型的收益来的更大。那在这之后才是模型的设计和挑选以及训练技巧上的事情。</p>
<p>1、探索和清洗数据。探索数据集是设计算法之前最为重要的一步，以图像分类为例，我们需要重点知道给定的数据集样本类别和各类别样本数量是否平衡，图像之间是否存在跨域问题（例如网上爬取的图像通常质量各异，存在噪声）。若是类别数远远超过类别样本数（比如类别10000，每个类别却只有10张图像），那通常的方法可能效果并不显著，这时候few-shot learning或者对数据集做进一步增强可能是你比较不错的选择。再如目标检测，待检测目标在数据集中的尺度范围是对检测器的性能有很大影响的部分。因此重点是检测大目标还是小目标、目标是否密集完全取决于数据集本身。所以，探索和进一步清洗数据集一直都是深度学习中最重要的一步。这是很多新手通常会忽略的一点。</p>
<p>2、探索模型结果。探索模型的结果，通常是需要对模型在验证集上的性能进行进一步的分析，这是如何进一步提升模型性能很重要的步骤。将模型在训练集和验证集都进行结果的验证和可视化，可直观的分析出模型是否存在较大偏差以及结果的正确性。以图像分类为例，若类别间样本数量很不平衡时，我们需要重点关注少样本类别在验证集的结果是否和训练集的出入较大，对出错类别可进一步进行模型数值分析以及可视化结果分析，进一步确认模型的行为。</p>
<p>3、监控训练和验证误差。首先很多情况下，我们忽略代码的规范性和算法撰写正确性验证，这点上容易产生致命的影响。在训练和验证都存在问题时，首先请确认自己的代码是否正确。其次，根据训练和验证误差进一步追踪模型的拟合状态。若训练数据集很小，此时监控误差则显得格外重要。确定了模型的拟合状态对进一步调整学习率的策略的选择或者其他有效超参数的选择则会更得心应手。</p>
<p>4、反向传播数值的计算，这种情况通常适合自己设计一个新操作的情况。目前大部分流行框架都已包含自动求导部分，但并不一定是完全符合你的要求的。验证求导是否正确的方式是比较自动求导的结果和有限差分计算结果是否一致。所谓有限差分即导数的定义，使用一个极小的值近似导数。</p>
<script type="math/tex; mode=display">
f^{'}(x_0) = \lim_{n\rightarrow0}\frac{\Delta y}{\Delta x} = \lim_{n\rightarrow0}\frac{f(x_0+\Delta x -f(x_0))}{\Delta x}</script><h3 id="为什么要做学习率调整"><a href="#为什么要做学习率调整" class="headerlink" title="为什么要做学习率调整?"></a>为什么要做学习率调整?</h3><p>​    学习率可以说是模型训练最为重要的超参数。通常情况下，一个或者一组优秀的学习率既能加速模型的训练，又能得到一个较优甚至最优的精度。过大或者过小的学习率会直接影响到模型的收敛。我们知道，当模型训练到一定程度的时候，损失将不再减少，这时候模型的一阶梯度接近零，对应Hessian 矩阵通常是两种情况，一、正定，即所有特征值均为正，此时通常可以得到一个局部极小值，若这个局部极小值接近全局最小则模型已经能得到不错的性能了，但若差距很大，则模型性能还有待于提升，通常情况下后者在训练初最常见。二，特征值有正有负，此时模型很可能陷入了鞍点，若陷入鞍点，模型性能表现就很差。以上两种情况在训练初期以及中期，此时若仍然以固定的学习率，会使模型陷入左右来回的震荡或者鞍点，无法继续优化。所以，学习率衰减或者增大能帮助模型有效的减少震荡或者逃离鞍点。</p>
<h3 id="学习率调整策略有哪些？"><a href="#学习率调整策略有哪些？" class="headerlink" title="学习率调整策略有哪些？"></a>学习率调整策略有哪些？</h3><p>通常情况下，大部分学习率调整策略都是衰减学习率，但有时若增大学习率也同样起到奇效。这里结合TensorFlow的内置方法来举例。</p>
<p>1、<strong>exponential_decay</strong>和<strong>natural_exp_decay</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">exponential_decay(learning_rate, global_step, decay_steps, decay_rate,</span><br><span class="line">                   staircase=<span class="literal">False</span>, name=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">natural_exp_decay(learning_rate, global_step, decay_steps, decay_rate,</span><br><span class="line">                   staircase=<span class="literal">False</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>指数衰减是最常用的衰减方式，这种方式简单直接，在训练初期衰减较大利于收敛，在后期衰减较小利于精调。以上两种均为指数衰减，区别在于后者使用以自然指数下降。</p>
<p><img src="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\指数衰减.jpeg" class="lazyload" data-srcset="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\指数衰减.jpeg" srcset="data:image/png;base64,666" alt="./"></p>
<p>2、<strong>piecewise_constant</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">piecewise_constant(x, boundaries, values, name=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>分段设置学习率法，跟指数型类似，区别在于每个阶段的衰减并不是按指数调整。可在不同阶段设置手动不同的学习率。这种学习率重点在有利于精调。</p>
<p>3、<strong>polynomial_decay</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">polynomial_decay(learning_rate, global_step, decay_steps,</span><br><span class="line">                  end_learning_rate=<span class="number">0.0001</span>, power=<span class="number">1.0</span>,</span><br><span class="line">                  cycle=<span class="literal">False</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>多项式衰减，计算如下：</p>
<script type="math/tex; mode=display">
global setp = min(global step, decay steps)</script><script type="math/tex; mode=display">
lr_{decayed} = (lr-lr_{end})*(1-{globalstep\over decaysteps})^{power} +lr_{end}</script><p>有别于上述两种，多项式衰减则是在每一步迭代上都会调整学习率。主要看Power参数，若Power为1，则是下图中的红色直线；若power小于1，则是开1/power次方，为蓝色线；绿色线为指数，power大于1。</p>
<p><img src="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\多项式衰减.jpeg" class="lazyload" data-srcset="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\多项式衰减.jpeg" srcset="data:image/png;base64,666" alt=""></p>
<p>此外，需要注意的是参数cycle，cycle对应的是一种周期循环调整的方式。这种cycle策略主要目的在后期防止在一个局部极小值震荡，若跳出该区域或许能得到更有的结果。这里说明cycle的方式不止可以在多项式中应用，可配合类似的周期函数进行衰减，如下图。</p>
<p><img src="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\cycle衰减.jpeg" class="lazyload" data-srcset="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\cycle衰减.jpeg" srcset="data:image/png;base64,666" alt=""></p>
<p>4、<strong>inverse_time_decay</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">inverse_time_decay(learning_rate, global_step, decay_steps, decay_rate,</span><br><span class="line">                   staircase=<span class="literal">False</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>逆时衰减，这种方式和指数型类似。如图，<img src="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\逆时衰减.jpeg" class="lazyload" data-srcset="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\逆时衰减.jpeg" srcset="data:image/png;base64,666" alt=""></p>
<p>5、<strong>cosine_decay</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">cosine_decay(learning_rate, global_step, decay_steps, alpha=<span class="number">0.0</span>,</span><br><span class="line">                 name=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>余弦衰减，即按余弦函数的方式衰减学习率，如图</p>
<p><img src="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\余弦衰减.jpeg" class="lazyload" data-srcset="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\余弦衰减.jpeg" srcset="data:image/png;base64,666" alt=""></p>
<p>6、<strong>cosine_decay_restarts</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">cosine_decay_restarts(learning_rate, global_step, first_decay_steps,</span><br><span class="line">                           t_mul=<span class="number">2.0</span>, m_mul=<span class="number">1.0</span>, alpha=<span class="number">0.0</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>余弦衰减，即余弦版本的cycle策略，作用与多项式衰减中的cycle相同。区别在于余弦重启衰减会重新回到初始学习率，拉长周期，而多项式版本则会逐周期衰减。</p>
<p><img src="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\余弦cycle衰减.jpeg" class="lazyload" data-srcset="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\余弦cycle衰减.jpeg" srcset="data:image/png;base64,666" alt=""></p>
<p>7、<strong>linear_cosine_decay</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">linear_cosine_decay(learning_rate, global_step, decay_steps,</span><br><span class="line">                        num_periods=<span class="number">0.5</span>, alpha=<span class="number">0.0</span>, beta=<span class="number">0.001</span>,</span><br><span class="line">                        name=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>线性余弦衰减，主要应用于增强学习领域。</p>
<p><img src="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\线性余弦衰减.jpeg" class="lazyload" data-srcset="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\线性余弦衰减.jpeg" srcset="data:image/png;base64,666" alt=""></p>
<p>8、<strong>noisy_linear_cosine_decay</strong></p>
<p>噪声线性余弦衰减，即在线性余弦衰减中加入随机噪声，增大寻优的随机性。</p>
<p><img src="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\噪声线性余弦衰减.jpeg" class="lazyload" data-srcset="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\噪声线性余弦衰减.jpeg" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="极端批样本数量下，如何训练网络？"><a href="#极端批样本数量下，如何训练网络？" class="headerlink" title="极端批样本数量下，如何训练网络？"></a>极端批样本数量下，如何训练网络？</h3><p>​    极端批样本情况一般是指batch size为1或者batch size在6000以上的情况。这两种情况，在使用不合理的情况下都会导致模型最终性能无法达到最优甚至是崩溃的情况。</p>
<p>​    在目标检测、分割或者3D图像等输入图像尺寸较大的场景，通常batch size 会非常小。而在14.2.4中，我们已经讲到这种情况会导致梯度的不稳定以及batchnorm统计的不准确。针对梯度不稳定的问题，通常不会太致命，若训练中发现梯度不稳定导致性能的严重降低时可采用累计梯度的策略，即每次计算完不反向更新，而是累计多次的误差后进行一次更新，这是一种在内存有限情况下实现有效梯度更新的一个策略。batch size过小通常对batchnorm的影响是最大的，若网络模型中存在batchnorm，batch size若只为1或者2时会对训练结果产生非常大的影响。这时通常有两种策略，一、若模型使用了预训练网络，可冻结预训练网络中batchnorm的模型参数，有效降低batch size引起的统计量变化的影响。二、在网络不是过深或者过于复杂时可直接移除batchnorm或者使用groupnorm代替batchnorm，前者不多阐释，后者是有FAIR提出的一种用于减少batch对batchnorm影响，其主要策略是先将特征在通道上进行分组，然后在组内进行归一化。即归一化操作上完全与batch size无关。这种groupnorm的策略被证实在极小批量网络训练上能达到较优秀的性能。当然这里也引入里group这个超参数，一般情况下建议不宜取group为1或者各通道单独为组的group数量，可结合实际网络稍加调试。</p>
<p>​    为了降低训练时间的成本，多机多卡的分布式系统通常会使用超大的batch size进行网络训练。同样的在14.2.4中，我们提到了超大batch size会带来梯度方向过于一致而导致的精度大幅度降低的问题。这时通常可采用层自适应速率缩放（LARS）算法。从理论认知上将，batch size增大会减少反向传播的梯度更新次数，但为了达到相同的模型效果，需要增大学习率。但学习率一旦增大，又会引起模型的不收敛。为了解决这一矛盾，LARS算法就在各层上自适应的计算一个本地学习率用于更新本层的参数，这样能有效的提升训练的稳定性。目前利用LARS算法，腾讯公司使用65536的超大batch size能将ResNet50在ImageNet在4分钟完成训练，而谷歌使用32768的batch size使用TPU能将该时间缩短至2分钟。</p>
<h2 id="合理使用预训练网络"><a href="#合理使用预训练网络" class="headerlink" title="合理使用预训练网络"></a>合理使用预训练网络</h2><h3 id="什么是微调（fine-tune）"><a href="#什么是微调（fine-tune）" class="headerlink" title="什么是微调（fine-tune）"></a>什么是微调（fine-tune）</h3><p>​    微调（fine-tune），顾名思义指稍微调整参数即可得到优秀的性能，是迁移学习的一种实现方式。微调和从头训练（train from scratch）的本质区别在于模型参数的初始化，train from scratch通常指对网络各类参数进行随机初始化（当然随机初始化也存在一定技巧），随机初始化模型通常不具有任何预测能力，通常需要大量的数据或者特定域的数据进行从零开始的训练，这样需要训练到优秀的模型通常是稍困难的。而微调的网络，网络各类参数已经在其他数据集（例如ImageNet数据集）完成较好调整的，具备了较优秀的表达能力。因此，我们只需要以较小的学习速率在自己所需的数据集领域进行学习即可得到较为优秀的模型。微调通常情况下，无须再重新设计网络结构，预训练模型提供了优秀的结构，只需稍微修改部分层即可。在小数据集上，通常微调的效果比从头训练要好很多，原因在于数据量较小的前提下，训练更多参数容易导致过度拟合。</p>
<h3 id="微调有哪些不同方法？"><a href="#微调有哪些不同方法？" class="headerlink" title="微调有哪些不同方法？"></a>微调有哪些不同方法？</h3><p>​    以图像分类为例，通常情况下由于不同数据集需要的类别数不同，我们需要修改网络的输出顶层。这种情况下有两种微调方式：</p>
<ul>
<li><p>不冻结网络模型的任何层，对最后的改动层使用较大的学习率，对未改动层以较小的学习率进行训练全模型训练，进行多轮训练即可。即一步完成训练。</p>
</li>
<li><p>冻结除了顶部改动层以外的所有层参数，即不对冻结部分的层进行参数训练更新，进行若干轮的微调训练后，放开顶部层以下的若干层或者全部放开所有层的参数，再次进行若干轮训练即可。即分多步训练。</p>
<p>以上两种都属于微调。目前由于存在大量优秀的预训练模型，如何确定哪个模型适合自己的任务并能得到最佳性能需要花大量的时间探索。此时，上述的前者是种不错训练方式，你无须进行过多分步的操作。而当探索到一个比较适合的模型时，你不妨可以再次重新尝试下以第二种方式进行训练，或许能得到相比于前者稍高些的性能，因为小数据集上调整过多的参数过拟合的机率也会增大，当然这并不是绝对的。</p>
</li>
</ul>
<h3 id="微调先冻结底层，训练顶层的原因？"><a href="#微调先冻结底层，训练顶层的原因？" class="headerlink" title="微调先冻结底层，训练顶层的原因？"></a>微调先冻结底层，训练顶层的原因？</h3><p>​    14.12中第二种冻结多步训练的方式。首先冻结除了顶部改动层以外的所有层参数，对顶层进行训练，这个过程可以理解为顶层的域适应训练，主要用来训练适应模型的现有特征空间，防止顶层糟糕的初始化，对已经具备一定表达能力的层的干扰和破坏，影响最终的性能。之后，在很多深度学习框架教程中会使用放开顶层往下一半的层数，继续进行微调。这样的好处在于越底层的特征通常是越通用的特征，越往上其整体的高层次语义越完备，这通过感受野很容易理解。所以，若预训练模型的数据和微调训练的数据语义差异越大（例如ImageNet的预模型用于医学图像的训练），那越往顶层的特征语义差异就越大，因此通常也需要进行相应的调整。</p>
<h3 id="不同的数据集特性下如何微调？"><a href="#不同的数据集特性下如何微调？" class="headerlink" title="不同的数据集特性下如何微调？"></a>不同的数据集特性下如何微调？</h3><ul>
<li>数据集数据量少，数据和原数据集类似。这是通常做法只需修改最后的输出层，训练即可，训练过多参数容易过拟合。</li>
<li>数据集数据量少，数据和原数据集差异较大。由于数据差异较大，可以在完成输出顶层的微调后，微调顶层往下一半的层数，进行微调。</li>
<li>数据集数据量大，数据与原数据集差异较大。这种情况下，通常已经不需要用预训练模型进行微调，通常直接重新训练即可。</li>
<li>数据集数据量大，数据与原数据类似。这时预训练模型的参数是个很好的初始化，可利用预训练模型放开所有层以较小的学习率微调即可。</li>
</ul>
<h3 id="目标检测中使用预训练模型的优劣？"><a href="#目标检测中使用预训练模型的优劣？" class="headerlink" title="目标检测中使用预训练模型的优劣？"></a>目标检测中使用预训练模型的优劣？</h3><p>​    目标检测中无论是一阶段的YOLO、SSD或者RetinaNet 还是二阶段的Faster R-CNN、R-FCN 和 FPN都是基于ImageNet上预训练好的分类模型。</p>
<p>​    优势在于：</p>
<p>​    1、正如大部分微调的情况一样，使用预训练网络已拥有优秀的语义特征，能有效的加快训练速度；</p>
<p>​    2、其次，对于大部分二阶段的模型来说，并未实现严格意义上的完全端对端的训练，所以使用预训练模型能直接提取到语义特征，能使两个阶段的网络更容易实现模型的优化。</p>
<p>​    劣势在于，分类模型和检测模型之间仍然存在一定任务上的差异：</p>
<p>​    1、分类模型大部分训练于单目标数据，对同时进行多目标的捕捉能力稍弱，且不关注目标的位置，在一定程度上让模型损失部分空间信息，这对检测模型通常是不利的；</p>
<p>​    2、域适应问题，若预训练模型（ImageNet）和实际检测器的使用场景（医学图像，卫星图像）差异较大时，性能会受到影响；</p>
<p>​    3、使用预训练模型就意味着难以自由改变网络结构和参数限制了应用场合。</p>
<h3 id="目标检测中如何从零开始训练-train-from-scratch-？"><a href="#目标检测中如何从零开始训练-train-from-scratch-？" class="headerlink" title="目标检测中如何从零开始训练(train from scratch)？"></a>目标检测中如何从零开始训练(train from scratch)？</h3><p>​    结合FAIR相关的研究，我们可以了解目标检测和其他任务从零训练模型一样，只要拥有足够的数据以及充分而有效的训练，同样能训练出不亚于利用预训练模型的检测器。这里我们提供如下几点建议：</p>
<p>​    1、数据集不大时，同样需要进行数据集增强。</p>
<p>​    2、预训练模型拥有更好的初始化，train from scratch需要更多的迭代次数以及时间训练和优化检测器。而二阶段模型由于并不是严格的端对端训练，此时可能需要更多的迭代次数以及时间，而一阶段检测模型训练会相对更容易些（例如DSOD以ScratchDet及）。</p>
<p>​    3、目标检测中train from scratch最大的问题还是batch size过小。所以可采取的策略是增加GPU使用异步batchnorm增大batch size，若条件限制无法使用更多GPU时，可使用groupnorm代替batchnorm</p>
<p>​    4、由于分类模型存在对多目标的捕捉能力弱以及对物体空间位置信息不敏感等问题，可借鉴DetNet训练一个专属于目标检测的模型网络，增强对多目标、尺度和位置拥有更强的适应性。</p>
<h2 id="如何改善-GAN-的性能"><a href="#如何改善-GAN-的性能" class="headerlink" title="如何改善 GAN 的性能"></a>如何改善 GAN 的性能</h2><p>优化GAN性能通常需要在如下几个方面进行</p>
<ul>
<li>设计或选择更适合目的代价函数。</li>
<li>添加额外的惩罚。</li>
<li>避免判别器过度自信和生成器过度拟合。</li>
<li>更好的优化模型的方法。</li>
<li>添加标签明确优化目标。</li>
</ul>
<p>GAN常用训练技巧</p>
<ul>
<li><p>输入规范化到（-1，1）之间，最后一层的激活函数使用tanh（BEGAN除外）</p>
</li>
<li><p>使用wassertein GAN的损失函数，</p>
</li>
<li><p>如果有标签数据的话，尽量使用标签，也有人提出使用反转标签效果很好，另外使用标签平滑，单边标签平滑或者双边标签平滑</p>
</li>
<li><p>使用mini-batch norm， 如果不用batch norm 可以使用instance norm 或者weight norm</p>
</li>
<li><p>避免使用RELU和pooling层，减少稀疏梯度的可能性，可以使用leakrelu激活函数</p>
</li>
<li><p>优化器尽量选择ADAM，学习率不要设置太大，初始1e-4可以参考，另外可以随着训练进行不断缩小学习率，</p>
</li>
<li><p>给D的网络层增加高斯噪声，相当于是一种正则</p>
<h2 id="AutoML"><a href="#AutoML" class="headerlink" title="AutoML"></a>AutoML</h2></li>
</ul>
<h3 id="什么是AutoML？"><a href="#什么是AutoML？" class="headerlink" title="什么是AutoML？"></a>什么是AutoML？</h3><p>​    目前一个优秀的机器学习和深度学习模型，离不开这几个方面：</p>
<p>​    一、优秀的数据预处理；</p>
<p>​    二、合适的模型结构和功能；</p>
<p>​    三、优秀的训练策略和超参数；</p>
<p>​    四、合适的后处理操作；</p>
<p>​    五、严格的结果分析。</p>
<p>​    这几方面都对最终的结果有着举足轻重的影响，这也是目前的数据工程师和学者们的主要工作。但由于这每一方面都十分繁琐，尤其是在构建模型和训练模型上。而大部分情况下，这些工作有无须过深专业知识就能使用起来。所以AutoML主要的作用就是来帮助实现高效的模型构建和超参数调整。例如深度学习网络的架构搜索、超参数的重要性分析等等。当然AutoML并不简单的进行暴力或者随机的搜索，其仍然需要机器学习方面的知识，例如贝叶斯优化、强化学习、元学习以及迁移学习等等。目前也有些不错的AutoML工具包，例如Alex Honchar的Hyperopt、微软的NNI、Autokeras等。</p>
<h3 id="自动化超参数搜索方法有哪些？"><a href="#自动化超参数搜索方法有哪些？" class="headerlink" title="自动化超参数搜索方法有哪些？"></a>自动化超参数搜索方法有哪些？</h3><p>​    目前自动化搜索主要包含网格搜索，随机搜索，基于模型的超参优化</p>
<p>​    网格搜索：</p>
<p>​        通常当超参数量较少的时候，可以使用网格搜索法。即列出每个超参数的大致候选集合。利用这些集合        进行逐项组合优化。在条件允许的情况下，重复进行网格搜索会当优秀，当然每次重复需要根据上一步得到的最优参数组合，进行进一步的细粒度的调整。网格搜索最大的问题就在于计算时间会随着超参数的数量指数级的增长。</p>
<p>​    随机搜索：</p>
<p>​        随机搜索，是一种用来替代网格搜索的搜索方式。随机搜索有别于网格搜索的一点在于，我们不需要设定一个离散的超参数集合，而是对每个超参数定义一个分布函数来生成随机超参数。随机搜索相比于网格搜索在一些不敏感超参上拥有明显优势。例如网格搜索对于批样本数量（batch size），在[16,32,64]这些范围内进行逐项调试，这样的调试显然收益更低下。当然随机搜索也可以进行细粒度范围内的重复的搜索优化。</p>
<p><img src="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\14.14.png" class="lazyload" data-srcset="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\14.14.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    基于模型的超参优化：</p>
<p>​        有别于上述两种的搜索策略，基于模型的超参调优问题转化为了优化问题。直觉上会考虑是否进行一个可导建模，然后利用梯度下降进行优化。但不幸的是我们的超参数通常情况下是离散的，而且其计算代价依旧很高。</p>
<p>​        基于模型的搜索算法，最常见的就是贝叶斯超参优化。有别于的网格搜索和随机搜索独立于前几次搜索结果的搜索，贝叶斯则是利用历史的搜索结果进行优化搜索。其主要有四部分组成，1.目标函数，大部分情况下就是模型验证集上的损失。2、搜索空间，即各类待搜索的超参数。3、优化策略，建立的概率模型和选择超参数的方式。4、历史的搜索结果。首先对搜索空间进行一个先验性的假设猜想，即假设一种选择超参的方式，然后不断的优化更新概率模型，最终的目标是找到验证集上误差最小的一组超参数。</p>
<h3 id="什么是神经网络架构搜索（NAS）"><a href="#什么是神经网络架构搜索（NAS）" class="headerlink" title="什么是神经网络架构搜索（NAS）"></a>什么是神经网络架构搜索（NAS）</h3><p>2015至2017年间，是CNN网络设计最兴盛的阶段，大多都是由学者人工设计的网络结构。这个过程通常会很繁琐。其主要原因在于对不同模块组件的组成通常是个黑盒优化的问题，此外，在不同结构超参数以及训练超参数的选择优化上非凸优化问题，或者是个混合优化问题，既有离散空间又有连续空间。NAS（Neural Architecture Search）的出现就是为了解决如何通过机器策略和自动化的方式设计出优秀高效的网络。而这种策略通常不是统一的标准，不同的网络结合实际的需求通常会有不同的设计，比如移动端的模型会在效率和精度之间做平衡。目前，NAS也是AUTOML中最重要的部分。NAS通常会分为三个方面，搜索空间（在哪搜索），搜索策略（如何搜索）及评价预估。</p>
<ul>
<li><p>搜索空间，即在哪搜索，定义了优化问题所需变量。不同规模的搜索空间的变量其对于的难度也是不一样的。早期由于网络结构以及层数相对比较简单，参数量较少，因此会更多的使用遗传算法等进化算法对网络的超参数和权重进行优化。深度学习发展到目前，模型网络结构越来越复杂，参数量级越来越庞大，这些进化算法已经无法继续使用。但若我们先验给定一些网络结构和超参数，模型的性能已经被限制在给定的空间，此时搜索的空间已变得有限，所以只需对复杂模型的架构参数和对应的超参数进行优化即可。</p>
</li>
<li><p>搜索策略， 即如何搜索，定义了如何快速、准确找到最优的网络结构参数配置的策略。常见的搜索方法包括：随机搜索、贝叶斯优化以及基于模型的搜索算法。其中主要代表为2017 年谷歌大脑的使用强化学习的搜索方法。</p>
</li>
<li><p>评价预估，定义了如何高效对搜索的评估策略。深度学习中，数据规模往往是庞大的，模型要在如此庞大的数据规模上进行搜索，这无疑是非常耗时的，对优化也会造成非常大的困难，所以需要一些高效的策略做近似的评估。 这里一般会有如下三种思路：</p>
<p>一、使用些低保真的训练集来训练模型。低保真在实际中可以用不同的理解，比如较少的迭代次数，用一小部分数据集或者保证结构的同时减少通道数等。这些方法都可以在测试优化结构时大大降低计算时间，当然也会存在一定的偏差。但架构搜索从来并不是要一组固定的参数，而是一种优秀的模型结构。最终选取时，只需在较优秀的几组结构中进行全集训练，进行择优选取即可。</p>
<p>二、使用代理模型。除了低保真的训练方式外，学者们提出了一种叫做代理模型的回归模型，采用例如插值等策略对已知的一些参数范围进行预测，目的是为了用尽可能少的点预测到最佳的结果。</p>
<p>三、参数级别的迁移。例如知识蒸馏等。用已训练好的模型权重参数对目标问题搜索，通常会让搜索拥有一个优秀的起点。由于积累了大量的历史寻优数据，对新问题的寻优将会起到很大的帮助。</p>
</li>
</ul>
<h3 id="NASNet的设计策略"><a href="#NASNet的设计策略" class="headerlink" title="NASNet的设计策略"></a>NASNet的设计策略</h3><p>NASNet是最早由google brain 通过网络架构搜索策略搜索并成功训练ImageNet的网络，其性能超越所有手动设计的网络模型。关于NASNet的搜索策略，首先需要参考google brain发表在ICLR2017的论文《Neural Architecture Search with Reinforcement Learning》。该论文是最早成功通过架构搜索策略在cifar-10数据集上取得比较不错效果的工作。NASNet很大程度上是沿用该搜索框架的设计思想。</p>
<p>NASNet的核心思想是利用强化学习对搜索空间内的结构进行反馈探索。架构搜索图如下，定义了一个以RNN为核心的搜索控制器。在搜索空间以概率p对模型进行搜索采样。得到网络模型A后，对该模型进行训练，待模型收敛得到设定的准确率R后，将梯度传递给控制器RNN进行梯度更新。</p>
<p><img src="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\NAS搜索策略.png" class="lazyload" data-srcset="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\NAS搜索策略.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                    架构搜索策略流程</p>
<p>RNN控制器会对卷积层的滤波器的尺寸、数量以及滑动间隔进行预测。每次预测的结果都会作为下一级的输入，档层数达到设定的阈值时，会停止预测。而这个阈值也会随着训练的进行而增加。这里的控制器之预测了卷积，并没有对例如inception系列的分支结构或者ResNet的跳级结构等进行搜索。所以，控制器需要进一步扩展到预测这些跳级结构上，这样搜索空间相应的也会增大。为了预测这些结构，RNN控制器内每一层都增加了一个预测跳级结构的神经元，文中称为锚点，稍有不同的是该锚点的预测会由前面所有层的锚点状态决定。</p>
<p><img src="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\RNN控制器.png" class="lazyload" data-srcset="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\RNN控制器.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                    RNN控制器</p>
<p>NASNet大体沿用了上述生成网络结构的机器，并在此基础上做了如下两点改进：</p>
<p>1、先验行地加入inception系列和ResNet的堆叠模块的思想。其定义了两种卷积模块，Normal Cell和Reduction Cell，前者不进行降采样，而后者是个降采样的模块。而由这两种模块组成的结构可以很方便的通过不同数量的模块堆叠将其从小数据集搜索到的架构迁移到大数据集上，大大提高了搜索效率。</p>
<p><img src="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\NASNet的RNN控制器.png" class="lazyload" data-srcset="/zh-TW/ch14_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/img\ch14\NASNet的RNN控制器.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​                                    NASNet的RNN控制器</p>
<p>2、对RNN控制进行优化，先验性地将各种尺寸和类型的卷积和池化层加入到搜索空间内，用预测一个卷积模块代替原先预测一层卷积。如图，控制器RNN不在预测单个卷积内的超参数组成，而是对一个模块内的每一个部分进行搜索预测，搜索的空间则限定在如下这些操作中：</p>
<p>​                        • identity                      • 1x3 then 3x1 convolution<br>​                        • 1x7 then 7x1 convolution      • 3x3 dilated convolution<br>​                        • 3x3 average pooling               • 3x3 max pooling<br>​                        • 5x5 max pooling              • 7x7 max pooling<br>​                        • 1x1 convolution                  • 3x3 convolution<br>​                        • 3x3 depthwise-separable conv • 5x5 depthwise-seperable conv<br>​                        • 7x7 depthwise-separable conv</p>
<p>在模块内的连接方式上也提供了element-wise addition和concatenate两种方式。NASNet的搜索方式和过程对NAS的一些后续工作都具有非常好的参考借鉴意义。</p>
<h3 id="网络设计中，为什么卷积核设计尺寸都是奇数"><a href="#网络设计中，为什么卷积核设计尺寸都是奇数" class="headerlink" title="网络设计中，为什么卷积核设计尺寸都是奇数"></a>网络设计中，为什么卷积核设计尺寸都是奇数</h3><p>我们发现在很多大部分网络设计时都会使用例如3x3/5x5/7x7等奇数尺寸卷积核，主要原因有两点：</p>
<ul>
<li>保证像素点中心位置，避免位置信息偏移</li>
<li>填充边缘时能保证两边都能填充，原矩阵依然对称</li>
</ul>
<h3 id="网络设计中，权重共享的形式有哪些，为什么要权重共享"><a href="#网络设计中，权重共享的形式有哪些，为什么要权重共享" class="headerlink" title="网络设计中，权重共享的形式有哪些，为什么要权重共享"></a>网络设计中，权重共享的形式有哪些，为什么要权重共享</h3><p>权重共享的形式：</p>
<ul>
<li>深度学习中，权重共享最具代表性的就是卷积网络的卷积操作。卷积相比于全连接神经网络参数大大减少；</li>
<li>多任务网络中，通常为了降低每个任务的计算量，会共享一个骨干网络。</li>
<li>一些相同尺度下的结构化递归网络</li>
</ul>
<p>权重共享的好处：</p>
<p>​    权重共享一定程度上能增强参数之间的联系，获得更好的共性特征。同时很大程度上降低了网络的参数，节省计算量和计算所需内存（当然，结构化递归并不节省计算量）。此外权重共享能起到很好正则的作用。正则化的目的是为了降低模型复杂度，防止过拟合，而权重共享则正好降低了模型的参数和复杂度。</p>
<p>​    因此一个设计优秀的权重共享方式，在降低计算量的同时，通常会较独享网络有更好的效果。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>异构计算</title>
    <url>/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="异构计算，-GPU和框架选型指南"><a href="#异构计算，-GPU和框架选型指南" class="headerlink" title="异构计算， GPU和框架选型指南"></a>异构计算， GPU和框架选型指南</h1><p>  深度学习训练和推理的过程中，会涉及到大量的向量(vector)，矩阵(matrix)和张量(tensor)操作，通常需要大量的浮点计算，包括高精度（在训练的时候）和低精度（在推理和部署的时候）。GPU， 作为一种通用可编程的加速器，最初设计是用来进行图形处理和渲染功能，但是从2007年开始，英伟达(NVIDIA)公司提出了第一个可编程通用计算平台（GPGPU），同时提出了CUDA框架，从此开启了GPU用于通用计算的新纪元。此后，不计其数的科研人员和开发者，对各种不同类型的算法用CUDA进行（部分）改写，从而达到几倍到数百倍的加速效果。尤其是在机器学习，特别是深度学习的浪潮来临后，GPU加速已经是各类工具实现的基本底层构架之一。本章里，会简单介绍GPU的基本架构，性能指标，框架选择等等和深度学习相关的内容。</p>
<h2 id="什么是异构计算？"><a href="#什么是异构计算？" class="headerlink" title="什么是异构计算？"></a>什么是异构计算？</h2><p>异构计算是基于一个更加朴素的概念，”异构现象“，也就是不同计算平台之间，由于硬件结构（包括计算核心和内存），指令集和底层软件实现等方面的不同而有着不同的特性。异构计算就是使用结合了两个或者多个不同的计算平台，并进行协同运算。比如，比较常见的，在深度学习和机器学习中已经比较成熟的架构：CPU和GPU的异构计算;此外还有比较新的Google推出的协处理器（TPU），根据目的而定制的ASIC，可编程的FPGA等也都是现在在异构计算中使用比较多的协处理器。而，本章中会着重介绍和深度学习共同繁荣的图形加算器，也就是常说的GPU。</p>
<h2 id="什么是GPGPU？"><a href="#什么是GPGPU？" class="headerlink" title="什么是GPGPU？"></a>什么是GPGPU？</h2><p>GPU,就如名字所包含的内容，原本开发的目的是为了进行计算机图形渲染，而减少对于CPU的负载。由于图像的原始特性，也就是像素间的独立性，所以GPU在设计的时候就遵从了“单指令流多数据流（SIMD）”架构，使得同一个指令（比如图像的某种变换），可以同时在多一个像素点上进行计算，从而得到比较大的吞吐量，才能使得计算机可以实时渲染比较复杂的2D/3D场景。在最初的应用场景里，GPU并不是作为一种通用计算平台出现的，直到2007年左右，一家伟大的公司将GPU带到通用计算的世界里，使得其可以在相对比较友好的编程环境（CUDA/OpenCL）里加速通用程序成了可能。从此之后，GPU通用计算，也就是GPGPU就成了学界和工业界都频繁使用的技术，在深度学习爆发的年代里，GPGPU成了推动这股浪潮非常重要的力量。</p>
<h2 id="GPU架构简介"><a href="#GPU架构简介" class="headerlink" title="GPU架构简介"></a>GPU架构简介</h2><p>GPU，图形显示芯片作为不同于CPU的设计逻辑和应用场景，有着非常不同的架构，本部分将简单介绍GPU究竟是如何架构，其中的计算核心有哪些特性。</p>
<h3 id="如何通俗理解GPU的架构？"><a href="#如何通俗理解GPU的架构？" class="headerlink" title="如何通俗理解GPU的架构？"></a>如何通俗理解GPU的架构？</h3><p>首先，下图简单地展示了几个GPU不同于CPU的特性：</p>
<ul>
<li>计算核心： 图中的CPU,i7-5960，Intel的第五代Broadwell架构，其中包括了8个CPU核心(支持16线程)，也就是理论上可以有16个不同的运算同时进行。除了8个核心计算单元，大部分的芯片面积是被3级缓存，内存和控制电路占据了。同样的，来自Nvidia的GTX980GPU，在差不多的芯片面积上，大部分是计算单元，16个SM，也就是流处理单元，每个流处理单元中包含着128个CUDA计算核心，所以总共来说，有2048个GPU运算单元，相应地这颗GPU理论上可以在一个时钟周期内可以进行2048次单精度运算。</li>
</ul>
<p><img src="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/cpu_gpu.png" class="lazyload" data-srcset="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/cpu_gpu.png" srcset="data:image/png;base64,666" alt="CPU和GPU的简单架构对比图"></p>
<ul>
<li>计算核心频率：时钟频率，代表每一秒中内能进行同步脉冲次数，也是从一个侧面反映一个计算元件的工作速度。下图中对比了个别早期产品，比如Intel的x5650和几款Nvidia的GPU。可以看出核心频率而言，CPU要远高于GPU。对于CPU而言，在不考虑能源消耗和制程工艺限制的情况下，追求更高的主频。但，在GPU的设计中，采用了多核心设计，即使是提高一些频率，其实对于总体性能影像不会特别大。当然，其中还有能耗方面的考虑，避免发热过高，也进行了权衡。还有一个可能的原因是，在一个流处理器中的每个核心（CUDA核心）的运行共享非常有限的缓存和寄存器，由于共享内存也是有性能极限的，所以即使每个GPU核心频率提高，如果被缓存等拖累也是无法展现出高性能的。</li>
</ul>
<p><img src="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/cpu_specs.png" class="lazyload" data-srcset="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/cpu_specs.png" srcset="data:image/png;base64,666" alt="CPU简单信息"></p>
<p><img src="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/gpu_specs.png" class="lazyload" data-srcset="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/gpu_specs.png" srcset="data:image/png;base64,666" alt="GPU的简单信息对比"></p>
<ul>
<li>内存架构：GPU的多层内存架构包括全局内存（也就是通常意义上大部分比较关注的内存，在若干到16GB之间，截至到当前最新），2级缓存，和芯片上的存储（包括寄存器，和1级缓存共用的共享内存，只读/纹理缓存和常量缓存）。通常来说，最高速的共享内存/缓存和寄存器都是非常有限的，比如在Tesla的K20中，只有48K的缓存可以作为共享内存或者1级缓存使用，所以在很多用GPU加速算法实现的过程中，有效地利用这些高速缓存是使得性能提升的非常重要的方面。</li>
</ul>
<p><img src="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/gpu_memory_arch.png" class="lazyload" data-srcset="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/gpu_memory_arch.png" srcset="data:image/png;base64,666" alt="GPU的简单信息对比"></p>
<p><img src="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/gpu_memory.png" class="lazyload" data-srcset="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/gpu_memory.png" srcset="data:image/png;base64,666" alt="GPU的内存架构容量信息"></p>
<h3 id="CUDA-核心是什么？"><a href="#CUDA-核心是什么？" class="headerlink" title="CUDA 核心是什么？"></a>CUDA 核心是什么？</h3><p>上面提到在一个GPU芯片里，会很几千个CUDA核心，被分布在多个流处理单元（SM）中，比如上面提到早期的GTX980中的16个SM中各包含了128个CUDA核心。如下图所示，作为GPU架构中的最小单元，其实它的设计和CPU有着非常类似的结构，其中包括了一个浮点运算单元和整型运算单元，和控制单元。同一个流处理器中，所有的CUDA核心将同步执行同一个指令，但是作用于不同的数据点上。</p>
<p><img src="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/cudacore.jpg" class="lazyload" data-srcset="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/cudacore.jpg" srcset="data:image/png;base64,666" alt="CUDA简单介绍"></p>
<p>一般来说，更加多的CUDA核心意味着有更多的并行执行单元，所以也就可以片面地认为是有更加高的性能。但是，其实这个也是取决于很多方面，最重要的是算法在并行实现的时候有没有高效地调度和内存的使用优化。在现在我们使用的大部分GPU加速的深度学习框架里，包括Tensorflow，PyTorch等都是依赖于底层的GPU的矩阵加速代码的实现。为此Nvidia公司也是制定和实现了统一的接口，比如cuDNN，方便上层框架更好的利用GPU的性能。</p>
<h3 id="为什么要使用GPU？"><a href="#为什么要使用GPU？" class="headerlink" title="为什么要使用GPU？"></a>为什么要使用GPU？</h3><p>对于并行计算来说，可以非常粗略地分为：</p>
<ul>
<li>并行指令： 也就是多个指令可以同时分配到不同的计算核心上同时进行，而他们的操作是不同的，并且他们之间相互独立，不需要额外的同步和信息共享。</li>
<li>并行数据流： 如果数据本身存在的天然的独立性，比如图像中的每一个像素，那么在对这个图像做处理的过程中，同一个指令可以同时作用于每一个像素。在这种情况下，这个对于完整图像的操作可以并行化。理论上，如果内存不是问题，并且计算单元的数量大于整个图像中总像素点的话，这个操作可以在一个时钟周期内完成。</li>
</ul>
<p>GPU整体的架构而言，某种意义上是同时支持以上两种并行模式。在同一个流处理器中，采用了“单一指令并行数据流的模式”，而在多个流处理器中，同一时间可以派发不同的指令。从这一点出发，GPU芯片算是一个非常灵活的架构。一个芯片中，流处理器的个数和其中包含的CUDA核心的数量也是一种面向应用设计时候找到的一个平衡点。</p>
<p>基于深度学习中大部分的操作的天然并行性（大量的矩阵操作），GPU在当下还是一种非常适合的计算平台。一个非常典型的例子就是常见的矩阵相乘（如下图），要计算Z = X×Y，通过并行计算，X和Y中的行向量和列向量的逐元素相乘就可以同时进行，只要得到结果后再进行累加，而且累加的过程中也是可以进行并行化，使得效率有非常大的提高。Nvidia也是制定和开发了一套底层类库，CUBlas方便开发者。我们熟悉的几大框架(e.g. Tensorflow, PyTorch等)也是遵循和使用了这些并行类库，所以才使得训练和部署性能有了非常多的提高。</p>
<p><img src="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/mat_mul_gpu.png" class="lazyload" data-srcset="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/mat_mul_gpu.png" srcset="data:image/png;base64,666" alt="CUDA 矩阵乘法示例"></p>
<h3 id="深度学习中的GPU应用"><a href="#深度学习中的GPU应用" class="headerlink" title="深度学习中的GPU应用"></a>深度学习中的GPU应用</h3><p>深度学习在最近几年内出现的井喷现象背后也是GPU的存在和发展作为坚实的推动力量。</p>
<p>哪些场景使用GPU</p>
<p>ImageNet的例子</p>
<h3 id="新图灵架构里的tensor-core对深度学习有什么作用？"><a href="#新图灵架构里的tensor-core对深度学习有什么作用？" class="headerlink" title="新图灵架构里的tensor core对深度学习有什么作用？"></a>新图灵架构里的tensor core对深度学习有什么作用？</h3><h2 id="CUDA-框架"><a href="#CUDA-框架" class="headerlink" title="CUDA 框架"></a>CUDA 框架</h2><h3 id="做CUDA编程难不难？"><a href="#做CUDA编程难不难？" class="headerlink" title="做CUDA编程难不难？"></a>做CUDA编程难不难？</h3><h3 id="cuDNN"><a href="#cuDNN" class="headerlink" title="cuDNN"></a>cuDNN</h3><h2 id="GPU硬件环境配置推荐"><a href="#GPU硬件环境配置推荐" class="headerlink" title="GPU硬件环境配置推荐"></a>GPU硬件环境配置推荐</h2><h3 id="GPU主要性能指标"><a href="#GPU主要性能指标" class="headerlink" title="GPU主要性能指标"></a>GPU主要性能指标</h3><p>GPU的性能主要由以下三个参数构成：</p>
<ol>
<li>计算能力。通常我们关心的是32位浮点计算能力。16位浮点训练也开始流行，如果只做预测的话也可以用8位整数。</li>
<li>内存大小。当模型越大，或者训练时的批量越大时，所需要的GPU内存就越多。</li>
<li>内存带宽。只有当内存带宽足够时才能充分发挥计算能力。</li>
</ol>
<p>对于大部分用户来说，只要考虑计算能力就可以了。GPU内存尽量不小于4GB。但如果GPU要同时显示图形界面，那么推荐的内存大小至少为6GB。内存带宽通常相对固定，选择空间较小。</p>
<p>下图描绘了GTX 900和1000系列里各个型号的32位浮点计算能力和价格的对比。其中价格为Wikipedia的建议价格。</p>
<p><img src="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/gtx.png" class="lazyload" data-srcset="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/gtx.png" srcset="data:image/png;base64,666" alt="浮点计算能力和价格的对比。"></p>
<p>我们可以从图中读出两点信息：</p>
<ol>
<li>在同一个系列里面，价格和性能大体上成正比。但后发布的型号性价比更高，例如980 TI和1080 TI。</li>
<li>GTX 1000系列比900系列在性价比上高出2倍左右。</li>
</ol>
<p>如果大家继续比较GTX较早的系列，也可以发现类似的规律。据此，我们推荐大家在能力范围内尽可能买较新的GPU。</p>
<p>对于RTX系列，新增了Tensor Cores单元及支持FP16，使得显卡的可选择范围更加多元。</p>
<h3 id="购买建议"><a href="#购买建议" class="headerlink" title="购买建议"></a>购买建议</h3><p>首先给出一些总体的建议：</p>
<p>性价比高但较贵：RTX 2070，GTX 1080 Ti</p>
<p>性价比高又便宜：RTX 2060，GTX 1060（6GB）</p>
<p>当使用数据集&gt; 250GB：GTX Titan X（Maxwell） ，NVIDIA Titan X Pascal或NVIDIA Titan Xp</p>
<p>没有足够的钱：GTX 1060（6GB）</p>
<p>几乎没有钱，入门级：GTX 1050 Ti（4GB）</p>
<p>做Kaggle比赛：RTX 2070、GTX 1060（6GB）适用于任何“正常”比赛，GTX 1080 Ti（预算足够可以选择RTX 2080 Ti）用于“深度学习竞赛”</p>
<p>计算机视觉研究员：RTX 2080 Ti（涡轮散热或水冷散热较好，方便后期增加新的显卡）如果网络很深可以选择Titan RTX</p>
<p>一名NLP研究人员：RTX 2080 Ti，并使用FP16来训练</p>
<p>搭建一个GPU集群：这个有点复杂，另做探讨。</p>
<p>刚开始进行深度学习研究：从RTX 2060或GTX 1060（6GB）开始，根据你下一步兴趣（入门，Kaggle比赛，研究，应用深度学习）等等，再进行选择。目前，RTX 2060和GTX 1060都比较合适入门的选择。</p>
<p>想尝试下深度学习，但没有过多要求：GTX 1050 ti（4或2GB）</p>
<p>目前独立GPU主要有AMD和Nvidia两家厂商。其中Nvidia在深度学习布局较早，对深度学习框架支持更好。因此，目前大家主要会选择Nvidia的GPU。</p>
<p>Nvidia有面向个人用户（例如GTX系列）和企业用户（例如Tesla系列）的两类GPU。这两类GPU的计算能力相当。然而，面向企业用户的GPU通常使用被动散热并增加了内存校验，从而更适合数据中心，并通常要比面向个人用户的GPU贵上10倍。</p>
<p>如果你是拥有100台机器以上的大公司用户，通常可以考虑针对企业用户的Nvidia Tesla系列。如果你是拥有10到100台机器的实验室和中小公司用户，预算充足的情况下可以考虑Nvidia DGX系列，否则可以考虑购买如Supermicro之类的性价比比较高的服务器，然后再购买安装GTX系列的GPU。</p>
<p>Nvidia一般每一两年发布一次新版本的GPU，例如2017年发布的是GTX 1000系列。每个系列中会有数个不同的型号，分别对应不同的性能。</p>
<h2 id="软件环境搭建"><a href="#软件环境搭建" class="headerlink" title="软件环境搭建"></a>软件环境搭建</h2><p>深度学习其实就是指基于一套完整的软件系统来构建算法，训练模型。如何搭建一套完整的软件系统，比如操作系统的选择？安装环境中遇到的问题等等，本节做一个简单的总结。</p>
<h3 id="操作系统选择？"><a href="#操作系统选择？" class="headerlink" title="操作系统选择？"></a>操作系统选择？</h3><p>针对硬件厂商来说，比如NVIDIA，对各个操作系统的支持都是比较好的 ，比如Windows系列,Linux系列，但是由于Linux系统对专业技术人员比较友好，所以目前几乎所有的深度学习系统构建都是基于Linux的，比较常用的系统如Ubuntu系列，CentOS系列等等。<br>在构建系统的时候，如何选择合适的操作系是一个刚刚入门深度学习的工作者面临的问题，在这里给出几点建议：<br>（1）刚刚入门，熟悉Windows系统，但是对Linux和深度学习都不太熟，这个时候可以基于windows系列系统来做入门学习<br>（2）简单了解Linux的使用，不太懂深度学习相关知识，可以直接基于Linux系统来搭建框架，跑一些开源的项目，慢慢深入研究学习<br>（3）熟悉Linux，不熟悉深度学习理论，毫无疑问，强烈推荐使用Linux系统，安装软件简单，工作效率高<br>总之一句话，如果不熟悉Linux，就先慢慢熟悉，最终还是要回归到Linux系统来构建深度学习系统</p>
<h3 id="常用基础软件安装？"><a href="#常用基础软件安装？" class="headerlink" title="常用基础软件安装？"></a>常用基础软件安装？</h3><p>目前有众多深度学习框架可供大家使用，但是所有框架基本都有一个共同的特点，目前几乎都是基于Nvidia的GPU来训练模型，要想更好的使用Nvidia的GPU，cuda和cudnn就是必备的软件安装。  </p>
<ol>
<li><p><strong>安装cuda</strong><br>上文中有关于cuda的介绍，这里只是简单介绍基于Linux系统安装cuda的具体步骤，可以根据自己的需要安装cuda8.0或者cuda9.0，这两种版本的安装步骤基本一致，这里以最常用的ubuntu 16.04 lts版本为例：  </p>
<ol>
<li><p>官网下载，地址<br>cuda8.0<a href="https://developer.nvidia.com/cuda-80-ga2-download-archive">https://developer.nvidia.com/cuda-80-ga2-download-archive</a><br>cuda9.0<a href="https://developer.nvidia.com/cuda-90-download-archive">https://developer.nvidia.com/cuda-90-download-archive</a><br>进入网址之后选择对应的系统版本即可，如下图所示：<br><img src="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/cuda8.0.png" class="lazyload" data-srcset="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/cuda8.0.png" srcset="data:image/png;base64,666" alt="cuda8.0"></p>
<p><img src="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/cuda9.0.png" class="lazyload" data-srcset="/zh-TW/ch15_GPU%E5%92%8C%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/%E5%BC%82%E6%9E%84%E8%BF%90%E7%AE%97%E3%80%81GPU%E5%8F%8A%E6%A1%86%E6%9E%B6%E9%80%89%E5%9E%8B/ch15/cuda9.0.png" srcset="data:image/png;base64,666" alt="cuda9.0">  </p>
</li>
<li><p>命令行中进入到cuda所在的位置，授予运行权限：<br>cuda8.0: sudo chmod +x cuda_8.0.61_375.26_linux.run<br>cuda9.0:sudo chmod +x cuda_9.0.176_384.81_linux.run</p>
</li>
<li><p>执行命令安装cuda：<br>cuda8.0:sudo sh cuda_8.0.61_375.26_linux.run<br>cuda9.0:sudo sh cuda_9.0.176_384.81_linux.run<br>之后命令之后下面就是安装步骤，cuda8.0和cuda9.0几乎一致：  </p>
<ul>
<li><p>首先出现cuda软件的版权说明，可以直接按q键跳过阅读  </p>
</li>
<li><p>Do you accept the previously read EULA?<br>​accept/decline/quit: <strong>accept</strong></p>
</li>
<li><p>Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 384.81?<br>​(y)es/(n)o/(q)uit:<strong>no</strong></p>
</li>
<li><p>Install the CUDA 9.0 Toolkit?<br>​(y)es/(n)o/(q)uit:<strong>yes</strong></p>
</li>
<li><p>Enter Toolkit Location<br>​ [ default is /usr/local/cuda-9.0 ]:直接按enter键即可</p>
</li>
<li><p>Do you want to install a symbolic link at /usr/local/cuda?<br>​(y)es/(n)o/(q)uit:<strong>yes</strong></p>
</li>
<li><p>Install the CUDA 9.0 Samples?<br>​ (y)es/(n)o/(q)uit:<strong>yes</strong></p>
</li>
</ul>
<p>以上步骤基本就是cuda的安装步骤。</p>
</li>
</ol>
</li>
<li><p><strong>安装cudnn</strong><br>cudnn是Nvidia的专门针对深度学习的加速库。。。</p>
</li>
</ol>
<h3 id="本机安装还是使用docker？"><a href="#本机安装还是使用docker？" class="headerlink" title="本机安装还是使用docker？"></a>本机安装还是使用docker？</h3><h3 id="GPU驱动问题"><a href="#GPU驱动问题" class="headerlink" title="GPU驱动问题"></a>GPU驱动问题</h3><h2 id="框架选择"><a href="#框架选择" class="headerlink" title="框架选择"></a>框架选择</h2><h3 id="主流框架比较"><a href="#主流框架比较" class="headerlink" title="主流框架比较"></a>主流框架比较</h3><p>（一个大表格比较）</p>
<h3 id="框架详细信息"><a href="#框架详细信息" class="headerlink" title="框架详细信息"></a>框架详细信息</h3><ul>
<li>Tensorflow<br>Tensorflow是Google于2015年开源的基于数据流编程的深度学习框架，得益于Google强大的技术实力和品牌背书，目前Tensorflow发展迅猛，其用户量远远超过其它框架用户。<br>优点：  <ol>
<li>由谷歌开发、维护，因此可以保障支持、开发的持续性</li>
<li>巨大、活跃的社区</li>
<li>网络训练的低级、高级接口</li>
<li>「TensorBoard」是一款强大的可视化套件，旨在跟踪网络拓扑和性能，使调试更加简单</li>
<li>TensorFlow 不仅支持深度学习，还有支持强化学习和其他算法的工具<br>缺点：  </li>
<li>计算图是纯 Python 的，因此速度较慢</li>
<li>图构造是静态的，意味着图必须先被「编译」再运行</li>
</ol>
</li>
</ul>
<ul>
<li>PyTorch<br>pytorch是Facebook于2017年才推出的深度学习框架，相对于其它框架，算是比较晚的了，但是这个同时也是优势，在设计的时候就会避免很多之前框架的问题，所以一经推出，就收到大家极大的欢迎<br>优点：  <ol>
<li>接口简洁且规范，文档齐全，和python无缝结合，</li>
<li>社区非常活跃，开源实现较多</li>
<li>提供动态计算图（意味着图是在运行时生成的），允许你处理可变长度的输入和输出，例如，在使用 RNN 时非常有用</li>
<li>易于编写自己的图层类型，易于在 GPU 上运行</li>
<li>「TensorBoard」缺少一些关键功能时，「Losswise」可以作为 Pytorch 的替代品</li>
</ol>
</li>
</ul>
<p>缺点:  </p>
<ol>
<li>模型部署相对其它框架稍有劣势，不过后续的pytorch1.0版本应该会有很大改善，和caffe2合并后，caffe2的优秀的模型部署能力可以弥补这个不足</li>
<li></li>
<li></li>
</ol>
<p>相关资源链接：  </p>
<ol>
<li>官网教程：<a href="https://pytorch.org/tutorials/">https://pytorch.org/tutorials/</a></li>
<li>基于pytorch的开源项目汇总：<a href="https://github.com/bharathgs/Awesome-pytorch-list">https://github.com/bharathgs/Awesome-pytorch-list</a><br>3.</li>
</ol>
<ul>
<li>Keras<br>Keras 是一个更高级、对用户最友好的 API，具有可配置的后端，由 Google Brain 团队成员 Francis Chollet 编写和维护<br>优点：  <ol>
<li>提供高级 API 来构建深度学习模型，使其易于阅读和使用 </li>
<li>编写规范的文档</li>
<li>大型、活跃的社区</li>
<li>位于其他深度学习库（如 Theano 和 TensorFlow，可配置）之上</li>
<li>使用面向对象的设计，因此所有内容都被视为对象（如网络层、参数、优化器等）。所有模型参数都可以作为对象属性进行访问<br>缺点：  </li>
<li>由于用途非常普遍，所以在性能方面比较欠缺</li>
<li>与 TensorFlow 后端配合使用时会出现性能问题（因为并未针对其进行优化），但与 Theano 后端配合使用时效果良好</li>
<li>不像 TensorFlow 或 PyTorch 那样灵活</li>
</ol>
</li>
</ul>
<ul>
<li><p>Sonnet</p>
</li>
<li><p>Caffe<br>caffe是第一个主流产品级深度学习库，于 2014 年由 UC Berkeley 发布开源<br>优点：  </p>
<ol>
<li>简单网络结构无需编写代码，可快速实现</li>
<li>漂亮的 Matlab 和 Python 接口</li>
<li>完全由c++编程实现，部署方便</li>
</ol>
</li>
</ul>
<p>缺点：  </p>
<ol>
<li>不灵活。在 Caffe 中，每个节点被当做一个层，因此如果你想要一种新的层类型，你需要定义完整的前向、后向和梯度更新过程。这些层是网络的构建模块，你需要在无穷无尽的列表中进行选择。（相反，在 TensorFlow 中，每个节点被当做一个张量运算例如矩阵相加、相乘或卷积。你可以轻易地定义一个层作为这些运算的组合。因此 TensorFlow 的构建模块更小巧，允许更灵活的模块化。）</li>
<li>需要大量的非必要冗长代码。如果你希望同时支持 CPU 和 GPU，你需要为每一个实现额外的函数。你还需要使用普通的文本编辑器来定义你的模型。真令人头疼！几乎每个人都希望程序化地定义模型，因为这有利于不同组件之间的模块化。有趣的是，Caffe 的主要架构师现在在 TensorFlow 团队工作</li>
<li>专一性。仅定位在计算机视觉（但做得很不错）  </li>
<li>不是以 Python 编写！如果你希望引入新的变动，你需要在 C++和 CUDA 上编程（对于更小的变动，你可以使用它的 Python 和 Matlab 接口）</li>
<li>糟糕的文档</li>
<li>安装比较困难！有大量的依赖包</li>
</ol>
<ul>
<li>Caffe2</li>
</ul>
<ul>
<li>MxNet<br>MxNet是dmlc社区推出的深度学习框架，MXNet由学术界发起，包括数个顶尖大学的多个学科的研究人员的贡献，在2017年被亚马逊指定为官方框架。<br>mxnet的最知名的优点就是其对多GPU的支持和扩展性强，其优秀的性能使之在工业界占有一席之地，在amazon支持之后，其文档和开发进度明显好很多。除了高可扩展性，MXNet 还提供混合编程模型（命令式和声明式），同时兼容多种编程语言（包括 Python、C ++、R、Scala、Julia、Matlab 和 JavaScript）的代码，目前主要在推python高层接口gluon</li>
</ul>
<p>优点：  </p>
<ol>
<li>多GPU支持好，扩展性强，支持多种编程语言接口，主要是由华人团队开发，中文社区活跃，中文文档资源和课程丰富</li>
<li>针对两大热门领域推出gluoncv和gluonNLP模块，复现经典论文，达到State-of-the-art，接口设计简单，文档齐全，拿来就可以用<br>缺点:  </li>
<li>现在mxnet官方社区主要在推gluon接口，接口稍有混乱，坑较多，入手门槛稍高</li>
<li>偏小众，经典网络和项目的开源实现相对于tensorflow和pytorch还是比较少，很多还是需要自己手动实现<br>相关资源链接：  </li>
<li>官方教程：<a href="http://mxnet.incubator.apache.org">http://mxnet.incubator.apache.org</a> 提供有快速入门教程和详细文档说明</li>
<li>中文教程：<a href="http://zh.gluon.ai/">http://zh.gluon.ai/</a> 官方的中文教程，此课程有对应的中文版视频，主要由李沐大神讲课</li>
<li>中文论坛：<a href="https://discuss.gluon.ai/">https://discuss.gluon.ai/</a> 官方发中文论坛，mxnet的主要作者都在这里，论坛比较活跃，可及时得到作者的回答</li>
<li>基于mxnet的开源项目实现：<a href="https://github.com/chinakook/Awesome-MXNet这里主要列举了mxnet在各个领域的项目的开源实现">https://github.com/chinakook/Awesome-MXNet这里主要列举了mxnet在各个领域的项目的开源实现</a></li>
</ol>
<ul>
<li><p>CNTK</p>
</li>
<li><p>PaddlePaddle</p>
</li>
<li><p>其他国内自主开发开源框架</p>
</li>
</ul>
<h3 id="哪些框架对于部署环境友好？"><a href="#哪些框架对于部署环境友好？" class="headerlink" title="哪些框架对于部署环境友好？"></a>哪些框架对于部署环境友好？</h3><ul>
<li><p>Tensorflow Serving</p>
</li>
<li><p>ONNX 标准</p>
</li>
<li><p>TensorRT</p>
</li>
<li><p>ONNPACK</p>
</li>
<li><p>Clipper</p>
</li>
</ul>
<h3 id="移动平台的框架如何选择？"><a href="#移动平台的框架如何选择？" class="headerlink" title="移动平台的框架如何选择？"></a>移动平台的框架如何选择？</h3><ul>
<li><p>Tensorflow Lite</p>
</li>
<li><p>Caffe2</p>
</li>
</ul>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="多GPU环境的配置"><a href="#多GPU环境的配置" class="headerlink" title="多GPU环境的配置"></a>多GPU环境的配置</h3><ul>
<li><p>Tensorflow</p>
</li>
<li><p>PyTorch</p>
</li>
</ul>
<h3 id="是不是可以分布式训练？"><a href="#是不是可以分布式训练？" class="headerlink" title="是不是可以分布式训练？"></a>是不是可以分布式训练？</h3><h3 id="可以在SPARK环境里训练或者部署模型吗？"><a href="#可以在SPARK环境里训练或者部署模型吗？" class="headerlink" title="可以在SPARK环境里训练或者部署模型吗？"></a>可以在SPARK环境里训练或者部署模型吗？</h3><h3 id="怎么进一步优化性能？"><a href="#怎么进一步优化性能？" class="headerlink" title="怎么进一步优化性能？"></a>怎么进一步优化性能？</h3><ul>
<li><p>TVM</p>
</li>
<li><p>nGraph</p>
</li>
</ul>
<h3 id="TPU和GPU的区别？"><a href="#TPU和GPU的区别？" class="headerlink" title="TPU和GPU的区别？"></a>TPU和GPU的区别？</h3><h3 id="未来量子计算对于深度学习等AI技术的影响？"><a href="#未来量子计算对于深度学习等AI技术的影响？" class="headerlink" title="未来量子计算对于深度学习等AI技术的影响？"></a>未来量子计算对于深度学习等AI技术的影响？</h3><hr>
<h2 id="GPU购买指南"><a href="#GPU购买指南" class="headerlink" title="GPU购买指南"></a>GPU购买指南</h2><p>深度学习训练通常需要大量的计算资源。GPU目前是深度学习最常使用的计算加速硬件。相对于CPU来说，GPU更便宜且计算更加密集。一方面，相同计算能力的GPU的价格一般是CPU价格的十分之一。另一方面，一台服务器通常可以搭载8块或者16块GPU。因此，GPU数量可以看作是衡量一台服务器的深度学习计算能力的一个标准。</p>
<h3 id="如何选择GPU"><a href="#如何选择GPU" class="headerlink" title="如何选择GPU"></a>如何选择GPU</h3><h3 id="GPU的主要性能指标"><a href="#GPU的主要性能指标" class="headerlink" title="GPU的主要性能指标"></a>GPU的主要性能指标</h3><p>在选择GPU时，首先要考虑的第一个GPU性能问题是什么呢：是否为cuda核心？时钟速度多大？内存大小多少？<br>这些都不是，对于深度学习性能而言，最重要的特征是内存带宽（memory bandwidth）。<br>简而言之：GPU针对内存带宽进行了优化，但同时牺牲了内存访问时间（延迟）。CPU的设计恰恰相反：如果涉及少量内存（例如几个数字相乘（3 <em> 6 </em> 9）），CPU可以快速计算，但是对于大量内存（如矩阵乘法（A <em> B </em> C）则很慢。由于内存带宽的限制，当涉及大量内存的问题时，GPU快速计算的优势往往会受到限制。当然，GPU和CPU之间还有更复杂的区别，关于为何GPU如此适用于处理深度学习问题，另做探讨。</p>
<p>所以如果你想购买一个快速的GPU，首先要关注的是GPU的带宽（bandwidth）。</p>
<h3 id="整机配置"><a href="#整机配置" class="headerlink" title="整机配置"></a>整机配置</h3><p>通常，我们主要用GPU做深度学习训练。因此，不需要购买高端的CPU。至于整机配置，尽量参考网上推荐的中高档的配置就好。不过，考虑到GPU的功耗、散热和体积，我们在整机配置上也需要考虑以下三个额外因素。</p>
<ol>
<li>机箱体积。GPU尺寸较大，通常考虑较大且自带风扇的机箱。</li>
<li>电源。购买GPU时需要查一下GPU的功耗，例如50W到300W不等。购买电源要确保功率足够，且不会过载机房的供电。</li>
<li>主板的PCIe卡槽。推荐使用PCIe 3.0 16x来保证充足的GPU到主内存的带宽。如果搭载多块GPU，要仔细阅读主板说明，以确保多块GPU一起使用时仍然是16x带宽。注意，有些主板搭载4块GPU时会降到8x甚至4x带宽。</li>
</ol>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul>
<li>在预算范围之内，尽可能买较新的GPU。</li>
<li>整机配置需要考虑到GPU的功耗、散热和体积。</li>
</ul>
<h2 id="框架选型"><a href="#框架选型" class="headerlink" title="框架选型"></a>框架选型</h2><p>目前常用的框架有tensorflow,keras,pytorch,mxnet等等，各个框架的优缺点在此简单介绍：</p>
<h3 id="常用框架简介"><a href="#常用框架简介" class="headerlink" title="常用框架简介"></a>常用框架简介</h3><ol>
<li><p>tensorflow：<br>tensorflow由于有google的强大背书，加上其优秀的分布式设计，丰富的教程资源和论坛，工业部署方便，基本很多人都是从tensorflow入门的<br>优点：google的强大背书，分布式训练，教程资源丰富，常见问题基本都可以在互联网中找到解决办法，工业部署方便<br>缺点: 接口混乱，官方文档不够简洁，清晰，</p>
</li>
<li><p>keras:<br>keras是一种高层编程接口，其可以选择不同的后端，比如tensorflow，therao等等<br>优点：接口简洁，上手快，文档好，资源多<br>缺点: 封装的太好了导致不理解其技术细节</p>
</li>
<li><p>pytorch:</p>
</li>
</ol>
<ol>
<li><p>caffe2:<br>caffe2是在caffe之后的第二代版本，同属于Facebook。。。<br>优点：支持模型的全平台部署，。。。。<br>缺点:使用人数相对较少，资源较少，和pytorch合并后应该会更受欢迎</p>
</li>
<li><p>mxnet<br>mxnet是dmlc社区推出的深度学习框架，在2017年被亚马逊指定为官方框架<br>优点：支持多种语言，代码设计优秀，省显存，华人团队开发，中文社区活跃，官方复现经典论文推出gluoncv和gluonNLP模块，非常方便，拿来就可以用。<br>缺点:现在mxnet官方社区主要在推gluon接口，接口稍有混乱，坑较多，入手门槛稍高</p>
</li>
<li><p>caffe：<br>目前很多做深度学习比较早的大厂基本都是在用caffe，因为在2013-2015年基本就是caffe的天下，并且caffe的代码设计很优秀，基本所有代码都被翻了很多遍了，被各种分析，大厂基本都是魔改caffe，基于caffe来进行二次开发，所在目前在很多大厂还是在使用caffe<br>优点：资源丰富，代码容易理解，部署方便<br>缺点：入门门槛高，文档较少</p>
</li>
</ol>
<p>框架选型总结:</p>
<ol>
<li>新手入门，首推pytorch，上手快，资源丰富,官方文档写的非常好(<a href="https://pytorch.org/tutorials/">https://pytorch.org/tutorials/</a>)</li>
<li>目前工业部署，tensorflow是首选,资源丰富，并且在分布式训练这一块基本一家独大</li>
<li>mxnet的gluon接口有比较丰富的中文资源（教程：zh.gluon.ai，论坛：discuss.gluon.ai）,gluoncv模块（<a href="https://gluon-cv.mxnet.io）,gluonNLP模块（https://gluon-nlp.mxnet.io）">https://gluon-cv.mxnet.io）,gluonNLP模块（https://gluon-nlp.mxnet.io）</a></li>
</ol>
<h2 id="模型部署"><a href="#模型部署" class="headerlink" title="模型部署"></a>模型部署</h2><p>我们一般都是通过python或者其他语言来编码训练模型，然后基于后端来进行部署<br>一般的框架都有自身的部署框架，比如tensorflow，pytorch，caffe2，mxnet等等<br>有一些框架是专门做推理部署使用的，比如<br> (1)tensorRT<br> (2)TVM<br> (3)ONNX</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>自然语言处理入门</title>
    <url>/zh-TW/ch16_%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><h2 id="NLP-发展史简述"><a href="#NLP-发展史简述" class="headerlink" title="NLP 发展史简述"></a>NLP 发展史简述</h2><pre><code>50多年来 NLP 的历史发展可以分为三个浪潮，前两波以理性主义和经验主义的形式出现，为当前的深度学习浪潮铺平了道路。NLP的深层学习革命的主要支柱是: （1）语言嵌入实体的分布式表征，（2）由于嵌入而产生的语义泛化， （3）自然语言的大跨度深序列建模，（4）能够从低到高表示语言层次的分层网络，以及（5）解决许多联合 NLP 问题的端对端深度学习方法。
</code></pre><h3 id="第一个浪潮：理性主义"><a href="#第一个浪潮：理性主义" class="headerlink" title="第一个浪潮：理性主义"></a>第一个浪潮：理性主义</h3><pre><code>在第一个浪潮中，NLP的实验持续了很长一段时间，可以追溯到20世纪50年代。1950年，阿兰·图灵提出了图灵测试，以评估计算机表现出与人类无法区分的智能行为的能力。这项测试是基于人类和计算机之间的自然语言对话，旨在生成类似人类的反应。1954年，George-IBM 实验产出了能够将60多个俄语句子翻译成英语的rrst机器翻译系统。

这些方法是基于这样一种信念，即人类思维中的语言知识是由泛型继承提前进行的，而这种信念，在大约1960年至1980年代后期，占据了NLP的大部分研究中的主导地位。这些方法被称为理性主义方法（Church 2007）。理性主义方法在 NLP 中的主导地位主要是由于诺姆·乔姆斯基（Noam Chomsky）关于先天语言结构的论点被广泛接受以及他对 N-grams 方法的批评（Chomsky 1957）。理性主义者一般假设语言的关键部分在出生时就被硬连接到大脑中，作为人类遗传遗传的一部分，因此他们试图设计手工制作的规则，将知识和推理机制纳入智能 NLP 系统。直到20世纪80年代，最著名的成功的NLP系统，如为模拟 Rogerian psychotherapist 的 ELIZA 系统和为了规则化真实世界信息为规则本体的 MARGIE 系统，都是基于复杂的手写规则。

这一时期恰逢以专家知识工程为特点的早期智能的早期发展，即领域专家根据其所掌握的（非常狭窄的）应用领域的知识设计计算机程序（Nilsson 1982; Winston 1993）。专家们使用符号逻辑规则设计了这些程序，这些规则基于对这些知识的仔细表征和工程。这些以知识为基础的智能系统往往通过检测"Head"或最重要的参数，并就每种特殊情况采取特定的解决办法，而这在解决狭义问题方面往往是有效的。这些“Head”参数由人类专家预先确定，使“tail”参数和案例不受影响。由于缺乏学习能力，他们有必要将解决方案推广到新的情况和领域。这一时期的典型方法是专家系统所提供的证据，这是一个模拟人类专家决策能力的计算机系统。这种系统旨在通过知识推理来解决复杂的问题（Nilsson 1982）。第一个专家系统建立于1970年代，然后在1980年代推广。使用的主要"算法"是以"if-then-else"为形式的推断规则（Jackson 1998）。这些智能系统的主要优点是其在进行逻辑推理方面（有限）能力的透明度和可解释性。像NLP系统，如 ELIZA 和 MARGIE ，一般专家系统在早期使用手工制作的专家知识，这往往是有效的狭隘的问题，虽然推理无法处理不确定性，是普遍存在的实际应用。

同样，语音识别研究和系统设计，这又是另一个长期存在的 NLP 和反智能挑战，在这个理性主义时代，主要基于专家知识工程的范式，如elegantly analyzed in（Church and Mercer 1993）。在1970年代和1980年代初，专家系统的语音识别方法相当流行（Reddy 1976; Zue 1985）。然而，研究人员敏锐地认识到，缺乏从数据中学习和处理推理不确定性的能力，导致了接下来描述的第二波语音识别、NLP和对于文本的人工智能浪潮也走向失败。
</code></pre><h3 id="第二波浪潮：经验主义"><a href="#第二波浪潮：经验主义" class="headerlink" title="第二波浪潮：经验主义"></a>第二波浪潮：经验主义</h3><pre><code>第二波 NLP 浪潮的特点是利用语料库数据以及基于（浅层）机器学习、统计学等来利用这些数据（Manning and Schtze 1999）。由于许多自然语言的结构和理论都被贬低或抛弃，而倾向于数据驱动的方法，这个时代发展的主要方法被称为经验或务实的方法（ChurchandMercer 1993;Church 2014）。NLP 的一个主要会议甚至被命名为“自然语言处理的经验方法（Empirical Methods in Natural Language Processing）（EMNLP）”，最直接地反映了NLP研究人员在那个时代对经验方法的强烈积极情绪。
与理性主义方法相反，经验方法认为人类的思维只是从关联、模式识别和泛化的常规操作开始。丰富的感官输入需要使大脑学习自然语言的详细结构。经验主义盛行于1920年至1960年间，自1990年以来一直在兴起。NLP的早期经验方法主要是开发生成模型，如隐马尔可夫模型 （HMM） （Baum and Petrie 1966）， IBM 翻译模型 （Brown et al. 1993）， 和 head-driven parsing 模型（Collins 1997），以发现大型语料库的规律性。自1990年代后期以来，在各种NLP任务中，歧视性模式已成为事实上的做法。NLP的典型判别模型和方法包括最大熵模型（ratnaparkhi 1997）、支持向量机（Vapnik 1998）、条件随机（Lafferty et al. 2001）、最大相互信息和最小区分器错误（He et al. 2008）还有感知器（Collins 2002）。
在这种经验主义时代中、NLP 与同样的智能方法如语音识别和计算机视觉是平行的。这是在明确的证据表明，学习和感知能力对复杂的智能系统至关重要，但在前一波流行的专家系统中却不存在。例如，当 DARPA 开始对自动驾驶提出重大挑战时，大多数车辆随后依赖于基于知识的智能智能。正如语音识别和NLP 一样，自主驾驶和计算机视觉研究人员意识到基于知识的范式的局限性，因为机器学习需要进行不确定性处理和泛化能力。
在第二波浪潮中，NLP的经验主义和语音识别是基于数据密集型机器学习的，我们现在称之为“shallow”，因为在下一节中描述的第三波浪潮中，数据的多层或“deep”表征通常缺乏抽象结构。在机器学习中，在第一次浪潮中，研究人员不需要考虑构造精确规则，为知识为基础的 NLP 和语音系统。相反，他们把重点放在统计模型（Bishop 2006; Murphy 2012）或作为一个基本引擎的简单的神经网络（Bishop 1995）。然后，他们使用足够的训练数据进行自动学习或“tune（调整）”系统的参数，使它们能够处理不确定性，并尝试从一个条件泛化到另一个条件，从一个领域泛化到另一个领域。机器学习的关键算法和方法包括EM （期望最大化）、贝叶斯网络、支持向量机、决策树以及神经网络的反向传播算法。
一般来说，基于机器学习的NLP、语音和其他智能系统的性能比早期的基于知识的智能系统要好得多。成功的例子包括语音识别 （Jelinek 1998）， 脸部识别 （Viola and Jones 2004）， 实体识别 （Fei-Fei and Perona 2005）， 手写字体识别 （Plamondon and Srihari 2000）， 以及机器翻译 （Och 2003）。
在语音识别方面，从20世纪80年代初到2010年前后近30年，利用基于 HMM 与高斯混合模型相结合的统计生成模型，以及其推广的各种版本（Baker et al. 2009a，b; Deng and O’Shaughnessy 2003; Rabiner and Juang 1993）的统计生成模式。泛化 HMM 的许多版本都是基于统计和神经网络的隐动态模型（Deng 1998; Bridle et al. 1998; Deng and Yu 2007）。前者采用 EM 和 switching extended Kalman ﬁlter 算法学习模型参数（Ma and Deng 2004; Lee et al. 2004），后者采用反向传播（Picone et al. 1999），两者都广泛地利用多个潜在层表示法进行语音分析的生成过程。将这种“深度”生成过程转化为端到端过程的对应方案，导致了深度学习的工业化成功（Deng et al. 2010， 2013; Hinton et al. 2012） ，从而形成了第三波浪潮的驱动力。
</code></pre><h3 id="第三波浪潮：深度学习"><a href="#第三波浪潮：深度学习" class="headerlink" title="第三波浪潮：深度学习"></a>第三波浪潮：深度学习</h3><pre><code>在第二波浪潮中开发的 NLP 系统，包括语音识别、语言理解和机器翻译，表现得比在第一波浪潮时更好，鲁棒性更高，但它们远远没有达到人的水平，而这留下了很多需求。除了少数例外，NLP的（浅层）机器学习模型通常没有足够的容量来吸收大量的训练数据。此外，学习算法、方法和基础设施也都不够强大。所有这一切都在几年前发生了变化，而这导致了第三波 NLP 浪潮，这股浪潮是由深层机器学习或深度学习的新范式推动的（Bengio 2009; Deng and Yu 2014; LeCun et al. 2015; Goodfellow et al. 2016）。
深度学习起源于人工神经网络，它可以被看作是受生物神经系统启发的细胞类型的级联模型。随着反向传播算法的出现（Rumelhart et al. 1986），90年代对深度神经网络的训练引起了广泛关注。在没有大量训练数据和没有适当的设计和学习范式的情况下，在神经网络训练过程中，学习信号随着层次数（或更严格的信用分配深度）在层层传播时呈指数形式消失，使得调整深层神经网络特别是递归的版本的连接权重变得异常艰难。Hinton 等人（2006）克服了这个问题，使用无人监督的预训练模型来进行学习有用的特征探测器。然后，通过监督学习进一步训练网络，对标记数据进行分类。因此，可以学习使用低维表征的方式来学习高维的表征的分布。这项开创性的工作标志着神经网络的复兴。此后提出和发展了各种网络结构，包括 Deep Belief 网络（Hinton et al.2006）、堆积自编码器（Vincent et al.2010）、深层玻尔兹曼机（Hinton and Salakhutdinov 2012）、深度卷积神经网络（Krizhevsky et al. 2012），深层堆积网络 （Deng et al. 2012），和深层 Q-networks （Mnih et al. 2015）。深度学习自2010年以来已成功地应用于实际智能领域的实际任务，包括语音识别（Yu et al. 2010; Hinton et al. 2012），图像识别（Krizhevsky et al. 2012; He et al. 2016），以及 NLP 绝大多数领域。 
其中由于微软公司在工业化上的成功，以及愈来愈高的准确率等迹象，这些2010-2011年语音识别的惊人成功预示着 NLP 的第三波浪潮和人工智能的到来。随着深度学习在语音识别方面取得成功，计算机视觉（Krizhevsky et al. 2012）和机器翻译（Bahdanau et al. 2015）被类似的深度学习范式所取代。特别是，虽然 Bengio 等人在2001的工作，在2011年就开发了强大的神经词嵌入技术（Bengio et al. 2001），但由于大数据的可用性和更快的计算，它直到10多年后才被证明在一个大规模和实际有用的规模上才能够实际有用（Mikolov et al. 2013）。此外，许多其他现实世界的NLP应用，如图像字幕（Karpathy and Fei-Fei 2015; Fang et al. 2015; Gan et al. 2017），视觉问题回答（Fei-Fei and Perona 2016），语音理解系统（Mesnil et al. 2013），网络搜索（Huang et al. 2013b）和推荐系统由于深度学习而取得成功，此外还有许多非NLP任务，包括药物发现和药理学、客户关系管理、推荐系统、手势识别、医学信息、广告投放、医学图像分析、机器人、自动驾驶车辆、纸板和电子游戏（例如 Atari， Go， Poker， and the latest， DOTA2）等。详情请参阅维基上的深度学习领域。
在更多基于文本的应用领域中，机器翻译可能受到深度学习的影响最大。从 NLP 第二波浪潮中发展起来的浅层——统计机器翻译开始看起的话，目前在实际应用中最好的机器翻译系统是基于深神经网络的。例如，谷歌在2016年9月宣布了其转向神经机器翻译的阶段，两个月后微软也发布了类似的声明。Facebook已经进行了大约一年的机器神经网络翻译的转换工作，到2017年8月它已经完全将这个系统部署成功。
在口语理解和对话系统领域，深度学习也正在产生巨大影响。目前流行的技术以多种方式维护和扩展了第二波时代浪潮中发展起来的统计方法。与经验（浅层）机器学习方法一样，深度学习也是基于数据密集型方法，以降低手工制作规则的成本，对噪声环境下的语音识别错误和语言理解错误具有很强的鲁棒性，并利用决策过程和强化学习的力量来设计对话策略，例如（Gasic et al. 2017; Dhingra et al. 2017）。与早期的方法相比，深度神经网络模型和表征方法更强大，它们使端到端学习成为可能。然而，深度学习也没有解决可解释性和领域泛化问题。
将深度学习应用于 NLP 问题方面的最近的两个重要技术突破是序列到序列学习（Sutskevar et al. 2014）和注意力机制建模（Bahdanau et al. 2015），以及最近的 BERT模型（Jacob el al.2018） 。序列到序列学习引入了一个强大的学习范式，即使用递归神经网络以端到端的方式进行编码和解码。注意力机制建模最初是为了克服编码一个长序列的难度而开发的，后来的持续发展又扩展了它的能力，提供了两个任意序列的高度可塑对齐能力，而其两个可以同时学习神经网络参数。而 BERT 则是实现了双向建模获取以得到更好的语言表征能力。序列到序列学习和注意力机制的关键概念在基于统计学习和词局部表征的最佳系统上提高了基于分布式单词嵌入的神经机器翻译的性能，而 BERT 更重要的意义是双向获取同一文段的高维意义。在这一成功之后，这些概念也被成功地应用到许多其他与NLP相关的任务中，如图像字幕（Karpathy and Fei-Fei 2015; Devlin et al. 2015）、语音识别（Chorowski et al. 2015）、一次性学习、句法分析、唇读、文本理解、摘要以及问答系统等。撇开他们巨大的经验成功不谈，基于神经网络的深度学习模型往往比早期浪潮中的传统机器学习模型更简单、更容易设计。在许多应用中，在端到端的任务中，模型的所有部分都同时进行深度学习，从特征抽取到预测。导致神经网络模型相对简单的另一个因素是，相同的模型构建成的块（即不同类型的层）通常在许多不同的应用中使用。为多种任务使用相同的构建块，这种方法使得模型更容易迁移到其它任务和数据上。此外，谷歌等公司还开发了软件工具包，以便更快、更有效地实现这些模型。由于以上这些原因，神经网络在数据量大而且基于云的方式上，是更常用的。
尽管深度学习在重塑语音、图像和视频的处理方面被证明是有效的，而且具有它的革命性，但在将深度学习与基于文本的 NLP 相结合方面的有效性并不那么明确，尽管它在一些实用的 NLP 任务中取得了经验上的成功。在语音、图像和视频处理中，深度学习通过直接从原始数据学习规律来解决语义差距问题。然而，在 NLP 中，人们提出了更强的理论和结构化模型，即语音、语法和语义，来提取理解和生成自然语言的基本机制，这些机制与神经网络不那么容易兼容。与语音、图像和视频信号相比，从文本数据中学习的神经表征可以对自然语言提供同样直接的见解，但是这个也不够直接。因此，将神经网络，特别是那些具有复杂层次结构的神经网络应用于 NLP，已成为 NLP 和深度学习社区中最活跃的领域，近年来取得了非常显著的进展（Deng 2016; Manning and Socher 2017;Jacob el al.2018）。
</code></pre>]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>NCNN部署</title>
    <url>/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/NCNN%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="NCNN部署"><a href="#NCNN部署" class="headerlink" title="NCNN部署"></a>NCNN部署</h1><h5 id="1-在电脑端使用ncnn实现分类-alexnet"><a href="#1-在电脑端使用ncnn实现分类-alexnet" class="headerlink" title="1.在电脑端使用ncnn实现分类(alexnet)"></a>1.在电脑端使用ncnn实现分类(alexnet)</h5><p>s1，安装g++，cmake，protobuf，opencv</p>
<p>s2，对源码进行编译</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">git clone https://github.com/Tencent/ncnn</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mkdir</span> -p build</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> build</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">cmake ..</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">make -j4</span></span><br></pre></td></tr></tbody></table></figure>
<p>s3，准备caffe模型文件(alexnet)</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">deploy.prototxt  </span><br><span class="line">snapshot_10000.caffemodel  </span><br></pre></td></tr></tbody></table></figure>
<p>alexnet   <a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet">deploy.prototxt</a>， <a href="http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel">caffemodel</a></p>
<p>s4，使用ncnn转换工具将旧caffe版本的prototxt和caffemodel转新版本</p>
<p>将旧caffe版本的prototxt和caffemodel存放在caffe/build/tools目录下，执行如下命令完成转换</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">./upgrade_net_proto_text [old prototxt] [new prototxt]</span><br><span class="line">./upgrade_net_proto_binary [old caffemodel] [new caffemodel]</span><br></pre></td></tr></tbody></table></figure>
<p>s5，将deploy.prototxt中输入层替换成input层(如果只读取一张图片将dim设置为1)</p>
<figure class="highlight protobuf"><table><tbody><tr><td class="code"><pre><span class="line">layer {</span><br><span class="line">  name: <span class="string">"data"</span></span><br><span class="line">  type: <span class="string">"Input"</span></span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  input_param { shape: { dim: <span class="number">1</span> dim: <span class="number">3</span> dim: <span class="number">227</span> dim: <span class="number">227</span> } }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>s6，使用caffe2ncnn工具将caffe model转成ncnn model</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">./caffe2ncnn deploy.prototxt bvlc_alexnet.caffemodel alexnet.param alexnet.bin</span><br></pre></td></tr></tbody></table></figure>
<p>在ncnn/build/tools目录下生成param和bin文件。</p>
<p>s7，对模型参数进行加密</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">./ncnn2mem alexnet.param alexnet.bin alexnet.id.h alexnet.mem.h</span><br></pre></td></tr></tbody></table></figure>
<p>在ncnn/build/tools目录下生成.param、.bin和.h文件。</p>
<p>alexnet.param    网络的模型参数</p>
<p>alexnet.bin   网络的权重</p>
<p>alexnet.id.h   在预测图片的时候使用到</p>
<p>s8，编写pc端代码(参考<a href="https://blog.csdn.net/qq_36982160/article/details/79929869">https://blog.csdn.net/qq_36982160/article/details/79929869</a>)</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">"gesture.id.h"</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"net.h"</span> </span></span><br><span class="line"><span class="comment">//使用ncnn，传入的参数第一个是你需要预测的数据，第二个参数是各个类别的得分vector，注意传入的是地址，这样才能在这个函数中改变其值 </span></span><br><span class="line"><span class="function"><span class="type">static</span> <span class="type">int</span> <span class="title">detect_squeezenet</span><span class="params">( <span class="type">float</span> *data, std::vector&lt;<span class="type">float</span>&gt;&amp; cls_scores)</span> </span>{ </span><br><span class="line">	<span class="comment">//实例化ncnn：Net，注意include "net.h"，不要在意这时候因为找不到net.h文件而include&lt;net.h&gt;报错，后文会介绍正确的打开方式 </span></span><br><span class="line">	ncnn::Net squeezenet; </span><br><span class="line">	</span><br><span class="line">	<span class="comment">//加载二进制文件，也是照写，后面会介绍对应文件应该放的正确位置 </span></span><br><span class="line">	<span class="type">int</span> a=squeezenet.<span class="built_in">load_param</span>(<span class="string">"demo.param"</span>); </span><br><span class="line">	<span class="type">int</span> b=squeezenet.<span class="built_in">load_param_bin</span>(<span class="string">"demo.bin"</span>); </span><br><span class="line"></span><br><span class="line">	<span class="comment">//实例化Mat，前三个参数是维度，第四个参数是传入的data，维度的设置根据你自己的数据进行设置，顺序是w、h、c </span></span><br><span class="line">	ncnn::Mat in = ncnn::<span class="built_in">Mat</span>(<span class="number">550</span>, <span class="number">8</span>, <span class="number">2</span>, data); </span><br><span class="line">	</span><br><span class="line">	<span class="comment">//实例化Extractor </span></span><br><span class="line">	ncnn::Extractor ex = squeezenet.<span class="built_in">create_extractor</span>(); </span><br><span class="line">	ex.<span class="built_in">set_light_mode</span>(<span class="literal">true</span>); </span><br><span class="line">	<span class="comment">//注意把"data"换成你deploy中的数据层名字 </span></span><br><span class="line">	<span class="type">int</span> d= ex.<span class="built_in">input</span>(<span class="string">"data"</span>, in); </span><br><span class="line">	</span><br><span class="line">	ncnn::Mat out; </span><br><span class="line">	<span class="comment">//这里是真正的终点，不多说了，只能仰天膜拜nihui大牛，重点是将prob换成你deploy中最后一层的名字 </span></span><br><span class="line">	<span class="type">int</span> c=ex.<span class="built_in">extract</span>(<span class="string">"prob"</span>, out); </span><br><span class="line">	</span><br><span class="line">	<span class="comment">//将out中的值转化为我们的cls_scores，这样就可以返回不同类别的得分了 </span></span><br><span class="line">	cls_scores.<span class="built_in">resize</span>(out.w); </span><br><span class="line">	<span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">0</span>; j&lt;out.w; j++) { </span><br><span class="line">		cls_scores[j] = out[j]; </span><br><span class="line">		} </span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> </span>{ </span><br><span class="line">	<span class="comment">//注意，这里的argv是之后从终端输入的参数，我这里是数据源的路径,因为我是从两个文件中生成一个总的数据，所以用了argv[1]和argv[2]，你也可以自己根据需求改变 </span></span><br><span class="line">	<span class="type">const</span> <span class="type">char</span>* imagepath1 = argv[<span class="number">1</span>]; </span><br><span class="line">	<span class="type">const</span> <span class="type">char</span>* imagepath2=argv[<span class="number">2</span>]; </span><br><span class="line">	</span><br><span class="line">	FILE *fopeni=<span class="literal">NULL</span>; </span><br><span class="line">	FILE *fopenq=<span class="literal">NULL</span>; </span><br><span class="line">	</span><br><span class="line">	fopeni=<span class="built_in">fopen</span>(imagepath1,<span class="string">"r"</span>); </span><br><span class="line">	fopenq=<span class="built_in">fopen</span>(imagepath2,<span class="string">"r"</span>); </span><br><span class="line">	</span><br><span class="line">	<span class="comment">//这是我的数据，i和q相当于图片的两个通道 </span></span><br><span class="line">	<span class="type">float</span> i[<span class="number">4400</span>]; </span><br><span class="line">	<span class="type">float</span> q[<span class="number">4400</span>]; </span><br><span class="line">	<span class="type">float</span> data[<span class="number">8800</span>]; </span><br><span class="line">	</span><br><span class="line">	<span class="type">int</span> count=<span class="number">4400</span>; </span><br><span class="line">	<span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; count; ++j) { </span><br><span class="line">		<span class="built_in">fscanf</span>(fopeni,<span class="string">"%f"</span>,&amp;i[j]); </span><br><span class="line">		<span class="built_in">fscanf</span>(fopenq,<span class="string">"%f"</span>,&amp;q[j]); </span><br><span class="line">	} </span><br><span class="line">	</span><br><span class="line">	<span class="comment">//这是将iq（相当于图片的两个通道的数据）转化为一个一维向量，需要特别注意的是数据维度的顺序 </span></span><br><span class="line">	<span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">8800</span>; ++j) { </span><br><span class="line">		<span class="keyword">if</span> (j&lt;<span class="number">4400</span>) { </span><br><span class="line">			data[j]=i[j]; </span><br><span class="line">		}</span><br><span class="line">		<span class="keyword">else</span>{ </span><br><span class="line">			data[j]=q[j<span class="number">-4400</span>]; </span><br><span class="line">		} </span><br><span class="line">	} </span><br><span class="line">	</span><br><span class="line">	<span class="type">char</span> a[<span class="number">13</span>]={<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'F'</span>,<span class="string">'G'</span>,<span class="string">'H'</span>,<span class="string">'I'</span>,<span class="string">'J'</span>,<span class="string">'K'</span>,<span class="string">'L'</span>,<span class="string">'M'</span>,<span class="string">'N'</span>,<span class="string">'O'</span>}; </span><br><span class="line">	</span><br><span class="line">	<span class="comment">//注意，这里是调用ncnn的代码 </span></span><br><span class="line">	std::vector&lt;<span class="type">float</span>&gt; cls_scores;  <span class="comment">//用来存储最终各类别的得分 </span></span><br><span class="line">	<span class="comment">//这个函数的实现在上面，快去看 </span></span><br><span class="line">	<span class="built_in">detect_squeezenet</span>(data, cls_scores); </span><br><span class="line">	<span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; cls_scores.<span class="built_in">size</span>(); ++i) { </span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"%c : %f\n"</span>, a[i],cls_scores[i]); </span><br><span class="line">	} </span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>代码是最简单的ncnn使用场景，可以根据自己需求加入代码。</p>
<p>s9，编译代码</p>
<p>(1) 编写CMakeLists.txt</p>
<p>在CMakeLists.txt增加如下两行代码</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line">add_executable(demo demo.cpp)</span><br><span class="line">target_link_libraries(demo ncnn)</span><br></pre></td></tr></tbody></table></figure>
<p>CMakeLists.txt如下</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">find_package(OpenCV QUIET COMPONENTS core highgui imgproc imgcodecs) </span><br><span class="line">if(NOT OpenCV_FOUND) </span><br><span class="line">	find_package(OpenCV REQUIRED COMPONENTS core highgui imgproc) </span><br><span class="line">endif() </span><br><span class="line"></span><br><span class="line">include_directories(${CMAKE_CURRENT_SOURCE_DIR}/../src) </span><br><span class="line">include_directories(${CMAKE_CURRENT_BINARY_DIR}/../src) </span><br><span class="line"></span><br><span class="line">add_executable(squeezenet squeezenet.cpp) </span><br><span class="line"></span><br><span class="line">target_link_libraries(squeezenet ncnn ${OpenCV_LIBS}) </span><br><span class="line"></span><br><span class="line">add_executable(fasterrcnn fasterrcnn.cpp) </span><br><span class="line">target_link_libraries(fasterrcnn ncnn ${OpenCV_LIBS}) </span><br><span class="line"></span><br><span class="line">add_executable(demo demo.cpp) </span><br><span class="line">target_link_libraries(demo ncnn) </span><br><span class="line"></span><br><span class="line">add_subdirectory(ssd)</span><br></pre></td></tr></tbody></table></figure>
<p>(2) 在ncnn根目录下CMakeLists.txt中编译examples语句的注释去掉</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">##############################################</span><br><span class="line"></span><br><span class="line"># add_subdirectory(examples)</span><br><span class="line"># add_subdirectory(benchmark)</span><br><span class="line">add_subdirectory(src)</span><br><span class="line">if(NOT ANDROID AND NOT IOS)</span><br><span class="line">add_subdirectory(tools)</span><br><span class="line">endif()</span><br></pre></td></tr></tbody></table></figure>
<p>(3)ncnn/build目录下进行编译，生成demo可执行文件</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">make</span><br></pre></td></tr></tbody></table></figure>
<p>s10，执行</p>
<p>将生成的.param和.bin文件复制到ncnn/build/examples目录下，然后终端cd到ncnn/build/examples，执行：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">./demo data_path1 data_path2</span><br></pre></td></tr></tbody></table></figure>
<h5 id="2-Win-x64-Visual-Studio-Community-2017"><a href="#2-Win-x64-Visual-Studio-Community-2017" class="headerlink" title="2. Win x64 (Visual Studio Community 2017)"></a>2. Win x64 (Visual Studio Community 2017)</h5><p>s1，安装Visual Studio Community 2017</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">download Visual Studio Community 2017 from https://visualstudio.microsoft.com/vs/community/</span><br><span class="line">install it</span><br><span class="line">Start → Programs → Visual Studio 2017 → Visual Studio Tools → x64 Native Tools Command Prompt for VS 2017</span><br></pre></td></tr></tbody></table></figure>
<p>s2，编译protobuf</p>
<figure class="highlight powershell"><table><tbody><tr><td class="code"><pre><span class="line">download protobuf<span class="literal">-3</span>.<span class="number">4.0</span> from https://github.com/google/protobuf/archive/v3.<span class="number">4.0</span>.zip</span><br><span class="line">&gt; <span class="built_in">cd</span> &lt;protobuf<span class="literal">-root-dir</span>&gt;</span><br><span class="line">&gt; mkdir <span class="built_in">build-vs2017</span></span><br><span class="line">&gt; <span class="built_in">cd</span> <span class="built_in">build-vs2017</span></span><br><span class="line">&gt; cmake <span class="literal">-G</span><span class="string">"NMake Makefiles"</span> <span class="literal">-DCMAKE_BUILD_TYPE</span>=Release <span class="literal">-DCMAKE_INSTALL_PREFIX</span>=%<span class="built_in">cd</span>%/install <span class="literal">-Dprotobuf_BUILD_TESTS</span>=OFF <span class="literal">-Dprotobuf_MSVC_STATIC_RUNTIME</span>=OFF ../cmake</span><br><span class="line">&gt; nmake</span><br><span class="line">&gt; nmake install</span><br></pre></td></tr></tbody></table></figure>
<p>s3，编译ncnn库</p>
<figure class="highlight powershell"><table><tbody><tr><td class="code"><pre><span class="line">&gt; <span class="built_in">cd</span> &lt;ncnn<span class="literal">-root-dir</span>&gt;</span><br><span class="line">&gt; mkdir <span class="literal">-p</span> <span class="built_in">build-vs2017</span></span><br><span class="line">&gt; <span class="built_in">cd</span> <span class="built_in">build-vs2017</span></span><br><span class="line">&gt; cmake <span class="literal">-G</span><span class="string">"NMake Makefiles"</span> <span class="literal">-DCMAKE_BUILD_TYPE</span>=Release <span class="literal">-DCMAKE_INSTALL_PREFIX</span>=%<span class="built_in">cd</span>%/install <span class="literal">-DProtobuf_INCLUDE_DIR</span>=&lt;protobuf<span class="literal">-root-dir</span>&gt;/<span class="built_in">build-vs2017</span>/install/include <span class="literal">-DProtobuf_LIBRARIES</span>=&lt;protobuf<span class="literal">-root-dir</span>&gt;/<span class="built_in">build-vs2017</span>/install/lib/libprotobuf.lib <span class="literal">-DProtobuf_PROTOC_EXECUTABLE</span>=&lt;protobuf<span class="literal">-root-dir</span>&gt;/<span class="built_in">build-vs2017</span>/install/bin/protoc.exe ..</span><br><span class="line">&gt; nmake</span><br><span class="line">&gt; nmake install</span><br><span class="line"></span><br><span class="line">pick <span class="built_in">build-vs2017</span>/install folder <span class="keyword">for</span> further usage</span><br></pre></td></tr></tbody></table></figure>
<h5 id="3-Android端使用ncnn"><a href="#3-Android端使用ncnn" class="headerlink" title="3. Android端使用ncnn"></a>3. Android端使用ncnn</h5><p>参考：</p>
<p><a href="https://blog.csdn.net/qq_33200967/article/details/82421089">https://blog.csdn.net/qq_33200967/article/details/82421089</a></p>
<p><a href="https://blog.csdn.net/qq_36982160/article/details/79931741">https://blog.csdn.net/qq_36982160/article/details/79931741</a></p>
<p>s1，使用Android Studio安装ndk</p>
<p>1）打开Android Studio，依次点击File-&gt;Settings-&gt;Appearance&amp;Behavior-&gt;System Settings-&gt;Android SDK-&gt;SDK Tool，选中NDK，点击apply自动下载安装(如果安装成功会在SDK目录下生成ndk-bundle文件夹)；</p>
<p>2）配置ndk的环境变量</p>
<ul>
<li><p>打开profile</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">sudo vim /etc/profile</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>在profile文件末尾添加ndk路径</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">export NDK_HOME=sdkroot/ndk-bundle</span><br><span class="line">PATH=$NDK_HOME:$PATH</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>保存退出，使用source命令使环境变量生效</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>验证ndk是否配置成功</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">ndk-build -v</span><br></pre></td></tr></tbody></table></figure>
</li>
</ul>
<p>s2，编译ncnn sdk</p>
<p>通过如下命令编译ncnn sdk，会在ncnn/build-android下生成install文件夹，其中包括：include(调用ncnn所需的头文件)和lib(编译得到的ncnn库libncnn.a)</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">mkdir build-android</span><br><span class="line">cd build-android</span><br><span class="line">cmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \</span><br><span class="line">    -DANDROID_ABI="armeabi-v7a" -DANDROID_ARM_NEON=ON \</span><br><span class="line">    -DANDROID_PLATFORM=android-14 ..</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">make package</span><br></pre></td></tr></tbody></table></figure>
<p>参数设置请参考：<a href="https://github.com/Tencent/ncnn/wiki/cmake-%E6%89%93%E5%8C%85-android-sdk">https://github.com/Tencent/ncnn/wiki/cmake-%E6%89%93%E5%8C%85-android-sdk</a></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">ANDROID_ABI 是架构名字，"armeabi-v7a" 支持绝大部分手机硬件</span><br><span class="line">ANDROID_ARM_NEON 是否使用 NEON 指令集，设为 ON 支持绝大部分手机硬件</span><br><span class="line">ANDROID_PLATFORM 指定最低系统版本，"android-14" 就是 android-4.0</span><br></pre></td></tr></tbody></table></figure>
<p>s3，对源码进行编译</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">git clone https://github.com/Tencent/ncnn</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mkdir</span> -p build</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> build</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">cmake ..</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">make -j4</span></span><br></pre></td></tr></tbody></table></figure>
<p>s4，准备caffe模型文件(alexnet)</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">deploy.prototxt  </span><br><span class="line">snapshot_10000.caffemodel  </span><br></pre></td></tr></tbody></table></figure>
<p>alexnet   <a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet">deploy.prototxt</a>， <a href="http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel">caffemodel</a></p>
<p>s5，使用ncnn转换工具将旧caffe版本的prototxt和caffemodel转新版本</p>
<p>将旧caffe版本的prototxt和caffemodel存放在caffe/build/tools目录下，执行如下命令完成转换</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">./upgrade_net_proto_text [old prototxt] [new prototxt]</span><br><span class="line">./upgrade_net_proto_binary [old caffemodel] [new caffemodel]</span><br></pre></td></tr></tbody></table></figure>
<p>s6，将deploy.prototxt中输入层替换成input层(如果只读取一张图片将dim设置为1)</p>
<figure class="highlight protobuf"><table><tbody><tr><td class="code"><pre><span class="line">layer {</span><br><span class="line">  name: <span class="string">"data"</span></span><br><span class="line">  type: <span class="string">"Input"</span></span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  input_param { shape: { dim: <span class="number">1</span> dim: <span class="number">3</span> dim: <span class="number">227</span> dim: <span class="number">227</span> } }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>s7，使用caffe2ncnn工具将caffe model转成ncnn model</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">./caffe2ncnn deploy.prototxt bvlc_alexnet.caffemodel alexnet.param alexnet.bin</span><br></pre></td></tr></tbody></table></figure>
<p>在ncnn/build/tools目录下生成param和bin文件。</p>
<p>s8，对模型参数进行加密</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">./ncnn2mem alexnet.param alexnet.bin alexnet.id.h alexnet.mem.h</span><br></pre></td></tr></tbody></table></figure>
<p>在ncnn/build/tools目录下生成.param、.bin和.h文件。</p>
<p>alexnet.param    网络的模型参数</p>
<p>alexnet.bin   网络的权重</p>
<p>alexnet.id.h   在预测图片的时候使用到</p>
<p>s9，开发安卓项目</p>
<p>(1)在Android Studio上创建一个NCNN1，并选择Include C++ support</p>
<p><img src="https://img-blog.csdn.net/20180905204155999?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjAwOTY3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" class="lazyload" data-srcset="https://img-blog.csdn.net/20180905204155999?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjAwOTY3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" srcset="data:image/png;base64,666" alt="这里写图片描述"></p>
<p>(2)在main目录下创建assets目录，并将alexnet.param、alexnet.bin、label.txt拷贝其中</p>
<p>(3)将include文件夹和 alexnet.id.h拷贝到cpp目录下</p>
<p>(4)在main目录下创建jniLibs/armeabi-v7a/目录，并将alexnet.id.h 拷贝其中</p>
<p>(5)在cpp文件夹下创建C++文件，用于加载模型和预测图片</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;android/bitmap.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;android/log.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;jni.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="comment">// ncnn</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"include/net.h"</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"alexnet.id.h"</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/time.h&gt;</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span> </span></span><br><span class="line"><span class="type">static</span> ncnn::UnlockedPoolAllocator g_blob_pool_allocator; </span><br><span class="line"><span class="type">static</span> ncnn::PoolAllocator g_workspace_pool_allocator; </span><br><span class="line"><span class="type">static</span> ncnn::Mat ncnn_param; </span><br><span class="line"><span class="type">static</span> ncnn::Mat ncnn_bin; </span><br><span class="line"><span class="type">static</span> ncnn::Net ncnn_net; </span><br><span class="line"><span class="keyword">extern</span> <span class="string">"C"</span> { </span><br><span class="line"><span class="comment">// public native boolean Init(byte[] param, byte[] bin, byte[] words); JNIEXPORT jboolean JNICALL </span></span><br><span class="line"><span class="built_in">Java_com_example_ncnn1_NcnnJni_Init</span>(JNIEnv *env, jobject thiz, jbyteArray param, jbyteArray bin) { </span><br><span class="line">	<span class="comment">// init param </span></span><br><span class="line">	{ </span><br><span class="line">		<span class="type">int</span> len = env-&gt;<span class="built_in">GetArrayLength</span>(param); </span><br><span class="line">		ncnn_param.<span class="built_in">create</span>(len, (<span class="type">size_t</span>) <span class="number">1u</span>); </span><br><span class="line">		env-&gt;<span class="built_in">GetByteArrayRegion</span>(param, <span class="number">0</span>, len, (jbyte *) ncnn_param); <span class="type">int</span> ret = ncnn_net.<span class="built_in">load_param</span>((<span class="type">const</span> <span class="type">unsigned</span> <span class="type">char</span> *) ncnn_param); </span><br><span class="line">		__android_log_print(ANDROID_LOG_DEBUG, <span class="string">"NcnnJni"</span>, <span class="string">"load_param %d %d"</span>, ret, len); </span><br><span class="line">	} </span><br><span class="line">	</span><br><span class="line">	<span class="comment">// init bin </span></span><br><span class="line">	{ </span><br><span class="line">		<span class="type">int</span> len = env-&gt;<span class="built_in">GetArrayLength</span>(bin); </span><br><span class="line">		ncnn_bin.<span class="built_in">create</span>(len, (<span class="type">size_t</span>) <span class="number">1u</span>); </span><br><span class="line">		env-&gt;<span class="built_in">GetByteArrayRegion</span>(bin, <span class="number">0</span>, len, (jbyte *) ncnn_bin); </span><br><span class="line">		<span class="type">int</span> ret = ncnn_net.<span class="built_in">load_model</span>((<span class="type">const</span> <span class="type">unsigned</span> <span class="type">char</span> *) ncnn_bin); </span><br><span class="line">		__android_log_print(ANDROID_LOG_DEBUG, <span class="string">"NcnnJni"</span>, <span class="string">"load_model %d %d"</span>, ret, len); </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line">	ncnn::Option opt; </span><br><span class="line">	opt.lightmode = <span class="literal">true</span>; </span><br><span class="line">	opt.num_threads = <span class="number">4</span>; </span><br><span class="line">	opt.blob_allocator = &amp;g_blob_pool_allocator; </span><br><span class="line">	opt.workspace_allocator = &amp;g_workspace_pool_allocator; </span><br><span class="line"></span><br><span class="line">	ncnn::<span class="built_in">set_default_option</span>(opt); </span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> JNI_TRUE; </span><br><span class="line">} </span><br><span class="line"></span><br><span class="line"><span class="comment">// public native String Detect(Bitmap bitmap); </span></span><br><span class="line"><span class="function">JNIEXPORT jfloatArray JNICALL <span class="title">Java_com_example_ncnn1_NcnnJni_Detect</span><span class="params">(JNIEnv* env, jobject thiz, jobject bitmap)</span> </span></span><br><span class="line"><span class="function"></span>{ </span><br><span class="line">	<span class="comment">// ncnn from bitmap </span></span><br><span class="line">	ncnn::Mat in; </span><br><span class="line">	{ </span><br><span class="line">		AndroidBitmapInfo info; </span><br><span class="line">		<span class="built_in">AndroidBitmap_getInfo</span>(env, bitmap, &amp;info); </span><br><span class="line">		<span class="type">int</span> width = info.width; <span class="type">int</span> height = info.height; </span><br><span class="line">		<span class="keyword">if</span> (info.format != ANDROID_BITMAP_FORMAT_RGBA_8888) </span><br><span class="line">			<span class="keyword">return</span> <span class="literal">NULL</span>; </span><br><span class="line"></span><br><span class="line">		<span class="type">void</span>* indata; </span><br><span class="line">		<span class="built_in">AndroidBitmap_lockPixels</span>(env, bitmap, &amp;indata); </span><br><span class="line">		<span class="comment">// 把像素转换成data，并指定通道顺序 </span></span><br><span class="line">		in = ncnn::Mat::<span class="built_in">from_pixels</span>((<span class="type">const</span> <span class="type">unsigned</span> <span class="type">char</span>*)indata, ncnn::Mat::PIXEL_RGBA2BGR, width, height); </span><br><span class="line">		<span class="built_in">AndroidBitmap_unlockPixels</span>(env, bitmap); </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line">	<span class="comment">// ncnn_net </span></span><br><span class="line">	std::vector&lt;<span class="type">float</span>&gt; cls_scores; </span><br><span class="line">	{ </span><br><span class="line">		<span class="comment">// 减去均值和乘上比例 </span></span><br><span class="line">		<span class="type">const</span> <span class="type">float</span> mean_vals[<span class="number">3</span>] = {<span class="number">103.94f</span>, <span class="number">116.78f</span>, <span class="number">123.68f</span>}; </span><br><span class="line">		<span class="type">const</span> <span class="type">float</span> scale[<span class="number">3</span>] = {<span class="number">0.017f</span>, <span class="number">0.017f</span>, <span class="number">0.017f</span>}; </span><br><span class="line"></span><br><span class="line">		in.<span class="built_in">substract_mean_normalize</span>(mean_vals, scale); </span><br><span class="line"></span><br><span class="line">		ncnn::Extractor ex = ncnn_net.<span class="built_in">create_extractor</span>(); </span><br><span class="line">		<span class="comment">// 如果时不加密是使用ex.input("data", in); </span></span><br><span class="line">		ex.<span class="built_in">input</span>(mobilenet_v2_param_id::BLOB_data, in); </span><br><span class="line"></span><br><span class="line">		ncnn::Mat out; </span><br><span class="line">		<span class="comment">// 如果时不加密是使用ex.extract("prob", out); </span></span><br><span class="line">		ex.<span class="built_in">extract</span>(mobilenet_v2_param_id::BLOB_prob, out); </span><br><span class="line"></span><br><span class="line">		<span class="type">int</span> output_size = out.w; </span><br><span class="line">		jfloat *output[output_size]; </span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; out.w; j++) { </span><br><span class="line">			output[j] = &amp;out[j]; </span><br><span class="line">		} </span><br><span class="line"></span><br><span class="line">		jfloatArray jOutputData = env-&gt;<span class="built_in">NewFloatArray</span>(output_size); </span><br><span class="line">		<span class="keyword">if</span> (jOutputData == <span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="literal">nullptr</span>; </span><br><span class="line"></span><br><span class="line">		env-&gt;<span class="built_in">SetFloatArrayRegion</span>(jOutputData, <span class="number">0</span>, output_size, <span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> jfloat *&gt;(*output)); <span class="comment">// copy </span></span><br><span class="line">		<span class="keyword">return</span> jOutputData; </span><br><span class="line">	} </span><br><span class="line">} </span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>(6)在项目包com.example.ncnn1下，修改MainActivity.java中的代码</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.ncnn1; </span><br><span class="line"><span class="keyword">import</span> android.Manifest; </span><br><span class="line"><span class="keyword">import</span> android.app.Activity; </span><br><span class="line"><span class="keyword">import</span> android.content.Intent; </span><br><span class="line"><span class="keyword">import</span> android.content.pm.PackageManager; </span><br><span class="line"><span class="keyword">import</span> android.content.res.AssetManager; </span><br><span class="line"><span class="keyword">import</span> android.graphics.Bitmap; </span><br><span class="line"><span class="keyword">import</span> android.graphics.BitmapFactory; </span><br><span class="line"><span class="keyword">import</span> android.net.Uri; </span><br><span class="line"><span class="keyword">import</span> android.os.Bundle; </span><br><span class="line"><span class="keyword">import</span> android.support.annotation.NonNull; </span><br><span class="line"><span class="keyword">import</span> android.support.annotation.Nullable; </span><br><span class="line"><span class="keyword">import</span> android.support.v4.app.ActivityCompat; </span><br><span class="line"><span class="keyword">import</span> android.support.v4.content.ContextCompat; </span><br><span class="line"><span class="keyword">import</span> android.text.method.ScrollingMovementMethod; </span><br><span class="line"><span class="keyword">import</span> android.util.Log; </span><br><span class="line"><span class="keyword">import</span> android.view.View; </span><br><span class="line"><span class="keyword">import</span> android.widget.Button; </span><br><span class="line"><span class="keyword">import</span> android.widget.ImageView; </span><br><span class="line"><span class="keyword">import</span> android.widget.TextView; </span><br><span class="line"><span class="keyword">import</span> android.widget.Toast; </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.bumptech.glide.Glide; </span><br><span class="line"><span class="keyword">import</span> com.bumptech.glide.load.engine.DiskCacheStrategy; </span><br><span class="line"><span class="keyword">import</span> com.bumptech.glide.request.RequestOptions; </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader; </span><br><span class="line"><span class="keyword">import</span> java.io.FileNotFoundException; </span><br><span class="line"><span class="keyword">import</span> java.io.IOException; </span><br><span class="line"><span class="keyword">import</span> java.io.InputStream; </span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader; </span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList; </span><br><span class="line"><span class="keyword">import</span> java.util.Arrays; </span><br><span class="line"><span class="keyword">import</span> java.util.List; </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MainActivity</span> <span class="keyword">extends</span> <span class="title class_">Activity</span> { </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">TAG</span> <span class="operator">=</span> MainActivity.class.getName(); </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">USE_PHOTO</span> <span class="operator">=</span> <span class="number">1001</span>; </span><br><span class="line">	<span class="keyword">private</span> String camera_image_path; </span><br><span class="line">	<span class="keyword">private</span> ImageView show_image; </span><br><span class="line">	<span class="keyword">private</span> TextView result_text; </span><br><span class="line">	<span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">load_result</span> <span class="operator">=</span> <span class="literal">false</span>; <span class="keyword">private</span> <span class="type">int</span>[] ddims = {<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>}; </span><br><span class="line">	<span class="keyword">private</span> <span class="type">int</span> <span class="variable">model_index</span> <span class="operator">=</span> <span class="number">1</span>; </span><br><span class="line">	<span class="keyword">private</span> List&lt;String&gt; resultLabel = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(); </span><br><span class="line">	<span class="keyword">private</span> <span class="type">NcnnJni</span> <span class="variable">squeezencnn</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NcnnJni</span>();</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span> </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCreate</span><span class="params">(Bundle savedInstanceState)</span> { </span><br><span class="line">		<span class="built_in">super</span>.onCreate(savedInstanceState); </span><br><span class="line">		setContentView(R.layout.activity_main); </span><br><span class="line"></span><br><span class="line">		<span class="keyword">try</span> { </span><br><span class="line">			initSqueezeNcnn(); </span><br><span class="line">		} <span class="keyword">catch</span> (IOException e) { </span><br><span class="line">			Log.e(<span class="string">"MainActivity"</span>, <span class="string">"initSqueezeNcnn error"</span>); </span><br><span class="line">		} </span><br><span class="line"></span><br><span class="line">		init_view(); </span><br><span class="line">		readCacheLabelFromLocalFile(); </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">initSqueezeNcnn</span><span class="params">()</span> <span class="keyword">throws</span> IOException { </span><br><span class="line">		<span class="type">byte</span>[] param = <span class="literal">null</span>; </span><br><span class="line">		<span class="type">byte</span>[] bin = <span class="literal">null</span>; </span><br><span class="line"></span><br><span class="line">		{ </span><br><span class="line">			<span class="type">InputStream</span> <span class="variable">assetsInputStream</span> <span class="operator">=</span> getAssets().open(<span class="string">"mobilenet_v2.param.bin"</span>); </span><br><span class="line">			<span class="type">int</span> <span class="variable">available</span> <span class="operator">=</span> assetsInputStream.available(); </span><br><span class="line">			param = <span class="keyword">new</span> <span class="title class_">byte</span>[available]; </span><br><span class="line">			<span class="type">int</span> <span class="variable">byteCode</span> <span class="operator">=</span> assetsInputStream.read(param); </span><br><span class="line">			assetsInputStream.close(); </span><br><span class="line">		}</span><br><span class="line"></span><br><span class="line">		{ </span><br><span class="line">			<span class="type">InputStream</span> <span class="variable">assetsInputStream</span> <span class="operator">=</span> getAssets().open(<span class="string">"mobilenet_v2.bin"</span>); </span><br><span class="line">			<span class="type">int</span> <span class="variable">available</span> <span class="operator">=</span> assetsInputStream.available(); </span><br><span class="line">			bin = <span class="keyword">new</span> <span class="title class_">byte</span>[available]; </span><br><span class="line">			<span class="type">int</span> <span class="variable">byteCode</span> <span class="operator">=</span> assetsInputStream.read(bin); </span><br><span class="line">			assetsInputStream.close(); </span><br><span class="line">		} </span><br><span class="line"></span><br><span class="line">		load_result = squeezencnn.Init(param, bin); </span><br><span class="line">		Log.d(<span class="string">"load model"</span>, <span class="string">"result:"</span> + load_result); </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line">	<span class="comment">// initialize view </span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">init_view</span><span class="params">()</span> { </span><br><span class="line">		request_permissions(); </span><br><span class="line">		show_image = (ImageView) findViewById(R.id.show_image); </span><br><span class="line">		result_text = (TextView) findViewById(R.id.result_text);</span><br><span class="line">		result_text.setMovementMethod(ScrollingMovementMethod.getInstance()); </span><br><span class="line">		<span class="type">Button</span> <span class="variable">use_photo</span> <span class="operator">=</span> (Button) findViewById(R.id.use_photo); </span><br><span class="line"></span><br><span class="line">		<span class="comment">// use photo click </span></span><br><span class="line">		use_photo.setOnClickListener(<span class="keyword">new</span> <span class="title class_">View</span>.OnClickListener() { </span><br><span class="line">			<span class="meta">@Override</span> </span><br><span class="line">			<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onClick</span><span class="params">(View view)</span> { </span><br><span class="line">				<span class="keyword">if</span> (!load_result) { </span><br><span class="line">					Toast.makeText(MainActivity.<span class="built_in">this</span>, <span class="string">"never load model"</span>, Toast.LENGTH_SHORT).show(); </span><br><span class="line">					<span class="keyword">return</span>; </span><br><span class="line">				} </span><br><span class="line">				PhotoUtil.use_photo(MainActivity.<span class="built_in">this</span>, USE_PHOTO); </span><br><span class="line">			} </span><br><span class="line">		}); </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line">	<span class="comment">// load label's name </span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">readCacheLabelFromLocalFile</span><span class="params">()</span> { </span><br><span class="line">		<span class="keyword">try</span> { </span><br><span class="line">			<span class="type">AssetManager</span> <span class="variable">assetManager</span> <span class="operator">=</span> getApplicationContext().getAssets(); </span><br><span class="line">			<span class="type">BufferedReader</span> <span class="variable">reader</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(assetManager.open(<span class="string">"synset.txt"</span>))); </span><br><span class="line">			<span class="type">String</span> <span class="variable">readLine</span> <span class="operator">=</span> <span class="literal">null</span>; </span><br><span class="line">			<span class="keyword">while</span> ((readLine = reader.readLine()) != <span class="literal">null</span>) { </span><br><span class="line">				resultLabel.add(readLine); </span><br><span class="line">			} </span><br><span class="line">			reader.close(); </span><br><span class="line">		} <span class="keyword">catch</span> (Exception e) { </span><br><span class="line">			Log.e(<span class="string">"labelCache"</span>, <span class="string">"error "</span> + e); </span><br><span class="line">		} </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span> </span><br><span class="line">	<span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">onActivityResult</span><span class="params">(<span class="type">int</span> requestCode, <span class="type">int</span> resultCode, <span class="meta">@Nullable</span> Intent data)</span> { </span><br><span class="line">		String image_path; </span><br><span class="line">		<span class="type">RequestOptions</span> <span class="variable">options</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RequestOptions</span>().skipMemoryCache(<span class="literal">true</span>).diskCacheStrategy(DiskCacheStrategy.NONE); </span><br><span class="line">		<span class="keyword">if</span> (resultCode == Activity.RESULT_OK) { </span><br><span class="line">			<span class="keyword">switch</span> (requestCode) { </span><br><span class="line">				<span class="keyword">case</span> USE_PHOTO: </span><br><span class="line">					<span class="keyword">if</span> (data == <span class="literal">null</span>) { </span><br><span class="line">						Log.w(TAG, <span class="string">"user photo data is null"</span>); </span><br><span class="line">						<span class="keyword">return</span>; </span><br><span class="line">					} </span><br><span class="line">					<span class="type">Uri</span> <span class="variable">image_uri</span> <span class="operator">=</span> data.getData(); </span><br><span class="line">					Glide.with(MainActivity.<span class="built_in">this</span>).load(image_uri).apply(options).into(show_image); </span><br><span class="line">					<span class="comment">// get image path from uri </span></span><br><span class="line">					image_path = PhotoUtil.get_path_from_URI(MainActivity.<span class="built_in">this</span>, image_uri); </span><br><span class="line">					<span class="comment">// predict image </span></span><br><span class="line">					predict_image(image_path);</span><br><span class="line">					<span class="keyword">break</span>; </span><br><span class="line">			} </span><br><span class="line">		} </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line">	<span class="comment">//  predict image </span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">predict_image</span><span class="params">(String image_path)</span> { </span><br><span class="line">		<span class="comment">// picture to float array </span></span><br><span class="line">		<span class="type">Bitmap</span> <span class="variable">bmp</span> <span class="operator">=</span> PhotoUtil.getScaleBitmap(image_path); </span><br><span class="line">		<span class="type">Bitmap</span> <span class="variable">rgba</span> <span class="operator">=</span> bmp.copy(Bitmap.Config.ARGB_8888, <span class="literal">true</span>); </span><br><span class="line"></span><br><span class="line">		<span class="comment">// resize to 227x227 </span></span><br><span class="line">		<span class="type">Bitmap</span> <span class="variable">input_bmp</span> <span class="operator">=</span> Bitmap.createScaledBitmap(rgba, ddims[<span class="number">2</span>], ddims[<span class="number">3</span>], <span class="literal">false</span>); </span><br><span class="line">		<span class="keyword">try</span> { </span><br><span class="line">			<span class="comment">// Data format conversion takes too long </span></span><br><span class="line">			<span class="comment">// Log.d("inputData", Arrays.toString(inputData)); </span></span><br><span class="line">			<span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> System.currentTimeMillis(); </span><br><span class="line">			<span class="comment">// get predict result </span></span><br><span class="line">			<span class="type">float</span>[] result = squeezencnn.Detect(input_bmp); </span><br><span class="line">			<span class="type">long</span> <span class="variable">end</span> <span class="operator">=</span> System.currentTimeMillis(); </span><br><span class="line">			Log.d(TAG, <span class="string">"origin predict result:"</span> + Arrays.toString(result)); </span><br><span class="line">			<span class="type">long</span> <span class="variable">time</span> <span class="operator">=</span> end - start; Log.d(<span class="string">"result length"</span>, String.valueOf(result.length)); </span><br><span class="line">			<span class="comment">// show predict result and time </span></span><br><span class="line">			<span class="type">int</span> <span class="variable">r</span> <span class="operator">=</span> get_max_result(result); </span><br><span class="line">			<span class="type">String</span> <span class="variable">show_text</span> <span class="operator">=</span> <span class="string">"result："</span> + r + <span class="string">"\nname："</span> + resultLabel.get(r) + <span class="string">"\nprobability："</span> + result[r] + <span class="string">"\ntime："</span> + time + <span class="string">"ms"</span>; </span><br><span class="line">			result_text.setText(show_text); </span><br><span class="line">		} <span class="keyword">catch</span> (Exception e) { </span><br><span class="line">			e.printStackTrace(); </span><br><span class="line">		} </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line">	<span class="comment">// get max probability label </span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">int</span> <span class="title function_">get_max_result</span><span class="params">(<span class="type">float</span>[] result)</span> { </span><br><span class="line">		<span class="type">float</span> <span class="variable">probability</span> <span class="operator">=</span> result[<span class="number">0</span>]; </span><br><span class="line">		<span class="type">int</span> <span class="variable">r</span> <span class="operator">=</span> <span class="number">0</span>; </span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; result.length; i++) { </span><br><span class="line">			<span class="keyword">if</span> (probability &lt; result[i]) { </span><br><span class="line">				probability = result[i]; </span><br><span class="line">				r = i; </span><br><span class="line">			}</span><br><span class="line">		} </span><br><span class="line">		<span class="keyword">return</span> r; </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line">	<span class="comment">// request permissions </span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">request_permissions</span><span class="params">()</span> { </span><br><span class="line">		List&lt;String&gt; permissionList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(); </span><br><span class="line">		<span class="keyword">if</span> (ContextCompat.checkSelfPermission(<span class="built_in">this</span>, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) { </span><br><span class="line">			permissionList.add(Manifest.permission.CAMERA); </span><br><span class="line">		} </span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (ContextCompat.checkSelfPermission(<span class="built_in">this</span>, Manifest.permission.WRITE_EXTERNAL_STORAGE) != PackageManager.PERMISSION_GRANTED) { </span><br><span class="line">			permissionList.add(Manifest.permission.WRITE_EXTERNAL_STORAGE); </span><br><span class="line">		} </span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (ContextCompat.checkSelfPermission(<span class="built_in">this</span>, Manifest.permission.READ_EXTERNAL_STORAGE) != PackageManager.PERMISSION_GRANTED) { </span><br><span class="line">			permissionList.add(Manifest.permission.READ_EXTERNAL_STORAGE); </span><br><span class="line">		} </span><br><span class="line"></span><br><span class="line">		<span class="comment">// if list is not empty will request permissions </span></span><br><span class="line">		<span class="keyword">if</span> (!permissionList.isEmpty()) { </span><br><span class="line">			ActivityCompat.requestPermissions(<span class="built_in">this</span>, permissionList.toArray(<span class="keyword">new</span> <span class="title class_">String</span>[permissionList.size()]), <span class="number">1</span>); </span><br><span class="line">		} </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span> </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onRequestPermissionsResult</span><span class="params">(<span class="type">int</span> requestCode, <span class="meta">@NonNull</span> String[] permissions, <span class="meta">@NonNull</span> <span class="type">int</span>[] grantResults)</span> { </span><br><span class="line">		<span class="built_in">super</span>.onRequestPermissionsResult(requestCode, permissions, grantResults); </span><br><span class="line">		<span class="keyword">switch</span> (requestCode) { </span><br><span class="line">			<span class="keyword">case</span> <span class="number">1</span>: </span><br><span class="line">				<span class="keyword">if</span> (grantResults.length &gt; <span class="number">0</span>) { </span><br><span class="line">					<span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; grantResults.length; i++) { </span><br><span class="line">						<span class="type">int</span> <span class="variable">grantResult</span> <span class="operator">=</span> grantResults[i]; </span><br><span class="line">						<span class="keyword">if</span> (grantResult == PackageManager.PERMISSION_DENIED) { </span><br><span class="line">							<span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> permissions[i]; </span><br><span class="line">							Toast.makeText(<span class="built_in">this</span>, s + <span class="string">" permission was denied"</span>, Toast.LENGTH_SHORT).show(); </span><br><span class="line">						} </span><br><span class="line">					} </span><br><span class="line">				} </span><br><span class="line">				<span class="keyword">break</span>; </span><br><span class="line">		} </span><br><span class="line">	} </span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>(7)在项目的包com.example.ncnn1下，创建一个NcnnJni.java类，用于提供JNI接口，代码如下：</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.ncnn1; </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> android.graphics.Bitmap; </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">NcnnJni</span> { </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">native</span> <span class="type">boolean</span> <span class="title function_">Init</span><span class="params">(<span class="type">byte</span>[] param, <span class="type">byte</span>[] bin)</span>; </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">native</span> <span class="type">float</span>[] Detect(Bitmap bitmap); </span><br><span class="line">	</span><br><span class="line">	<span class="keyword">static</span> { </span><br><span class="line">		System.loadLibrary(<span class="string">"ncnn_jni"</span>); </span><br><span class="line">	} </span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>(8)在项目的包com.example.ncnn1下，创建一个PhotoUtil.java类，这个是图片的工具类，代码如下：</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.ncnn1; </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> android.app.Activity; </span><br><span class="line"><span class="keyword">import</span> android.content.Context; </span><br><span class="line"><span class="keyword">import</span> android.content.Intent; </span><br><span class="line"><span class="keyword">import</span> android.database.Cursor; </span><br><span class="line"><span class="keyword">import</span> android.graphics.Bitmap; </span><br><span class="line"><span class="keyword">import</span> android.graphics.BitmapFactory; </span><br><span class="line"><span class="keyword">import</span> android.net.Uri; </span><br><span class="line"><span class="keyword">import</span> android.provider.MediaStore; </span><br><span class="line"><span class="keyword">import</span> java.nio.FloatBuffer; </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">PhotoUtil</span> { </span><br><span class="line">	<span class="comment">// get picture in photo </span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">use_photo</span><span class="params">(Activity activity, <span class="type">int</span> requestCode)</span> {</span><br><span class="line">		<span class="type">Intent</span> <span class="variable">intent</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Intent</span>(Intent.ACTION_PICK); </span><br><span class="line">		intent.setType(<span class="string">"image/*"</span>); </span><br><span class="line">		activity.startActivityForResult(intent, requestCode); </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line">	<span class="comment">// get photo from Uri </span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> String <span class="title function_">get_path_from_URI</span><span class="params">(Context context, Uri uri)</span> { </span><br><span class="line">		String result; </span><br><span class="line">		<span class="type">Cursor</span> <span class="variable">cursor</span> <span class="operator">=</span> context.getContentResolver().query(uri, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>); </span><br><span class="line">		<span class="keyword">if</span> (cursor == <span class="literal">null</span>) { </span><br><span class="line">			result = uri.getPath(); </span><br><span class="line">		} <span class="keyword">else</span> { </span><br><span class="line">			cursor.moveToFirst(); </span><br><span class="line">			<span class="type">int</span> <span class="variable">idx</span> <span class="operator">=</span> cursor.getColumnIndex(MediaStore.Images.ImageColumns.DATA); </span><br><span class="line">			result = cursor.getString(idx); </span><br><span class="line">			cursor.close(); </span><br><span class="line">		} </span><br><span class="line">		<span class="keyword">return</span> result; </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line">	<span class="comment">// compress picture </span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> Bitmap <span class="title function_">getScaleBitmap</span><span class="params">(String filePath)</span> { </span><br><span class="line">		BitmapFactory.<span class="type">Options</span> <span class="variable">opt</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BitmapFactory</span>.Options(); </span><br><span class="line">		opt.inJustDecodeBounds = <span class="literal">true</span>; </span><br><span class="line">		BitmapFactory.decodeFile(filePath, opt); </span><br><span class="line"></span><br><span class="line">		<span class="type">int</span> <span class="variable">bmpWidth</span> <span class="operator">=</span> opt.outWidth; </span><br><span class="line">		<span class="type">int</span> <span class="variable">bmpHeight</span> <span class="operator">=</span> opt.outHeight; </span><br><span class="line">		</span><br><span class="line">		<span class="type">int</span> <span class="variable">maxSize</span> <span class="operator">=</span> <span class="number">500</span>; </span><br><span class="line">		</span><br><span class="line">		<span class="comment">// compress picture with inSampleSize </span></span><br><span class="line">		opt.inSampleSize = <span class="number">1</span>; </span><br><span class="line">		<span class="keyword">while</span> (<span class="literal">true</span>) { </span><br><span class="line">			<span class="keyword">if</span> (bmpWidth / opt.inSampleSize &lt; maxSize || bmpHeight / opt.inSampleSize &lt; maxSize) { </span><br><span class="line">				<span class="keyword">break</span>; </span><br><span class="line">			} </span><br><span class="line">			opt.inSampleSize *= <span class="number">2</span>; </span><br><span class="line">		} </span><br><span class="line">		opt.inJustDecodeBounds = <span class="literal">false</span>; </span><br><span class="line">		<span class="keyword">return</span> BitmapFactory.decodeFile(filePath, opt); </span><br><span class="line">	} </span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>(9)修改启动页面的布局，修改如下：</p>
<figure class="highlight xml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"utf-8"</span>?&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">RelativeLayout</span> <span class="attr">xmlns:android</span>=<span class="string">"http://schemas.android.com/apk/res/android"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">xmlns:app</span>=<span class="string">"http://schemas.android.com/apk/res-auto"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">xmlns:tools</span>=<span class="string">"http://schemas.android.com/tools"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">android:layout_width</span>=<span class="string">"match_parent"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">android:layout_height</span>=<span class="string">"match_parent"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">tools:context</span>=<span class="string">".MainActivity"</span>&gt;</span> </span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">LinearLayout</span></span></span><br><span class="line"><span class="tag">        <span class="attr">android:id</span>=<span class="string">"@+id/btn_ll"</span></span></span><br><span class="line"><span class="tag">        <span class="attr">android:layout_alignParentBottom</span>=<span class="string">"true"</span></span></span><br><span class="line"><span class="tag">        <span class="attr">android:layout_width</span>=<span class="string">"match_parent"</span></span></span><br><span class="line"><span class="tag">        <span class="attr">android:layout_height</span>=<span class="string">"wrap_content"</span></span></span><br><span class="line"><span class="tag">        <span class="attr">android:orientation</span>=<span class="string">"horizontal"</span>&gt;</span> </span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">Button</span></span></span><br><span class="line"><span class="tag">            <span class="attr">android:id</span>=<span class="string">"@+id/use_photo"</span></span></span><br><span class="line"><span class="tag">            <span class="attr">android:layout_weight</span>=<span class="string">"1"</span></span></span><br><span class="line"><span class="tag">            <span class="attr">android:layout_width</span>=<span class="string">"0dp"</span></span></span><br><span class="line"><span class="tag">            <span class="attr">android:layout_height</span>=<span class="string">"wrap_content"</span></span></span><br><span class="line"><span class="tag">            <span class="attr">android:text</span>=<span class="string">"相册"</span> /&gt;</span> </span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">LinearLayout</span>&gt;</span> </span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">TextView</span></span></span><br><span class="line"><span class="tag">    	<span class="attr">android:layout_above</span>=<span class="string">"@id/btn_ll"</span></span></span><br><span class="line"><span class="tag">    	<span class="attr">android:id</span>=<span class="string">"@+id/result_text"</span></span></span><br><span class="line"><span class="tag">    	<span class="attr">android:textSize</span>=<span class="string">"16sp"</span></span></span><br><span class="line"><span class="tag">    	<span class="attr">android:layout_width</span>=<span class="string">"match_parent"</span></span></span><br><span class="line"><span class="tag">    	<span class="attr">android:hint</span>=<span class="string">"预测结果会在这里显示"</span></span></span><br><span class="line"><span class="tag">    	<span class="attr">android:layout_height</span>=<span class="string">"100dp"</span> /&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">ImageView</span></span></span><br><span class="line"><span class="tag">    	<span class="attr">android:layout_alignParentTop</span>=<span class="string">"true"</span></span></span><br><span class="line"><span class="tag">    	<span class="attr">android:layout_above</span>=<span class="string">"@id/result_text"</span></span></span><br><span class="line"><span class="tag">    	<span class="attr">android:id</span>=<span class="string">"@+id/show_image"</span></span></span><br><span class="line"><span class="tag">    	<span class="attr">android:layout_width</span>=<span class="string">"match_parent"</span></span></span><br><span class="line"><span class="tag">    	<span class="attr">android:layout_height</span>=<span class="string">"match_parent"</span> /&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">RelativeLayout</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<p>(10)修改APP目录下的CMakeLists.txt文件，修改如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"># For more information about using CMake with Android Studio, read the </span><br><span class="line"># documentation: https://d.android.com/studio/projects/add-native-code.html </span><br><span class="line"></span><br><span class="line"># Sets the minimum version of CMake required to build the native library. </span><br><span class="line"></span><br><span class="line">cmake_minimum_required(VERSION 3.4.1) </span><br><span class="line"></span><br><span class="line"># Creates and names a library, sets it as either STATIC </span><br><span class="line"># or SHARED, and provides the relative paths to its source code. </span><br><span class="line"># You can define multiple libraries, and CMake builds them for you. </span><br><span class="line"># Gradle automatically packages shared libraries with your APK. </span><br><span class="line">set(ncnn_lib ${CMAKE_SOURCE_DIR}/src/main/jniLibs/armeabi-v7a/libncnn.a) </span><br><span class="line">add_library (ncnn_lib STATIC IMPORTED) </span><br><span class="line">set_target_properties(ncnn_lib PROPERTIES IMPORTED_LOCATION ${ncnn_lib}) </span><br><span class="line"></span><br><span class="line">add_library( # Sets the name of the library. </span><br><span class="line">			ncnn_jni </span><br><span class="line">			# Sets the library as a shared library. </span><br><span class="line">			SHARED </span><br><span class="line">			</span><br><span class="line">			# Provides a relative path to your source file(s). </span><br><span class="line">			src/main/cpp/ncnn_jni.cpp ) </span><br><span class="line"></span><br><span class="line"># Searches for a specified prebuilt library and stores the path as a </span><br><span class="line"># variable. Because CMake includes system libraries in the search path by </span><br><span class="line"># default, you only need to specify the name of the public NDK library </span><br><span class="line"># you want to add. CMake verifies that the library exists before </span><br><span class="line"># completing its build. </span><br><span class="line"></span><br><span class="line">find_library( # Sets the name of the path variable. </span><br><span class="line">				log-lib </span><br><span class="line">				</span><br><span class="line">				# Specifies the name of the NDK library that </span><br><span class="line">				# you want CMake to locate. </span><br><span class="line">				log ) </span><br><span class="line"></span><br><span class="line"># Specifies libraries CMake should link to your target library. You </span><br><span class="line"># can link multiple libraries, such as libraries you define in this </span><br><span class="line"># build script, prebuilt third-party libraries, or system libraries. </span><br><span class="line"></span><br><span class="line">target_link_libraries( # Specifies the target library. </span><br><span class="line">						ncnn_jni </span><br><span class="line">						ncnn_lib </span><br><span class="line">						jnigraphics </span><br><span class="line"></span><br><span class="line">						# Links the target library to the log library </span><br><span class="line">						# included in the NDK. </span><br><span class="line">						${log-lib} )</span><br></pre></td></tr></tbody></table></figure>
<p>(11)修改APP目录下的build.gradle文件，修改如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">apply plugin: 'com.android.application' </span><br><span class="line"></span><br><span class="line">android { </span><br><span class="line">	compileSdkVersion 28 </span><br><span class="line">	defaultConfig { </span><br><span class="line">		applicationId "com.example.ncnn1" </span><br><span class="line">		minSdkVersion 21 </span><br><span class="line">		targetSdkVersion 28 </span><br><span class="line">		versionCode 1 </span><br><span class="line">		versionName "1.0" </span><br><span class="line">		testInstrumentationRunner "android.support.test.runner.AndroidJUnitRunner" </span><br><span class="line">		externalNativeBuild { </span><br><span class="line">			cmake { </span><br><span class="line">				cppFlags "-std=c++11 -fopenmp" </span><br><span class="line">				abiFilters "armeabi-v7a" </span><br><span class="line">			}</span><br><span class="line">		}</span><br><span class="line">	} </span><br><span class="line">	buildTypes { </span><br><span class="line">		release { </span><br><span class="line">			minifyEnabled false </span><br><span class="line">			proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro' </span><br><span class="line">		}</span><br><span class="line">	} </span><br><span class="line">	externalNativeBuild { </span><br><span class="line">		cmake { </span><br><span class="line">			path "CMakeLists.txt" </span><br><span class="line">		} </span><br><span class="line">	} </span><br><span class="line"></span><br><span class="line">	sourceSets { </span><br><span class="line">		main { </span><br><span class="line">			jniLibs.srcDirs = ["src/main/jniLibs"] </span><br><span class="line">			jni.srcDirs = ['src/cpp'] </span><br><span class="line">		}</span><br><span class="line">	} </span><br><span class="line">} </span><br><span class="line"></span><br><span class="line">dependencies { </span><br><span class="line">	implementation fileTree(dir: 'libs', include: ['*.jar']) </span><br><span class="line">	implementation 'com.android.support:appcompat-v7:28.0.0-rc02' </span><br><span class="line">	implementation 'com.android.support.constraint:constraint-layout:1.1.3' </span><br><span class="line">	testImplementation 'junit:junit:4.12' </span><br><span class="line">	implementation 'com.github.bumptech.glide:glide:4.3.1' </span><br><span class="line">	androidTestImplementation 'com.android.support.test:runner:1.0.2' </span><br><span class="line">	androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.0.2' </span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>(12)最后在配置文件中添加权限</p>
<figure class="highlight xml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">uses-permission</span> <span class="attr">android:name</span>=<span class="string">"android.permission.READ_EXTERNAL_STORAGE"</span>/&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">uses-permission</span> <span class="attr">android:name</span>=<span class="string">"android.permission.WRITE_EXTERNAL_STORAGE"</span>/&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<p>(13)编译完成</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>后端架构选型及应用场景</title>
    <url>/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="后端架构选型及应用场景"><a href="#后端架构选型及应用场景" class="headerlink" title="后端架构选型及应用场景"></a>后端架构选型及应用场景</h1><h2 id="为什么需要分布式计算？"><a href="#为什么需要分布式计算？" class="headerlink" title="为什么需要分布式计算？"></a>为什么需要分布式计算？</h2><p>  在这个数据爆炸的时代，产生的数据量不断地在攀升，从GB,TB,PB,ZB.挖掘其中数据的价值也是企业在不断地追求的终极目标。但是要想对海量的数据进行挖掘，首先要考虑的就是海量数据的存储问题，比如Tb量级的数据。</p>
<p>  谈到数据的存储，则不得不说的是磁盘的数据读写速度问题。早在上个世纪90年代初期，普通硬盘的可以存储的容量大概是1G左右，硬盘的读取速度大概为4.4MB/s.读取一张硬盘大概需要5分钟时间，但是如今硬盘的容量都在1TB左右了,相比扩展了近千倍。但是硬盘的读取速度大概是100MB/s。读完一个硬盘所需要的时间大概是2.5个小时。所以如果是基于TB级别的数据进行分析的话，光硬盘读取完数据都要好几天了，更谈不上计算分析了。那么该如何处理大数据的存储，计算分析呢？</p>
<p>  一个很简单的减少数据读写时间的方法就是同时从多个硬盘上读写数据，比如，如果我们有100个硬盘，每个硬盘存储1%的数据 ，并行读取，那么不到两分钟就可以完成之前需要2.5小时的数据读写任务了。这就是大数据中的分布式存储的模型。当然实现分布式存储还需要解决很多问题，比如硬件故障的问题，使用多台主机进行分布式存储时，若主机故障，会出现数据丢失的问题，所以有了副本机制：系统中保存数据的副本。一旦有系统发生故障，就可以使用另外的副本进行替换（著名的RAID冗余磁盘阵列就是按这个原理实现的）。其次比如一个很大的文件如何进行拆分存储，读取拆分以后的文件如何进行校验都是要考虑的问题。比如我们使用Hadoop中的HDFS也面临这个问题，只是框架给我们实现了这些问题的解决办法，开发中开发者不用考虑这些问题，底层框架已经实现了封装。</p>
<p>  同样假如有一个10TB的文件，我们要统计其中某个关键字的出现次数，传统的做法是遍历整个文件，然后统计出关键字的出现次数，这样效率会特别特别低。基于分布式存储以后，数据被分布式存储在不同的服务器上，那么我们就可以使用分布式计算框架（比如MapReduce,Spark等）来进行并行计算（或者说是分布式计算），即：每个服务器上分别统计自己存储的数据中关键字出现的次数，最后进行一次汇总，那么假如数据分布在100台服务器上，即同时100台服务器同时进行关键字统计工作，效率一下子可以提高几十倍。</p>
<h2 id="目前有哪些深度学习分布式计算框架？"><a href="#目前有哪些深度学习分布式计算框架？" class="headerlink" title="目前有哪些深度学习分布式计算框架？"></a>目前有哪些深度学习分布式计算框架？</h2><h3 id="PaddlePaddle"><a href="#PaddlePaddle" class="headerlink" title="PaddlePaddle"></a>PaddlePaddle</h3><p>  PaddlePaddle【1】是百度开源的一个深度学习平台。PaddlePaddle为深度学习研究人员提供了丰富的API，可以轻松地完成神经网络配置，模型训练等任务。<br>官方文档中简易介绍了如何使用框架在</p>
<ul>
<li>线性回归</li>
<li>识别数字</li>
<li>图像分类</li>
<li>词向量</li>
<li>个性化推荐</li>
<li>情感分析</li>
<li>语义角色标注</li>
<li>机器翻译</li>
</ul>
<p>等方面的应用</p>
<p>  Github地址：<a href="https://github.com/PaddlePaddle/Paddle">https://github.com/PaddlePaddle/Paddle</a></p>
<h3 id="Deeplearning4j"><a href="#Deeplearning4j" class="headerlink" title="Deeplearning4j"></a>Deeplearning4j</h3><p>  DeepLearning4J（DL4J）【2】是一套基于Java语言的神经网络工具包，可以构建、定型和部署神经网络。DL4J与Hadoop和Spark集成，支持分布式CPU和GPU。</p>
<p>  Deeplearning4j包括了分布式、多线程的深度学习框架，以及普通的单线程深度学习框架。定型过程以集群进行，也就是说，Deeplearning4j可以快速处理大量数据。Deeplearning4j在开放堆栈中作为模块组件的功能，使之成为为微服务架构打造的深度学习框架。</p>
<p>  Deeplearning4j从各类浅层网络出发，设计深层神经网络。这一灵活性使用户可以根据所需，在分布式、生产级、能够在分布式CPU或GPU的基础上与Spark和Hadoop协同工作的框架内，整合受限玻尔兹曼机、其他自动编码器、卷积网络或递归网络。</p>
<p>  Deeplearning4j在已建立的各个库及其在系统整体中的所处位置</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-2.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-2.png" srcset="data:image/png;base64,666" alt="Deeplearning4j"></p>
<p>  Github地址：<a href="https://github.com/deeplearning4j/deeplearning4j">https://github.com/deeplearning4j/deeplearning4j</a></p>
<h3 id="Mahout"><a href="#Mahout" class="headerlink" title="Mahout"></a>Mahout</h3><p>  Mahout【3】是基于Hadoop的机器学习和数据挖掘的一个分布式框架。Mahout用MapReduce实现了部分数据挖掘算法，解决了并行挖掘的问题。</p>
<p>  Mahout包含许多实现，包括聚类、分类、推荐过滤、频繁子项挖掘等。</p>
<p>  Mahout算法库：</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-3-1.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-3-1.png" srcset="data:image/png;base64,666" alt="Mahout"></p>
<p>  Mahout应用场景：</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-3-2.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-3-2.png" srcset="data:image/png;base64,666" alt="Mahout"></p>
<p>  Github地址：<a href="https://github.com/apache/mahout">https://github.com/apache/mahout</a></p>
<h3 id="Spark-MLllib"><a href="#Spark-MLllib" class="headerlink" title="Spark MLllib"></a>Spark MLllib</h3><p>MLlib(Machine Learnig lib) 【4】是Spark对常用的机器学习算法的实现库，同时包括相关的测试和数据生成器。</p>
<p>MLlib是MLBase一部分，其中MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。</p>
<ul>
<li>ML Optimizer会选择它认为最适合的已经在内部实现好了的机器学习算法和相关参数，来处理用户输入的数据，并返回模型或别的帮助分析的结果；</li>
<li>MLI 是一个进行特征抽取和高级ML编程抽象的算法实现的API或平台；</li>
<li>MLlib是Spark实现一些常见的机器学习算法和实用程序，包括分类、回归、聚类、协同过滤、降维以及底层优化，该算法可以进行可扩充； </li>
<li>MLRuntime 基于Spark计算框架，将Spark的分布式计算应用到机器学习领域。</li>
</ul>
<p>MLlib主要包含三个部分：</p>
<ul>
<li>底层基础：包括Spark的运行库、矩阵库和向量库</li>
<li>算法库：包含广义线性模型、推荐系统、聚类、决策树和评估的算法</li>
<li>实用程序：包括测试数据的生成、外部数据的读入等功能</li>
</ul>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-4-1.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-4-1.png" srcset="data:image/png;base64,666" alt="架构图"></p>
<center>架构图</center>


<p>  MLlib目前支持4种常见的机器学习问题: 分类、回归、聚类和协同过滤，MLlib在Spark整个生态系统中的位置如图下图所示。</p>
<h3 id="Ray"><a href="#Ray" class="headerlink" title="Ray"></a>Ray</h3><p>  Ray【5】是加州大学伯克利分校实时智能安全执行实验室(RISELab)的研究人员针对机器学习领域开发的一种新的分布式计算框架，该框架旨在让基于Python的机器学习和深度学习工作负载能够实时执行，并具有类似消息传递接口(MPI)的性能和细粒度。</p>
<p>  增强学习的场景，按照原理定义，因为没有预先可用的静态标签信息，所以通常需要引入实际的目标系统（为了加快训练，往往是目标系统的模拟环境）来获取反馈信息，用做损失/收益判断，进而完成整个训练过程的闭环反馈。典型的步骤是通过观察特定目标系统的状态，收集反馈信息，判断收益，用这些信息来调整参数，训练模型，并根据新的训练结果产出可用于调整目标系统的行为Action，输出到目标系统，进而影响目标系统状态变化，完成闭环，如此反复迭代，最终目标是追求某种收益的最大化（比如对AlphoGo来说，收益是赢得一盘围棋的比赛）。</p>
<p>  在这个过程中，一方面，模拟目标系统，收集状态和反馈信息，判断收益，训练参数，生成Action等等行为可能涉及大量的任务和计算（为了选择最佳Action，可能要并发模拟众多可能的行为）。而这些行为本身可能也是千差万别的异构的任务，任务执行的时间也可能长短不一，执行过程有些可能要求同步，也有些可能更适合异步。</p>
<p>  另一方面，整个任务流程的DAG图也可能是动态变化的，系统往往可能需要根据前一个环节的结果，调整下一个环节的行为参数或者流程。这种调整，可能是目标系统的需要（比如在自动驾驶过程中遇到行人了，那么我们可能需要模拟计算刹车的距离来判断该采取的行动是刹车还是拐弯，而平时可能不需要这个环节），也可能是增强学习特定训练算法的需要（比如根据多个并行训练的模型的当前收益，调整模型超参数，替换模型等等）。</p>
<p>  此外，由于所涉及到的目标系统可能是具体的，现实物理世界中的系统，所以对时效性也可能是有强要求的。举个例子，比如你想要实现的系统是用来控制机器人行走，或者是用来打视频游戏的。那么整个闭环反馈流程就需要在特定的时间限制内完成（比如毫秒级别）。</p>
<p>  总结来说，就是增强学习的场景，对分布式计算框架的任务调度延迟，吞吐量和动态修改DAG图的能力都可能有很高的要求。按照官方的设计目标，Ray需要支持异构计算任务，动态计算链路，毫秒级别延迟和每秒调度百万级别任务的能力。</p>
<p>  Ray的目标问题，主要是在类似增强学习这样的场景中所遇到的工程问题。那么增强学习的场景和普通的机器学习，深度学习的场景又有什么不同呢？简单来说，就是对整个处理链路流程的时效性和灵活性有更高的要求。</p>
<p>Ray框架优点:</p>
<ul>
<li>海量任务调度能力</li>
<li>毫秒级别的延迟</li>
<li>异构任务的支持</li>
<li>任务拓扑图动态修改的能力</li>
</ul>
<p>  Ray没有采用中心任务调度的方案，而是采用了类似层级（hierarchy）调度的方案，除了一个全局的中心调度服务节点（实际上这个中心调度节点也是可以水平拓展的），任务的调度也可以在具体的执行任务的工作节点上，由本地调度服务来管理和执行。<br>与传统的层级调度方案，至上而下分配调度任务的方式不同的是，Ray采用了至下而上的调度策略。也就是说，任务调度的发起，并不是先提交给全局的中心调度器统筹规划以后再分发给次级调度器的。而是由任务执行节点直接提交给本地的调度器，本地的调度器如果能满足该任务的调度需求就直接完成调度请求，在无法满足的情况下，才会提交给全局调度器，由全局调度器协调转发给有能力满足需求的另外一个节点上的本地调度器去调度执行。</p>
<p>  架构设计一方面减少了跨节点的RPC开销，另一方面也能规避中心节点的瓶颈问题。当然缺点也不是没有，由于缺乏全局的任务视图，无法进行全局规划，因此任务的拓扑逻辑结构也就未必是最优的了。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-5-1.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-5-1.png" srcset="data:image/png;base64,666" alt="架构图"></p>
<center>架构图</center>

<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-5-2.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-5-2.png" srcset="data:image/png;base64,666" alt="任务调度图"></p>
<center>任务调度图</center>

<p>  Ray架构现状：</p>
<ul>
<li>API层以上 的部分还比较薄弱，Core模块核心逻辑估需要时间打磨。</li>
<li>国内目前除了蚂蚁金服和RISELab有针对性的合作以外，关注程度还很低，没有实际的应用实例看到，整体来说还处于比较早期的框架构建阶段。</li>
</ul>
<p>  Github地址：<a href="https://github.com/ray-project/ray">https://github.com/ray-project/ray</a></p>
<h3 id="Spark-stream"><a href="#Spark-stream" class="headerlink" title="Spark stream"></a>Spark stream</h3><p>  随着大数据的发展，人们对大数据的处理要求也越来越高，原有的批处理框架MapReduce适合离线计算，却无法满足实时性要求较高的业务，如实时推荐、用户行为分析等。 Spark Streaming是建立在Spark上的实时计算框架，通过它提供的丰富的API、基于内存的高速执行引擎，用户可以结合流式、批处理和交互试查询应用。</p>
<p>  Spark是一个类似于MapReduce的分布式计算框架，其核心是弹性分布式数据集，提供了比MapReduce更丰富的模型，可以在快速在内存中对数据集进行多次迭代，以支持复杂的数据挖掘算法和图形计算算法。Spark Streaming【6】是一种构建在Spark上的实时计算框架，它扩展了Spark处理大规模流式数据的能力。</p>
<p>  Spark Streaming的优势在于：</p>
<ul>
<li>能运行在100+的结点上，并达到秒级延迟。</li>
<li>使用基于内存的Spark作为执行引擎，具有高效和容错的特性。</li>
<li>能集成Spark的批处理和交互查询。</li>
<li>为实现复杂的算法提供和批处理类似的简单接口。</li>
</ul>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-6-1.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-6-1.png" srcset="data:image/png;base64,666" alt="Spark Streaming架构图"></p>
<p>  Spark Streaming把实时输入数据流以时间片Δt （如1秒）为单位切分成块。Spark Streaming会把每块数据作为一个RDD，并使用RDD操作处理每一小块数据。每个块都会生成一个Spark Job处理，最终结果也返回多块。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-6-2.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-6-2.png" srcset="data:image/png;base64,666" alt="Spark Streaming基本原理图"></p>
<p>  正如Spark Streaming最初的目标一样，它通过丰富的API和基于内存的高速计算引擎让用户可以结合流式处理，批处理和交互查询等应用。因此Spark Streaming适合一些需要历史数据和实时数据结合分析的应用场合。当然，对于实时性要求不是特别高的应用也能完全胜任。另外通过RDD的数据重用机制可以得到更高效的容错处理。</p>
<h3 id="Horovod"><a href="#Horovod" class="headerlink" title="Horovod"></a>Horovod</h3><p>  Horovod【7】 是 Uber 开源的又一个深度学习工具，它的发展吸取了 Facebook「一小时训练 ImageNet 论文」与百度 Ring Allreduce 的优点，可为用户实现分布式训练提供帮助。</p>
<p>  Horovod 支持通过用于高性能并行计算的低层次接口 – 消息传递接口 (MPI) 进行分布式模型训练。有了 MPI，就可以利用分布式 Kubernetes 集群来训练 TensorFlow 和 PyTorch 模型。</p>
<p>  分布式 TensorFlow 的参数服务器模型（parameter server paradigm）通常需要对大量样板代码进行认真的实现。但是 Horovod 仅需要几行。下面是一个分布式 TensorFlow 项目使用 Horovod 的示例：</p>
<pre><code>import  tensorflow as tf
import horovod.tensorflow as hvd
# Initialize Horovod
hvd.init()
# Pin GPU to be used to process local rank (one GPU per process)
config = tf.ConfigProto()
config.gpu_options.visible_device_list = str(hvd.local_rank())
# Build model…
loss = …
opt = tf.train.AdagradOptimizer(0.01)
# Add Horovod Distributed Optimizer
opt = hvd.DistributedOptimizer(opt)
# Add hook to broadcast variables from rank 0 to all other processes during
# initialization.
hooks = [hvd.BroadcastGlobalVariablesHook(0)]
# Make training operation
train_op = opt.minimize(loss)
# The MonitoredTrainingSession takes care of session initialization,
# restoring from a checkpoint, saving to a checkpoint, and closing when done
# or an error occurs.
with tf.train.MonitoredTrainingSession(checkpoint_dir=“/tmp/train_logs”,
  config=config,
  hooks=hooks) as mon_sess:
 while not mon_sess.should_stop():
   # Perform synchronous training.
   mon_sess.run(train_op)
</code></pre><p>  在该示例中，粗体文字指进行单个 GPU 分布式项目时必须做的改变：</p>
<ul>
<li>hvd.init() 初始化 Horovod。</li>
<li>config.gpu_options.visible_device_list = str(hvd.local_rank()) 向每个 TensorFlow 流程分配一个 GPU。</li>
<li>opt=hvd.DistributedOptimizer(opt) 使用 Horovod 优化器包裹每一个常规 TensorFlow 优化器，Horovod 优化器使用 ring-allreduce 平均梯度。</li>
<li>hvd.BroadcastGlobalVariablesHook(0) 将变量从第一个流程向其他流程传播，以实现一致性初始化。如果该项目无法使用 MonitoredTrainingSession，则用户可以运行 hvd.broadcast_global_variables(0)。</li>
</ul>
<p>  之后，可以使用 mpirun 命令使该项目的多个拷贝在多个服务器中运行：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">$ mpirun -np 16 -x LD_LIBRARY_PATH -H </span><br><span class="line">server1:4,server2:4,server3:4,server4:4 python train.py</span><br></pre></td></tr></tbody></table></figure>
<p>  mpirun 命令向四个节点分布 train.py，然后在每个节点的四个 GPU 上运行 train.py。</p>
<p>  Github地址：<a href="https://github.com/uber/horovod">https://github.com/uber/horovod</a></p>
<h3 id="BigDL"><a href="#BigDL" class="headerlink" title="BigDL"></a>BigDL</h3><p>  BigDL【9】是一种基于Apache Spark的分布式深度学习框架。它可以无缝的直接运行在现有的Apache Spark和Hadoop集群之上。BigDL的设计吸取了Torch框架许多方面的知识，为深度学习提供了全面的支持；包括数值计算和高级神经网络；借助现有的Spark集群来运行深度学习计算，并简化存储在Hadoop中的大数据集的数据加载。</p>
<p>  BigDL优点：</p>
<ul>
<li>丰富的深度学习支持。模拟Torch之后，BigDL为深入学习提供全面支持，包括数字计算（通过Tensor）和高级神经网络 ; 此外，用户可以使用BigDL将预先训练好的Caffe或Torch模型加载到Spark程序中。</li>
<li>极高的性能。为了实现高性能，BigDL在每个Spark任务中使用英特尔MKL和多线程编程。因此，在单节点Xeon（即与主流GPU 相当）上，它比开箱即用开源Caffe，Torch或TensorFlow快了数量级。</li>
<li>有效地横向扩展。BigDL可以通过利用Apache Spark（快速分布式数据处理框架），以及高效实施同步SGD和全面减少Spark的通信，从而有效地扩展到“大数据规模”上的数据分析</li>
</ul>
<p>  BigDL缺点：</p>
<ul>
<li>对机器要求高 jdk7上运行性能差 在CentOS 6和7上，要将最大用户进程增加到更大的值（例如514585）; 否则，可能会看到错误，如“无法创建新的本机线程”。 </li>
<li>训练和验证的数据会加载到内存，挤占内存</li>
</ul>
<p>  BigDL满足的应用场景：</p>
<ul>
<li>直接在Hadoop/Spark框架下使用深度学习进行大数据分析（即将数据存储在HDFS、HBase、Hive等数据库上）；</li>
<li>在Spark程序中/工作流中加入深度学习功能；</li>
<li>利用现有的 Hadoop/Spark 集群来运行深度学习程序，然后将代码与其他的应用场景进行动态共享，例如ETL（Extract、Transform、Load，即通常所说的数据抽取）、数据仓库（data warehouse）、功能引擎、经典机器学习、图表分析等。</li>
</ul>
<h3 id="Petastorm"><a href="#Petastorm" class="headerlink" title="Petastorm"></a>Petastorm</h3><p>  Petastorm是一个由 Uber ATG 开发的开源数据访问库。这个库可以直接基于数 TB Parquet 格式的数据集进行单机或分布式训练和深度学习模型评估。Petastorm 支持基于 Python 的机器学习框架，如 Tensorflow、Pytorch 和 PySpark，也可以直接用在 Python 代码中。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-9-1.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-9-1.png" srcset="data:image/png;base64,666" alt="深度学习集群"></p>
<p>  即使是在现代硬件上训练深度模型也很耗时，而且在很多情况下，很有必要在多台机器上分配训练负载。典型的深度学习集群需要执行以下几个步骤：</p>
<ul>
<li>一台或多台机器读取集中式或本地数据集。</li>
<li>每台机器计算损失函数的值，并根据模型参数计算梯度。在这一步通常会使用 GPU。</li>
<li>通过组合估计的梯度（通常由多台机器以分布式的方式计算得出）来更新模型系数。</li>
</ul>
<p>  通常，一个数据集是通过连接多个数据源的记录而生成的。这个由 Apache Spark 的 Python 接口 PySpark 生成的数据集稍后将被用在机器学习训练中。Petastorm 提供了一个简单的功能，使用 Petastorm 特定的元数据对标准的 Parquet 进行了扩展，从而让它可以与 Petastorm 兼容。<br>有了 Petastorm，消费数据就像在 HDFS 或文件系统中创建和迭代读取对象一样简单。Petastorm 使用 PyArrow 来读取 Parquet 文件。</p>
<p>  将多个数据源组合到单个表格结构中，从而生成数据集。可以多次使用相同的数据集进行模型训练和评估。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-9-2.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-9-2.png" srcset="data:image/png;base64,666" alt="深度学习集群"></p>
<p>  为分布式训练进行分片<br>在分布式训练环境中，每个进程通常负责训练数据的一个子集。一个进程的数据子集与其他进程的数据子集正交。Petastorm 支持将数据集的读时分片转换为正交的样本集。<br><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-9-3.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-9-3.png" srcset="data:image/png;base64,666" alt="Petastorm 将数据集的非重叠子集提供给参与分布式训练的不同机器"><br>                Petastorm 将数据集的非重叠子集提供给参与分布式训练的不同机器</p>
<p>  本地缓存<br>Petastorm 支持在本地存储中缓存数据。当网络连接速度较慢或带宽很昂贵时，这会派上用场。<br><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-9-4.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-9-4.png" srcset="data:image/png;base64,666" alt="本地缓存"></p>
<p>Github地址：<a href="https://github.com/uber/petastorm">https://github.com/uber/petastorm</a></p>
<h3 id="TensorFlowOnSpark"><a href="#TensorFlowOnSpark" class="headerlink" title="TensorFlowOnSpark"></a>TensorFlowOnSpark</h3><p>  TensorFlowOnSpark【10】为 Apache Hadoop 和 Apache Spark 集群带来可扩展的深度学习。 通过结合深入学习框架 TensorFlow 和大数据框架 Apache Spark 、Apache Hadoop 的显着特征，TensorFlowOnSpark 能够在 GPU 和 CPU 服务器集群上实现分布式深度学习。</p>
<p>  满足的应用场景：<br>为了利用TensorFlow在现有的Spark和Hadoop集群上进行深度学习。而不需要为深度学习设置单独的集群。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-10-1.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-10-1.png" srcset="data:image/png;base64,666" alt="架构图"></p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-10-2.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-2-10-2.png" srcset="data:image/png;base64,666" alt="运行流程图"></p>
<p>  优点：</p>
<ul>
<li>轻松迁移所有现有的TensorFlow程序，&lt;10行代码更改; </li>
<li>支持所有TensorFlow功能：同步/异步训练，模型/数据并行，推理和TensorBoard; </li>
<li>服务器到服务器的直接通信在可用时实现更快的学习; </li>
<li>允许数据集在HDFS和由Spark推动的其他来源或由TensorFlow拖动; </li>
<li>轻松集成您现有的数据处理流水线和机器学习算法（例如，MLlib，CaffeOnSpark）; </li>
<li>轻松部署在云或内部部署：CPU和GPU，以太网和Infiniband。</li>
<li>TensorFlowOnSpark是基于google的TensorFlow的实现，而TensorFlow有着一套完善的教程，内容丰富。 </li>
</ul>
<p>  劣势：</p>
<ul>
<li>开源时间不长，未得到充分的验证。</li>
</ul>
<p>  Github 地址:<a href="https://github.com/yahoo/TensorFlowOnSpark">https://github.com/yahoo/TensorFlowOnSpark</a></p>
<h2 id="如何进行实时计算？"><a href="#如何进行实时计算？" class="headerlink" title="如何进行实时计算？"></a>如何进行实时计算？</h2><h3 id="什么是实时流计算？"><a href="#什么是实时流计算？" class="headerlink" title="什么是实时流计算？"></a>什么是实时流计算？</h3><p>  所谓实时流计算，就是近几年由于数据得到广泛应用之后，在数据持久性建模不满足现状的情况下，急需数据流的瞬时建模或者计算处理。这种实时计算的应用实例有金融服务、网络监控、电信数据管理、 Web 应用、生产制造、传感检测，等等。在这种数据流模型中，单独的数据单元可能是相关的元组（Tuple），如网络测量、呼叫记录、网页访问等产生的数据。但是，这些数据以大量、快速、时变（可能是不可预知）的数据流持续到达，由此产生了一些基础性的新的研究问题——实时计算。实时计算的一个重要方向就是实时流计算。</p>
<h3 id="实时流计算过程"><a href="#实时流计算过程" class="headerlink" title="实时流计算过程"></a>实时流计算过程</h3><p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-4-1.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-4-1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>  我们以热卖产品的统计为例，看下传统的计算手段：</p>
<ul>
<li>将用户行为、log等信息清洗后保存在数据库中.</li>
<li>将订单信息保存在数据库中.</li>
<li>利用触发器或者协程等方式建立本地索引，或者远程的独立索引.</li>
<li>join订单信息、订单明细、用户信息、商品信息等等表，聚合统计20分钟内热卖产品，并返回top-10.</li>
<li>web或app展示.</li>
</ul>
<p>  这是一个假想的场景，但假设你具有处理类似场景的经验，应该会体会到这样一些问题和难处：</p>
<ul>
<li>水平扩展问题（scale-out）<br>显然，如果是一个具有一定规模的电子商务网站，数据量都是很大的。而交易信息因为涉及事务，所以很难直接舍弃关系型数据库的事务能力，迁移到具有更好的scale-out能力的NoSQL数据库中。</li>
</ul>
<p>  那么，一般都会做sharding。历史数据还好说，我们可以按日期来归档，并可以通过批处理式的离线计算，将结果缓存起来。<br>但是，这里的要求是20分钟内，这很难。</p>
<ul>
<li>性能问题<br>这个问题，和scale-out是一致的，假设我们做了sharding，因为表分散在各个节点中，所以我们需要多次入库，并在业务层做聚合计算。</li>
</ul>
<p>  问题是，20分钟的时间要求，我们需要入库多少次呢？10分钟呢？5分钟呢？实时呢？</p>
<p>  而且，业务层也同样面临着单点计算能力的局限，需要水平扩展，那么还需要考虑一致性的问题。<br>所以，到这里一切都显得很复杂。</p>
<ul>
<li>业务扩展问题</li>
</ul>
<p>  假设我们不仅仅要处理热卖商品的统计，还要统计广告点击、或者迅速根据用户的访问行为判断用户特征以调整其所见的信息，更加符合用户的潜在需求等，那么业务层将会更加复杂。<br>也许你有更好的办法，但实际上，我们需要的是一种新的认知：<br>这个世界发生的事，是实时的。<br>所以我们需要一种实时计算的模型，而不是批处理模型。<br>我们需要的这种模型，必须能够处理很大的数据，所以要有很好的scale-out能力，最好是，我们都不需要考虑太多一致性、复制的问题。</p>
<p>  那么，这种计算模型就是实时计算模型，也可以认为是流式计算模型。<br>现在假设我们有了这样的模型，我们就可以愉快地设计新的业务场景：</p>
<ul>
<li>转发最多的微博是什么？</li>
<li>最热卖的商品有哪些？</li>
<li>大家都在搜索的热点是什么？</li>
<li>我们哪个广告，在哪个位置，被点击最多？</li>
</ul>
<p>或者说，我们可以问：<br>  这个世界，在发生什么？</p>
<p>  最热的微博话题是什么？<br>我们以一个简单的滑动窗口计数的问题，来揭开所谓实时计算的神秘面纱。<br>假设，我们的业务要求是：<br>统计20分钟内最热的10个微博话题。</p>
<p>  解决这个问题，我们需要考虑：</p>
<ul>
<li>数据源</li>
</ul>
<p>  这里，假设我们的数据，来自微博长连接推送的话题。</p>
<ul>
<li>问题建模</li>
</ul>
<p>  我们认为的话题是#号扩起来的话题，最热的话题是此话题出现的次数比其它话题都要多。<br>比如：@foreach_break : 你好,#世界#,我爱你，#微博#。<br>“世界”和“微博”就是话题。</p>
<ul>
<li><p>计算引擎采用storm</p>
</li>
<li><p>定义时间</p>
</li>
</ul>
<p>  时间的定义是一件很难的事情，取决于所需的精度是多少。<br>根据实际，我们一般采用tick来表示时刻这一概念。<br>在storm的基础设施中，executor启动阶段，采用了定时器来触发“过了一段时间”这个事件。<br>如下所示：<br></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">(defn setup-ticks! [worker executor-data]</span><br><span class="line">  (let [storm-conf (:storm-conf executor-data)</span><br><span class="line">        tick-time-secs (storm-conf TOPOLOGY-TICK-TUPLE-FREQ-SECS)</span><br><span class="line">        receive-queue (:receive-queue executor-data)</span><br><span class="line">        context (:worker-context executor-data)]</span><br><span class="line">    (when tick-time-secs</span><br><span class="line">      (if (or (system-id? (:component-id executor-data))</span><br><span class="line">              (and (= false (storm-conf TOPOLOGY-ENABLE-MESSAGE-TIMEOUTS))</span><br><span class="line">                   (= :spout (:type executor-data))))</span><br><span class="line">        (log-message "Timeouts disabled for executor " (:component-id executor-data) ":" (:executor-id executor-data))</span><br><span class="line">        (schedule-recurring</span><br><span class="line">          (:user-timer worker)</span><br><span class="line">          tick-time-secs</span><br><span class="line">          tick-time-secs</span><br><span class="line">          (fn []</span><br><span class="line">            (disruptor/publish</span><br><span class="line">              receive-queue</span><br><span class="line">              [[nil (TupleImpl. context [tick-time-secs] Constants/SYSTEM_TASK_ID Constants/SYSTEM_TICK_STREAM_ID)]]</span><br><span class="line">              )))))))</span><br></pre></td></tr></tbody></table></figure><br>之前的博文中，已经详细分析了这些基础设施的关系，不理解的童鞋可以翻看前面的文章。<br>每隔一段时间，就会触发这样一个事件，当流的下游的bolt收到一个这样的事件时，就可以选择是增量计数还是将结果聚合并发送到流中。<br>bolt如何判断收到的tuple表示的是“tick”呢？<br>负责管理bolt的executor线程，从其订阅的消息队列消费消息时，会调用到bolt的execute方法，那么，可以在execute中这样判断：<br><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">public static boolean isTick(Tuple tuple) {</span><br><span class="line">    return tuple != null</span><br><span class="line">           &amp;&amp; Constants.SYSTEM_COMPONENT_ID  .equals(tuple.getSourceComponent())</span><br><span class="line">           &amp;&amp; Constants.SYSTEM_TICK_STREAM_ID.equals(tuple.getSourceStreamId());</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><br>结合上面的setup-tick!的clojure代码，我们可以知道SYSTEM_TICK_STREAM_ID在定时事件的回调中就以构造函数的参数传递给了tuple，那么SYSTEM_COMPONENT_ID是如何来的呢？<br>可以看到，下面的代码中，SYSTEM_TASK_ID同样传给了tuple：<br>;; 请注意SYSTEM_TASK_ID和SYSTEM_TICK_STREAM_ID<br>(TupleImpl. context [tick-time-secs] Constants/SYSTEM_TASK_ID Constants/SYSTEM_TICK_STREAM_ID)<br>然后利用下面的代码，就可以得到SYSTEM_COMPONENT_ID：<br><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">public String getComponentId(int taskId) {</span><br><span class="line">    if(taskId==Constants.SYSTEM_TASK_ID) {</span><br><span class="line">        return Constants.SYSTEM_COMPONENT_ID;</span><br><span class="line">    } else {</span><br><span class="line">        return _taskToComponent.get(taskId);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><br>滑动窗口<br>有了上面的基础设施，我们还需要一些手段来完成“工程化”，将设想变为现实。<br>这里，我们看看Michael G. Noll的滑动窗口设计。<p></p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-4-2.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-4-2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>Topology</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">String spoutId = "wordGenerator";</span><br><span class="line">String counterId = "counter";</span><br><span class="line">String intermediateRankerId = "intermediateRanker";</span><br><span class="line">String totalRankerId = "finalRanker";</span><br><span class="line">// 这里，假设TestWordSpout就是我们发送话题tuple的源</span><br><span class="line">builder.setSpout(spoutId, new TestWordSpout(), 5);</span><br><span class="line">// RollingCountBolt的时间窗口为9秒钟，每3秒发送一次统计结果到下游</span><br><span class="line">builder.setBolt(counterId, new RollingCountBolt(9, 3), 4).fieldsGrouping(spoutId, new Fields("word"));</span><br><span class="line">// IntermediateRankingsBolt，将完成部分聚合，统计出top-n的话题</span><br><span class="line">builder.setBolt(intermediateRankerId, new IntermediateRankingsBolt(TOP_N), 4).fieldsGrouping(counterId, new Fields(</span><br><span class="line">    "obj"));</span><br><span class="line">    // TotalRankingsBolt， 将完成完整聚合，统计出top-n的话题</span><br><span class="line">builder.setBolt(totalRankerId, new TotalRankingsBolt(TOP_N)).globalGrouping(intermediateRankerId);</span><br></pre></td></tr></tbody></table></figure>
<p>上面的topology设计如下：</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-4-3.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-4-3.png" srcset="data:image/png;base64,666" alt=""></p>
<p>将聚合计算与时间结合起来<br>前文，我们叙述了tick事件，回调中会触发bolt的execute方法，那可以这么做：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">RollingCountBolt:</span><br><span class="line">  @Override</span><br><span class="line">  public void execute(Tuple tuple) {</span><br><span class="line">    if (TupleUtils.isTick(tuple)) {</span><br><span class="line">      LOG.debug("Received tick tuple, triggering emit of current window counts");</span><br><span class="line">      // tick来了，将时间窗口内的统计结果发送，并让窗口滚动</span><br><span class="line">      emitCurrentWindowCounts();</span><br><span class="line">    }</span><br><span class="line">    else {</span><br><span class="line">      // 常规tuple，对话题计数即可</span><br><span class="line">      countObjAndAck(tuple);</span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line"></span><br><span class="line">  // obj即为话题，增加一个计数 count++</span><br><span class="line">  // 注意，这里的速度基本取决于流的速度，可能每秒百万，也可能每秒几十.</span><br><span class="line">  // 内存不足？ bolt可以scale-out.</span><br><span class="line">  private void countObjAndAck(Tuple tuple) {</span><br><span class="line">    Object obj = tuple.getValue(0);</span><br><span class="line">    counter.incrementCount(obj);</span><br><span class="line">    collector.ack(tuple);</span><br><span class="line">  }</span><br><span class="line">  </span><br><span class="line">  // 将统计结果发送到下游</span><br><span class="line">  private void emitCurrentWindowCounts() {</span><br><span class="line">    Map&lt;Object, Long&gt; counts = counter.getCountsThenAdvanceWindow();</span><br><span class="line">    int actualWindowLengthInSeconds = lastModifiedTracker.secondsSinceOldestModification();</span><br><span class="line">    lastModifiedTracker.markAsModified();</span><br><span class="line">    if (actualWindowLengthInSeconds != windowLengthInSeconds) {</span><br><span class="line">      LOG.warn(String.format(WINDOW_LENGTH_WARNING_TEMPLATE, actualWindowLengthInSeconds, windowLengthInSeconds));</span><br><span class="line">    }</span><br><span class="line">    emit(counts, actualWindowLengthInSeconds);</span><br><span class="line">  }</span><br></pre></td></tr></tbody></table></figure>
<p>上面的代码可能有点抽象，看下这个图就明白了，tick一到，窗口就滚动：</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-4-4.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-4-4.png" srcset="data:image/png;base64,666" alt=""></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">IntermediateRankingsBolt &amp; TotalRankingsBolt：</span><br><span class="line">  public final void execute(Tuple tuple, BasicOutputCollector collector) {</span><br><span class="line">    if (TupleUtils.isTick(tuple)) {</span><br><span class="line">      getLogger().debug("Received tick tuple, triggering emit of current rankings");</span><br><span class="line">      // 将聚合并排序的结果发送到下游</span><br><span class="line">      emitRankings(collector);</span><br><span class="line">    }</span><br><span class="line">    else {</span><br><span class="line">      // 聚合并排序</span><br><span class="line">      updateRankingsWithTuple(tuple);</span><br><span class="line">    }</span><br><span class="line">  }</span><br></pre></td></tr></tbody></table></figure>
<p>  其中，IntermediateRankingsBolt和TotalRankingsBolt的聚合排序方法略有不同：</p>
<p>IntermediateRankingsBolt的聚合排序方法：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">@Override</span><br><span class="line">void updateRankingsWithTuple(Tuple tuple) {</span><br><span class="line">  // 这一步，将话题、话题出现的次数提取出来</span><br><span class="line">  Rankable rankable = RankableObjectWithFields.from(tuple);</span><br><span class="line">  // 这一步，将话题出现的次数进行聚合，然后重排序所有话题</span><br><span class="line">  super.getRankings().updateWith(rankable);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>TotalRankingsBolt的聚合排序方法：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">@Override</span><br><span class="line">void updateRankingsWithTuple(Tuple tuple) {</span><br><span class="line">// 提出来自IntermediateRankingsBolt的中间结果</span><br><span class="line">  Rankings rankingsToBeMerged = (Rankings) tuple.getValue(0);</span><br><span class="line">// 聚合并排序</span><br><span class="line">  super.getRankings().updateWith(rankingsToBeMerged);</span><br><span class="line">// 去0，节约内存</span><br><span class="line">  super.getRankings().pruneZeroCounts();</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>而重排序方法比较简单粗暴，因为只求前N个，N不会很大：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">private void rerank() {</span><br><span class="line">  Collections.sort(rankedItems);</span><br><span class="line">  Collections.reverse(rankedItems);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>  结语</p>
<p>  下图可能就是我们想要的结果，我们完成了t0 - t1时刻之间的热点话题统计. </p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-4-5.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-4-5.png" srcset="data:image/png;base64,666" alt=""></p>
<h2 id="如何进行离线计算？"><a href="#如何进行离线计算？" class="headerlink" title="如何进行离线计算？"></a>如何进行离线计算？</h2><p>相对于实时计算，有充裕的时间进行运算挖掘</p>
<p><strong>应用场景</strong></p>
<ul>
<li>用户流失预警系统</li>
<li>基于用户购买的挽回系统</li>
<li>用户特征和规则提取系统</li>
<li>数据分析系统</li>
<li>用户画像系统</li>
</ul>
<p><strong>流程</strong></p>
<ul>
<li>数据采集</li>
<li>数据预处理</li>
<li>数据建模</li>
<li>ETL</li>
<li>数据导出</li>
<li>工作流调度</li>
</ul>
<h3 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h3><blockquote>
<p>Flume 收集服务器日志到hdfs</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">type=taildir taildir可以监控一个目录, 也可以用一个正则表达式匹配文件名进行实时收集</span><br><span class="line">taildir=spooldir + exec + 支持断点续传</span><br><span class="line">agent1.sources = source1</span><br><span class="line">agent1.sinks = sink1</span><br><span class="line">agent1.channels = channel1</span><br><span class="line"></span><br><span class="line">agent1.sources.source1.type = TAILDIR </span><br><span class="line">agent1.sources.source1.positionFile = /var/log/flume/taildir_position.json</span><br><span class="line">agent1.sources.source1.filegroups = f1 f2</span><br><span class="line"></span><br><span class="line"># 监控文件内容的改变</span><br><span class="line"></span><br><span class="line">agent1.sources.source1.filegroups.f1 = /usr/local/nginx/logs/example.log</span><br><span class="line"></span><br><span class="line"># 监控生成的文件</span><br><span class="line"></span><br><span class="line">agent1.sources.source1.filegroups.f1 = /usr/local/nginx/logs/.*log.*</span><br><span class="line"></span><br><span class="line">agent1.sources.source1.interceptors = i1</span><br><span class="line">agent1.sources.source1.interceptors.i1.type = host</span><br><span class="line">agent1.sources.source1.interceptors.i1.hostHeader = hostname</span><br><span class="line"></span><br><span class="line"># 配置sink组件为hdfs</span><br><span class="line"></span><br><span class="line">agent1.sinks.sink1.type = hdfs</span><br><span class="line">agent1.sinks.sink1.hdfs.path=</span><br><span class="line">hdfs://node-1:9000/weblog/flume-collection/%y-%m-%d/%H-%M_%hostname</span><br><span class="line"></span><br><span class="line"># 指定文件名前缀</span><br><span class="line"></span><br><span class="line">agent1.sinks.sink1.hdfs.filePrefix = access_log</span><br><span class="line"></span><br><span class="line"># 指定每批下沉数据的记录条数</span><br><span class="line"></span><br><span class="line">agent1.sinks.sink1.hdfs.batchSize= 100</span><br><span class="line">agent1.sinks.sink1.hdfs.fileType = DataStream</span><br><span class="line">agent1.sinks.sink1.hdfs.writeFormat =Text</span><br><span class="line"></span><br><span class="line"># 指定下沉文件按1G大小滚动</span><br><span class="line"></span><br><span class="line">agent1.sinks.sink1.hdfs.rollSize = 1024*1024*1024</span><br><span class="line"></span><br><span class="line"># 指定下沉文件按1000000条数滚动</span><br><span class="line"></span><br><span class="line">agent1.sinks.sink1.hdfs.rollCount = 1000000</span><br><span class="line"></span><br><span class="line"># 指定下沉文件按30分钟滚动</span><br><span class="line"></span><br><span class="line">agent1.sinks.sink1.hdfs.rollInterval = 30</span><br><span class="line"></span><br><span class="line"># agent1.sinks.sink1.hdfs.round = true</span><br><span class="line"></span><br><span class="line"># agent1.sinks.sink1.hdfs.roundValue = 10</span><br><span class="line"></span><br><span class="line"># agent1.sinks.sink1.hdfs.roundUnit = minute</span><br><span class="line"></span><br><span class="line">agent1.sinks.sink1.hdfs.useLocalTimeStamp = true</span><br><span class="line"></span><br><span class="line"># 使用memory类型channel</span><br><span class="line"></span><br><span class="line">agent1.channels.channel1.type = memory</span><br><span class="line">agent1.channels.channel1.capacity = 500000</span><br><span class="line">agent1.channels.channel1.transactionCapacity = 600</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line"></span><br><span class="line">agent1.sources.source1.channels = channel1</span><br><span class="line">agent1.sinks.sink1.channel = channel1</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><blockquote>
<p>数据预处理编程技巧</p>
<ul>
<li>对于本次分析无利用的数据 通常采用逻辑删除 建立标记位 通过01或者true false表示数据是否有效</li>
<li>对于最后一个字段不固定的情况 可以采用动态拼接的方式</li>
<li>静态资源过滤<ul>
<li>js css img (静态数据) 只关心真正请求页面的（index.html）</li>
<li>data（动态数据）</li>
</ul>
</li>
<li>在mr中，如果涉及小且频繁使用的数据，如何优化？<ul>
<li>每次都从数据库查询 效率极低</li>
<li>可以通过数据结构保存在内存中 方便查询 一般在setup方法中进行初始化操作</li>
</ul>
</li>
<li>关于mr程序输出文件名<ul>
<li>part-r-00000 表示是reducetask的输出</li>
<li>part-m-00000 表示是maptask的输出</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="数据建模"><a href="#数据建模" class="headerlink" title="数据建模"></a>数据建模</h3><blockquote>
<ul>
<li><p>维度建模</p>
<p>专门适用于OLAP的设计模式存在着两种类型的表：事实表 维度表</p>
<ul>
<li>事实表：主题的客观度量 能够以记录主题为准 信息多不精准</li>
<li>维度表：看问题分析问题的角度 信息精但是不全 可跟事实表关系</li>
</ul>
</li>
<li><p>维度建模三种常见模型</p>
<ul>
<li>星型模型 一个事实表带多个维度表 维度之间没关系 数仓发展建立初期（一个主题）</li>
<li>雪花模型 一个事实表带多个维度表 维度之间可以继续关系维度 不利于维护 少用</li>
<li>星座模型 多个事实表带多个维度 有些维度可以共用 数仓发展后期（多个主题）</li>
</ul>
<p>不管什么模型，在数仓中，一切有利于数据分析即可为，不用考虑数据冗余性和其他设计规范。</p>
</li>
<li><p>模块设计–维度建模</p>
<p>在本项目中，因为分析主题只有一个（网站流量日志），所有采用星型模型<br>事实表——&gt;对应清洗完之后的数据<br>维度表——-&gt;来自于提前通过工具生成 维度表范围要横跨事实表分析维度<br>点击流模型属于业务模型数据 既不是事实表 也不是维度表 是为了后续计算某些业务指标方便而由业务指定</p>
</li>
<li><p>宽表：为了分析，把原来表中某些字段属性提取出来，构成新的字段 也称之为明细表</p>
<p>窄表：没有扩宽的表 原始表<br>宽表数据来自于窄表 insert（宽）+select (窄)<br>总结：hive中，有几种方式可以创建出带有数据的表？</p>
<ul>
<li><p>create+load data 创建表加载数据（内部表）</p>
</li>
<li><p>create +external +location 创建外部表指定数据路径</p>
</li>
<li><p>create+insert+select 表的数据来自于后面查询语句返回的结果</p>
</li>
<li><p>create+select 创建的表结构和数据来自于后面的查询语句</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"># -- hive内置解析url的函数</span><br><span class="line"></span><br><span class="line">parse_url_tuple（url,host path,query,queryvalue）</span><br><span class="line"></span><br><span class="line"># -- 通常用于把后面的表挂接在左边的表之上 返回成为一个新表</span><br><span class="line"></span><br><span class="line">a LATERAL VIEW b </span><br><span class="line">LATERAL VIEW</span><br><span class="line"></span><br><span class="line">create table t_ods_tmp_referurl as SELECT a.*,b.* FROM ods_weblog_origin a LATERAL VIEW parse_url_tuple(regexp_replace(http_referer, "\"", ""), 'HOST', 'PATH','QUERY', 'QUERY:id') b as host, path, query, query_id; </span><br></pre></td></tr></tbody></table></figure>
</li>
</ul>
</li>
<li><p>group by 语法限制</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">select count(*) as pvs from ods_weblog_detail t where datestr='20130918' group by t.hour</span><br><span class="line"></span><br><span class="line">select t.hour,count(*) as pvs from ods_weblog_detail t where datestr='20130918' group by t.hour</span><br><span class="line"></span><br><span class="line"># -- 在有group by的语句中，出现在select后面的字段要么是分组的字段要么是被聚合函数包围的字段。</span><br><span class="line">解决：</span><br><span class="line">select t.day,t.hour,count(*) as pvs from ods_weblog_detail t where datestr='20130918' group by t.day,t.hour;</span><br></pre></td></tr></tbody></table></figure>
</li>
</ul>
</blockquote>
<h3 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h3><blockquote>
<p><strong>宽表生成</strong></p>
<ul>
<li>生成ods+url解析表</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">create table t_ods_tmp_referurl as</span><br><span class="line">SELECT a.*,b.*</span><br><span class="line">FROM ods_weblog_origin a </span><br><span class="line">LATERAL VIEW parse_url_tuple(regexp_replace(http_referer, "\"", ""), 'HOST', 'PATH','QUERY', 'QUERY:id') b as host, path, query, query_id;</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>生成ods+url+date解析表</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">create table t_ods_tmp_detail as </span><br><span class="line">select b.*,substring(time_local,0,10) as daystr,</span><br><span class="line">substring(time_local,12) as tmstr,</span><br><span class="line">substring(time_local,6,2) as month,</span><br><span class="line">substring(time_local,9,2) as day,</span><br><span class="line">substring(time_local,11,3) as hour</span><br><span class="line">From t_ods_tmp_referurl b;</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>综合</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">create table ods_weblog_detail(</span><br><span class="line">valid           string, --有效标识</span><br><span class="line">remote_addr     string, --来源IP</span><br><span class="line">remote_user     string, --用户标识</span><br><span class="line">time_local      string, --访问完整时间</span><br><span class="line">daystr          string, --访问日期</span><br><span class="line">timestr         string, --访问时间</span><br><span class="line">month           string, --访问月</span><br><span class="line">day             string, --访问日</span><br><span class="line">hour            string, --访问时</span><br><span class="line">request         string, --请求的url</span><br><span class="line">status          string, --响应码</span><br><span class="line">body_bytes_sent string, --传输字节数</span><br><span class="line">http_referer    string, --来源url</span><br><span class="line">ref_host        string, --来源的host</span><br><span class="line">ref_path        string, --来源的路径</span><br><span class="line">ref_query       string, --来源参数query</span><br><span class="line">ref_query_id    string, --来源参数query的值</span><br><span class="line">http_user_agent string --客户终端标识</span><br><span class="line">)</span><br><span class="line">partitioned by(datestr string);</span><br><span class="line"></span><br><span class="line">insert into table  ods_weblog_detail partition(datestr='20130918') </span><br><span class="line">select c.valid,c.remote_addr,c.remote_user,c.time_local,</span><br><span class="line">substring(c.time_local,0,10) as daystr,</span><br><span class="line">substring(c.time_local,12) as tmstr,</span><br><span class="line">substring(c.time_local,6,2) as month,</span><br><span class="line">substring(c.time_local,9,2) as day,</span><br><span class="line">substring(c.time_local,12,2) as hour,</span><br><span class="line">c.request,c.status,c.body_bytes_sent,c.http_referer,c.ref_host,c.ref_path,c.ref_query,c.ref_query_id,c.http_user_agent</span><br><span class="line">from </span><br><span class="line">(select a.*,b.*</span><br><span class="line">from ods_weblog_origin a</span><br><span class="line">LATERAL view </span><br><span class="line">parse_url_tuple(regexp_replace(a.http_referer,"\"",""),'HOST','PATH','QUERY','QUERY_ID')b as ref_host, ref_path, ref_query, ref_query_id) c;</span><br></pre></td></tr></tbody></table></figure>
<p><strong>DML分析</strong></p>
<ul>
<li>计算该处理批次（一天）中的各小时 pvs</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">select </span><br><span class="line">t.month,t.day,t.hour,count(*)</span><br><span class="line">from ods_weblog_detail t</span><br><span class="line">where t.datestr='20130918'</span><br><span class="line">group by t.month,t.day,t.hour;</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>计算每天的pvs</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">select t.month,t.day,count(*) from ods_weblog_detail t where t.datestr='20130918' group by t.month,t.day;</span><br><span class="line"></span><br><span class="line">select a.month,a.day,sum(a.pvs)</span><br><span class="line">from </span><br><span class="line">(</span><br><span class="line">    select </span><br><span class="line">    t.month as month,t.day as day,t.hour as hour,count(*)  as pvs</span><br><span class="line">    from ods_weblog_detail t</span><br><span class="line">    where t.datestr='20130918'</span><br><span class="line">    group by t.month,t.day,t.hour</span><br><span class="line">) a </span><br><span class="line">group by a.month,a.day;</span><br></pre></td></tr></tbody></table></figure>
<p>统计每小时各来访url产生的pvs</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">select </span><br><span class="line">t.day,t.hour,t.http_referer,t.ref_host,count(*)</span><br><span class="line">from ods_weblog_detail t</span><br><span class="line">where datestr='20130918'</span><br><span class="line">group by t.day,t.hour,t.http_referer,t.ref_host</span><br><span class="line">having t.ref_host is not null;</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>统计每小时各来访host的产生的pv数并排序</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">select </span><br><span class="line">t.month,t.day,t.hour,t.ref_host,count(*) as pvs</span><br><span class="line">from ods_weblog_detail t</span><br><span class="line">where datestr='20130918'</span><br><span class="line">group by t.month,t.day,t.hour,t.ref_host</span><br><span class="line">having t.ref_host is not null</span><br><span class="line">order by t.hour asc ,pvs desc;</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">按照时间维度，统计一天内各小时产生最多pvs的来源（host）topN(分组Top)</span><br><span class="line">select </span><br><span class="line">a.month,a.day,a.hour,a.host,a.pvs,a.rmp</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">    select </span><br><span class="line">    t.month as month,t.day as day,t.hour as hour,t.ref_host as host,count(*) as pvs,</span><br><span class="line">    row_number()over(partition by concat(t.month,t.day,t.hour) order by pvs desc) rmp</span><br><span class="line">    from ods_weblog_detail t</span><br><span class="line">    where datestr='20130918'</span><br><span class="line">    group by t.month,t.day,t.hour,t.ref_host</span><br><span class="line">    having t.ref_host is not null</span><br><span class="line">    order by hour asc ,pvs desc</span><br><span class="line">)a </span><br><span class="line">where a.rmp &lt; 4;</span><br></pre></td></tr></tbody></table></figure>
<p>统计今日所有来访者平均请求的页面数。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">select count(*)/count(distinct remote_addr) from ods_weblog_detail where datestr='20130918';</span><br><span class="line"></span><br><span class="line">select</span><br><span class="line">sum(a.pvs)/count(a.ip)</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">    t.remote_addr as ip,count(*) as pvs</span><br><span class="line">    from ods_weblog_detail t</span><br><span class="line">    where t.datestr='20130918'</span><br><span class="line">    group by t.remote_addr</span><br><span class="line">) a;</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>统计每日最热门的页面 top10</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">select </span><br><span class="line">t.request,count(*) as counts</span><br><span class="line">from ods_weblog_detail t</span><br><span class="line">where datestr='20130918'</span><br><span class="line">group by t.request</span><br><span class="line">order by counts desc</span><br><span class="line">limit 10;</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>每日新访客</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">select </span><br><span class="line">today.ip</span><br><span class="line">from </span><br><span class="line">(</span><br><span class="line">    select distinct t.remote_addr as ip </span><br><span class="line">    from ods_weblog_detail t</span><br><span class="line">) today </span><br><span class="line">left join history</span><br><span class="line">on today.ip=history.ip</span><br><span class="line">where  history.ip is null;</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>查询今日所有回头访客及其访问次数（session）</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">select</span><br><span class="line">remote_addr,count(session) as cs</span><br><span class="line">from ods_click_stream_visit</span><br><span class="line">where datestr='20130918'</span><br><span class="line">group by remote_addr</span><br><span class="line">having cs &gt;1;</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>人均访问频次</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">select </span><br><span class="line">count(session)/count(distinct remote_addr)</span><br><span class="line">from ods_click_stream_visit</span><br><span class="line">where datestr='20130918';</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>级联查询自join</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">select </span><br><span class="line">rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs  </span><br><span class="line">from  dw_oute_numbs rn</span><br><span class="line">inner join </span><br><span class="line">dw_oute_numbs rr;</span><br><span class="line"></span><br><span class="line"># -- 绝对转化</span><br><span class="line"></span><br><span class="line">select </span><br><span class="line">a.rrstep,a.rrnumbs/a.rnnumbs</span><br><span class="line">from </span><br><span class="line">(</span><br><span class="line">    select </span><br><span class="line">    rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs  </span><br><span class="line">    from  dw_oute_numbs rn</span><br><span class="line">    inner join dw_oute_numbs rr</span><br><span class="line">)a</span><br><span class="line">where a.rnstep='step1';</span><br><span class="line"></span><br><span class="line"># -- 相对转化</span><br><span class="line"></span><br><span class="line">select </span><br><span class="line">tmp.rrstep as step,tmp.rrnumbs/tmp.rnnumbs as leakage_rate</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">    select rn.step as rnstep,rn.numbs as rnnumbs,rr.step as rrstep,rr.numbs as rrnumbs from 	dw_oute_numbs rn</span><br><span class="line">    inner join </span><br><span class="line">    dw_oute_numbs rr</span><br><span class="line">) tmp</span><br><span class="line">where cast(substr(tmp.rnstep,5,1) as int)=cast(substr(tmp.rrstep,5,1) as int)-1;</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<h3 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h3><blockquote>
<p>Sqoop可以对HDFS文件进行导入导出到关系型数据库<br>Sqoop 工作机制是将导入或导出命令翻译成 mapreduce 程序来实现。<br>在翻译出的 mapreduce 中主要是对 inputformat 和 outputformat 进行定制<br>sqoop实际生产环境中 关于mysql地址 尽量不要使用: localhost 可用ip或者域名代替</p>
<p><strong>导入</strong></p>
<ul>
<li>mysql——-&gt;hdfs 导入的文件分隔符为逗号</li>
<li>mysql——-&gt;hive<ul>
<li>需要先复制表结构到hive 再向表中导入数据</li>
<li>导入的文件分隔符为 ‘\001’</li>
<li>sqoop中增量导入的判断是通过上次导入到某个列的某个值来标识 的，这个值由用户自己维护，一般企业中选择不重复且自增长的主键最多，自增长的时间也可以。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"># 导入mysql表到hdfs</span><br><span class="line"></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node-1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123 \</span><br><span class="line">--target-dir /sqoopresult \</span><br><span class="line">--table emp --m 1</span><br><span class="line"></span><br><span class="line"># 支持条件导入数据</span><br><span class="line"></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node-1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123 \</span><br><span class="line">--where "id &gt; 1202" \</span><br><span class="line">--target-dir /sqoopresult/t1 \</span><br><span class="line">--table emp --m 1</span><br><span class="line"></span><br><span class="line"># 将关系型数据的表结构复制到hive中</span><br><span class="line"></span><br><span class="line">bin/sqoop create-hive-table \</span><br><span class="line">--connect jdbc:mysql://node-1:3306/userdb \</span><br><span class="line">--table emp_add \</span><br><span class="line">--username root \</span><br><span class="line">--password 123 \</span><br><span class="line">--hive-table default.emp_add_sp</span><br><span class="line"></span><br><span class="line"># 从关系数据库导入文件到hive中</span><br><span class="line"></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node-1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123 \</span><br><span class="line">--table emp_add \</span><br><span class="line">--hive-table default.emp_add_sp \</span><br><span class="line">--hive-import \</span><br><span class="line">--m 1</span><br><span class="line"></span><br><span class="line"># 增量导入</span><br><span class="line"></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node-1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123 \</span><br><span class="line">--table emp_add  \</span><br><span class="line">--target-dir '/user/hive/warehouse/emp_add_sp' \</span><br><span class="line">--incremental append \</span><br><span class="line">--check-column id \</span><br><span class="line">--last-value 1205 \</span><br><span class="line">--fields-terminated-by '\001' \</span><br><span class="line">--m 1</span><br></pre></td></tr></tbody></table></figure>
<p><strong>导出</strong></p>
<ul>
<li><p>hdfs导出到mysql</p>
<ul>
<li><p>要先在mysql中手动创建对应的表结构</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"># hdfs文件导出到mysql</span><br><span class="line"></span><br><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node-1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123 \</span><br><span class="line">--table employee \</span><br><span class="line">--export-dir /hivedata/employee.txt \</span><br><span class="line">--fields-terminated-by '\001'</span><br></pre></td></tr></tbody></table></figure>
</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="工作流调度"><a href="#工作流调度" class="headerlink" title="工作流调度"></a>工作流调度</h3><blockquote>
<p>azkaban工作流程</p>
<ul>
<li>配置job文件（注意文件的第一行头信息）</li>
<li>把job配置连同其他资源一起打成.zip压缩包</li>
<li>页面上创建工程project</li>
<li>上传.zip压缩包</li>
<li>execute/schedule</li>
</ul>
</blockquote>
<h2 id="如何设计一个人机交互系统？"><a href="#如何设计一个人机交互系统？" class="headerlink" title="如何设计一个人机交互系统？"></a>如何设计一个人机交互系统？</h2><h3 id="什么是人机交互系统？"><a href="#什么是人机交互系统？" class="headerlink" title="什么是人机交互系统？"></a>什么是人机交互系统？</h3><p>  人机交互系统(Human-computer interaction，简称HCI)是研究人与计算机之间通过相互理解的交流与通信，在最大程度上为人们完成信息管理，服务和处理等功能，使计算机真正成为人们工作学习的和谐助手的一门技术科学。</p>
<p>  通俗来讲，就是指人与计算机之间使用某种对话语言，以一定的交互方式，为完成确定任务的人与计算机之间的信息交换过程。</p>
<p>  目前工业界落地的产品包括阿里巴巴集团的云小蜜、天猫精灵；百度的UNIT、小度；小米的小爱同学；京东的叮咚智能音箱等。</p>
<p>【产品图】</p>
<p>人机交互发展阶段大致可分为四个阶段</p>
<ul>
<li>第一代人机交互技术：基于键盘和字符显示器的交互阶段</li>
<li>第二代人机交互技术：基于鼠标和图形显示器的交互阶段</li>
<li>第三代人机交互技术：基于多媒体技术的交互阶段</li>
<li>第四代人机交互技术：人机自然交互与通信</li>
</ul>
<p>本章节重点介绍第四代人机交互技术，传统人机交互模型主要组成部分包括：</p>
<p>1.多模态输入/输出：多模态输入/输出是第四代人机交互与通信的主要标志之一。多模态输入包括键盘、鼠标、文字、语音、手势、表情、注视等多种输入方式；而多模态输出包括文字、图形、语音、手势、表情等多种交互信息</p>
<p>2.智能接口代理：智能接口代理是实现人与计算机交互的媒介</p>
<p>3.视觉获取：视觉系统主要用于实时获取外部视觉信息</p>
<p>4.视觉合成：使人机交互能够在一个仿真或虚拟的环境中进行，仿佛现实世界中人与人之间的交互</p>
<p>5.对话系统：目前主要由两种研究趋势，一种以语音为主，另一种从某一特定任务域入手，引入对话管理概念，建立类似于人人对话的人机对话</p>
<p>6.Internet信息服务：扮演信息交流媒介的角色</p>
<p>7.知识处理：自动地提取有组织的，可为人们利用的知识</p>
<p>本章节介绍的人机交互系统在传统人机交互功能基础上提供：</p>
<ul>
<li>对话管理：业界领先的自然语言交互系统，支持多意图自由跳转以及状态跟踪等技术</li>
<li>意图识别：基于fasttext与textcnn、bert等nlp处理技术提供多意图识别框架以及策略</li>
<li>实验系统：多重叠实验框架，支持流量多种划分方式以及灰度白名单功能</li>
<li>分布式训练：基于k8s+tensorflow+kebuflow的分布式gpu训练集群</li>
<li>日志分析：基于spark+flume等流式大数据处理成熟方案，提供在线日志分析系统</li>
<li>语料平台：基于标注数据以及线上数据汇总的语料存储平台</li>
</ul>
<p>等功能。</p>
<p></p><center>【架构设计图】</center><br><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-1-1.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-1-1.jpg" srcset="data:image/png;base64,666" alt="架构设计图"><p></p>
<h3 id="如何设计人机交互系统的问答引擎算法架构？"><a href="#如何设计人机交互系统的问答引擎算法架构？" class="headerlink" title="如何设计人机交互系统的问答引擎算法架构？"></a>如何设计人机交互系统的问答引擎算法架构？</h3><p>人机交互系统中的核心是问答引擎，引擎架构设计的好坏直接影响整个系统的用户体验，地位就像人的大脑。</p>
<p>本章节介绍的问答引擎架构由预处理模块、检索模块、知识图谱、排序模块、用户画像组成。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-2-2.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-2-2.jpg" srcset="data:image/png;base64,666" alt="架构图"></p>
<p>检索模块如果能完全命中语料库答案则直接返回，不在经过排序。</p>
<p>ES检索和深度语义匹配是比较粗比较弱的召回模型，不用考虑用户问内部的相对顺序，也不用考虑用户问与匹配问之间的位置编码，ngram信息。精排解决这些问题，且有更强的拟合性，但精排涉及到效率问题，所以在此之前有实体对齐过滤一些明显不对的答案。</p>
<p>回答不了的问题可以坐意图澄清或者转移到另一个问题。</p>
<p>借鉴推荐系统的思想：</p>
<p>1.粗排阶段根据用户长期兴趣画像召回相关度较高的答案，同时减轻精排阶段的压力 </p>
<p>2.精排阶段则根据粗排召回的答案列表，通过离线训练好的排序模型预测CTR,最终下发前Top N 个答案作为推荐结果。</p>
<h3 id="如何处理长难句？"><a href="#如何处理长难句？" class="headerlink" title="如何处理长难句？"></a>如何处理长难句？</h3><p><strong>长难句压缩</strong></p>
<pre><code>输入：“嗯你好，我之前06年的时候买了一个保险，嗯一年只交了518元，然后后面我就再也没有买了，是06年的事情，然后现在我打电话给那个服务热线吧，我想退保就是”

输出：“我之前06年的时候买了保险，我想退保”

输入：“一年半之前做过小腿骨折手术，现在已修复，只是固定钢板还没取，准备8月取，医生说也可以不取，不影响正常生活和工作，能否投保相互保？”

输出：“做过腿骨折手术，不影响正常生活和工作，能否投保相互保？”
</code></pre><p><strong>传统处理方案</strong></p>
<ul>
<li>语法树分析+关键词典</li>
</ul>
<p>步骤：</p>
<p>1.通过标点或空格分割长句成若干个断句，然后对短句分类，去掉口水语句</p>
<p>2.基于概率和句法分析的句子压缩方案，只保留主谓宾等核心句子成分。配合特定的关键词典，确保关键词被保留。</p>
<p><strong>深度学习处理方案</strong></p>
<ul>
<li>文本摘要和句子压缩主流方法：一种是抽取式（extractive），另一种是生成式（abstractive）</li>
</ul>
<p>从传统的TextRank抽取式，到深度学习中采用RNN、CNN单元处理，再引入Attention、Self-Attention、机器生成摘要的方式，这些跟人类思维越来越像，都建立在对整段句子的理解之上，生成摘要的效果，常常让我们惊艳。</p>
<p>注意：TextRank算法对较短的文本效果不好。</p>
<h3 id="如何纠错？"><a href="#如何纠错？" class="headerlink" title="如何纠错？"></a>如何纠错？</h3><ul>
<li><strong>字典纠错：字典+规则，特定数据驱动型纠错</strong></li>
</ul>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-4-1.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-4-1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<ul>
<li><strong>通用纠错模型：神经网络模型，其他纠错保底</strong></li>
</ul>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-4-2.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-4-2.jpg" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="什么是指代消解？如何指代消解？"><a href="#什么是指代消解？如何指代消解？" class="headerlink" title="什么是指代消解？如何指代消解？"></a>什么是指代消解？如何指代消解？</h3><p>  指代消解，广义上说，就是在篇章中确定代词指向哪个名词短语的问题。按照指向，可以分为回指和预指。回指就是代词的先行语在代词前面，预指就是代词的先行语在代词后面。按照指代的类型可以分为三类：人称代词、指示代词、有定描述、省略、部分－整体指代、普通名词短语。</p>
<ol>
<li>主谓关系SBV（subject-verb）</li>
<li>状中结构ADV（adverbial）</li>
<li>核心HED（head）</li>
<li>动宾关系VOB（verb-object）</li>
<li>标点 WP</li>
<li>nhd:疾病，v:动词，vn:名动词 ，nbx:自定义专有名词，rzv:谓词性指示代词</li>
</ol>
<p>例子：</p>
<pre><code>输入1: 感冒/nhd 可以/v 投保/vn 相互保/nbx 吗/y？
输入2: 那/rzv 癌症/nhd 呢/y？
输出：癌症可以投保相互保吗？

输入1:相互保很好
输入2:这个产品比e生保好在哪？
输出：相互保比e生保好在哪？

输入1:相互保、e生保都是医疗险吗
输入2:前者能报销啥 
输出：相互保能报销啥

输入1:相互保、e生保都是医疗险吗
输入2:第一种能报销啥 
输出：相互保能报销啥
输入3:第二种呢 
输出：e生保能报销啥
</code></pre><p>分析：</p>
<pre><code>感冒 --(SBV)--&gt; 投保
可以 --(ADV)--&gt; 投保
投保 --(HED)--&gt; ##核心##
相互保 --(VOB)--&gt; 投保
吗 --(RAD)--&gt; 投保
？ --(WP)--&gt; 投保
</code></pre><p>实现思路：</p>
<pre><code>分词-&gt;词性标注-&gt;依存句法分析-&gt;主谓宾提取-&gt;实体替换/指代消解
</code></pre><p>待消解项，先行语可通过句法分析找出。</p>
<h3 id="如何做语义匹配？"><a href="#如何做语义匹配？" class="headerlink" title="如何做语义匹配？"></a>如何做语义匹配？</h3><ul>
<li>孪生网络（Siamese network）</li>
</ul>
<p>孪生神经网络是一类包含两个或更多个相同子网络的神经网络架构。 这里相同是指它们具有相同的配置即具有相同的参数和权重。 参数更新在两个子网上共同进行。 </p>
<p>孪生神经网络在涉及发现相似性或两个可比较的事物之间的关系的任务中流行。 一些例子是复述评分，其中输入是两个句子，输出是它们是多么相似的得分; 或签名验证，确定两个签名是否来自同一个人。 通常，在这样的任务中，使用两个相同的子网络来处理两个输入，并且另一个模块将取得它们的输出并产生最终输出。 下面的图片来自Bromley et al (1993)【11】。 他们为签名验证任务提出了一个孪生体系结构。 </p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-1.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>孪生结构之所以在这些任务中表现的比较好，有如下几个原因 ：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">1.子网共享权重意味着训练需要更少的参数，也就意味着需要更少的数据并且不容易过拟合。</span><br><span class="line"></span><br><span class="line">2.每个子网本质上产生其输入的表示。 （图片中的"签名特征向量"）。如果输入是相同类型的，例如匹配两个句子或匹配两个图片，使用类似的模型来处理类似的输入是有意义的。 这样，就有了具有相同语义的表示向量，使得它们更容易比较。</span><br></pre></td></tr></tbody></table></figure>
<p>问题回答，一些最近的研究使用孪生体系结构来评分问题和答案候选人之间的相关性[2]。 所以一个输入是一个问句，另一个输入是一个答案，输出和问题的答案是相关的。 问题和答案看起来不完全相同，但如果目标是提取相似性或它们之间的联系，孪生体系结构也可以很好地工作。 </p>
<ul>
<li>交互矩阵（MatchPyramid）【12】</li>
</ul>
<p>对于文本匹配，基本思路如下述公式：</p>
<script type="math/tex; mode=display">
match(T1,T2)=F(θ(T1),θ(T2))</script><p>其中T为文本，函数θ代表将文本转换为对应的表示，函数F则代表两个文本表示之间的交互关系。<br>由侧重点不同可分为表示方法与交互方法，即注重θ或者F，而MatchPyramid应属于后一种。</p>
<p>基本方法：构建文本与文本的相似度矩阵，采用CNN对矩阵进行特征抽取，最后用softmax获得分类概率，评价方法为交叉熵。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-2.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>流程：</p>
<ul>
<li>[x] 相似度矩阵</li>
</ul>
<p>由于CNN针对的是网格型数据，而文本显然属于序列数据，那么就有必要对数据进行转换，以下三种构建相似度矩阵的方法，其中距离矩阵使用点积的效果相对较好。</p>
<ul>
<li>0-1类型，每个序列对应的词相同为1，不同为0</li>
</ul>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-3.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-3.png" srcset="data:image/png;base64,666" alt=""></p>
<ul>
<li>cosine距离，使用预训练的Glove将词转为向量，之后计算序列对应的词的cosine距离</li>
</ul>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-4.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-4.png" srcset="data:image/png;base64,666" alt=""></p>
<ul>
<li><p>点积，同上，但是将cosine距离改为点积</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-5.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-5.png" srcset="data:image/png;base64,666" alt=""></p>
</li>
<li><p>[x] 两层CNN</p>
</li>
</ul>
<p>后续利用两层的CNN对相似度矩阵进行特征抽取，这里要注意的是由于上一层的相似度矩阵shape不一致，在第一层CNN后面进行maxpool的时候，要使用动态pool，有没有其他的小trick就不可得知了。</p>
<ul>
<li>[x] 两层MLP</li>
</ul>
<p>最后用两层的全连接对CNN的结果进行转换，使用softmax函数得到最终分类概率。 </p>
<blockquote>
<p>作者使用论文中的模型，在kaggle的quora数据集中得到一个相当不错的分数，最终小组成绩达到了第四名。<br>附实现地址：<a href="https://github.com/faneshion/MatchZoo">https://github.com/faneshion/MatchZoo</a></p>
</blockquote>
<h3 id="如何在海量的向量中查找相似的TopN向量？"><a href="#如何在海量的向量中查找相似的TopN向量？" class="headerlink" title="如何在海量的向量中查找相似的TopN向量？"></a>如何在海量的向量中查找相似的TopN向量？</h3><ul>
<li><strong>Annoy搜索算法</strong></li>
</ul>
<p>  Annoy的目标是建立一个数据结构，使得查询一个点的最近邻点的时间复杂度是次线性。Annoy通过建立一个二叉树来使得每个点查找时间复杂度是O(log n)。 看下面这个图，随机选择两个点，以这两个节点为初始中心节点，执行聚类数为2的kmeans过程，最终产生收敛后两个聚类中心点。这两个聚类中心点之间连一条线段（灰色短线），建立一条垂直于这条灰线，并且通过灰线中心点的线（黑色粗线）。这条黑色粗线把数据空间分成两部分。在多维空间的话，这条黑色粗线可以看成等距垂直超平面。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-1.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>  通过多次递归迭代划分的话，最终原始数据会形成类似下面这样一个二叉树结构。二叉树底层是叶子节点记录原始数据节点，其他中间节点记录的是分割超平面的信息。Annoy建立这样的二叉树结构是希望满足这样的一个假设: 相似的数据节点应该在二叉树上位置更接近，一个分割超平面不应该把相似的数据节点分割二叉树的不同分支上。</p>
<pre><code>总结：Annoy建立一个数据结构，使得查询一个向量的最近邻向量的时间复杂度是次线性

步骤：随机选两个点聚类-&gt;超平面内分割-&gt;构造二叉树-&gt;构造多棵树-&gt;检索答案合并排序

注意：第一次查询速度比较慢，是个近似算法，准确率逼近100%
</code></pre><h3 id="什么是话术澄清？"><a href="#什么是话术澄清？" class="headerlink" title="什么是话术澄清？"></a>什么是话术澄清？</h3><p>  问题有歧义或者匹配答案有置信度但不够高的时候触发话术澄清。简单来说，就是明确意图，意图不明确的时候可以反问用户以确认。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-7-1.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-7-1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>  意图图谱的节点代表一个个意图节点。这些“意图”之间的关系包括需求澄清（disambiguation）、需求细化（depth extension）、需求横向延展（breadth extension ）等。在图所示例子中，当“阿拉斯加”的意思是“阿拉斯加州”时，与之关联的意图是城市、旅游等信息。当“阿拉斯加”的含义是“阿拉斯加犬”时，它延伸的意图是宠物狗、宠物狗护理，以及如何喂食等。</p>
<h3 id="如何对结果进行排序打分？"><a href="#如何对结果进行排序打分？" class="headerlink" title="如何对结果进行排序打分？"></a>如何对结果进行排序打分？</h3><ul>
<li><p><strong>DRMM+PACRR</strong>【13】</p>
<pre><code> **DRMM+PACRR**是针对文档相关性排序的新模型，这几种模型基于此前的DRMM模型。具体来说，DRMM 模型使用的是上下文无关的term encoding编码方式，改进模型则借鉴了PACRR的思想，融合了n-grams 和不同方式编码的上下文信息。

 Context-sensitive Term Encodings构造qd相似度矩阵，卷积提取ngram信息。

 两层max-pooling获取最强相似信息并拼接。

 使用相同的MLP网络独立地计算每一个q-term encoding（矩阵的每一行）的分数，再通过一个线性层得到query与doc的相关性得分 。
</code></pre></li>
</ul>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-9-0.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-9-0.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>​    github地址：<a href="https://github.com/nlpaueb/deep-relevance-ranking">https://github.com/nlpaueb/deep-relevance-ranking</a></p>
<ul>
<li><p><strong>N-grams</strong></p>
<pre><code> **N-grams**是机器学习中NLP处理中的一个较为重要的语言模型，常用来做句子相似度比较，模糊查询，以及句子合理性，句子矫正等。

 如果你是一个玩LOL的人，那么当我说“正方形打野”、“你是真的皮”，“你皮任你皮”这些词或词组时，你应该能想到的下一个词可能是“大司马”，而不是“五五开”。如果你不是LOL玩家，没关系，当我说“上火”、“金罐”这两个词，你能想到的下一个词应该更可能“加多宝”，而不是“可口可乐”。
</code></pre><p> <strong>N-grams</strong>正是基于这样的想法，它的第一个特点是某个词的出现依赖于其他若干个词，第二个特点是我们获得的信息越多，预测越准确。我想说，我们每个人的大脑中都有一个N-gram模型，而且是在不断完善和训练的。我们的见识与经历，都在丰富着我们的阅历，增强着我们的联想能力。<br> <strong>N-grams</strong>模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。</p>
<pre><code> **N-grams中的概率计算**

 假设我们有一个有n个词组成的句子$S=(w1,w2,...,wn)$ , 如何衡量它的概率呢？让我们假设，每个单词$wi$都要依赖于从第一个单词$w1$到它之前一个单词$wi−1$的影响：
</code></pre><script type="math/tex; mode=display">
p(S)=p(w1w2...wn)=p(w1)p(w2|w1)...p(wn|wn1...w2w1)</script><p>这个衡量方法有两个缺陷：</p>
</li>
<li><p><strong>参数空间过大</strong>，概率$p(wn|wn-1…w2w1)$的参数有$O(n)$个。</p>
</li>
<li><strong>数据稀疏严重</strong>，词同时出现的情况可能没有，组合阶数高时尤其明显。 </li>
</ul>
<p>为了解决第一个问题，我们引入<strong>马尔科夫假设（Markov Assumption）</strong>：<strong>一个词的出现仅与它之前的若干个词有关</strong>。 </p>
<script type="math/tex; mode=display">
p(w1⋯wn)=∏p(wi∣wi−1⋯w1)≈∏p(wi∣wi−1⋯wi−N+1)</script><p>如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为 <strong>Bi-gram</strong>： </p>
<script type="math/tex; mode=display">
p(S)=p(w 
1

 w 
2

 ⋯w 
n

 )=p(w 
1

 )p(w 
2

 ∣w 
1

 )⋯p(w 
n

 ∣w 
n−1

 )</script><p>如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为 <strong>Tri-gram</strong>： </p>
<script type="math/tex; mode=display">
p(S)=p(w 
1

 w 
2

 ⋯w 
n

 )=p(w 
1

 )p(w 
2

 ∣w 
1

 )⋯p(w 
n

 ∣w 
n−1

 w 
n−2

 )</script><p>N-gram的$N$可以取很高，然而现实中一般 bi-gram 和 tri-gram 就够用了。</p>
<p>那么，如何计算其中的每一项条件概率 $p(wn∣wn−1⋯w2w1) $答案是<strong>极大似然估计（Maximum Likelihood Estimation，MLE）</strong>，即数频数： </p>
<script type="math/tex; mode=display">
p(w 
n

 ∣w 
n−1

 )= 
C(w 
n−1

 )
C(w 
n−1

 w 
n

 )</script><script type="math/tex; mode=display">
p(w 
n

 ∣w 
n−1

 w 
n−2

 )= 
C(w 
n−2

 w 
n−1

 )
C(w 
n−2

 w 
n−1

 w 
n

 )</script><script type="math/tex; mode=display">
p(w 
n

 ∣w 
n−1

 ⋯w 
2

 w 
1

 )= 
C(w 
1

 w 
2

 ⋯w 
n−1

 )
C(w 
1

 w 
2

 ⋯w 
n

 )</script><p><strong>具体地</strong>，以Bi-gram为例，我们有这样一个由三句话组成的语料库： </p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-9-1.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-9-1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>容易统计，“I”出现了3次，“I am”出现了2次，因此能计算概率： </p>
<script type="math/tex; mode=display">
p(am∣I)= 
3
2</script><p>同理，还能计算出如下概率： </p>
<p>$p(I∣<s>)=0.67p(do∣I)=0.33p(Sam∣am)=0.5p(not∣do)=1p(<s>∣Sam)=0.5p(like∣not)=1 $</s></s></p>
<h3 id="如何评估人机交互系统的效果？"><a href="#如何评估人机交互系统的效果？" class="headerlink" title="如何评估人机交互系统的效果？"></a>如何评估人机交互系统的效果？</h3><p>​    人机交互系统可通过有效问题数量、推荐Top1答案的准确率、推荐Top3答案的准确率、有效问题响应的准确率、知识覆盖率等指标衡量其效果。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">类型</th>
<th style="text-align:center">指标</th>
<th style="text-align:center">收集方式收集方式</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">问答评估指标</td>
<td style="text-align:center">有效问题数</td>
<td style="text-align:center">日志抽样统计</td>
</tr>
<tr>
<td style="text-align:center">问答评估指标</td>
<td style="text-align:center">Top1准确率</td>
<td style="text-align:center">日志抽样统计</td>
</tr>
<tr>
<td style="text-align:center">问答评估指标</td>
<td style="text-align:center">Top3准确率</td>
<td style="text-align:center">日志抽样统计</td>
</tr>
<tr>
<td style="text-align:center">问答评估指标</td>
<td style="text-align:center">有效问题响应准确率</td>
<td style="text-align:center">日志抽样统计</td>
</tr>
<tr>
<td style="text-align:center">问答评估指标</td>
<td style="text-align:center">知识覆盖率</td>
<td style="text-align:center">日志抽样统计</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">类型</th>
<th style="text-align:center">指标指标</th>
<th style="text-align:center">计算公式</th>
<th style="text-align:center">指标用途</th>
<th style="text-align:center">收集方式</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">总体指标</td>
<td style="text-align:center">请求量占比</td>
<td style="text-align:center">FAQ、任务型、寒暄FAQ机器人的请求量/总请求量</td>
<td style="text-align:center">运维</td>
<td style="text-align:center">日志可得出</td>
</tr>
<tr>
<td style="text-align:center">总体指标</td>
<td style="text-align:center">总用户量</td>
<td style="text-align:center">FAQ、任务型、寒暄引擎的独立访客数</td>
<td style="text-align:center">运维</td>
<td style="text-align:center">日志可得出</td>
</tr>
<tr>
<td style="text-align:center">总体指标</td>
<td style="text-align:center">解决率</td>
<td style="text-align:center">（点击解决button量+不点击）/FAQ咨询重量=1-点击未解决button量/FAQ总咨询量</td>
<td style="text-align:center">分析</td>
<td style="text-align:center">日志可得出</td>
</tr>
<tr>
<td style="text-align:center">总体指标</td>
<td style="text-align:center">未解决率</td>
<td style="text-align:center">点击解决button/FAQ总咨询量=1-解决率</td>
<td style="text-align:center">分析</td>
<td style="text-align:center">日志可得出</td>
</tr>
<tr>
<td style="text-align:center">总体指标</td>
<td style="text-align:center">转人工率</td>
<td style="text-align:center">FAQ转人工量/FAQ总咨询量</td>
<td style="text-align:center">分析</td>
<td style="text-align:center">日志可得出</td>
</tr>
<tr>
<td style="text-align:center">总体指标</td>
<td style="text-align:center">任务达成率</td>
<td style="text-align:center">业务办理成功量/业务办理请求量</td>
<td style="text-align:center">分析</td>
<td style="text-align:center">日志可得出</td>
</tr>
<tr>
<td style="text-align:center">总体指标</td>
<td style="text-align:center">平均会话时长</td>
<td style="text-align:center">FAQ、任务、向量对话时长/总用户数</td>
<td style="text-align:center">分析</td>
<td style="text-align:center">日志可得出</td>
</tr>
<tr>
<td style="text-align:center">总体指标</td>
<td style="text-align:center">平均对话轮数</td>
<td style="text-align:center">总对话论数/总用户数</td>
<td style="text-align:center">分析</td>
<td style="text-align:center">日志可得出</td>
</tr>
<tr>
<td style="text-align:center">总体指标</td>
<td style="text-align:center">断点问题分析</td>
<td style="text-align:center">转空客问题</td>
<td style="text-align:center">分析</td>
<td style="text-align:center">日志可得出</td>
</tr>
<tr>
<td style="text-align:center">总体指标</td>
<td style="text-align:center">无应答问题分析</td>
<td style="text-align:center">机器人返回兜底话术的问题</td>
<td style="text-align:center">分析</td>
<td style="text-align:center">日志可得出</td>
</tr>
<tr>
<td style="text-align:center">总体指标</td>
<td style="text-align:center">热点问题分析</td>
<td style="text-align:center">问题咨询总量TopN</td>
<td style="text-align:center">分析</td>
<td style="text-align:center">日志可得出</td>
</tr>
</tbody>
</table>
</div>
<h2 id="如何设计个性化推荐系统？"><a href="#如何设计个性化推荐系统？" class="headerlink" title="如何设计个性化推荐系统？"></a>如何设计个性化推荐系统？</h2><h3 id="什么是个性化推荐系统？"><a href="#什么是个性化推荐系统？" class="headerlink" title="什么是个性化推荐系统？"></a>什么是个性化推荐系统？</h3><p>​    个性化推荐系统就是根据用户的历史，社交关系，兴趣点，上下文环境等信息去判断用户当前需要或潜在感兴趣的内容的一类应用。 </p>
<p>​    大数据时代，我们的生活的方方面面都出现了信息过载的问题：电子商务、电影或者视频网站、个性化音乐网络电台、社交网络、个性化阅读、基于位置的服务、个性化邮件、个性化广告…….逛淘宝、订外卖、听网络电台、看剧等等等。推荐系统在你不知不觉中将你感兴趣的内容推送给你，甚至有的时候，推荐系统比你本人更了解你自己。 </p>
<p>​    推荐系统的业务主要包括四个部分：</p>
<ul>
<li><p>物料组装：生产广告，实现文案、图片等内容的个性化</p>
</li>
<li><p>物料召回：在大量内容中召回一个子集作为推荐的内容</p>
</li>
<li>物料排序：将召回的子集的内容按照某种标准进行精细排序</li>
<li><p>运营策略：加入一些运营策略进行一部分的重新排序，再下发内容</p>
<pre><code>      推荐系统必须要实现收集与分析数据的功能。数据收集体现为：埋点、上报、存储。而数据分析则体现为：构造画像（用户与内容）、行为归因。 
</code></pre><p>  ​    推荐系统的算法体现在两部分：<strong>召回、排序</strong>。召回的算法多种多样：itemCF、userCF、关联规则、embedding、序列匹配、同类型收集等等。排序的算法可以从多个角度来描述，这里我们从一个宏观的角度来描述，即排序算法可以分成五个部分：<strong>构造样本、设计模型、确定目标函数、选择优化方法、评估</strong>。 </p>
</li>
</ul>
<h3 id="如何设计个性化推荐系统的推荐引擎架构？"><a href="#如何设计个性化推荐系统的推荐引擎架构？" class="headerlink" title="如何设计个性化推荐系统的推荐引擎架构？"></a>如何设计个性化推荐系统的推荐引擎架构？</h3><p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-2-1.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-2-1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="召回模块"><a href="#召回模块" class="headerlink" title="召回模块"></a>召回模块</h3><ul>
<li>热点召回和人工运营：兜底策略</li>
<li>用户画像（CB）召回：标签排序、倒排截取</li>
<li><p>CF召回算法：user-based算法和item-based算法</p>
</li>
<li><p>如何做大规模在线用户CF召回？</p>
<ul>
<li>离线计算每个用户的相似用户top k，存入cache</li>
<li>在线存储每个用户的点击记录</li>
<li>在线检索相似用户点击记录</li>
</ul>
</li>
</ul>
<h3 id="排序模块"><a href="#排序模块" class="headerlink" title="排序模块"></a>排序模块</h3><ul>
<li>模型选择：LR、FM、GBDT、DNN …</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>复杂特征+简单模型</th>
<th>简单特征+复杂模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>线性模型：LR</td>
<td>非线性模型：GBDT,DNN</td>
</tr>
<tr>
<td>训练快，解析性好</td>
<td>表达能力强，起点高</td>
</tr>
<tr>
<td>在线预测简单</td>
<td>训练慢，解析性差</td>
</tr>
<tr>
<td>人工构造大规模特征才能提升效果，后期难维护</td>
<td>容易过拟合，难优化</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p>排序算法演进</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-2-2.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-2-2.jpg" srcset="data:image/png;base64,666" alt=""></p>
</li>
</ul>
<h3 id="离线训练"><a href="#离线训练" class="headerlink" title="离线训练"></a>离线训练</h3><ul>
<li><p>离线训练流程</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-5-1.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-5-1.jpg" srcset="data:image/png;base64,666" alt=""></p>
</li>
<li><p>如何线上实时反馈特征？</p>
<ul>
<li>在线计算，与曝光日志一起上报，离线直接使用</li>
</ul>
</li>
<li><p>如何解决曝光不足问题？</p>
<ul>
<li><p>使用CTR的贝叶斯平滑（CTR = 曝光次数 / 点击次数）【15】</p>
<blockquote>
<ul>
<li><p>所有新闻自身CTR(r)服从Beta分布:</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-5-2.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-5-2.png" srcset="data:image/png;base64,666" alt=""></p>
</li>
<li><p>某一新闻，给定展示次数时和自身CTR,点击次数服从伯努利分布，曝光次数为I,点击次数为C:</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-5-3.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-5-3.png" srcset="data:image/png;base64,666" alt=""></p>
</li>
<li><p>对最大似然函数求解参数α，β，则i新闻CTR后验估计：</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-6.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-5-6-6.png" srcset="data:image/png;base64,666" alt=""></p>
</li>
<li><p>对曝光不足的做平滑，曝光充分的影响不大</p>
</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ul>
<h3 id="用户画像"><a href="#用户画像" class="headerlink" title="用户画像"></a>用户画像</h3><ul>
<li>用户标签</li>
<li><p>统计方法</p>
<ul>
<li>用户feeds内行为，标签 计数（点击率），缺点：无法加入更多特征，不方便后续优化</li>
</ul>
</li>
<li><p>基于机器学习的方法</p>
<ul>
<li>对用户长期兴趣建模</li>
<li>LR模型</li>
<li>用户标签作为特征</li>
</ul>
</li>
</ul>
<h3 id="GBDT粗排"><a href="#GBDT粗排" class="headerlink" title="GBDT粗排"></a>GBDT粗排</h3><ul>
<li>为什么需要粗排？<ul>
<li>快速筛选高质量的候选集</li>
<li>方便利用在线实时反馈特征</li>
</ul>
</li>
<li>如何做粗排的特征设计？<ul>
<li>特征要相对稠密</li>
</ul>
</li>
<li>如何选择合适的算法模型？<ul>
<li>lightgbm</li>
<li>xgboost</li>
<li>lightgbm比xgboot速度更快；在线预测时，线程更安全</li>
</ul>
</li>
</ul>
<h3 id="在线FM精排"><a href="#在线FM精排" class="headerlink" title="在线FM精排"></a>在线FM精排</h3><ul>
<li><p>为什么需要在线学习？</p>
<ul>
<li>feeds内容更新快</li>
<li>用户兴趣会随时间变化</li>
<li>排序模型需要快速反应用户的兴趣变化</li>
</ul>
</li>
<li><p><strong>FM模型</strong></p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-8-1.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-8-1.png" srcset="data:image/png;base64,666" alt=""></p>
<script type="math/tex; mode=display">
</script><script type="math/tex; mode=display">
𝜎(𝑦)=1/(1+exp⁡(−𝑦))</script></li>
<li><p><strong>采用FTRL(Follow The Regularized Leader)更新模型</strong></p>
<ul>
<li><p>算法概述 </p>
<p>​    FTRL是一种适用于处理超大规模数据的,含大量稀疏特征的在线学习的常见优化算法，方便实用，而且效果很好，常用于更新在线的CTR预估模型。</p>
<p>　　FTRL算法兼顾了FOBOS和RDA两种算法的优势，既能同FOBOS保证比较高的精度，又能在损失一定精度的情况下产生更好的稀疏性。</p>
<p>　　FTRL在处理带非光滑正则项（如L1正则）的凸优化问题上表现非常出色，不仅可以通过L1正则控制模型的稀疏度，而且收敛速度快。</p>
</li>
<li><p>算法要点与推导 </p>
</li>
<li><p>算法特性及优缺点 </p>
<p>​    在线学习，实时性高；可以处理大规模稀疏数据；有大规模模型参数训练能力；根据不同的特征特征学习率 。</p>
<p>​    FTRL-Proximal工程实现上的tricks：</p>
<p>　　1.saving memory</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">  　	方案1）Poisson Inclusion：对某一维度特征所来的训练样本，以p的概率接受并更新模型。</span><br><span class="line">    　	方案2）Bloom Filter Inclusion：用bloom filter从概率上做某一特征出现k次才更新。</span><br></pre></td></tr></tbody></table></figure>
<p>　　2.浮点数重新编码</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">1）特征权重不需要用32bit或64bit的浮点数存储，存储浪费空间。</span><br><span class="line"> 	2）16bit encoding，但是要注意处理rounding技术对regret带来的影响(注：python可以尝试用numpy.float16格式)</span><br></pre></td></tr></tbody></table></figure>
<p>　　3.训练若干相似model</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">1）对同一份训练数据序列，同时训练多个相似的model。</span><br><span class="line"> 	2）这些model有各自独享的一些feature，也有一些共享的feature。</span><br><span class="line"> 	3）出发点：有的特征维度可以是各个模型独享的，而有的各个模型共享的特征，可以用同样的数据训练。</span><br></pre></td></tr></tbody></table></figure>
<p>　　4.Single Value Structure</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">1）多个model公用一个feature存储（例如放到cbase或redis中），各个model都更新这个共有的feature结构。</span><br><span class="line"> 	2）对于某一个model，对于他所训练的特征向量的某一维，直接计算一个迭代结果并与旧值做一个平均。</span><br></pre></td></tr></tbody></table></figure>
<p>　　5.使用正负样本的数目来计算梯度的和（所有的model具有同样的N和P）</p>
<p>　　6.subsampling Training Data</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">1）在实际中，CTR远小于50%，所以正样本更加有价值。通过对训练数据集进行subsampling，可以大大减小训练数据集的大小。</span><br><span class="line">   	2）正样本全部采（至少有一个广告被点击的query数据），负样本使用一个比例r采样（完全没有广告被点击的query数据）。但是直接在这种采样上进行训练，会导致比较大的biased prediction。</span><br><span class="line"> 	3）解决办法：训练的时候，对样本再乘一个权重。权重直接乘到loss上面，从而梯度也会乘以这个权重。</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>适合场景</p>
<p>点击率模型 </p>
</li>
<li><p>案例</p>
<p><a href="">https://www.kaggle.com/jiweiliu/ftrl-starter-code/output</a></p>
<p><a href="https://github.com/Angel-ML/angel/blob/master/docs/algo/ftrl_lr_spark.md">https://github.com/Angel-ML/angel/blob/master/docs/algo/ftrl_lr_spark.md</a></p>
</li>
</ul>
</li>
<li><p>如何选择精排特征？</p>
<ul>
<li>新增特征需保证已有特征索引不变</li>
<li>定期离线训练淘汰无用特征，防止特征无线膨胀</li>
<li>使用GBDT粗排预测的CTR分段结果作为特征</li>
</ul>
</li>
</ul>
<h3 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h3><ul>
<li><p>协同过滤算法</p>
<p>​    协同过滤(Collaborative filtering, CF)算法是目前个性化推荐系统比较流行的算法之一。</p>
<p>​    协同算法分为两个基本算法：<strong>基于用户的协同过滤（UserCF）和基于项目的协同过滤（ItemCF）。</strong> </p>
</li>
<li><p>基于属性的推荐算法</p>
<ul>
<li><p>基于用户标签的推荐</p>
<p>​    统计用户最常用的标签，对于每个标签，统计被打过这个标签次数最多的物品，然后将具有这些标签的最热门的物品推荐给这个用户。这个方法非常适合新用户或者数据很少的冷启动，目前许多的app都会在新用户最初进入时让用户添加喜好标签方便为用户推送内容。 </p>
</li>
<li><p>基于商品内容的推荐算法</p>
<p>​    利用商品的内容属性计算商品之间的相似度，是物推物的算法。这种算法不依赖用户行为，只要获取到item的内容信息就可以计算语义级别上的相似性，不存在iterm冷启动问题。缺点就是不是所有iterm都可以非常容易的抽取成有意义的特征，而且中文一词多义和一义多词的复杂性也是需要攻克的一个难题。</p>
</li>
</ul>
</li>
<li><p>基于矩阵分解的推荐算法</p>
<p>​    原理：根据已有的评分矩阵（非常稀疏），分解为低维的用户特征矩阵（评分者对各个因子的喜好程度）以及商品特征矩阵（商品包含各个因子的程度），最后再反过来分析数据（用户特征矩阵与商品特征矩阵相乘得到新的评分矩阵）得出预测结果；这是一个非常优雅的推荐算法，因为当涉及到矩阵分解时，我们通常不会太多地去思考哪些项目将停留在所得到矩阵的列和行中。但是使用这个推荐引擎，我们清楚地看到，u是第i个用户的兴趣向量，v是第j个电影的参数向量。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-2-4.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-2-4.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    所以我们可以用u和v的点积来估算x(第i个用户对第j个电影的评分)。我们用已知的分数构建这些向量，并使用它们来预测未知的得分。</p>
<p>​    例如，在矩阵分解之后，Ted的向量是(1.4; .8)，电影A的向量是(1.4; .9)，现在，我们可以通过计算(1.4; .8)和(1.4; .9)的点积，来还原电影A-Ted的得分。结果，我们得到2.68分。</p>
<p> <img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-2-5.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-2-5.png" srcset="data:image/png;base64,666" alt=""></p>
</li>
<li><p>基于热门内容的推荐算法</p>
<p>​    为用户推荐流行度高的物品，或者说新热物品。例如最近北方天气突然降温，一大堆用户开始在淘宝搜索购买大衣或者羽绒服，淘宝就会为北方用户推荐大衣。55度杯新出时，所有人都会搜索购买，然后用户的瀑布流中就会出现55度杯。流行度算法很好的解决冷启动问题，但推荐的物品有限，不能很好的命中用户的兴趣点；其推荐列表通常会作为候补列表推荐给用户；在微博、新闻等产品推荐时是常用的方法。基本流程就是：确定物品的流行周期，计算物品在流行周期内的流行度，流行度高的物品作为被推荐的物品。</p>
</li>
</ul>
<h3 id="如何评价个性化推荐系统的效果？"><a href="#如何评价个性化推荐系统的效果？" class="headerlink" title="如何评价个性化推荐系统的效果？"></a>如何评价个性化推荐系统的效果？</h3><ul>
<li><strong>准确率与召回率（Precision &amp; Recall）</strong></li>
</ul>
<p>  准确率和召回率是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量。其中精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率；召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-3-0.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-3-0.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>  一般来说，Precision就是检索出来的条目（比如：文档、网页等）有多少是准确的，Recall就是所有准确的条目有多少被检索出来了。</p>
<p>  正确率、召回率和 F 值是在鱼龙混杂的环境中，选出目标的重要评价指标。不妨看看这些指标的定义先：</p>
<pre><code>正确率 = 提取出的正确信息条数 /  提取出的信息条数 

召回率 = 提取出的正确信息条数 /  样本中的信息条数    
</code></pre><p>  两者取值在0和1之间，数值越接近1，查准率或查全率就越高。   </p>
<pre><code>F值  = 正确率 * 召回率 * 2 / (正确率 + 召回率) （F 值即为正确率和召回率的调和平均值）
</code></pre><p>  不妨举这样一个例子：某池塘有1400条鲤鱼，300只虾，300只鳖。现在以捕鲤鱼为目的。撒一大网，逮着了700条鲤鱼，200只虾，100只鳖。那么，这些指标分别如下：</p>
<pre><code>正确率 = 700 / (700 + 200 + 100) = 70%

召回率 = 700 / 1400 = 50%

F值 = 70% * 50% * 2 / (70% + 50%) = 58.3%
</code></pre><p>  不妨看看如果把池子里的所有的鲤鱼、虾和鳖都一网打尽，这些指标又有何变化：</p>
<pre><code>正确率 = 1400 / (1400 + 300 + 300) = 70%

召回率 = 1400 / 1400 = 100%

F值 = 70% * 100% * 2 / (70% + 100%) = 82.35%        
</code></pre><p>  由此可见，正确率是评估捕获的成果中目标成果所占得比例；召回率，顾名思义，就是从关注领域中，召回目标类别的比例；而F值，则是综合这二者指标的评估指标，用于综合反映整体的指标。</p>
<p>  当然希望检索结果Precision越高越好，同时Recall也越高越好，但事实上这两者在某些情况下有矛盾的。比如极端情况下，我们只搜索出了一个结果，且是准确的，那么Precision就是100%，但是Recall就很低；而如果我们把所有结果都返回，那么比如Recall是100%，但是Precision就会很低。因此在不同的场合中需要自己判断希望Precision比较高或是Recall比较高。如果是做实验研究，可以绘制Precision-Recall曲线来帮助分析。</p>
<p>​    <strong>注意：准确率和召回率是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下准确率高、召回率就低，召回率低、准确率高，当然如果两者都低，那是什么地方出问题了</strong>。一般情况，用不同的阀值，统计出一组不同阀值下的精确率和召回率，如下图： </p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-3-1.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-3-1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>如果是做搜索，那就是保证召回的情况下提升准确率；如果做疾病监测、反垃圾，则是保准确率的条件下，提升召回。</strong></p>
<p>所以，在两者都要求高的情况下，可以用F1值来衡量。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-9-2-2.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-9-2-2.png" srcset="data:image/png;base64,666" alt="F-Measure"></p>
<p>公式基本上就是这样，但是如何算图中的A、B、C、D呢？<strong>这需要人工标注，人工标注数据需要较多时间且枯燥，如果仅仅是做实验可以用用现成的语料。当然，还有一个办法，找个一个比较成熟的算法作为基准，用该算法的结果作为样本来进行比照</strong>，这个方法也有点问题，如果有现成的很好的算法，就不用再研究了。 </p>
<ul>
<li><strong>综合评价指标（F-Measure）</strong></li>
</ul>
<p>  P和R指标有时候会出现的矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure（又称为F-Score）。</p>
<p>  F-Measure是Precision和Recall加权调和平均：</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-9-2-1.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-9-2-1.png" srcset="data:image/png;base64,666" alt="F-Measure"></p>
<p>  当参数α=1时，就是最常见的F1，也即</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-9-2-2.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-9-2-2.png" srcset="data:image/png;base64,666" alt="F-Measure"></p>
<p>  可知F1综合了P和R的结果，当F1较高时则能说明试验方法比较有效。</p>
<ul>
<li><strong>E值</strong></li>
</ul>
<p>  E值表示查准率P和查全率R的加权平均值，当其中一个为0时，E值为1，其计算公式：</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-9-3-1.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-9-3-1.png" srcset="data:image/png;base64,666" alt="E值"></p>
<p>  b越大，表示查准率的权重越大。</p>
<ul>
<li><p><strong>平均正确率（Average Precision）</strong></p>
<p>  平均正确率表示不同查全率的点上的正确率的平均。</p>
</li>
<li><p><strong>AP和mAP(mean Average Precision)</strong> </p>
<p>​    mAP是为解决P（准确率），R（召回率），F-measure的单点值局限性的。为了得到 一个能够反映全局性能的指标，可以看考察下图，其中两条曲线(方块点与圆点)分布对应了两个检索系统的准确率-召回率曲线 。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-3-2.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-3-2.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>​    可以看出，虽然两个系统的性能曲线有所交叠但是以圆点标示的系统的性能在绝大多数情况下要远好于用方块标示的系统。</p>
<p>​    从中我们可以 发现一点，如果一个系统的性能较好，其曲线应当尽可能的向上突出。</p>
<p>​    更加具体的，曲线与坐标轴之间的面积应当越大。</p>
<p>​    最理想的系统， 其包含的面积应当是1，而所有系统的包含的面积都应当大于0。这就是用以评价信息检索系统的最常用性能指标，平均准确率mAP其规范的定义如下:</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-3-3.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-3-3.jpg" srcset="data:image/png;base64,666" alt=""></p>
</li>
<li><p><strong>ROC和AUC</strong> </p>
<p>​    ROC和AUC是评价分类器的指标，上面第一个图的ABCD仍然使用，只是需要稍微变换。 </p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-3-4.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-3-4.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>ROC关注两个指标</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">True  Positive Rate( TPR ) = TP / [ TP + FN] ，TPR代表能将正例分对的概率</span><br><span class="line">False Positive Rate( FPR ) = FP / [ FP + TN] ，FPR代表将负例错分为正例的概率</span><br></pre></td></tr></tbody></table></figure>
<p>​    在ROC 空间中，每个点的横坐标是FPR，纵坐标是TPR，这也就描绘了分类器在TP（真正的正例）和FP（错误的正例）间的trade-off。ROC的主要分析工具是一个画在ROC空间的曲线——ROC curve。我们知道，对于二值分类问题，实例的值往往是连续值，我们通过设定一个阈值，将实例分类到正类或者负类（比如大于阈值划分为正类）。因此我们可以变化阈值，根据不同的阈值进行分类，根据分类结果计算得到ROC空间中相应的点，连接这些点就形成ROC curve。ROC curve经过（0,0）（1,1），实际上(0, 0)和(1, 1)连线形成的ROC curve实际上代表的是一个随机分类器。一般情况下，这个曲线都应该处于(0, 0)和(1, 1)连线的上方。如图所示。 </p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-3-5.jpg" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-3-5.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>​    用ROC curve来表示分类器的performance很直观好用。可是，人们总是希望能有一个数值来标志分类器的好坏。</p>
<p>​    于是<strong>Area Under roc Curve(AUC)</strong>就出现了。顾名思义，AUC的值就是处于ROC curve下方的那部分面积的大小。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。</p>
<p>​    AUC计算工具：<a href="http://mark.goadrich.com/programs/AUC/">http://mark.goadrich.com/programs/AUC/</a></p>
<p>​    <strong>P/R和ROC是两个不同的评价指标和计算方式，一般情况下，检索用前者，分类、识别等用后者。</strong></p>
</li>
</ul>
<h3 id="个性化推荐系统案例分析"><a href="#个性化推荐系统案例分析" class="headerlink" title="个性化推荐系统案例分析"></a>个性化推荐系统案例分析</h3><p>​    在过去的十年中，神经网络已经取得了巨大的飞跃。如今，神经网络已经得以广泛应用，并逐渐取代传统的机器学习方法。 接下来，我要介绍一下YouTube如何使用深度学习方法来做个性化推荐。 </p>
<p>​    由于体量庞大、动态库和各种观察不到的外部因素，为YouTube用户提供推荐内容是一项非常具有挑战性的任务。 </p>
<p>​    <em>YouTube的推荐系统算法由两个神经网络组成：一个用于候选生成，一个用于排序。如果你没时间仔细研究论文，可以看看我们下面给出的简短总结。</em> 【14】</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-4-1.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-4-1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    以用户的浏览历史为输入，候选生成网络可以显著减小可推荐的视频数量，从庞大的库中选出一组最相关的视频。这样生成的候选视频与用户的相关性最高，然后我们会对用户评分进行预测。</p>
<p>​    这个网络的目标，只是通过协同过滤提供更广泛的个性化。</p>
<p>​    <img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-4-2.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-4-2.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    进行到这一步，我们得到一组规模更小但相关性更高的内容。我们的目标是仔细分析这些候选内容，以便做出最佳的选择。</p>
<p>​    这个任务由排序网络完成。</p>
<p>​    所谓排序就是根据视频描述数据和用户行为信息，使用设计好的目标函数为每个视频打分，得分最高的视频会呈献给用户。</p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-4-3.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-4-3.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    通过这两步，我们可以从非常庞大的视频库中选择视频，并面向用户进行有针对性的推荐。这个方法还能让我们把其他来源的内容也容纳进来。 </p>
<p><img src="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-4-4.png" class="lazyload" data-srcset="/zh-TW/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/18-6-4-4.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    推荐任务是一个极端的多类分类问题。这个预测问题的实质，是基于用户(U)和语境(C)，在给定的时间t精确地从库(V)中上百万的视频类(i)中，对特定的视频观看(Wt)情况进行分类。 </p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>模型压缩及移动端部署</title>
    <url>/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="模型压缩及移动端部署"><a href="#模型压缩及移动端部署" class="headerlink" title="模型压缩及移动端部署"></a>模型压缩及移动端部署</h1><p>​    深度神经网络在人工智能的应用中，包括语音识别、计算机视觉、自然语言处理等各方面，在取得巨大成功的同时，这些深度神经网络需要巨大的计算开销和内存开销，严重阻碍了资源受限下的使用。本章总结了模型压缩、加速一般原理和方法，以及在移动端如何部署。</p>
<h2 id="模型压缩理解"><a href="#模型压缩理解" class="headerlink" title="模型压缩理解"></a>模型压缩理解</h2><p>​    模型压缩是指利用数据集对已经训练好的深度模型进行精简，进而得到一个轻量且准确率相当的网络，压缩后的网络具有更小的结构和更少的参数，可以有效降低计算和存储开销，便于部署再受限的硬件环境中。</p>
<h2 id="为什么需要模型压缩和加速？"><a href="#为什么需要模型压缩和加速？" class="headerlink" title="为什么需要模型压缩和加速？"></a>为什么需要模型压缩和加速？</h2><p>（1）随着AI技术的飞速发展，越来越多的公司希望在自己的移动端产品中注入AI能力。</p>
<p>（2）对于在线学习和增量学习等实时应用而言，如何减少含有大量层级及结点的大型神经网络所需要的内存和计算量显得极为重要。  </p>
<p>（3）模型的参数在一定程度上能够表达其复杂性,相关研究表明,并不是所有的参数都在模型中发挥作用,部分参数作用有限、表达冗余,甚至会降低模型的性能。</p>
<p>（4）复杂的模型固然具有更好的性能，但是高额的存储空间、计算资源消耗是使其难以有效的应用在各硬件平台上的重要原因。</p>
<p>（5）智能设备的流行提供了内存、CPU、能耗和宽带等资源，使得深度学习模型部署在智能移动设备上变得可行。<br>（6）高效的深度学习方法可以有效的帮助嵌入式设备、分布式系统完成复杂工作，在移动端部署深度学习有很重要的意义。   </p>
<h2 id="模型压缩的必要性及可行性"><a href="#模型压缩的必要性及可行性" class="headerlink" title="模型压缩的必要性及可行性"></a>模型压缩的必要性及可行性</h2><div class="table-container">
<table>
<thead>
<tr>
<th>必要性</th>
<th>首先是资源受限，其次在许多网络结构中，如VGG-16网络，参数数量1亿3千多万，占用500MB空间，需要进行309亿次浮点运算才能完成一次图像识别任务。</th>
</tr>
</thead>
<tbody>
<tr>
<td>可行性</td>
<td>模型的参数在一定程度上能够表达其复杂性,相关研究表明,并不是所有的参数都在模型中发挥作用,部分参数作用有限、表达冗余,甚至会降低模型的性能。论文<predicting parameters="" in="" deep="" learning="">提出，很多的深度神经网络仅仅使用很少一部分（5%）权值就足以预测剩余的权值。该论文还提出这些剩下的权值甚至可以直接不用被学习。也就是说，仅仅训练一小部分原来的权值参数就有可能达到和原来网络相近甚至超过原来网络的性能（可以看作一种正则化）。</predicting></td>
</tr>
<tr>
<td>最终目的</td>
<td>最大程度的减小模型复杂度，减少模型存储需要的空间，也致力于加速模型的训练和推测</td>
</tr>
</tbody>
</table>
</div>
<h2 id="目前有哪些深度学习模型压缩方法？"><a href="#目前有哪些深度学习模型压缩方法？" class="headerlink" title="目前有哪些深度学习模型压缩方法？"></a>目前有哪些深度学习模型压缩方法？</h2><p>​    目前深度学习模型压缩方法主要分为更精细化模型设计、模型裁剪、核的稀疏化、量化、低秩分解、迁移学习等方法，而这些方法又可分为前端压缩和后端压缩。</p>
<h3 id="前端压缩和后端压缩对比"><a href="#前端压缩和后端压缩对比" class="headerlink" title="前端压缩和后端压缩对比"></a>前端压缩和后端压缩对比</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">对比项目</th>
<th style="text-align:center">前端压缩</th>
<th style="text-align:center">后端压缩</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">含义</td>
<td style="text-align:center">不会改变原始网络结构的压缩技术</td>
<td style="text-align:center">会大程度上改变原始网络结构的压缩技术</td>
</tr>
<tr>
<td style="text-align:center">主要方法</td>
<td style="text-align:center">知识蒸馏、紧凑的模型结构设计、滤波器层面的剪枝</td>
<td style="text-align:center">低秩近似、未加限制的剪枝、参数量化、二值网络</td>
</tr>
<tr>
<td style="text-align:center">实现难度</td>
<td style="text-align:center">较简单</td>
<td style="text-align:center">较难</td>
</tr>
<tr>
<td style="text-align:center">是否可逆</td>
<td style="text-align:center">可逆</td>
<td style="text-align:center">不可逆</td>
</tr>
<tr>
<td style="text-align:center">成熟应用</td>
<td style="text-align:center">剪枝</td>
<td style="text-align:center">低秩近似、参数量化</td>
</tr>
<tr>
<td style="text-align:center">待发展应用</td>
<td style="text-align:center">知识蒸馏</td>
<td style="text-align:center">二值网络</td>
</tr>
</tbody>
</table>
</div>
<h3 id="网络剪枝"><a href="#网络剪枝" class="headerlink" title="网络剪枝"></a>网络剪枝</h3><p>深度学习模型因其<strong>稀疏性</strong>，可以被裁剪为结构精简的网络模型，具体包括结构性剪枝与非结构性剪枝。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
<th>举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>非结构化剪枝</td>
<td>通常是连接级、细粒度的剪枝方法，精度相对较高，但依赖于特定算法库或硬件平台的支持</td>
<td>Deep Compression [5], Sparse-Winograd [6] 算法等；</td>
</tr>
<tr>
<td>结构化剪枝</td>
<td>是filter级或layer级、粗粒度的剪枝方法，精度相对较低，但剪枝策略更为有效，不需要特定算法库或硬件平台的支持，能够直接在成熟深度学习框架上运行。</td>
<td>如局部方式的、通过layer by layer方式的、最小化输出FM重建误差的Channel Pruning [7], ThiNet [8], Discrimination-aware Channel Pruning [9]；全局方式的、通过训练期间对BN层Gamma系数施加L1正则约束的Network Slimming [10]；全局方式的、按Taylor准则对Filter作重要性排序的Neuron Pruning [11]；全局方式的、可动态重新更新pruned filters参数的剪枝方法 [12];<br><a href="https://blog.csdn.net/baidu_31437863/article/details/84474847">https://blog.csdn.net/baidu_31437863/article/details/84474847</a></td>
</tr>
</tbody>
</table>
</div>
<p>如果按剪枝粒度分，从粗到细，可分为中间隐含层剪枝、通道剪枝、卷积核剪枝、核内剪枝、单个权重剪枝。下面按照剪枝粒度的分类从粗（左）到细（右）。</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/剪枝粒度分类.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/剪枝粒度分类.png" srcset="data:image/png;base64,666" alt=""></p>
<p>（a）层间剪枝   （b）特征图剪枝    （c）k*k核剪枝   （d）核内剪枝</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>单个权重粒度</td>
<td>早期 Le Cun[16]提出的 OBD(optimal brain damage)将网络中的任意权重参数都看作单个参数,能够有效地提高预测准确率,却不能减小运行时间;同时,剪枝代价过高,只适用于小网络</td>
</tr>
<tr>
<td>核内权重粒度</td>
<td>网络中的任意权重被看作是单个参数并进行随机非结构化剪枝,该粒度的剪枝导致网络连接不规整,需要通过稀疏表达来减少内存占用,进而导致在前向传播预测时,需要大量的条件判断和额外空间来标明零或非零参数的位置,因此不适用于并行计算</td>
</tr>
<tr>
<td>卷积核粒度与通道粒度</td>
<td>卷积核粒度与通道粒度属于粗粒度剪枝,不依赖任何稀疏卷积计算库及专用硬件;同时,能够在获得高压缩率的同时大量减小测试阶段的计算时间.由</td>
</tr>
</tbody>
</table>
</div>
<p>从剪枝目标上分类，可分为减少参数/网络复杂度、减少过拟合/增加泛化能力/提高准确率、减小部署运行时间/提高网络效率及减小训练时间等。</p>
<h3 id="典型剪枝方法对比"><a href="#典型剪枝方法对比" class="headerlink" title="典型剪枝方法对比"></a>典型剪枝方法对比</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">剪枝方法</th>
<th style="text-align:center">修剪对象</th>
<th style="text-align:center">修剪方式</th>
<th style="text-align:center">效果</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Deep Compression</td>
<td style="text-align:center">权重</td>
<td style="text-align:center">随机修剪</td>
<td style="text-align:center">50倍压缩</td>
</tr>
<tr>
<td style="text-align:center">Structured Pruning</td>
<td style="text-align:center">权重</td>
<td style="text-align:center">组稀疏+排他性稀疏</td>
<td style="text-align:center">性能提升</td>
</tr>
<tr>
<td style="text-align:center">Network Slimming</td>
<td style="text-align:center">特征图通道</td>
<td style="text-align:center">根据尺度因子修剪</td>
<td style="text-align:center">节省计算资源</td>
</tr>
<tr>
<td style="text-align:center">mProp</td>
<td style="text-align:center">梯度</td>
<td style="text-align:center">修剪幅值小的梯度</td>
<td style="text-align:center">加速</td>
</tr>
</tbody>
</table>
</div>
<h3 id="网络蒸馏"><a href="#网络蒸馏" class="headerlink" title="网络蒸馏"></a>网络蒸馏</h3><p>​    网络精馏是指利用大量未标记的迁移数据(transfer data),让小模型去拟合大模型,从而让小模型学到与大模型相似的函数映射.网络精馏可以看成在同一个域上迁移学习[34]的一种特例,目的是获得一个比原模型更为精简的网络,整体的框架图如图 4所示. </p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/网络蒸馏.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/网络蒸馏.png" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="前端压缩"><a href="#前端压缩" class="headerlink" title="前端压缩"></a>前端压缩</h3><p>（1）知识蒸馏</p>
<p>​    一个复杂模型可由多个简单模型或者强约束条件训练得到。复杂模型特点是性能好，但其参数量大，计算效率低。小模型特点是计算效率高，但是其性能较差。知识蒸馏是让复杂模型学习到的知识迁移到小模型当中,使其保持其快速的计算速度前提下，同时拥有复杂模型的性能，达到模型压缩的目的。<br>（2）紧凑的模型结构设计<br>​    紧凑的模型结构设计主要是对神经网络卷积的方式进行改进，比如使用两个3x3的卷积替换一个5x5的卷积、使用深度可分离卷积等等方式降低计算参数量。  目前很多网络基于模块化设计思想，在深度和宽度两个维度上都很大，导致参数冗余。因此有很多关于模型设计的研究，如SqueezeNet、MobileNet等，使用更加细致、高效的模型设计，能够很大程度的减少模型尺寸，并且也具有不错的性能。<br>（3）滤波器层面的剪枝<br>​    滤波器层面的剪枝属于非结构花剪枝，主要是对较小的权重矩阵整个剔除，然后对整个神经网络进行微调。此方式由于剪枝过于粗放，容易导致精度损失较大，而且部分权重矩阵中会存留一些较小的权重造成冗余，剪枝不彻底。  具体操作是在训练时使用稀疏约束（加入权重的稀疏正则项，引导模型的大部分权重趋向于0）。完成训练后，剪去滤波器上的这些 0 。</p>
<p>​    优点是简单，缺点是剪得不干净，非结构化剪枝会增加内存访问成本。</p>
<h3 id="后端压缩"><a href="#后端压缩" class="headerlink" title="后端压缩"></a>后端压缩</h3><p>（1）低秩近似<br>​    在卷积神经网络中，卷积运算都是以矩阵相乘的方式进行。对于复杂网络，权重矩阵往往非常大，非常消耗存储和计算资源。低秩近似就是用若干个低秩矩阵组合重构大的权重矩阵，以此降低存储和计算资源消耗。  </p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">事项</th>
<th style="text-align:left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">优点</td>
<td style="text-align:left">可以降低存储和计算消耗；<br>一般可以压缩2-3倍；精度几乎没有损失；</td>
</tr>
<tr>
<td style="text-align:left">缺点</td>
<td style="text-align:left">模型越复杂，权重矩阵越大，利用低秩近似重构参数矩阵不能保证模型的性能 ；   <br>超参数的数量随着网络层数的增加呈线性变化趋势，例如中间层的特征通道数等等。 <br>随着模型复杂度的提升，搜索空间急剧增大。</td>
</tr>
</tbody>
</table>
</div>
<p>（2）未加限制的剪枝    </p>
<p>​    完成训练后，不加限制地剪去那些冗余参数。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>保持模型性能不损失的情况下，减少参数量9-11倍； <br>剔除不重要的权重，可以加快计算速度，同时也可以提高模型的泛化能力；</td>
</tr>
<tr>
<td>缺点</td>
<td>极度依赖专门的运行库和特殊的运行平台，不具有通用性；<br> 压缩率过大时，破坏性能；</td>
</tr>
</tbody>
</table>
</div>
<p>（3）参数量化    </p>
<p>​    神经网络的参数类型一般是32位浮点型，使用较小的精度代替32位所表示的精度。或者是将多个权重映射到同一数值，权重共享。<strong>量化其实是一种权值共享的策略</strong>。量化后的权值张量是一个高度稀疏的有很多共享权值的矩阵，对非零参数，我们还可以进行定点压缩，以获得更高的压缩率。 </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>模型性能损失很小，大小减少8-16倍；</td>
</tr>
<tr>
<td>缺点</td>
<td>压缩率大时，性能显著下降； <br>依赖专门的运行库，通用性较差；</td>
</tr>
<tr>
<td>举例</td>
<td>二值化网络：XNORnet [13], ABCnet with Multiple Binary Bases [14], <br>Bin-net with High-Order Residual Quantization [15], Bi-Real Net [16]；<br>三值化网络：Ternary weight networks [17], Trained Ternary Quantization [18]；</td>
</tr>
</tbody>
</table>
</div>
<p>W1-A8 或 W2-A8量化： Learning Symmetric Quantization [19]；<br>INT8量化：TensorFlow-lite [20], TensorRT [21]；<br>其他（非线性）：Intel INQ [22], log-net, CNNPack [23] 等；<br>原文：<a href="https://blog.csdn.net/baidu_31437863/article/details/84474847">https://blog.csdn.net/baidu_31437863/article/details/84474847</a> |<br>| 总结 | 最为典型就是二值网络、XNOR网络等。其主要原理就是采用1bit对网络的输入、权重、响应进行编码。减少模型大小的同时，原始网络的卷积操作可以被bit-wise运算代替，极大提升了模型的速度。但是，如果原始网络结果不够复杂（模型描述能力），由于二值网络会较大程度降低模型的表达能力。因此现阶段有相关的论文开始研究n-bit编码方式成为n值网络或者多值网络或者变bit、组合bit量化来克服二值网络表达能力不足的缺点。 |</p>
<p>（4）二值网络</p>
<p>​    相对量化更为极致，对于32bit浮点型数用1bit二进制数-1或者1表示，可大大减小模型尺寸。  </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>网络体积小，运算速度快，有时可避免部分网络的overfitting</td>
</tr>
<tr>
<td>缺点</td>
<td>二值神经网络损失的信息相对于浮点精度是非常大；<br>粗糙的二值化近似导致训练时模型收敛速度非常慢</td>
</tr>
</tbody>
</table>
</div>
<p>（5）三值网络</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>相对于二值神经网络，三值神经网络(Ternary Weight Networks)在同样的模型结构下可以达到成百上千倍的表达能力提升;并且，在计算时间复杂度上，三元网络和二元网络的计算复杂度是一样的。<br>例如，对于ResNet-18层网络中最常出现的卷积核(3x3大小)，二值神经网络模型最多可以表达2的3x3次方(=512)种结构，而三元神经网络则可以表达3的3x3次方(=19683)种卷积核结构。在表达能力上，三元神经网络相对要高19683/512 = 38倍。因此，三元神经网络模型能够在保证计算复杂度很低的情况下大幅的提高网络的表达能力，进而可以在精度上相对于二值神经网络有质的飞跃。另外，由于对中间信息的保存更多，三元神经网络可以极大的加快网络训练时的收敛速度，从而更快、更稳定的达到最优的结果。</td>
</tr>
<tr>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="低秩分解"><a href="#低秩分解" class="headerlink" title="低秩分解"></a>低秩分解</h3><p>基于低秩分解的深度神经网络压缩与加速的核心思想是利用矩阵或张量分解技术估计并分解深度模型中的原始卷积核．卷积计算是整个卷积神经网络中计算复杂 度 最 高 的 计 算 操 作，通 过 分 解４Ｄ 卷积核张量，可以有效地减少模型内部的冗余性．此外对于２Ｄ的全 连 接 层 矩 阵 参 数，同样可以利用低秩分解技术进行处理．但由于卷积层与全连接层的分解方式不同，本文分别从卷积层和全连接层２个不同角度回顾与分析低秩分解技术在深度神经网络中的应用.</p>
<p>在２０１３年，Ｄｅｎｉｌ等人［５７］从理论上利用低秩分解的技术并分析了深度神经网络存在大量的冗余信<br>息，开创了基于低秩分解的深度网络模型压缩与加速的新思路．如图７所示，展示了主流的张量分解后卷积 计 算．</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/img\ch17\低秩分解模型压缩加速.jpg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/img\ch17\低秩分解模型压缩加速.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>(出自《深度神经网络压缩与加速综述》)</p>
<h3 id="总体压缩效果评价指标有哪些？"><a href="#总体压缩效果评价指标有哪些？" class="headerlink" title="总体压缩效果评价指标有哪些？"></a>总体压缩效果评价指标有哪些？</h3><p>​    网络压缩评价指标包括运行效率、参数压缩率、准确率.与基准模型比较衡量性能提升时,可以使用提升倍数(speedup)或提升比例(ratio)。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>评价指标</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>准确率</td>
<td>目前,大部分研究工作均会测量 Top-1 准确率,只有在 ImageNet 这类大型数据集上才会只用 Top-5 准确率.为方便比较</td>
</tr>
<tr>
<td>参数压缩率</td>
<td>统计网络中所有可训练的参数,根据机器浮点精度转换为字节(byte)量纲,通常保留两位有效数字以作近似估计.</td>
</tr>
<tr>
<td>运行效率</td>
<td>可以从网络所含浮点运算次数(FLOP)、网络所含乘法运算次数(MULTS)或随机实验测得的网络平均前向传播所需时间这 3 个角度来评价</td>
</tr>
</tbody>
</table>
</div>
<h3 id="几种轻量化网络结构对比"><a href="#几种轻量化网络结构对比" class="headerlink" title="几种轻量化网络结构对比"></a>几种轻量化网络结构对比</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">网络结构</th>
<th style="text-align:center">TOP1 准确率/%</th>
<th style="text-align:center">参数量/M</th>
<th style="text-align:center">CPU运行时间/ms</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">MobileNet V1</td>
<td style="text-align:center">70.6</td>
<td style="text-align:center">4.2</td>
<td style="text-align:center">123</td>
</tr>
<tr>
<td style="text-align:center">ShuffleNet(1.5)</td>
<td style="text-align:center">69.0</td>
<td style="text-align:center">2.9</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">ShuffleNet(x2)</td>
<td style="text-align:center">70.9</td>
<td style="text-align:center">4.4</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">MobileNet V2</td>
<td style="text-align:center">71.7</td>
<td style="text-align:center">3.4</td>
<td style="text-align:center">80</td>
</tr>
<tr>
<td style="text-align:center">MobileNet V2(1.4)</td>
<td style="text-align:center">74.7</td>
<td style="text-align:center">6.9</td>
<td style="text-align:center">149</td>
</tr>
</tbody>
</table>
</div>
<h3 id="网络压缩未来研究方向有哪些？"><a href="#网络压缩未来研究方向有哪些？" class="headerlink" title="网络压缩未来研究方向有哪些？"></a>网络压缩未来研究方向有哪些？</h3><p>网络剪枝、网络精馏和网络分解都能在一定程度上实现网络压缩的目的.回归到深度网络压缩的本质目的上,即提取网络中的有用信息,以下是一些值得研究和探寻的方向.<br>(1) 权重参数对结果的影响度量.深度网络的最终结果是由全部的权重参数共同作用形成的,目前,关于单个卷积核/卷积核权重的重要性的度量仍然是比较简单的方式,尽管文献[14]中给出了更为细节的分析,但是由于计算难度大,并不实用.因此,如何通过更有效的方式来近似度量单个参数对模型的影响,具有重要意义.<br>(2) 学生网络结构的构造.学生网络的结构构造目前仍然是由人工指定的,然而,不同的学生网络结构的训练难度不同,最终能够达到的效果也有差异.因此,如何根据教师网络结构设计合理的网络结构在精简模型的条件下获取较高的模型性能,是未来的一个研究重点.<br>(3) 参数重建的硬件架构支持.通过分解网络可以无损地获取压缩模型,在一些对性能要求高的场景中是非常重要的.然而,参数的重建步骤会拖累预测阶段的时间开销,如何通过硬件的支持加速这一重建过程,将是未来的一个研究方向.<br>(4) 任务或使用场景层面的压缩.大型网络通常是在量级较大的数据集上训练完成的,比如,在 ImageNet上训练的模型具备对 1 000 类物体的分类,但在一些具体场景的应用中,可能仅需要一个能识别其中几类的小型模型.因此,如何从一个全功能的网络压缩得到部分功能的子网络,能够适应很多实际应用场景的需求.<br>(5) 网络压缩效用的评价.目前,对各类深度网络压缩算法的评价是比较零碎的,侧重于和被压缩的大型网络在参数量和运行时间上的比较.未来的研究可以从提出更加泛化的压缩评价标准出发,一方面平衡运行速度和模型大小在不同应用场景下的影响;另一方面,可以从模型本身的结构性出发,对压缩后的模型进行评价. </p>
<p>（出自《深度网络模型压缩综述》）</p>
<h2 id="目前有哪些深度学习模型优化加速方法？"><a href="#目前有哪些深度学习模型优化加速方法？" class="headerlink" title="目前有哪些深度学习模型优化加速方法？"></a>目前有哪些深度学习模型优化加速方法？</h2><p><a href="https://blog.csdn.net/nature553863/article/details/81083955">https://blog.csdn.net/nature553863/article/details/81083955</a></p>
<h3 id="模型优化加速方法"><a href="#模型优化加速方法" class="headerlink" title="模型优化加速方法"></a>模型优化加速方法</h3><p>模型优化加速能够提升网络的计算效率，具体包括：<br>（1）Op-level的快速算法：FFT Conv2d (7x7, 9x9), Winograd Conv2d (3x3, 5x5) 等；<br>（2）Layer-level的快速算法：Sparse-block net [1] 等；<br>（3）优化工具与库：TensorRT (Nvidia), Tensor Comprehension (Facebook) 和 Distiller (Intel) 等；   </p>
<p>原文：<a href="https://blog.csdn.net/nature553863/article/details/81083955">https://blog.csdn.net/nature553863/article/details/81083955</a>   </p>
<h3 id="TensorRT加速原理"><a href="#TensorRT加速原理" class="headerlink" title="TensorRT加速原理"></a>TensorRT加速原理</h3><p><a href="https://blog.csdn.net/xh_hit/article/details/79769599">https://blog.csdn.net/xh_hit/article/details/79769599</a></p>
<p>​    在计算资源并不丰富的嵌入式设备上，TensorRT之所以能加速神经网络的的推断主要得益于两点：</p>
<ul>
<li><p>首先是TensorRT支持int8和fp16的计算，通过在减少计算量和保持精度之间达到一个理想的trade-off，达到加速推断的目的。</p>
</li>
<li><p>更为重要的是TensorRT对于网络结构进行了重构和优化，主要体现在一下几个方面。</p>
<p>(1) TensorRT通过解析网络模型将网络中无用的输出层消除以减小计算。</p>
<p>(2) 对于网络结构的垂直整合，即将目前主流神经网络的Conv、BN、Relu三个层融合为了一个层，例如将图1所示的常见的Inception结构重构为图2所示的网络结构。</p>
<p>(3) 对于网络结构的水平组合，水平组合是指将输入为相同张量和执行相同操作的层融合一起，例如图2向图3的转化。</p>
</li>
</ul>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/tensorRT1.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/tensorRT1.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/tensorRT2.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/tensorRT2.png" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/tensorRT3.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/tensorRT3.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    以上3步即是TensorRT对于所部署的深度学习网络的优化和重构，根据其优化和重构策略，第一和第二步适用于所有的网络架构，但是第三步则对于含有Inception结构的神经网络加速效果最为明显。</p>
<p>​    Tips: 想更好地利用TensorRT加速网络推断，可在基础网络中多采用Inception模型结构，充分发挥TensorRT的优势。</p>
<h3 id="TensorRT如何优化重构模型？"><a href="#TensorRT如何优化重构模型？" class="headerlink" title="TensorRT如何优化重构模型？"></a>TensorRT如何优化重构模型？</h3><div class="table-container">
<table>
<thead>
<tr>
<th>条件</th>
<th>方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>若训练的网络模型包含TensorRT支持的操作</td>
<td>1、对于Caffe与TensorFlow训练的模型，若包含的操作都是TensorRT支持的，则可以直接由TensorRT优化重构</td>
</tr>
<tr>
<td></td>
<td>2、对于MXnet, PyTorch或其他框架训练的模型，若包含的操作都是TensorRT支持的，可以采用TensorRT API重建网络结构，并间接优化重构；</td>
</tr>
<tr>
<td>若训练的网络模型包含TensorRT不支持的操作</td>
<td>1、TensorFlow模型可通过tf.contrib.tensorrt转换，其中不支持的操作会保留为TensorFlow计算节点；</td>
</tr>
<tr>
<td></td>
<td>2、不支持的操作可通过Plugin API实现自定义并添加进TensorRT计算图；</td>
</tr>
<tr>
<td></td>
<td>3、将深度网络划分为两个部分，一部分包含的操作都是TensorRT支持的，可以转换为TensorRT计算图。另一部则采用其他框架实现，如MXnet或PyTorch；</td>
</tr>
</tbody>
</table>
</div>
<h3 id="TensorRT加速效果如何？"><a href="#TensorRT加速效果如何？" class="headerlink" title="TensorRT加速效果如何？"></a>TensorRT加速效果如何？</h3><p>以下是在TitanX (Pascal)平台上，TensorRT对大型分类网络的优化加速效果：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Network</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Framework/GPU:TitanXP</th>
<th style="text-align:center">Avg.Time(Batch=8,unit:ms)</th>
<th style="text-align:center">Top1 Val.Acc.(ImageNet-1k)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Resnet50</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">TensorFlow</td>
<td style="text-align:center">24.1</td>
<td style="text-align:center">0.7374</td>
</tr>
<tr>
<td style="text-align:center">Resnet50</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">MXnet</td>
<td style="text-align:center">15.7</td>
<td style="text-align:center">0.7374</td>
</tr>
<tr>
<td style="text-align:center">Resnet50</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">TRT4.0.1</td>
<td style="text-align:center">12.1</td>
<td style="text-align:center">0.7374</td>
</tr>
<tr>
<td style="text-align:center">Resnet50</td>
<td style="text-align:center">int8</td>
<td style="text-align:center">TRT4.0.1</td>
<td style="text-align:center">6</td>
<td style="text-align:center">0.7226</td>
</tr>
<tr>
<td style="text-align:center">Resnet101</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">TensorFlow</td>
<td style="text-align:center">36.7</td>
<td style="text-align:center">0.7612</td>
</tr>
<tr>
<td style="text-align:center">Resnet101</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">MXnet</td>
<td style="text-align:center">25.8</td>
<td style="text-align:center">0.7612</td>
</tr>
<tr>
<td style="text-align:center">Resnet101</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">TRT4.0.1</td>
<td style="text-align:center">19.3</td>
<td style="text-align:center">0.7612</td>
</tr>
<tr>
<td style="text-align:center">Resnet101</td>
<td style="text-align:center">int8</td>
<td style="text-align:center">TRT4.0.1</td>
<td style="text-align:center">9</td>
<td style="text-align:center">0.7574</td>
</tr>
</tbody>
</table>
</div>
<h2 id="影响神经网络速度的4个因素（再稍微详细一点）"><a href="#影响神经网络速度的4个因素（再稍微详细一点）" class="headerlink" title="影响神经网络速度的4个因素（再稍微详细一点）"></a>影响神经网络速度的4个因素（再稍微详细一点）</h2><ol>
<li><p>FLOPs(FLOPs就是网络执行了多少multiply-adds操作)；  </p>
</li>
<li><p>MAC(内存访问成本)；   </p>
</li>
<li><p>并行度(如果网络并行度高，速度明显提升)；   </p>
</li>
<li><p>计算平台(GPU，ARM)   </p>
</li>
</ol>
<h2 id="压缩和加速方法如何选择？"><a href="#压缩和加速方法如何选择？" class="headerlink" title="压缩和加速方法如何选择？"></a>压缩和加速方法如何选择？</h2><p>​    １）对于在线计算内存存储有限的应用场景或设备，可以选择参数共享和参数剪枝方法，特别是二值量化权值和激活、结构化剪枝．其他方法虽然能够有效的压缩模型中的权值参数，但无法减小计算中隐藏的内存大小（如特征图）．<br>​    ２）如果在应用中用到的紧性模型需要利用预训练模型，那么参数剪枝、参数共享以及低秩分解将成为首要考虑的方法．相反地，若不需要借助预训练模型，则可以考虑紧性滤波设计及知识蒸馏方法．<br>​    ３）若需要一次性端对端训练得到压缩与加速后模型，可以利用基于紧性滤波设计的深度神经网络压缩与加速方法．<br>​    ４）一般情况下，参数剪枝，特别是非结构化剪枝，能大大压缩模型大小，且不容易丢失分类精度．对于需要稳定的模型分类的应用，非结构化剪枝成为首要选择．<br>​    ５）若采用的数据集较小时，可以考虑知识蒸馏方法．对于小样本的数据集，学生网络能够很好地迁移教师模型的知识，提高学生网络的判别性．<br>​    ６）主流的５个深度神经网络压缩与加速算法相互之间是正交的，可以结合不同技术进行进一步的压缩与加速．如：韩 松 等 人［３０］结合了参数剪枝和参数共享；温伟等人［６４］以及 Ａｌｖａｒｅｚ等人［８５］结合了参数剪枝和低秩分解．此外对于特定的应用场景，如目标检测，可以对卷积层和全连接层使用不同的压缩与加速技术分别处理．</p>
<p>参考《深度神经网络压缩与加速综述》</p>
<h2 id="改变网络结构设计为什么会实现模型压缩、加速？"><a href="#改变网络结构设计为什么会实现模型压缩、加速？" class="headerlink" title="改变网络结构设计为什么会实现模型压缩、加速？"></a>改变网络结构设计为什么会实现模型压缩、加速？</h2><h3 id="Group-convolution"><a href="#Group-convolution" class="headerlink" title="Group convolution"></a>Group convolution</h3><p>​    Group convolution最早出现在AlexNet中，是为了解决单卡显存不够，将网络部署到多卡上进行训练而提出。Group convolution可以减少单个卷积1/g的参数量。如何计算的呢？  </p>
<p>​    假设</p>
<ul>
<li>输入特征的的维度为$H<em>W</em>C_1$;</li>
<li>卷积核的维度为$H_1<em>W_1</em>C_1$，共$C_2$个；</li>
<li>输出特征的维度为$H_1<em>W_1</em>C_2$ 。  </li>
</ul>
<p>传统卷积计算方式如下：<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/1.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/1.png" srcset="data:image/png;base64,666" alt="image"><br>传统卷积运算量为：  </p>
<script type="math/tex; mode=display">
A = H*W * h1 * w1 * c1 * c2</script><p>Group convolution是将输入特征的维度c1分成g份，每个group对应的channel数为c1/g，特征维度H * W * c1/g；，每个group对应的卷积核的维度也相应发生改变为h1 * w1 * c1/9，共c2/g个；每个group相互独立运算，最后将结果叠加在一起。<br>Group convolution计算方式如下：<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/2.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/2.png" srcset="data:image/png;base64,666" alt="image"><br>Group convolution运算量为：  </p>
<script type="math/tex; mode=display">
B = H * W * h1 * w1 * c1/g * c2/g * g</script><p>Group卷积相对于传统卷积的运算量：  </p>
<script type="math/tex; mode=display">
\dfrac{B}{A} = \dfrac{ H * W * h1 * w1 * c1/g * c2/g * g}{H * W * h1 * w1 * c1 * c2} = \dfrac{1}{g}</script><p>由此可知：group卷积相对于传统卷积减少了1/g的参数量。</p>
<h3 id="Depthwise-separable-convolution"><a href="#Depthwise-separable-convolution" class="headerlink" title="Depthwise separable convolution"></a>Depthwise separable convolution</h3><p>Depthwise separable convolution是由depthwise conv和pointwise conv构成。<br>depthwise conv(DW)有效减少参数数量并提升运算速度。但是由于每个feature map只被一个卷积核卷积，因此经过DW输出的feature map不能只包含输入特征图的全部信息，而且特征之间的信息不能进行交流，导致“信息流通不畅”。<br>pointwise conv(PW)实现通道特征信息交流，解决DW卷积导致“信息流通不畅”的问题。<br>假设输入特征的的维度为H * W * c1；卷积核的维度为h1 * w1 * c1，共c2个；输出特征的维度为 H1 * W1 * c2。<br>传统卷积计算方式如下：<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/3.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/3.png" srcset="data:image/png;base64,666" alt="image"><br>传统卷积运算量为：  </p>
<script type="math/tex; mode=display">
A = H * W * h1 * w1 * c1 * c2</script><p>DW卷积的计算方式如下：<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/4.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/4.png" srcset="data:image/png;base64,666" alt="image"><br>DW卷积运算量为： </p>
<script type="math/tex; mode=display">
B_DW = H * W * h1 * w1 * 1 * c1</script><p>PW卷积的计算方式如下：<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/5.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/5.png" srcset="data:image/png;base64,666" alt="image"></p>
<script type="math/tex; mode=display">
B_PW = H_m * W_m * 1 * 1 * c_1 * c_2</script><p>Depthwise separable convolution运算量为：</p>
<script type="math/tex; mode=display">
B = B_DW + B_PW</script><p>Depthwise separable convolution相对于传统卷积的运算量：</p>
<script type="math/tex; mode=display">
\dfrac{B}{A} = \dfrac{ H * W * h_1 * w_1 * 1 * c_1 + H_m * W_m * 1 * 1 * c_1 * c_2}{H * W * h1 * w1 * c_1 * c_2}  

= \dfrac{1}{c_2} + \dfrac{1}{h_1 * w_1}</script><p>由此可知，随着卷积通道数的增加，Depthwise separable convolution的运算量相对于传统卷积更少。</p>
<h3 id="输入输出的channel相同时，MAC最小"><a href="#输入输出的channel相同时，MAC最小" class="headerlink" title="输入输出的channel相同时，MAC最小"></a>输入输出的channel相同时，MAC最小</h3><p><strong>卷积层的输入和输出特征通道数相等时MAC最小，此时模型速度最快。</strong><br>假设feature map的大小为h*w，输入通道$c_1$，输出通道$c_2$。<br>已知：</p>
<script type="math/tex; mode=display">
FLOPs = B = h * w * c1 * c2   
=> c1 * c2 = \dfrac{B}{h * w}</script><script type="math/tex; mode=display">
MAC = h * w * (c1 + c2) + c1 * c2</script><script type="math/tex; mode=display">
=> MAC \geq 2 * h * w \sqrt{\dfrac{B}{h * w}} + \dfrac{B}{h * w}</script><p>根据均值不等式得到(c1-c2)^2&gt;=0，等式成立的条件是c1=c2，也就是输入特征通道数和输出特征通道数相等时，在给定FLOPs前提下，MAC达到取值的下界。</p>
<h3 id="减少组卷积的数量"><a href="#减少组卷积的数量" class="headerlink" title="减少组卷积的数量"></a>减少组卷积的数量</h3><p><strong>过多的group操作会增大MAC，从而使模型速度变慢</strong><br>由以上公式可知，group卷积想比与传统的卷积可以降低计算量，提高模型的效率；如果在相同的FLOPs时，group卷积为了满足FLOPs会是使用更多channels，可以提高模型的精度。但是随着channel数量的增加，也会增加MAC。<br>FLOPs：</p>
<script type="math/tex; mode=display">
B = \dfrac{h * w * c1 * c2}{g}</script><p>MAC：</p>
<script type="math/tex; mode=display">
MAC = h * w * (c1 + c2) + \dfrac{c1 * c2}{g}</script><p>由MAC，FLOPs可知：</p>
<script type="math/tex; mode=display">
MAC = h * w * c1 + \dfrac{B*g}{c1} + \dfrac{B}{h * w}</script><p>当FLOPs固定(B不变)时，g越大，MAC越大。</p>
<h3 id="减少网络碎片化程度-分支数量"><a href="#减少网络碎片化程度-分支数量" class="headerlink" title="减少网络碎片化程度(分支数量)"></a>减少网络碎片化程度(分支数量)</h3><p><strong>模型中分支数量越少，模型速度越快</strong><br>此结论主要是由实验结果所得。<br>以下为网络分支数和各分支包含的卷积数目对神经网络速度的影响。<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/6.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/6.png" srcset="data:image/png;base64,666" alt="image"><br>实验中使用的基本网络结构，分别将它们重复10次，然后进行实验。实验结果如下：<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/7.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/7.png" srcset="data:image/png;base64,666" alt="image"><br>由实验结果可知，随着网络分支数量的增加，神经网络的速度在降低。网络碎片化程度对GPU的影响效果明显，对CPU不明显，但是网络速度同样在降低。</p>
<h3 id="减少元素级操作"><a href="#减少元素级操作" class="headerlink" title="减少元素级操作"></a>减少元素级操作</h3><p><strong>元素级操作所带来的时间消耗也不能忽视</strong><br>ReLU ，Tensor 相加，Bias相加的操作，分离卷积（depthwise convolution）都定义为元素级操作。<br>FLOPs大多数是对于卷积计算而言的，因为元素级操作的FLOPs相对要低很多。但是过的元素级操作也会带来时间成本。ShuffleNet作者对ShuffleNet v1和MobileNet v2的几种层操作的时间消耗做了分析，发现元素级操作对于网络速度的影响也很大。<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/8.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/8.png" srcset="data:image/png;base64,666" alt="image"></p>
<h2 id="常用的轻量级网络有哪些？"><a href="#常用的轻量级网络有哪些？" class="headerlink" title="常用的轻量级网络有哪些？"></a>常用的轻量级网络有哪些？</h2><h3 id="SequeezeNet"><a href="#SequeezeNet" class="headerlink" title="SequeezeNet"></a>SequeezeNet</h3><p>SqueenzeNet出自F. N. Iandola, S.Han等人发表的论文《SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt; 0.5MB model size》，作者在保证精度不损失的同时，将原始AlexNet压缩至原来的510倍。  </p>
<h4 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h4><p>在网络结构设计方面主要采取以下三种方式：</p>
<ul>
<li>用1*1卷积核替换3*3卷积<ul>
<li>理论上一个1*1卷积核的参数是一个3*3卷积核的1/9，可以将模型尺寸压缩9倍。</li>
</ul>
</li>
<li>减小3*3卷积的输入通道数<ul>
<li>根据上述公式，减少输入通道数不仅可以减少卷积的运算量，而且输入通道数与输出通道数相同时还可以减少MAC。</li>
</ul>
</li>
<li>延迟降采样<ul>
<li>分辨率越大的输入能够提供更多特征的信息，有利于网络的训练判断，延迟降采样可以提高网络精度。<h4 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h4>SqueezeNet提出一种多分支结构——fire model，其中是由Squeeze层和expand层构成。Squeeze层是由s1个1*1卷积组成，主要是通过1*1的卷积降低expand层的输入维度；expand层利用e1个1*1和e3个3*3卷积构成多分支结构提取输入特征，以此提高网络的精度(其中e1=e3=4*s1)。<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/9.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/9.png" srcset="data:image/png;base64,666" alt="image"><br>SqueezeNet整体网络结构如下图所示：<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/10.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/10.png" srcset="data:image/png;base64,666" alt="image"></li>
</ul>
</li>
</ul>
<h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>不同压缩方法在ImageNet上的对比实验结果<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/11.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/11.png" srcset="data:image/png;base64,666" alt="image"><br>由实验结果可知，SqueezeNet不仅保证了精度，而且将原始AlexNet从240M压缩至4.8M，压缩50倍，说明此轻量级网络设计是可行。</p>
<h3 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h3><p>MobileNet 是Google团队于CVPR-2017的论文《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications》中针对手机等嵌入式设备提出的一种轻量级的深层神经网络，该网络结构在VGG的基础上使用DW+PW的组合，在保证不损失太大精度的同时，降低模型参数量。</p>
<h4 id="设计思想-1"><a href="#设计思想-1" class="headerlink" title="设计思想"></a>设计思想</h4><ul>
<li>采用深度可分离卷积代替传统卷积<ul>
<li>采用DW卷积在减少参数数量的同时提升运算速度。但是由于每个feature map只被一个卷积核卷积，因此经过DW输出的feature map不能只包含输入特征图的全部信息，而且特征之间的信息不能进行交流，导致“信息流通不畅”。</li>
<li>采用PW卷积实现通道特征信息交流，解决DW卷积导致“信息流通不畅”的问题。</li>
</ul>
</li>
<li>使用stride=2的卷积替换pooling<ul>
<li>直接在卷积时利用stride=2完成了下采样，从而节省了需要再去用pooling再去进行一次下采样的时间，可以提升运算速度。同时，因为pooling之前需要一个stride=1的 conv，而与stride=2 conv的计算量想比要高近4倍(<strong>个人理解</strong>)。<h4 id="网络架构-1"><a href="#网络架构-1" class="headerlink" title="网络架构"></a>网络架构</h4></li>
</ul>
</li>
<li><p>DW conv和PW conv<br>MobileNet的网络架构主要是由DW conv和PW conv组成，相比于传统卷积可以降低$\dfrac{1}{N} + \dfrac{1}{Dk}$倍的计算量。<br>标准卷积与DW conv和PW conv如图所示:<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/12.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/12.png" srcset="data:image/png;base64,666" alt="image"><br>深度可分离卷积与传统卷积运算量对比：<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/13.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/13.png" srcset="data:image/png;base64,666" alt="image"><br>网络结构：<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/14.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/14.png" srcset="data:image/png;base64,666" alt="image"></p>
</li>
<li><p>MobileNets的架构<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/15.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/15.png" srcset="data:image/png;base64,666" alt="image"></p>
</li>
</ul>
<h4 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a>实验结果</h4><p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/16.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/16.png" srcset="data:image/png;base64,666" alt="image"><br>由上表可知，使用相同的结构，深度可分离卷积虽然准确率降低1%，但是参数量减少了6/7。</p>
<h3 id="MobileNet-v2"><a href="#MobileNet-v2" class="headerlink" title="MobileNet-v2"></a>MobileNet-v2</h3><p>MobileNet-V2是2018年1月公开在arXiv上论文《Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation》，是对MobileNet-V1的改进，同样是一个轻量化卷积神经网络。</p>
<h4 id="设计思想-2"><a href="#设计思想-2" class="headerlink" title="设计思想"></a>设计思想</h4><ul>
<li>采用Inverted residuals<ul>
<li>为了保证网络可以提取更多的特征，在residual block中第一个1*1 Conv和3*3 DW Conv之前进行通道扩充</li>
</ul>
</li>
<li>Linear bottlenecks<ul>
<li>为了避免Relu对特征的破坏，在residual block的Eltwise sum之前的那个 1*1 Conv 不再采用Relu</li>
</ul>
</li>
<li>stride=2的conv不使用shot-cot，stride=1的conv使用shot-cut</li>
</ul>
<h4 id="网络架构-2"><a href="#网络架构-2" class="headerlink" title="网络架构"></a>网络架构</h4><ul>
<li>Inverted residuals<br>ResNet中Residuals block先经过1*1的Conv layer，把feature map的通道数降下来，再经过3*3 Conv layer，最后经过一个1*1 的Conv layer，将feature map 通道数再“扩张”回去。即采用先压缩，后扩张的方式。而 inverted residuals采用先扩张，后压缩的方式。<br>MobileNet采用DW conv提取特征，由于DW conv本身提取的特征数就少，再经过传统residuals block进行“压缩”，此时提取的特征数会更少，因此inverted residuals对其进行“扩张”，保证网络可以提取更多的特征。<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/17.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/17.png" srcset="data:image/png;base64,666" alt="image"></li>
<li>Linear bottlenecks<br>ReLu激活函数会破坏特征。ReLu对于负的输入，输出全为0，而本来DW conv特征通道已经被“压缩”，再经过ReLu的话，又会损失一部分特征。采用Linear，目的是防止Relu破坏特征。<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/18.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/18.png" srcset="data:image/png;base64,666" alt="image"></li>
<li>shortcut<br>stride=2的conv不使用shot-cot，stride=1的conv使用shot-cut<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/19.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/19.png" srcset="data:image/png;base64,666" alt="image"></li>
<li>网络架构<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/20.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/20.png" srcset="data:image/png;base64,666" alt="image"></li>
</ul>
<h3 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a>Xception</h3><p>Xception是Google提出的，arXiv 的V1 于2016年10月公开《Xception: Deep Learning with Depthwise Separable Convolutions 》，Xception是对Inception v3的另一种改进，主要是采用depthwise separable convolution来替换原来Inception v3中的卷积操作。</p>
<h4 id="设计思想-3"><a href="#设计思想-3" class="headerlink" title="设计思想"></a>设计思想</h4><ul>
<li>采用depthwise separable convolution来替换原来Inception v3中的卷积操作<br>  与原版的Depth-wise convolution有两个不同之处：<ul>
<li>第一个：原版Depth-wise convolution，先逐通道卷积，再1<em>1卷积; 而Xception是反过来，先1\</em>1卷积，再逐通道卷积；</li>
<li>第二个：原版Depth-wise convolution的两个卷积之间是不带激活函数的，而Xception在经过1*1卷积之后会带上一个Relu的非线性激活函数；</li>
</ul>
</li>
</ul>
<h4 id="网络架构-3"><a href="#网络架构-3" class="headerlink" title="网络架构"></a>网络架构</h4><p>feature map在空间和通道上具有一定的相关性，通过Inception模块和非线性激活函数实现通道之间的解耦。增多3*3的卷积的分支的数量，使它与1*1的卷积的输出通道数相等，此时每个3*3的卷积只作用与一个通道的特征图上，作者称之为“极致的Inception（Extream Inception）”模块，这就是Xception的基本模块。<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/21.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/21.png" srcset="data:image/png;base64,666" alt="image"></p>
<h3 id="ShuffleNet-v1"><a href="#ShuffleNet-v1" class="headerlink" title="ShuffleNet-v1"></a>ShuffleNet-v1</h3><p>ShuffleNet 是Face++团队提出的，晚于MobileNet两个月在arXiv上公开《ShuffleNet： An Extremely Efficient Convolutional Neural Network for Mobile Devices 》用于移动端前向部署的网络架构。ShuffleNet基于MobileNet的group思想，将卷积操作限制到特定的输入通道。而与之不同的是，ShuffleNet将输入的group进行打散，从而保证每个卷积核的感受野能够分散到不同group的输入中，增加了模型的学习能力。</p>
<h4 id="设计思想-4"><a href="#设计思想-4" class="headerlink" title="设计思想"></a>设计思想</h4><ul>
<li>采用group conv减少大量参数<ul>
<li>roup conv与DW conv存在相同的“信息流通不畅”问题 </li>
</ul>
</li>
<li>采用channel shuffle解决上述问题<ul>
<li>MobileNet中采用PW conv解决上述问题，SheffleNet中采用channel shuffle</li>
</ul>
</li>
<li>采用concat替换add操作<ul>
<li>avg pooling和DW conv(s=2)会减小feature map的分辨率，采用concat增加通道数从而弥补分辨率减小而带来信息的损失</li>
</ul>
</li>
</ul>
<h4 id="网络架构-4"><a href="#网络架构-4" class="headerlink" title="网络架构"></a>网络架构</h4><p>MobileNet中1*1卷积的操作占据了约95%的计算量，所以作者将1*1也更改为group卷积，使得相比MobileNet的计算量大大减少。<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/22.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/22.png" srcset="data:image/png;base64,666" alt="image"><br>group卷积与DW存在同样使“通道信息交流不畅”的问题，MobileNet中采用PW conv解决上述问题，SheffleNet中采用channel shuffle。<br>ShuffleNet的shuffle操作如图所示<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/24.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/24.png" srcset="data:image/png;base64,666" alt="image"><br>avg pooling和DW conv(s=2)会减小feature map的分辨率，采用concat增加通道数从而弥补分辨率减小而带来信息的损失；实验表明：多多使用通道(提升通道的使用率)，有助于提高小模型的准确率。<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/23.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/23.png" srcset="data:image/png;base64,666" alt="image"><br>网络结构：<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/25.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/25.png" srcset="data:image/png;base64,666" alt="image"></p>
<h3 id="ShuffleNet-v2"><a href="#ShuffleNet-v2" class="headerlink" title="ShuffleNet-v2"></a>ShuffleNet-v2</h3><p>huffleNet-v2 是Face++团队提出的《ShuffleNet V2: Practical Guidelines for Ecient CNN Architecture Design》，旨在设计一个轻量级但是保证精度、速度的深度网络。</p>
<h4 id="设计思想-5"><a href="#设计思想-5" class="headerlink" title="设计思想"></a>设计思想</h4><ul>
<li>文中提出影响神经网络速度的4个因素：<ul>
<li>a. FLOPs(FLOPs就是网络执行了多少multiply-adds操作)</li>
<li>b. MAC(内存访问成本)</li>
<li>c. 并行度(如果网络并行度高，速度明显提升)</li>
<li>d. 计算平台(GPU，ARM)</li>
</ul>
</li>
<li>ShuffleNet-v2 提出了4点网络结构设计策略：<ul>
<li>G1.输入输出的channel相同时，MAC最小</li>
<li>G2.过度的组卷积会增加MAC</li>
<li>G3.网络碎片化会降低并行度</li>
<li>G4.元素级运算不可忽视  </li>
</ul>
</li>
</ul>
<h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>depthwise convolution 和 瓶颈结构增加了 MAC，用了太多的 group，跨层连接中的 element-wise Add 操作也是可以优化的点。所以在 shuffleNet V2 中增加了几种新特性。<br>所谓的 channel split 其实就是将通道数一分为2，化成两分支来代替原先的分组卷积结构（G2），并且每个分支中的卷积层都是保持输入输出通道数相同（G1），其中一个分支不采取任何操作减少基本单元数（G3），最后使用了 concat 代替原来的 elementy-wise add，并且后面不加 ReLU 直接（G4），再加入channle shuffle 来增加通道之间的信息交流。 对于下采样层，在这一层中对通道数进行翻倍。 在网络结构的最后，即平均值池化层前加入一层 1x1 的卷积层来进一步的混合特征。<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/26.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/26.png" srcset="data:image/png;base64,666" alt="image"><br>网络结构<br><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/27.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/27.png" srcset="data:image/png;base64,666" alt="image"></p>
<h4 id="ShuffleNet-v2具有高精度的原因"><a href="#ShuffleNet-v2具有高精度的原因" class="headerlink" title="ShuffleNet-v2具有高精度的原因"></a>ShuffleNet-v2具有高精度的原因</h4><ul>
<li>由于高效，可以增加更多的channel，增加网络容量</li>
<li>采用split使得一部分特征直接与下面的block相连，特征复用(DenseNet)</li>
</ul>
<h2 id="现有移动端开源框架及其特点"><a href="#现有移动端开源框架及其特点" class="headerlink" title="现有移动端开源框架及其特点"></a>现有移动端开源框架及其特点</h2><h3 id="NCNN"><a href="#NCNN" class="headerlink" title="NCNN"></a>NCNN</h3><p>１、开源时间：2017年7月　　　</p>
<p>２、开源用户：腾讯优图　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/Tencent/ncnn">https://github.com/Tencent/ncnn</a> 　　</p>
<p>4、特点：</p>
<ul>
<li>1）NCNN考虑了手机端的硬件和系统差异以及调用方式，架构设计以手机端运行为主要原则。</li>
<li>2）无第三方依赖，跨平台，手机端 CPU 的速度快于目前所有已知的开源框架（以开源时间为参照对象）。</li>
<li>3）基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行，开发出人工智能 APP。   </li>
</ul>
<p>5、功能：    </p>
<ul>
<li>1、NCNN支持卷积神经网络、多分支多输入的复杂网络结构，如vgg、googlenet、resnet、squeezenet 等。</li>
<li>2、NCNN无需依赖任何第三方库。    </li>
<li>3、NCNN全部使用C/C++实现，以及跨平台的cmake编译系统，可轻松移植到其他系统和设备上。    </li>
<li>4、汇编级优化，计算速度极快。使用ARM NEON指令集实现卷积层，全连接层，池化层等大部分 CNN 关键层。 </li>
<li>5、精细的数据结构设计，没有采用需消耗大量内存的通常框架——im2col + 矩阵乘法，使得内存占用极低。   </li>
<li>6、支持多核并行计算，优化CPU调度。   </li>
<li>7、整体库体积小于500K，可精简到小于300K。   </li>
<li>8、可扩展的模型设计，支持8bit 量化和半精度浮点存储。   </li>
<li>9、支持直接内存引用加载网络模型。   </li>
<li>10、可注册自定义层实现并扩展。   </li>
</ul>
<p>6、NCNN在Android端部署示例</p>
<ul>
<li>1）选择合适的Android Studio版本并安装。</li>
<li>2）根据需求选择NDK版本并安装。</li>
<li>3）在Android Studio上配置NDK的环境变量。</li>
<li>4）根据自己需要编译NCNN sdk</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">mkdir build-android cd build-android cmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \ -DANDROID_ABI="armeabi-v7a" -DANDROID_ARM_NEON=ON \ -DANDROID_PLATFORM=android-14 .. make make install</span><br></pre></td></tr></tbody></table></figure>
<p>​    安装完成之后，install下有include和lib两个文件夹。</p>
<p>​    备注：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">ANDROID_ABI 是架构名字，"armeabi-v7a" 支持绝大部分手机硬件 </span><br><span class="line">ANDROID_ARM_NEON 是否使用 NEON 指令集，设为 ON 支持绝大部分手机硬件 </span><br><span class="line">ANDROID_PLATFORM 指定最低系统版本，"android-14" 就是 android-4.0</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>5）进行NDK开发。</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">1）assets文件夹下放置你的bin和param文件。</span><br><span class="line">2）jni文件夹下放置你的cpp和mk文件。</span><br><span class="line">3）修改你的app gradle文件。</span><br><span class="line">4）配置Android.mk和Application.mk文件。</span><br><span class="line">5）进行java接口的编写。</span><br><span class="line">6）读取拷贝bin和param文件（有些则是pb文件，根据实际情况）。</span><br><span class="line">7）进行模型的初始化和执行预测等操作。</span><br><span class="line">8）build。</span><br><span class="line">9）cd到src/main/jni目录下，执行ndk-build，生成.so文件。</span><br><span class="line">10）接着就可写自己的操作处理需求。</span><br></pre></td></tr></tbody></table></figure>
<h3 id="QNNPACK"><a href="#QNNPACK" class="headerlink" title="QNNPACK"></a>QNNPACK</h3><p>全称：Quantized Neural Network PACKage（量化神经网络包）　　　</p>
<p>１、开源时间：2018年10月　　　</p>
<p>２、开源用户：Facebook　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/pytorch/QNNPACK">https://github.com/pytorch/QNNPACK</a>　　　　</p>
<p>４、特点：　　　</p>
<p>​    １）低密度卷积优化函数库；　　　</p>
<p>　    ２）可在手机上实时运行Mask R-CNN 和 DensePose;</p>
<p>​    ３） 能在性能受限的移动设备中用 100ms 以内的时间实施图像分类；　　　</p>
<p>5、QNNPACK 如何提高效率？</p>
<p>1)<strong>QNNPACK 使用与安卓神经网络 API 兼容的线性量化方案</strong></p>
<p>QNNPACK 的输入矩阵来自低精度、移动专用的计算机视觉模型。其它库在计算A和B矩阵相乘时，重新打包 A 和 B 矩阵以更好地利用缓存层次结构，希望在大量计算中分摊打包开销，QNNPACK 删除所有计算非必需的内存转换，针对 A和B矩阵相乘适用于一级缓存的情况进行了优化。</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/QNNPACK1.jpeg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/QNNPACK1.jpeg" srcset="data:image/png;base64,666" alt=""></p>
<p>​    1）优化了L1缓存计算，不需要输出中间结果，直接输出最终结果，节省内存带宽和缓存占用。</p>
<p>具体分析：</p>
<ul>
<li>常规实现：在量化矩阵-矩阵乘法中，8位整数的乘积通常会被累加至 32 位的中间结果中，随后重新量化以产生 8 位的输出。遇到大矩阵尺寸时，比如有时K太大，A和B的面板无法直接转入缓存，此时，需利用缓存层次结构，借助GEMM将A和B的面板沿着K维分割成固定大小的子面板，以便于每个子面板都能适应L1缓存，随后为每个子面板调用微内核。这一缓存优化需要 PDOT 为内核输出 32  位中间结果，最终将它们相加并重新量化为 8 位整数。</li>
<li>优化实现：由于  ONNPACK 对于面板 A 和 B 总是适应 L1 缓存的移动神经网络进行了优化，因此它在调用微内核时处理整个 A 和 B  的面板。而由于无需在微内核之外积累 32 位的中间结果，QNNPACK 会将 32 位的中间结果整合进微内核中并写出 8  位值，这节省了内存带宽和缓存占用。</li>
</ul>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/QNNPACK2.jpeg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/QNNPACK2.jpeg" srcset="data:image/png;base64,666" alt=""></p>
<p>​    2）取消了矩阵 A 的重新打包。</p>
<ul>
<li><p>常规实现：</p>
<pre><code>  矩阵 B 包含静态权重，可以一次性转换成任何内存布局，但矩阵  A 包含卷积输入，每次推理运行都会改变。因此，重新打包矩阵 A 在每次运行时都会产生开销。尽管存在开销，传统的 GEMM实现还是出于以下两个原因对矩阵 A 进行重新打包：

  a 缓存关联性及微内核效率受限。如果不重新打包，微内核将不得不读取被潜在的大跨距隔开的几行A。如果这个跨距恰好是 2 的许多次幂的倍数，面板中不同行 A  的元素可能会落入同一缓存集中。如果冲突的行数超过了缓存关联性，它们就会相互驱逐，性能也会大幅下降。

  b 打包对微内核效率的影响与当前所有移动处理器支持的  SIMD  向量指令的使用密切相关。这些指令加载、存储或者计算小型的固定大小元素向量，而不是单个标量（scalar）。在矩阵相乘中，充分利用向量指令达到高性能很重要。在传统的  GEMM 实现中，微内核把 MR 元素重新打包到向量暂存器里的 MR 线路中。
</code></pre></li>
<li><p>优化实现：</p>
<pre><code>  a 当面板适配一级缓存时，不会存在缓存关联性及微内核效率受限的问题。

  b 在 QNNPACK 实现中，MR  元素在存储中不是连续的，微内核需要把它们加载到不同的向量暂存器中。越来越大的暂存器压力迫使 QNNPACK 使用较小的 MRxNR  拼贴，但实际上这种差异很小，而且可以通过消除打包开销来补偿。例如，在 32 位 ARM 架构上，QNNPACK 使用 4×8 微内核，其中  57% 的向量指令是乘-加；另一方面，gemmlowp 库使用效率稍高的 4×12 微内核，其中 60% 的向量指令是乘-加。微内核加载 A  的多个行，乘以 B 的满列，结果相加，然后完成再量化并记下量化和。A 和 B 的元素被量化为 8 位整数，但乘积结果相加到 32 位。大部分  ARM 和 ARM64 处理器没有直接完成这一运算的指令，所以它必须分解为多个支持运算。QNNPACK  提供微内核的两个版本，其不同之处在于用于乘以 8 位值并将它们累加到 32 位的指令序列。
</code></pre></li>
</ul>
<p>2)<strong>从矩阵相乘到卷积</strong></p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/QNNPACK3.jpeg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/QNNPACK3.jpeg" srcset="data:image/png;base64,666" alt=""></p>
<p>​    传统实现：</p>
<p>​    简单的 1×1  卷积可直接映射到矩阵相乘</p>
<p>​    但对于具备较大卷积核、padding 或子采样（步幅）的卷积而言则并非如此。但是，这些较复杂的卷积能够通过记忆变换  im2col 映射到矩阵相乘。对于每个输出像素，im2col 复制输入图像的图像块并将其计算为 2D 矩阵。由于每个输出像素都受 KHxKWxC  输入像素值的影响（KH 和 KW 分别指卷积核的高度和宽度，C 指输入图像中的通道数），因此该矩阵的大小是输入图像的 KHxKW  倍，im2col 给内存占用和性能都带来了一定的开销。和 Caffe 一样，大部分深度学习框架转而使用基于 im2col  的实现，利用现有的高度优化矩阵相乘库来执行卷积操作。</p>
<p>​    优化实现：</p>
<p>​    Facebook  研究者在 QNNPACK 中实现了一种更高效的算法。</p>
<ul>
<li>他们没有变换卷积输入使其适应矩阵相乘的实现，而是调整 PDOT 微内核的实现，在运行中执行  im2col 变换。这样就无需将输入张量的实际输入复制到 im2col 缓存，而是使用输入像素行的指针设置 indirection  buffer，输入像素与每个输出像素的计算有关。</li>
<li>研究者还修改了矩阵相乘微内核，以便从 indirection buffer  加载虚构矩阵（imaginary matrix）A 的行指针，indirection buffer 通常比 im2col buffer  小得多。</li>
<li>此外，如果两次推断运行的输入张量存储位置不变，则 indirection buffer  还可使用输入张量行的指针进行初始化，然后在多次推断运行中重新使用。研究者观察到具备 indirection buffer 的微内核不仅消除了  im2col 变换的开销，其性能也比矩阵相乘微内核略好（可能由于输入行在计算不同输出像素时被重用）。</li>
</ul>
<p>3)<strong>深度卷积</strong></p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/QNNPACK4.jpeg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/QNNPACK4.jpeg" srcset="data:image/png;base64,666" alt=""></p>
<p>分组卷积（grouped   convolution）将输入和输出通道分割成多组，然后对每个组进行分别处理。在有限条件下，当组数等于通道数时，该卷积就是深度卷积，常用于当前的神经网络架构中。深度卷积对每个通道分别执行空间滤波，展示了与正常卷积非常不同的计算模式。因此，通常要向深度卷积提供单独实现，QNNPACK  包括一个高度优化版本 3×3 深度卷积。</p>
<p>深度卷积的传统实现是每次都在卷积核元素上迭代，然后将一个卷积核行和一个输入行的结果累加到输出行。对于一个  3×3 的深度卷积，此类实现将把每个输出行更新 9 次。在 QNNPACK 中，研究者计算所有 3×3 卷积核行和 3×3  输入行的结果，一次性累加到输出行，然后再处理下个输出行。</p>
<p>QNNPACK  实现高性能的关键因素在于完美利用通用暂存器（GPR）来展开卷积核元素上的循环，同时避免在 hot loop 中重新加载地址寄存器。32-bit  ARM 架构将实现限制在 14 个 GPR。在 3×3 深度卷积中，需要读取 9 个输入行和 9 个卷积核行。这意味着如果想完全展开循环必须存储  18 个地址。然而，实践中推断时卷积核不会发生变化。因此 Facebook 研究者使用之前在 CxKHxKW 中的滤波器，将它们封装进  [C/8]xKWxKHx8，这样就可以仅使用具备地址增量（address increment）的一个 GPR 访问所有滤波器。（研究者使用数字 8  的原因在于，在一个命令中加载 8 个元素然后减去零，在 128-bit NEON 暂存器中生成 8 个 16-bit 值。）然后使用 9  个输入行指针，指针将滤波器重新装进 10 个 GPR，完全展开滤波器元素上的循环。64-bit ARM 架构相比 32-bit 架构，GPR  的数量翻了一倍。QNNPACK 利用额外的 ARM64 GPR，一次性存储 3×5 输入行的指针，并计算 3 个输出行。</p>
<p>7、性能优势：</p>
<p>​    测试结果显示出 QNNPACK 在端到端基准上的性能优势。在量化当前最优 MobileNetV2 架构上，基于QNNPACK 的 Caffe2 算子的速度大约是 TensorFlow Lite 速度的 2 倍，在多种手机上都是如此。除了 QNNPACK 之外，Facebook 还开源了 Caffe2 quantized MobileNet v2 模型，其 top-1 准确率比相应的 TensorFlow 模型高出 1.3%。    </p>
<p><strong>MobileNetV1</strong></p>
<p>MobileNetV1  架构在使用深度卷积（depthwise convolution）使模型更适合移动设备方面具备开创性。MobileNetV1 包括几乎整个  1×1 卷积和 3×3 卷积。Facebook 研究者将量化 MobileNetV1 模型从 TensorFlow Lite 转换而来，并在  TensorFlow Lite 和 QNNPACK 的 32-bit ARM 设备上对 MobileNetV1 进行基准测试。二者运行时均使用 4  线程，研究者观察到 QNNPACK 的运行速度几何平均值是 TensorFlow Lite 的 1.8 倍。</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/mv1.jpg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/mv1.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>MobileNetV2</strong></p>
<p>作为移动视觉任务的当前最优架构之一，MobileNetV2  引入了瓶颈构造块和瓶颈之间的捷径连接。研究者在 MobileNetV2 分类模型的量化版上对比基于 QNNPACK 的 Caffe2 算子和  TensorFlow Lite 实现。使用的量化 Caffe2 MobileNetV2 模型已开源，量化 TensorFlow Lite  模型来自官方库：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md。下表展示了二者在常用测试集上的">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md。下表展示了二者在常用测试集上的</a>  top1 准确率：</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/mv2.jpg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/mv2.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>​    Facebook 研究者利用这些模型建立了 Facebook AI 性能评估平台（<a href="https://github.com/facebook/FAI-PEP）的基准，该基准基于">https://github.com/facebook/FAI-PEP）的基准，该基准基于</a> 32-bit ARM 环境的大量手机设备。对于 TensorFlow Lite 线程设置，研究者尝试了一到四个线程，并报告了最快速的结果。结果显示 TensorFlow Lite 使用四线程的性能最优，因此后续研究中使用四线程来对比 TensorFlow Lite 和 QNNPACK。下表展示了结果，以及在典型智能手机和高端机上，基于 QNNPACK 的算子速度比 TensorFlow Lite 快得多。</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/mv3.jpg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/mv3.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>Facebook开源高性能内核库QNNPACK<br><a href="https://baijiahao.baidu.com/s?id=1615725346726413945&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1615725346726413945&amp;wfr=spider&amp;for=pc</a><br><a href="http://www.sohu.com/a/272158070_610300">http://www.sohu.com/a/272158070_610300</a></p>
<p>支持移动端深度学习的几种开源框架<br><a href="https://blog.csdn.net/zchang81/article/details/74280019">https://blog.csdn.net/zchang81/article/details/74280019</a></p>
<h3 id="Prestissimo"><a href="#Prestissimo" class="headerlink" title="Prestissimo"></a>Prestissimo</h3><p>１、开源时间：2017年11月　　　</p>
<p>２、开源用户：九言科技　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/in66-dev/In-Prestissimo">https://github.com/in66-dev/In-Prestissimo</a>　　</p>
<p>４、功能特点：　</p>
<p><strong>基础功能</strong></p>
<ul>
<li>支持卷积神经网络，支持多输入和多分支结构</li>
<li>精炼简洁的API设计，使用方便</li>
<li>提供调试接口，支持打印各个层的数据以及耗时</li>
<li>不依赖任何第三方计算框架，整体库体积 500K 左右（32位 约400k，64位 约600k）</li>
<li>纯 C++ 实现，跨平台，支持 android 和 ios</li>
<li>模型为纯二进制文件，不暴露开发者设计的网络结构</li>
</ul>
<p><strong>极快的速度</strong></p>
<ul>
<li>大到框架设计，小到汇编书写上全方位的优化，iphone7 上跑 SqueezeNet 仅需 26ms（单线程）</li>
<li>支持浮点(float)和整型(int)两种运算模式，float模式精度与caffe相同，int模式运算速度快，大部分网络用int的精度便已经足够</li>
<li>以巧妙的内存布局提升cpu的cache命中率，在中低端机型上性能依然强劲</li>
<li>针对 float-arm32, float-arm64, int-arm32, int-arm64 四个分支均做了细致的优化，保证arm32位和arm64位版本都有非常好的性能</li>
</ul>
<p><strong>SqueezeNet-v1.1 测试结果</strong></p>
<p><strong>Note</strong>: 手机测试性能存在一定的抖动，连续多次运算取平均时间</p>
<p><strong>Note</strong>: 像华为mate8, mate9，Google nexus 6 虽然是64位的CPU，但测试用的是 32位的库，因此cpu架构依然写 arm-v7a</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">CPU架构</th>
<th style="text-align:center">机型</th>
<th style="text-align:center">CPU</th>
<th style="text-align:center">ncnn（4线程）</th>
<th style="text-align:center">mdl</th>
<th style="text-align:center">Prestissimo_float(单线程)</th>
<th style="text-align:center">Prestissimo_int(单线程)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">小米2</td>
<td style="text-align:center">高通APQ8064 1.5GHz</td>
<td style="text-align:center">185 ms</td>
<td style="text-align:center">370 ms</td>
<td style="text-align:center">184 ms</td>
<td style="text-align:center">115 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">小米2s</td>
<td style="text-align:center">四核 骁龙APQ8064 Pro 1.7GHz</td>
<td style="text-align:center">166 ms</td>
<td style="text-align:center">-</td>
<td style="text-align:center">136 ms</td>
<td style="text-align:center">96 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">红米Note 4x</td>
<td style="text-align:center">骁龙625 四核2.0GHz</td>
<td style="text-align:center">124 ms</td>
<td style="text-align:center">306 ms</td>
<td style="text-align:center">202 ms</td>
<td style="text-align:center">110 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">Google Nexus 6</td>
<td style="text-align:center">骁龙805 四核 2.7GHz</td>
<td style="text-align:center">84 ms</td>
<td style="text-align:center">245 ms</td>
<td style="text-align:center">103 ms</td>
<td style="text-align:center">63 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">Vivo x6d</td>
<td style="text-align:center">联发科 MT6752 1.7GHz</td>
<td style="text-align:center">245 ms</td>
<td style="text-align:center">502 ms</td>
<td style="text-align:center">370 ms</td>
<td style="text-align:center">186 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">华为 Mate 8</td>
<td style="text-align:center">海思麒麟950 4大4小 2.3GHz 1.8GHz</td>
<td style="text-align:center">75 ms</td>
<td style="text-align:center">180 ms</td>
<td style="text-align:center">95 ms</td>
<td style="text-align:center">57 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">华为 Mate 9</td>
<td style="text-align:center">海思麒麟960 4大4小 2.4GHz 1.8GHz</td>
<td style="text-align:center">61 ms</td>
<td style="text-align:center">170 ms</td>
<td style="text-align:center">94 ms</td>
<td style="text-align:center">48 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v8</td>
<td style="text-align:center">iphone7</td>
<td style="text-align:center">Apple A10 Fusion 2.34GHz</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">27 ms</td>
<td style="text-align:center">26 ms</td>
</tr>
</tbody>
</table>
</div>
<p><strong>未开放特性</strong></p>
<ul>
<li>多核并行加速（多核机器可以再提升30%-100% 的速度）</li>
<li>depthwise卷积运算（支持mobilenet）</li>
<li>模型压缩功能，压缩后的模型体积可缩小到20%以下</li>
<li>GPU 运算模式（Android 基于opengl es 3.1，ios 基于metal）</li>
</ul>
<p><strong>同类框架对比</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">框架</th>
<th style="text-align:center">caffe</th>
<th style="text-align:center">tensorflow</th>
<th style="text-align:center">mdl-android</th>
<th style="text-align:center">mdl-ios</th>
<th style="text-align:center">ncnn</th>
<th style="text-align:center">CoreML</th>
<th style="text-align:center">Prestissimo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">计算硬件</td>
<td style="text-align:center">cpu</td>
<td style="text-align:center">cpu</td>
<td style="text-align:center">cpu</td>
<td style="text-align:center">gpu</td>
<td style="text-align:center">cpu</td>
<td style="text-align:center">gpu</td>
<td style="text-align:center">cpu （gpu版本未开放）</td>
</tr>
<tr>
<td style="text-align:center">计算速度</td>
<td style="text-align:center">慢</td>
<td style="text-align:center">慢</td>
<td style="text-align:center">慢</td>
<td style="text-align:center">很快</td>
<td style="text-align:center">很快</td>
<td style="text-align:center">极快</td>
<td style="text-align:center">极快</td>
</tr>
<tr>
<td style="text-align:center">库大小</td>
<td style="text-align:center">大</td>
<td style="text-align:center">较大</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">小</td>
<td style="text-align:center">小</td>
<td style="text-align:center">小</td>
<td style="text-align:center">小</td>
</tr>
<tr>
<td style="text-align:center">兼容性</td>
<td style="text-align:center">好</td>
<td style="text-align:center">好</td>
<td style="text-align:center">好</td>
<td style="text-align:center">限ios8以上</td>
<td style="text-align:center">很好</td>
<td style="text-align:center">仅支持 ios11</td>
<td style="text-align:center">很好</td>
</tr>
<tr>
<td style="text-align:center">模型支持度</td>
<td style="text-align:center">很好</td>
<td style="text-align:center">好</td>
<td style="text-align:center">-</td>
<td style="text-align:center">差（仅限指定模型）</td>
<td style="text-align:center">较好</td>
<td style="text-align:center">-</td>
<td style="text-align:center">中等（当前版本不支持mobilenet）</td>
</tr>
</tbody>
</table>
</div>
<p><strong>使用方法-模型转换</strong></p>
<p>绝影支持的是私有的模型文件格式，需要把 caffe 训练出来的模型转换为 .prestissimo 格式，模型转换工具为 caffe2Prestissimo.out。caffe2Prestissimo.out 依赖 protobuf 3.30。将 XXX.prototxt 和 YYY.caffemodel 转化为 Prestissimo 模型 ZZZ.prestissimo：（得到）./caffe2Prestissimo.out XXX.prototxt YYY.caffemodel ZZZ.prestissimo</p>
<h3 id="MDL（mobile-deep-learning）"><a href="#MDL（mobile-deep-learning）" class="headerlink" title="MDL（mobile-deep-learning）"></a>MDL（mobile-deep-learning）</h3><p>１、开源时间：2017年9月（已暂停更新）　　　</p>
<p>２、开源用户：百度　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/allonli/mobile-deep-learning">https://github.com/allonli/mobile-deep-learning</a></p>
<p>４、功能特点：</p>
<ul>
<li>一键部署，脚本参数就可以切换ios或者android</li>
<li>支持iOS  gpu运行MobileNet、squeezenet模型</li>
<li>已经测试过可以稳定运行MobileNet、GoogLeNet v1、squeezenet、ResNet-50模型</li>
<li>体积极小，无任何第三方依赖。纯手工打造。</li>
<li>提供量化函数，对32位float转8位uint直接支持，模型体积量化后4M上下</li>
<li>与ARM相关算法团队线上线下多次沟通，针对ARM平台会持续优化</li>
<li>NEON使用涵盖了卷积、归一化、池化所有方面的操作</li>
<li>汇编优化，针对寄存器汇编操作具体优化</li>
<li>loop unrolling 循环展开，为提升性能减少不必要的CPU消耗，全部展开判断操作</li>
<li>将大量繁重的计算任务前置到overhead过程</li>
</ul>
<p>5、框架结构</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/MDL1.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/MDL1.png" srcset="data:image/png;base64,666" alt=""></p>
<p>MDL 框架主要包括：<strong>模型转换模块（MDL Converter）、模型加载模块（Loader）、网络管理模块（Net）、矩阵运算模块（Gemmers）及供 Android 端调用的 JNI 接口层（JNI Interfaces）。</strong></p>
<p>​    其中，模型转换模块主要负责将Caffe 模型转为 MDL 模型，同时支持将 32bit 浮点型参数量化为 8bit 参数，从而极大地压缩模型体积；模型加载模块主要完成模型的反量化及加载校验、网络注册等过程，网络管理模块主要负责网络中各层 Layer 的初始化及管理工作；MDL 提供了供 Android 端调用的 JNI 接口层，开发者可以通过调用 JNI 接口轻松完成加载及预测过程。</p>
<p>6、MDL 的性能及兼容性</p>
<ul>
<li>体积 armv7 300k+</li>
<li>速度 iOS GPU mobilenet 可以达到 40ms、squeezenet 可以达到 30ms</li>
</ul>
<p>​        MDL  从立项到开源，已经迭代了一年多。移动端比较关注的多个指标都表现良好，如体积、功耗、速度。百度内部产品线在应用前也进行过多次对比，和已开源的相关项目对比，MDL  能够在保证速度和能耗的同时支持多种深度学习模型，如 mobilenet、googlenet v1、squeezenet 等，且具有 iOS  GPU 版本，squeezenet 一次运行最快可以达到 3-40ms。</p>
<p><strong>同类框架对比</strong></p>
<p>​     框架Caffe2TensorFlowncnnMDL(CPU)MDL(GPU)硬件CPUCPUCPUCPUGPU速度慢慢快快极快体积大大小小小兼容Android&amp;iOSAndroid&amp;iOSAndroid&amp;iOSAndroid&amp;iOSiOS</p>
<p>​     与支持 CNN 的移动端框架对比，MDL 速度快、性能稳定、兼容性好、demo 完备。</p>
<p><strong>兼容性</strong></p>
<p>​     MDL 在 iOS 和 Android 平台均可以稳定运行，其中 iOS10 及以上平台有基于 GPU 运算的 API，性能表现非常出色，在 Android 平台则是纯 CPU 运行。高中低端机型运行状态和手机百度及其他 App 上的覆盖都有绝对优势。</p>
<p>​     MDL 同时也支持 Caffe 模型直接转换为 MDL 模型。</p>
<h3 id="Paddle-Mobile"><a href="#Paddle-Mobile" class="headerlink" title="Paddle-Mobile"></a>Paddle-Mobile</h3><p>１、开源时间：持续更新，已到3.0版本　　　</p>
<p>２、开源用户：百度　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/PaddlePaddle/paddle-mobile">https://github.com/PaddlePaddle/paddle-mobile</a>　</p>
<p>４、功能特点：</p>
<p><strong>功能特点</strong></p>
<ul>
<li><p>高性能支持ARM CPU </p>
</li>
<li><p>支持Mali GPU</p>
</li>
<li><p>支持Andreno GPU</p>
</li>
<li><p>支持苹果设备的GPU Metal实现</p>
</li>
<li><p>支持ZU5、ZU9等FPGA开发板</p>
</li>
<li><p>支持树莓派等arm-linux开发板</p>
</li>
</ul>
<h3 id="MACE（-Mobile-AI-Compute-Engine）"><a href="#MACE（-Mobile-AI-Compute-Engine）" class="headerlink" title="MACE（ Mobile AI Compute Engine）"></a>MACE（ Mobile AI Compute Engine）</h3><p>１、开源时间：2018年4月(持续更新，v0.9.0 (2018-07-20))　　　</p>
<p>２、开源用户：小米　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/XiaoMi/mace">https://github.com/XiaoMi/mace</a>    </p>
<p>４、简介：Mobile AI Compute Engine (MACE) 是一个专为移动端异构计算设备优化的深度学习前向预测框架。<br>MACE覆盖了常见的移动端计算设备（CPU，GPU和DSP），并且提供了完整的工具链和文档，用户借助MACE能够很方便地在移动端部署深度学习模型。MACE已经在小米内部广泛使用并且被充分验证具有业界领先的性能和稳定性。</p>
<p>5、MACE的基本框架：</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/mace-arch.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/mace-arch.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>MACE Model</strong></p>
<p>MACE定义了自有的模型格式（类似于Caffe2），通过MACE提供的工具可以将Caffe和TensorFlow的模型 转为MACE模型。</p>
<p><strong>MACE Interpreter</strong></p>
<p>MACE Interpreter主要负责解析运行神经网络图（DAG）并管理网络中的Tensors。</p>
<p><strong>Runtime</strong></p>
<p>CPU/GPU/DSP Runtime对应于各个计算设备的算子实现。</p>
<p>6、MACE使用的基本流程</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/mace-work-flow-zh.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/mace-work-flow-zh.png" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>1. 配置模型部署文件(.yml)</strong></p>
<p>模型部署文件详细描述了需要部署的模型以及生成库的信息，MACE根据该文件最终生成对应的库文件。</p>
<p><strong>2.编译MACE库</strong></p>
<p>编译MACE的静态库或者动态库。</p>
<p><strong>3.转换模型</strong></p>
<p>将TensorFlow 或者 Caffe的模型转为MACE的模型。</p>
<p><strong>4.1. 部署</strong></p>
<p>根据不同使用目的集成Build阶段生成的库文件，然后调用MACE相应的接口执行模型。</p>
<p><strong>4.2. 命令行运行</strong></p>
<p>MACE提供了命令行工具，可以在命令行运行模型，可以用来测试模型运行时间，内存占用和正确性。</p>
<p><strong>4.3. Benchmark</strong></p>
<p>MACE提供了命令行benchmark工具，可以细粒度的查看模型中所涉及的所有算子的运行时间。</p>
<p>7、MACE在哪些角度进行了优化?</p>
<p><strong>MACE</strong> 专为移动端异构计算平台优化的神经网络计算框架。主要从以下的角度做了专门的优化：</p>
<ul>
<li>性能<ul>
<li>代码经过NEON指令，OpenCL以及Hexagon HVX专门优化，并且采用<br><a href="https://arxiv.org/abs/1509.09308">Winograd算法</a>来进行卷积操作的加速。<br>此外，还对启动速度进行了专门的优化。</li>
</ul>
</li>
<li><p>功耗</p>
<ul>
<li>支持芯片的功耗管理，例如ARM的big.LITTLE调度，以及高通Adreno GPU功耗选项。</li>
</ul>
</li>
<li>系统响应<ul>
<li>支持自动拆解长时间的OpenCL计算任务，来保证UI渲染任务能够做到较好的抢占调度，<br>从而保证系统UI的相应和用户体验。</li>
</ul>
</li>
<li>内存占用<ul>
<li>通过运用内存依赖分析技术，以及内存复用，减少内存的占用。另外，保持尽量少的外部<br>依赖，保证代码尺寸精简。</li>
</ul>
</li>
<li><p>模型加密与保护</p>
<ul>
<li>模型保护是重要设计目标之一。支持将模型转换成C++代码，以及关键常量字符混淆，增加逆向的难度。</li>
</ul>
</li>
<li>硬件支持范围<ul>
<li>支持高通，联发科，以及松果等系列芯片的CPU，GPU与DSP(目前仅支持Hexagon)计算加速。</li>
<li>同时支持在具有POSIX接口的系统的CPU上运行。</li>
</ul>
</li>
</ul>
<p>8、性能对比：</p>
<p>MACE 支持 TensorFlow 和 Caffe 模型，提供转换工具，可以将训练好的模型转换成专有的模型数据文件，同时还可以选择将模型转换成C++代码，支持生成动态库或者静态库，提高模型保密性。</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/maca_com.jpg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/maca_com.jpg" srcset="data:image/png;base64,666" alt=""></p>
<h3 id="FeatherCNN"><a href="#FeatherCNN" class="headerlink" title="FeatherCNN"></a>FeatherCNN</h3><p>１、开源时间：持续更新，已到3.0版本　　　</p>
<p>２、开源用户：腾讯AI　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/Tencent/FeatherCNN">https://github.com/Tencent/FeatherCNN</a></p>
<p>４、功能特点：</p>
<p><strong>FeatherCNN 是由腾讯 AI 平台部研发的基于 ARM 架构的高效 CNN 推理库，该项目支持 Caffe 模型，且具有高性能、易部署、轻量级三大特性。</strong></p>
<p><strong>该项目具体特性如下：</strong></p>
<ul>
<li><p>高性能：无论是在移动设备（iOS / Android），嵌入式设备（Linux）还是基于 ARM 的服务器（Linux）上，FeatherCNN 均能发挥最先进的推理计算性能；</p>
</li>
<li><p>易部署：FeatherCNN 的所有内容都包含在一个代码库中，以消除第三方依赖关系。因此，它便于在移动平台上部署。FeatherCNN 自身的模型格式与 Caffe 模型完全兼容。</p>
</li>
<li><p>轻量级：编译后的 FeatherCNN 库的体积仅为数百 KB。</p>
</li>
</ul>
<h3 id="TensorFlow-Lite"><a href="#TensorFlow-Lite" class="headerlink" title="TensorFlow Lite"></a>TensorFlow Lite</h3><p>１、开源时间：2017年11月　　　</p>
<p>２、开源用户：谷歌　　　</p>
<p>３、GitHub地址：<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite</a></p>
<p>４、简介：</p>
<p>Google 表示 Lite 版本 TensorFlow 是 TensorFlow Mobile 的一个延伸版本。此前，通过TensorFlow Mobile API，TensorFlow已经支持手机上的模型嵌入式部署。TensorFlow Lite应该被视为TensorFlow Mobile的升级版。</p>
<p>TensorFlow Lite可以与Android 8.1中发布的神经网络API完美配合，即便在没有硬件加速时也能调用CPU处理，确保模型在不同设备上的运行。 而Android端版本演进的控制权是掌握在谷歌手中的，从长期看，TensorFlow Lite会得到Android系统层面上的支持。</p>
<p>5、架构：</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/tflite_artc.JPEG" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/tflite_artc.JPEG" srcset="data:image/png;base64,666" alt=""></p>
<p>其组件包括：</p>
<ul>
<li>TensorFlow 模型（TensorFlow Model）：保存在磁盘中的训练模型。</li>
<li>TensorFlow Lite 转化器（TensorFlow Lite Converter）：将模型转换成 TensorFlow Lite 文件格式的项目。</li>
<li>TensorFlow Lite 模型文件（TensorFlow Lite Model File）：基于 FlatBuffers，适配最大速度和最小规模的模型。</li>
</ul>
<p>6、移动端开发步骤：</p>
<p>Android Studio 3.0, SDK Version API26, NDK Version 14</p>
<p>步骤：</p>
<ol>
<li><p>将此项目导入到Android Studio：<br> <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo</a></p>
</li>
<li><p>下载移动端的模型（model）和标签数据（lables）：<br> <a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip">https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip</a></p>
</li>
<li><p>下载完成解压mobilenet_v1_224_android_quant_2017_11_08.zip文件得到一个xxx.tflite和labes.txt文件，分别是模型和标签文件，并且把这两个文件复制到assets文件夹下。</p>
</li>
<li><p>构建app，run……</p>
</li>
</ol>
<p>17.7.9 TensorFlow Lite和TensorFlow Mobile的区别？</p>
<ul>
<li>TensorFlow Lite是TensorFlow Mobile的进化版。</li>
<li>在大多数情况下，TensorFlow Lite拥有跟小的二进制大小，更少的依赖以及更好的性能。</li>
<li>相比TensorFlow Mobile是对完整TensorFlow的裁减，TensorFlow Lite基本就是重新实现了。从内部实现来说，在TensorFlow内核最基本的OP，Context等数据结构，都是新的。从外在表现来说，模型文件从PB格式改成了FlatBuffers格式，TensorFlow的size有大幅度优化，降至300K，然后提供一个converter将普通TensorFlow模型转化成TensorFlow Lite需要的格式。因此，无论从哪方面看，TensorFlow Lite都是一个新的实现方案。</li>
</ul>
<h3 id="PocketFlow"><a href="#PocketFlow" class="headerlink" title="PocketFlow"></a>PocketFlow</h3><p>１、开源时间：2018年9月　　　</p>
<p>２、开源用户：腾讯　　　</p>
<p>３、GitHub地址：<a href="https://github.com/Tencent/PocketFlow">https://github.com/Tencent/PocketFlow</a></p>
<p>４、简介：</p>
<p>全球首个自动模型压缩框架</p>
<p>一款面向移动端AI开发者的自动模型压缩框架，集成了当前主流的模型压缩与训练算法，结合自研超参数优化组件实现了全程自动化托管式的模型压缩与加速。开发者无需了解具体算法细节，即可快速地将AI技术部署到移动端产品上，实现了自动托管式模型压缩与加速，实现用户数据的本地高效处理。</p>
<p>5、框架介绍</p>
<p>PocketFlow 框架主要由两部分组件构成，分别是模型压缩/加速算法组件和超参数优化组件，具体结构如下图所示。</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/framework_design.png" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/framework_design.png" srcset="data:image/png;base64,666" alt=""></p>
<p>​    开发者将未压缩的原始模型作为 PocketFlow 框架的输入，同时指定期望的性能指标，例如模型的压缩和/或加速倍数；在每一轮迭代过程中，超参数优化组件选取一组超参数取值组合，之后模型压缩/加速算法组件基于该超参数取值组合，对原始模型进行压缩，得到一个压缩后的候选模型；基于对候选模型进行性能评估的结果，超参数优化组件调整自身的模型参数，并选取一组新的超参数取值组合，以开始下一轮迭代过程；当迭代终止时，PocketFlow 选取最优的超参数取值组合以及对应的候选模型，作为最终输出，返回给开发者用作移动端的模型部署。</p>
<p>6、PocketFlow如何实现模型压缩与加速？</p>
<p>​    具体地，PocketFlow 通过下列各个算法组件的有效结合，实现了精度损失更小、自动化程度更高的深度学习模型的压缩与加速：</p>
<ul>
<li><p>a) 通道剪枝（channel pruning）组件：在CNN网络中，通过对特征图中的通道维度进行剪枝，可以同时降低模型大小和计算复杂度，并且压缩后的模型可以直接基于现有的深度学习框架进行部署。在CIFAR-10图像分类任务中，通过对  ResNet-56 模型进行通道剪枝，可以实现2.5倍加速下分类精度损失0.4%，3.3倍加速下精度损失0.7%。</p>
</li>
<li><p>b) 权重稀疏化（weight sparsification）组件：通过对网络权重引入稀疏性约束，可以大幅度降低网络权重中的非零元素个数；压缩后模型的网络权重可以以稀疏矩阵的形式进行存储和传输，从而实现模型压缩。对于  MobileNet 图像分类模型，在删去50%网络权重后，在 ImageNet 数据集上的 Top-1 分类精度损失仅为0.6%。</p>
</li>
<li><p>c) 权重量化（weight quantization）组件：通过对网络权重引入量化约束，可以降低用于表示每个网络权重所需的比特数；团队同时提供了对于均匀和非均匀两大类量化算法的支持，可以充分利用  ARM 和 FPGA 等设备的硬件优化，以提升移动端的计算效率，并为未来的神经网络芯片设计提供软件支持。以用于 ImageNet  图像分类任务的 ResNet-18 模型为例，在8比特定点量化下可以实现精度无损的4倍压缩。</p>
</li>
<li><p>d)网络蒸馏（network distillation）组件：对于上述各种模型压缩组件，通过将未压缩的原始模型的输出作为额外的监督信息，指导压缩后模型的训练，在压缩/加速倍数不变的前提下均可以获得0.5%-2.0%不等的精度提升。</p>
</li>
<li><p>e) 多GPU训练（multi-GPU training）组件：深度学习模型训练过程对计算资源要求较高，单个GPU难以在短时间内完成模型训练，因此团队提供了对于多机多卡分布式训练的全面支持，以加快使用者的开发流程。无论是基于  ImageNet 数据的Resnet-50图像分类模型还是基于 WMT14 数据的 Transformer  机器翻译模型，均可以在一个小时内训练完毕。[1] </p>
</li>
<li><p>f) 超参数优化（hyper-parameter optimization）组件：多数开发者对模型压缩算法往往不甚了解，但超参数取值对最终结果往往有着巨大的影响，因此团队引入了超参数优化组件，采用了包括强化学习等算法以及  AI Lab 自研的 AutoML  自动超参数优化框架来根据具体性能需求，确定最优超参数取值组合。例如，对于通道剪枝算法，超参数优化组件可以自动地根据原始模型中各层的冗余程度，对各层采用不同的剪枝比例，在保证满足模型整体压缩倍数的前提下，实现压缩后模型识别精度的最大化。</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/packflow1.jpg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/packflow1.jpg" srcset="data:image/png;base64,666" alt=""></p>
</li>
</ul>
<p>7、PocketFlow 性能</p>
<p>​    通过引入超参数优化组件，不仅避免了高门槛、繁琐的人工调参工作，同时也使得  PocketFlow 在各个压缩算法上全面超过了人工调参的效果。以图像分类任务为例，在 CIFAR-10 和 ImageNet  等数据集上，PocketFlow 对 ResNet 和 MobileNet 等多种 CNN 网络结构进行有效的模型压缩与加速。</p>
<p>​    在  CIFAR-10 数据集上，PocketFlow 以 ResNet-56  作为基准模型进行通道剪枝，并加入了超参数优化和网络蒸馏等训练策略，实现了 2.5 倍加速下分类精度损失 0.4%，3.3 倍加速下精度损失  0.7%，且显著优于未压缩的 ResNet-44 模型； 在 ImageNet 数据集上，PocketFlow 可以对原本已经十分精简的  MobileNet 模型继续进行权重稀疏化，以更小的模型尺寸取得相似的分类精度；与 Inception-V1、ResNet-18  等模型相比，模型大小仅为后者的约 20~40%，但分类精度基本一致（甚至更高）。</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/packflow2.jpg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/packflow2.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/packflow3.jpg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/packflow3.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p>相比于费时费力的人工调参，PocketFlow 框架中的 AutoML 自动超参数优化组件仅需 10<br>余次迭代就能达到与人工调参类似的性能，在经过 100 次迭代后搜索得到的超参数组合可以降低约 0.6%<br>的精度损失；通过使用超参数优化组件自动地确定网络中各层权重的量化比特数，PocketFlow 在对用于 ImageNet 图像分类任务的<br>ResNet-18 模型进行压缩时，取得了一致性的性能提升；当平均量化比特数为 4 比特时，超参数优化组件的引入可以将分类精度从 63.6%<br>提升至 68.1%（原始模型的分类精度为 70.3%）。</p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/packflow4.jpg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/packflow4.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p><img src="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/packflow5.jpg" class="lazyload" data-srcset="/zh-TW/ch17_%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/ch17/packflow5.jpg" srcset="data:image/png;base64,666" alt=""></p>
<p><strong>参考文献</strong></p>
<p>[1]  Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Jiezhang Cao,  Qingyao Wu, Junzhou Huang, Jinhui Zhu,「Discrimination-aware Channel  Pruning for Deep Neural Networks”, In Proc. of the 32nd Annual  Conference on Neural Information Processing Systems, NIPS ‘18, Montreal,  Canada, December 2018.</p>
<p>[2] Jiaxiang  Wu, Weidong Huang, Junzhou Huang, Tong Zhang,「Error Compensated  Quantized SGD and its Applications to Large-scale Distributed  Optimization」, In Proc. of the 35th International Conference on Machine  Learning, ICML’18, Stockholm, Sweden, July 2018.</p>
<h3 id="其他几款支持移动端深度学习的开源框架"><a href="#其他几款支持移动端深度学习的开源框架" class="headerlink" title="其他几款支持移动端深度学习的开源框架"></a>其他几款支持移动端深度学习的开源框架</h3><p><a href="https://blog.csdn.net/zchang81/article/details/74280019">https://blog.csdn.net/zchang81/article/details/74280019</a></p>
<h3 id="MDL、NCNN和-TFLite比较"><a href="#MDL、NCNN和-TFLite比较" class="headerlink" title="MDL、NCNN和 TFLite比较"></a>MDL、NCNN和 TFLite比较</h3><p>百度-MDL框架、腾讯-NCNN框架和谷歌TFLite框架比较。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">MDL</th>
<th style="text-align:center">NCNN</th>
<th style="text-align:center">TFLite</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">代码质量</td>
<td style="text-align:center">中</td>
<td style="text-align:center">高</td>
<td style="text-align:center">很高</td>
</tr>
<tr>
<td style="text-align:center">跨平台</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">支持caffe模型</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">×</td>
</tr>
<tr>
<td style="text-align:center">支持TensorFlow模型</td>
<td style="text-align:center">×</td>
<td style="text-align:center">×</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">CPU NEON指令优化</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">GPU加速</td>
<td style="text-align:center">√</td>
<td style="text-align:center">×</td>
<td style="text-align:center">×</td>
</tr>
</tbody>
</table>
</div>
<p>相同点：</p>
<ul>
<li>只含推理（inference）功能，使用的模型文件需要通过离线的方式训练得到。</li>
<li>最终生成的库尺寸较小，均小于500kB。</li>
<li>为了提升执行速度，都使用了ARM NEON指令进行加速。</li>
<li>跨平台，iOS和Android系统都支持。</li>
</ul>
<p>不同点：</p>
<ul>
<li>MDL和NCNN均是只支持Caffe框架生成的模型文件，而TfLite则毫无意外的只支持自家大哥TensorFlow框架生成的模型文件。</li>
<li>MDL支持利用iOS系统的Matal框架进行GPU加速，能够显著提升在iPhone上的运行速度，达到准实时的效果。而NCNN和TFLite还没有这个功能。</li>
</ul>
<h2 id="17-11-移动端开源框架部署"><a href="#17-11-移动端开源框架部署" class="headerlink" title="17.11 移动端开源框架部署"></a>17.11 移动端开源框架部署</h2><h3 id="以NCNN为例"><a href="#以NCNN为例" class="headerlink" title="以NCNN为例"></a>以NCNN为例</h3><p>部署步骤   </p>
<h3 id="以QNNPACK为例"><a href="#以QNNPACK为例" class="headerlink" title="以QNNPACK为例"></a>以QNNPACK为例</h3><p>部署步骤     </p>
<h3 id="在Android手机上使用MACE实现图像分类"><a href="#在Android手机上使用MACE实现图像分类" class="headerlink" title="在Android手机上使用MACE实现图像分类"></a>在Android手机上使用MACE实现图像分类</h3><h3 id="在Android手机上使用PaddleMobile实现图像分类"><a href="#在Android手机上使用PaddleMobile实现图像分类" class="headerlink" title="在Android手机上使用PaddleMobile实现图像分类"></a>在Android手机上使用PaddleMobile实现图像分类</h3><p><strong>编译paddle-mobile库</strong></p>
<p>1）编译Android能够使用的CPP库：编译Android的paddle-mobile库，可选择使用Docker编译和Ubuntu交叉编译，这里介绍使用Ubuntu交叉编译paddle-mobile库。</p>
<p><em>注</em>：在Android项目，Java代码调用CPP代码，CPP的函数需要遵循一定的命名规范，比如Java<em>包名</em>类名_对应的Java的方法名。</p>
<p>​    目前官方提供了5个可以给Java调用的函数，该代码在：paddle-mobile/src/jni/paddle_mobile_jni.cpp，如果想要让这些函数能够在自己的包名下的类调用，就要修改CPP的函数名称修改如下：</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line">JNIEXPORT jboolean JNICALL <span class="title function_">Java_com_baidu_paddle_PML_load</span><span class="params">(JNIEnv *env, </span></span><br><span class="line"><span class="params">	jclass thiz,</span></span><br><span class="line"><span class="params">	jstring modelPath)</span> { </span><br><span class="line">		ANDROIDLOGI(<span class="string">"load invoked"</span>); </span><br><span class="line">		<span class="type">bool</span> <span class="variable">optimize</span> <span class="operator">=</span> <span class="literal">true</span>; </span><br><span class="line">		<span class="keyword">return</span> getPaddleMobileInstance()-&gt;Load(jstring2cppstring(env, modelPath), optimize); }</span><br></pre></td></tr></tbody></table></figure>
<p>​    笔者项目的包名为<code>com.example.paddlemobile1</code>，在这个包下有一个<code>ImageRecognition.java</code>的程序来对应这个CPP程序，那么修改<code>load</code>函数如下：</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line">JNIEXPORT jboolean JNICALL <span class="title function_">Java_com_example_paddlemobile1_ImageRecognition_load</span><span class="params">(JNIEnv *env,</span></span><br><span class="line"><span class="params">                                                          jclass thiz,</span></span><br><span class="line"><span class="params">                                                          jstring modelPath)</span> {</span><br><span class="line">  ANDROIDLOGI(<span class="string">"load invoked"</span>);</span><br><span class="line">  <span class="type">bool</span> <span class="variable">optimize</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">return</span> getPaddleMobileInstance()-&gt;Load(jstring2cppstring(env, modelPath),</span><br><span class="line">                                         optimize);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p><strong>使用Ubuntu交叉编译paddle-mobile库</strong></p>
<p>1、下载和解压NDK。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">wget https://dl.google.com/android/repository/android-ndk-r17b-linux-x86_64.zip</span><br><span class="line">unzip android-ndk-r17b-linux-x86_64.zip</span><br></pre></td></tr></tbody></table></figure>
<p>2、设置NDK环境变量，目录是NDK的解压目录。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">export NDK_ROOT="/home/test/paddlepaddle/android-ndk-r17b"</span><br></pre></td></tr></tbody></table></figure>
<p>设置好之后，可以使用以下的命令查看配置情况。</p>
<pre><code>root@test:/home/test/paddlepaddle# echo $NDK_ROOT
/home/test/paddlepaddle/android-ndk-r17b
</code></pre><p>3、安装cmake，需要安装较高版本的，笔者的cmake版本是3.11.2。</p>
<p>下载cmake源码</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">wget https://cmake.org/files/v3.11/cmake-3.11.2.tar.gz</span><br></pre></td></tr></tbody></table></figure>
<p>解压cmake源码</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">tar -zxvf cmake-3.11.2.tar.gz</span><br></pre></td></tr></tbody></table></figure>
<p>进入到cmake源码根目录，并执行bootstrap。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">cd cmake-3.11.2</span><br><span class="line">./bootstrap</span><br></pre></td></tr></tbody></table></figure>
<p>最后执行以下两条命令开始安装cmake。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></tbody></table></figure>
<p>安装完成之后，可以使用cmake —version是否安装成功.</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">root@test:/home/test/paddlepaddle# cmake --version</span><br><span class="line">cmake version 3.11.2</span><br><span class="line"></span><br><span class="line">CMake suite maintained and supported by Kitware (kitware.com/cmake).</span><br></pre></td></tr></tbody></table></figure>
<p>4、克隆paddle-mobile源码。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">git clone https://github.com/PaddlePaddle/paddle-mobile.git</span><br></pre></td></tr></tbody></table></figure>
<p>5、进入到paddle-mobile的tools目录下，执行编译。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">cd paddle-mobile/tools/</span><br><span class="line">sh build.sh android</span><br></pre></td></tr></tbody></table></figure>
<p>（可选）如果想编译针对某一个网络编译更小的库时，可以在命令后面加上相应的参数，如下：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">sh build.sh android googlenet</span><br></pre></td></tr></tbody></table></figure>
<p>6、最后会在paddle-mobile/build/release/arm-v7a/build目录下生产paddle-mobile库。</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line">root<span class="meta">@test</span>:/home/test/paddlepaddle/paddle-mobile/build/release/arm-v7a/build# ls</span><br><span class="line">libpaddle-mobile.so</span><br></pre></td></tr></tbody></table></figure>
<p>libpaddle-mobile.so就是我们在开发Android项目的时候使用到的paddle-mobile库。</p>
<p><strong>创建Android项目</strong></p>
<p>1、首先使用Android Studio创建一个普通的Android项目，包名为<code>com.example.paddlemobile1</code></p>
<p>2、在main目录下创建l两个assets/paddle_models文件夹，这个文件夹存放PaddleFluid训练好的预测模型。PaddleMobile支持量化模型，使用模型量化可以把模型缩小至原来的四分之一，如果使用量化模型，那加载模型的接口也有修改一下，使用以下的接口加载模型：</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="type">boolean</span> <span class="title function_">loadQualified</span><span class="params">(String modelDir)</span>;</span><br></pre></td></tr></tbody></table></figure>
<p>3、在<code>main</code>目录下创建一个<code>jniLibs</code>文件夹，这个文件夹是存放CPP编译库的，在本项目中就存放上一部分编译的<code>libpaddle-mobile.so</code></p>
<p>4、在Android项目的配置文件夹中加上权限声明，因为我们要使用到读取相册和使用相机，所以加上以下的权限声明：</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line">&lt;uses-permission android:name=<span class="string">"android.permission.CAMERA"</span> /&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">"android.permission.WRITE_EXTERNAL_STORAGE"</span> /&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">"android.permission.READ_EXTERNAL_STORAGE"</span> /&gt;</span><br></pre></td></tr></tbody></table></figure>
<p>5、修改<code>activity_main.xml</code>界面，修改成如下：</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"utf-8"</span>?&gt;</span><br><span class="line">&lt;RelativeLayout xmlns:android=<span class="string">"http://schemas.android.com/apk/res/android"</span></span><br><span class="line">    xmlns:app=<span class="string">"http://schemas.android.com/apk/res-auto"</span></span><br><span class="line">    xmlns:tools=<span class="string">"http://schemas.android.com/tools"</span></span><br><span class="line">    android:layout_width=<span class="string">"match_parent"</span></span><br><span class="line">    android:layout_height=<span class="string">"match_parent"</span></span><br><span class="line">    tools:context=<span class="string">".MainActivity"</span>&gt;</span><br><span class="line">&lt;LinearLayout</span><br><span class="line">    android:id=<span class="string">"@+id/btn_ll"</span></span><br><span class="line">    android:layout_alignParentBottom=<span class="string">"true"</span></span><br><span class="line">    android:layout_width=<span class="string">"match_parent"</span></span><br><span class="line">    android:layout_height=<span class="string">"wrap_content"</span></span><br><span class="line">    android:orientation=<span class="string">"horizontal"</span>&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Button</span><br><span class="line">        android:id=<span class="string">"@+id/use_photo"</span></span><br><span class="line">        android:layout_weight=<span class="string">"1"</span></span><br><span class="line">        android:layout_width=<span class="string">"0dp"</span></span><br><span class="line">        android:layout_height=<span class="string">"wrap_content"</span></span><br><span class="line">        android:text=<span class="string">"相册"</span> /&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Button</span><br><span class="line">        android:id=<span class="string">"@+id/start_camera"</span></span><br><span class="line">        android:layout_weight=<span class="string">"1"</span></span><br><span class="line">        android:layout_width=<span class="string">"0dp"</span></span><br><span class="line">        android:layout_height=<span class="string">"wrap_content"</span></span><br><span class="line">        android:text=<span class="string">"拍照"</span> /&gt;</span><br><span class="line">&lt;/LinearLayout&gt;</span><br><span class="line"></span><br><span class="line">&lt;TextView</span><br><span class="line">    android:layout_above=<span class="string">"@id/btn_ll"</span></span><br><span class="line">    android:id=<span class="string">"@+id/result_text"</span></span><br><span class="line">    android:textSize=<span class="string">"16sp"</span></span><br><span class="line">    android:layout_width=<span class="string">"match_parent"</span></span><br><span class="line">    android:hint=<span class="string">"预测结果会在这里显示"</span></span><br><span class="line">    android:layout_height=<span class="string">"100dp"</span> /&gt;</span><br><span class="line"></span><br><span class="line">&lt;ImageView</span><br><span class="line">    android:layout_alignParentTop=<span class="string">"true"</span></span><br><span class="line">    android:layout_above=<span class="string">"@id/result_text"</span></span><br><span class="line">    android:id=<span class="string">"@+id/show_image"</span></span><br><span class="line">    android:layout_width=<span class="string">"match_parent"</span></span><br><span class="line">    android:layout_height=<span class="string">"match_parent"</span> /&gt;</span><br><span class="line">&lt;/RelativeLayout&gt;</span><br></pre></td></tr></tbody></table></figure>
<p>6、创建一个<code>ImageRecognition.java</code>的Java程序，这个程序的作用就是调用<code>paddle-mobile/src/jni/paddle_mobile_jni.cpp</code>的函数，对应的是里面的函数。目前支持一下几个接口。</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.paddlemobile1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ImageRecognition</span> {</span><br><span class="line">    <span class="comment">// set thread num</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title function_">setThread</span><span class="params">(<span class="type">int</span> threadCount)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Load seperated parameters</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="type">boolean</span> <span class="title function_">load</span><span class="params">(String modelDir)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// load qualified model</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="type">boolean</span> <span class="title function_">loadQualified</span><span class="params">(String modelDir)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Load combined parameters</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="type">boolean</span> <span class="title function_">loadCombined</span><span class="params">(String modelPath, String paramPath)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// load qualified model</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="type">boolean</span> <span class="title function_">loadCombinedQualified</span><span class="params">(String modelPath, String paramPath)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// object detection</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="type">float</span>[] predictImage(<span class="type">float</span>[] buf, <span class="type">int</span>[]ddims);</span><br><span class="line"></span><br><span class="line"><span class="comment">// predict yuv image</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="type">float</span>[] predictYuv(<span class="type">byte</span>[] buf, <span class="type">int</span> imgWidth, <span class="type">int</span> imgHeight, <span class="type">int</span>[] ddims, <span class="type">float</span>[]meanValues);</span><br><span class="line"></span><br><span class="line"><span class="comment">// clear model</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title function_">clear</span><span class="params">()</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>7、然后编写一个<code>PhotoUtil.java</code>的工具类。</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.paddlemobile1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> android.app.Activity;</span><br><span class="line"><span class="keyword">import</span> android.content.Context;</span><br><span class="line"><span class="keyword">import</span> android.content.Intent;</span><br><span class="line"><span class="keyword">import</span> android.database.Cursor;</span><br><span class="line"><span class="keyword">import</span> android.graphics.Bitmap;</span><br><span class="line"><span class="keyword">import</span> android.graphics.BitmapFactory;</span><br><span class="line"><span class="keyword">import</span> android.net.Uri;</span><br><span class="line"><span class="keyword">import</span> android.os.Build;</span><br><span class="line"><span class="keyword">import</span> android.provider.MediaStore;</span><br><span class="line"><span class="keyword">import</span> android.support.v4.content.FileProvider;</span><br><span class="line"><span class="keyword">import</span> android.util.Log;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">PhotoUtil</span> {</span><br><span class="line"><span class="comment">// start camera</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Uri <span class="title function_">start_camera</span><span class="params">(Activity activity, <span class="type">int</span> requestCode)</span> {</span><br><span class="line">    Uri imageUri;</span><br><span class="line">    <span class="comment">// save image in cache path</span></span><br><span class="line">    <span class="type">File</span> <span class="variable">outputImage</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(activity.getExternalCacheDir(), <span class="string">"out_image.jpg"</span>);</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="keyword">if</span> (outputImage.exists()) {</span><br><span class="line">            outputImage.delete();</span><br><span class="line">        }</span><br><span class="line">        outputImage.createNewFile();</span><br><span class="line">    } <span class="keyword">catch</span> (IOException e) {</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">if</span> (Build.VERSION.SDK_INT &gt;= <span class="number">24</span>) {</span><br><span class="line">        <span class="comment">// compatible with Android 7.0 or over</span></span><br><span class="line">        imageUri = FileProvider.getUriForFile(activity,</span><br><span class="line">                <span class="string">"com.example.paddlemobile1"</span>, outputImage);</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        imageUri = Uri.fromFile(outputImage);</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// set system camera Action</span></span><br><span class="line">    <span class="type">Intent</span> <span class="variable">intent</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Intent</span>(MediaStore.ACTION_IMAGE_CAPTURE);</span><br><span class="line">    <span class="comment">// set save photo path</span></span><br><span class="line">    intent.putExtra(MediaStore.EXTRA_OUTPUT, imageUri);</span><br><span class="line">    <span class="comment">// set photo quality, min is 0, max is 1</span></span><br><span class="line">    intent.putExtra(MediaStore.EXTRA_VIDEO_QUALITY, <span class="number">0</span>);</span><br><span class="line">    activity.startActivityForResult(intent, requestCode);</span><br><span class="line">    <span class="keyword">return</span> imageUri;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// get picture in photo</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">use_photo</span><span class="params">(Activity activity, <span class="type">int</span> requestCode)</span>{</span><br><span class="line">    <span class="type">Intent</span> <span class="variable">intent</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Intent</span>(Intent.ACTION_PICK);</span><br><span class="line">    intent.setType(<span class="string">"image/*"</span>);</span><br><span class="line">    activity.startActivityForResult(intent, requestCode);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// get photo from Uri</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title function_">get_path_from_URI</span><span class="params">(Context context, Uri uri)</span> {</span><br><span class="line">    String result;</span><br><span class="line">    <span class="type">Cursor</span> <span class="variable">cursor</span> <span class="operator">=</span> context.getContentResolver().query(uri, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>);</span><br><span class="line">    <span class="keyword">if</span> (cursor == <span class="literal">null</span>) {</span><br><span class="line">        result = uri.getPath();</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        cursor.moveToFirst();</span><br><span class="line">        <span class="type">int</span> <span class="variable">idx</span> <span class="operator">=</span> cursor.getColumnIndex(MediaStore.Images.ImageColumns.DATA);</span><br><span class="line">        result = cursor.getString(idx);</span><br><span class="line">        cursor.close();</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Compress the image to the size of the training image，and change RGB</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">float</span>[] getScaledMatrix(Bitmap bitmap, <span class="type">int</span> desWidth,</span><br><span class="line">                               <span class="type">int</span> desHeight) {</span><br><span class="line">    <span class="type">float</span>[] dataBuf = <span class="keyword">new</span> <span class="title class_">float</span>[<span class="number">3</span> * desWidth * desHeight];</span><br><span class="line">    <span class="type">int</span> rIndex;</span><br><span class="line">    <span class="type">int</span> gIndex;</span><br><span class="line">    <span class="type">int</span> bIndex;</span><br><span class="line">    <span class="type">int</span>[] pixels = <span class="keyword">new</span> <span class="title class_">int</span>[desWidth * desHeight];</span><br><span class="line">    <span class="type">Bitmap</span> <span class="variable">bm</span> <span class="operator">=</span> Bitmap.createScaledBitmap(bitmap, desWidth, desHeight, <span class="literal">false</span>);</span><br><span class="line">    bm.getPixels(pixels, <span class="number">0</span>, desWidth, <span class="number">0</span>, <span class="number">0</span>, desWidth, desHeight);</span><br><span class="line">    <span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> <span class="variable">k</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; pixels.length; i++) {</span><br><span class="line">        <span class="type">int</span> <span class="variable">clr</span> <span class="operator">=</span> pixels[i];</span><br><span class="line">        j = i / desHeight;</span><br><span class="line">        k = i % desWidth;</span><br><span class="line">        rIndex = j * desWidth + k;</span><br><span class="line">        gIndex = rIndex + desHeight * desWidth;</span><br><span class="line">        bIndex = gIndex + desHeight * desWidth;</span><br><span class="line">        dataBuf[rIndex] = (<span class="type">float</span>) ((clr &amp; <span class="number">0x00ff0000</span>) &gt;&gt; <span class="number">16</span>) - <span class="number">148</span>;</span><br><span class="line">        dataBuf[gIndex] = (<span class="type">float</span>) ((clr &amp; <span class="number">0x0000ff00</span>) &gt;&gt; <span class="number">8</span>) - <span class="number">148</span>;</span><br><span class="line">        dataBuf[bIndex] = (<span class="type">float</span>) ((clr &amp; <span class="number">0x000000ff</span>)) - <span class="number">148</span>;</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">if</span> (bm.isRecycled()) {</span><br><span class="line">        bm.recycle();</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> dataBuf;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// compress picture</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Bitmap <span class="title function_">getScaleBitmap</span><span class="params">(String filePath)</span> {</span><br><span class="line">    BitmapFactory.<span class="type">Options</span> <span class="variable">opt</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BitmapFactory</span>.Options();</span><br><span class="line">    opt.inJustDecodeBounds = <span class="literal">true</span>;</span><br><span class="line">    BitmapFactory.decodeFile(filePath, opt);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> <span class="variable">bmpWidth</span> <span class="operator">=</span> opt.outWidth;</span><br><span class="line">    <span class="type">int</span> <span class="variable">bmpHeight</span> <span class="operator">=</span> opt.outHeight;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> <span class="variable">maxSize</span> <span class="operator">=</span> <span class="number">500</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// compress picture with inSampleSize</span></span><br><span class="line">    opt.inSampleSize = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) {</span><br><span class="line">        <span class="keyword">if</span> (bmpWidth / opt.inSampleSize &lt; maxSize || bmpHeight / opt.inSampleSize &lt; maxSize) {</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        }</span><br><span class="line">        opt.inSampleSize *= <span class="number">2</span>;</span><br><span class="line">    }</span><br><span class="line">    opt.inJustDecodeBounds = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">return</span> BitmapFactory.decodeFile(filePath, opt);</span><br><span class="line">}</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>start_camera()方法是启动相机并返回图片的URI。</li>
<li>use_photo()方法是打开相册，获取到的图片URI在回到函数中获取。</li>
<li>get_path_from_URI()方法是把图片的URI转换成绝对路径。</li>
<li>getScaledMatrix()方法是把图片压缩成跟训练时的大小，并转换成预测需要用的数据格式浮点数组。</li>
<li>getScaleBitmap()方法是对图片进行等比例压缩，减少内存的支出。</li>
</ul>
<p>8、最后修改<code>MainActivity.java</code>，修改如下：</p>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.paddlemobile1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> android.Manifest;</span><br><span class="line"><span class="keyword">import</span> android.annotation.SuppressLint;</span><br><span class="line"><span class="keyword">import</span> android.app.Activity;</span><br><span class="line"><span class="keyword">import</span> android.content.Context;</span><br><span class="line"><span class="keyword">import</span> android.content.Intent;</span><br><span class="line"><span class="keyword">import</span> android.content.pm.PackageManager;</span><br><span class="line"><span class="keyword">import</span> android.graphics.Bitmap;</span><br><span class="line"><span class="keyword">import</span> android.net.Uri;</span><br><span class="line"><span class="keyword">import</span> android.os.Bundle;</span><br><span class="line"><span class="keyword">import</span> android.os.Environment;</span><br><span class="line"><span class="keyword">import</span> android.support.annotation.NonNull;</span><br><span class="line"><span class="keyword">import</span> android.support.annotation.Nullable;</span><br><span class="line"><span class="keyword">import</span> android.support.v4.app.ActivityCompat;</span><br><span class="line"><span class="keyword">import</span> android.support.v4.content.ContextCompat;</span><br><span class="line"><span class="keyword">import</span> android.support.v7.app.AppCompatActivity;</span><br><span class="line"><span class="keyword">import</span> android.util.Log;</span><br><span class="line"><span class="keyword">import</span> android.view.View;</span><br><span class="line"><span class="keyword">import</span> android.widget.Button;</span><br><span class="line"><span class="keyword">import</span> android.widget.ImageView;</span><br><span class="line"><span class="keyword">import</span> android.widget.TextView;</span><br><span class="line"><span class="keyword">import</span> android.widget.Toast;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.bumptech.glide.Glide;</span><br><span class="line"><span class="keyword">import</span> com.bumptech.glide.load.engine.DiskCacheStrategy;</span><br><span class="line"><span class="keyword">import</span> com.bumptech.glide.request.RequestOptions;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStream;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MainActivity</span> <span class="keyword">extends</span> <span class="title class_">AppCompatActivity</span> {</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">TAG</span> <span class="operator">=</span> MainActivity.class.getName();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">USE_PHOTO</span> <span class="operator">=</span> <span class="number">1001</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">START_CAMERA</span> <span class="operator">=</span> <span class="number">1002</span>;</span><br><span class="line">    <span class="keyword">private</span> Uri image_uri;</span><br><span class="line">    <span class="keyword">private</span> ImageView show_image;</span><br><span class="line">    <span class="keyword">private</span> TextView result_text;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">String</span> <span class="variable">assets_path</span> <span class="operator">=</span> <span class="string">"paddle_models"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">load_result</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span>[] ddims = {<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>};</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String[] PADDLE_MODEL = {</span><br><span class="line">        <span class="string">"lenet"</span>,</span><br><span class="line">        <span class="string">"alexnet"</span>,</span><br><span class="line">        <span class="string">"vgg16"</span>,</span><br><span class="line">        <span class="string">"resnet"</span>,</span><br><span class="line">        <span class="string">"googlenet"</span>,</span><br><span class="line">        <span class="string">"mobilenet_v1"</span>,</span><br><span class="line">        <span class="string">"mobilenet_v2"</span>,</span><br><span class="line">        <span class="string">"inception_v1"</span>,</span><br><span class="line">        <span class="string">"inception_v2"</span>,</span><br><span class="line">        <span class="string">"squeezenet"</span></span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="comment">// load paddle-mobile api</span></span><br><span class="line"><span class="keyword">static</span> {</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        System.loadLibrary(<span class="string">"paddle-mobile"</span>);</span><br><span class="line"></span><br><span class="line">    } <span class="keyword">catch</span> (SecurityException e) {</span><br><span class="line">        e.printStackTrace();</span><br><span class="line"></span><br><span class="line">    } <span class="keyword">catch</span> (UnsatisfiedLinkError e) {</span><br><span class="line">        e.printStackTrace();</span><br><span class="line"></span><br><span class="line">    } <span class="keyword">catch</span> (NullPointerException e) {</span><br><span class="line">        e.printStackTrace();</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">onCreate</span><span class="params">(Bundle savedInstanceState)</span> {</span><br><span class="line">    <span class="built_in">super</span>.onCreate(savedInstanceState);</span><br><span class="line">    setContentView(R.layout.activity_main);</span><br><span class="line"></span><br><span class="line">    init();</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// initialize view</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> {</span><br><span class="line">    request_permissions();</span><br><span class="line">    show_image = (ImageView) findViewById(R.id.show_image);</span><br><span class="line">    result_text = (TextView) findViewById(R.id.result_text);</span><br><span class="line">    <span class="type">Button</span> <span class="variable">use_photo</span> <span class="operator">=</span> (Button) findViewById(R.id.use_photo);</span><br><span class="line">    <span class="type">Button</span> <span class="variable">start_photo</span> <span class="operator">=</span> (Button) findViewById(R.id.start_camera);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// use photo click</span></span><br><span class="line">    use_photo.setOnClickListener(<span class="keyword">new</span> <span class="title class_">View</span>.OnClickListener() {</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onClick</span><span class="params">(View view)</span> {</span><br><span class="line">            PhotoUtil.use_photo(MainActivity.<span class="built_in">this</span>, USE_PHOTO);</span><br><span class="line">            <span class="comment">//                load_model();</span></span><br><span class="line">            }</span><br><span class="line">        });</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line">    <span class="comment">// start camera click</span></span><br><span class="line">    start_photo.setOnClickListener(<span class="keyword">new</span> <span class="title class_">View</span>.OnClickListener() {</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onClick</span><span class="params">(View view)</span> {</span><br><span class="line">            image_uri = PhotoUtil.start_camera(MainActivity.<span class="built_in">this</span>, START_CAMERA);</span><br><span class="line">        }</span><br><span class="line">    });</span><br><span class="line"></span><br><span class="line">    <span class="comment">// copy file from assets to sdcard</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">sdcard_path</span> <span class="operator">=</span> Environment.getExternalStorageDirectory()</span><br><span class="line">            + File.separator + assets_path;</span><br><span class="line">    copy_file_from_asset(<span class="built_in">this</span>, assets_path, sdcard_path);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// load model</span></span><br><span class="line">    load_model();</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// load infer model</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">load_model</span><span class="params">()</span> {</span><br><span class="line">    <span class="type">String</span> <span class="variable">model_path</span> <span class="operator">=</span> Environment.getExternalStorageDirectory()</span><br><span class="line">            + File.separator + assets_path + File.separator + PADDLE_MODEL[<span class="number">4</span>];</span><br><span class="line">    Log.d(TAG, model_path);</span><br><span class="line">    load_result = ImageRecognition.load(model_path);</span><br><span class="line">    <span class="keyword">if</span> (load_result) {</span><br><span class="line">        Log.d(TAG, <span class="string">"model load success"</span>);</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        Log.d(TAG, <span class="string">"model load fail"</span>);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// clear infer model</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">clear_model</span><span class="params">()</span> {</span><br><span class="line">    ImageRecognition.clear();</span><br><span class="line">    Log.d(TAG, <span class="string">"model is clear"</span>);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// copy file from asset to sdcard</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">copy_file_from_asset</span><span class="params">(Context context, String oldPath, String newPath)</span> {</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        String[] fileNames = context.getAssets().list(oldPath);</span><br><span class="line">        <span class="keyword">if</span> (fileNames.length &gt; <span class="number">0</span>) {</span><br><span class="line">            <span class="comment">// directory</span></span><br><span class="line">            <span class="type">File</span> <span class="variable">file</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(newPath);</span><br><span class="line">            <span class="keyword">if</span> (!file.exists()) {</span><br><span class="line">                file.mkdirs();</span><br><span class="line">            }</span><br><span class="line">            <span class="comment">// copy recursivelyC</span></span><br><span class="line">            <span class="keyword">for</span> (String fileName : fileNames) {</span><br><span class="line">                copy_file_from_asset(context, oldPath + <span class="string">"/"</span> + fileName, newPath + <span class="string">"/"</span> + fileName);</span><br><span class="line">            }</span><br><span class="line">            Log.d(TAG, <span class="string">"copy files finish"</span>);</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="comment">// file</span></span><br><span class="line">            <span class="type">File</span> <span class="variable">file</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(newPath);</span><br><span class="line">            <span class="comment">// if file exists will never copy</span></span><br><span class="line">            <span class="keyword">if</span> (file.exists()) {</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">// copy file to new path</span></span><br><span class="line">            <span class="type">InputStream</span> <span class="variable">is</span> <span class="operator">=</span> context.getAssets().open(oldPath);</span><br><span class="line">            <span class="type">FileOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(file);</span><br><span class="line">            <span class="type">byte</span>[] buffer = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">1024</span>];</span><br><span class="line">            <span class="type">int</span> byteCount;</span><br><span class="line">            <span class="keyword">while</span> ((byteCount = is.read(buffer)) != -<span class="number">1</span>) {</span><br><span class="line">                fos.write(buffer, <span class="number">0</span>, byteCount);</span><br><span class="line">            }</span><br><span class="line">            fos.flush();</span><br><span class="line">            is.close();</span><br><span class="line">            fos.close();</span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">onActivityResult</span><span class="params">(<span class="type">int</span> requestCode, <span class="type">int</span> resultCode, <span class="meta">@Nullable</span> Intent data)</span> {</span><br><span class="line">    String image_path;</span><br><span class="line">    <span class="type">RequestOptions</span> <span class="variable">options</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RequestOptions</span>().skipMemoryCache(<span class="literal">true</span>).diskCacheStrategy(DiskCacheStrategy.NONE);</span><br><span class="line">    <span class="keyword">if</span> (resultCode == Activity.RESULT_OK) {</span><br><span class="line">        <span class="keyword">switch</span> (requestCode) {</span><br><span class="line">            <span class="keyword">case</span> USE_PHOTO:</span><br><span class="line">                <span class="keyword">if</span> (data == <span class="literal">null</span>) {</span><br><span class="line">                    Log.w(TAG, <span class="string">"user photo data is null"</span>);</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                }</span><br><span class="line">                image_uri = data.getData();</span><br><span class="line">                Glide.with(MainActivity.<span class="built_in">this</span>).load(image_uri).apply(options).into(show_image);</span><br><span class="line">                <span class="comment">// get image path from uri</span></span><br><span class="line">                image_path = PhotoUtil.get_path_from_URI(MainActivity.<span class="built_in">this</span>, image_uri);</span><br><span class="line">                <span class="comment">// show result</span></span><br><span class="line">                result_text.setText(image_path);</span><br><span class="line">                <span class="comment">// predict image</span></span><br><span class="line">                predict_image(PhotoUtil.get_path_from_URI(MainActivity.<span class="built_in">this</span>, image_uri));</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> START_CAMERA:</span><br><span class="line">                <span class="comment">// show photo</span></span><br><span class="line">                Glide.with(MainActivity.<span class="built_in">this</span>).load(image_uri).apply(options).into(show_image);</span><br><span class="line">                <span class="comment">// get image path from uri</span></span><br><span class="line">                image_path = PhotoUtil.get_path_from_URI(MainActivity.<span class="built_in">this</span>, image_uri);</span><br><span class="line">                <span class="comment">// show result</span></span><br><span class="line">                result_text.setText(image_path);</span><br><span class="line">                <span class="comment">// predict image</span></span><br><span class="line">                predict_image(PhotoUtil.get_path_from_URI(MainActivity.<span class="built_in">this</span>, image_uri));</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="meta">@SuppressLint("SetTextI18n")</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">predict_image</span><span class="params">(String image_path)</span> {</span><br><span class="line">    <span class="comment">// picture to float array</span></span><br><span class="line">    <span class="type">Bitmap</span> <span class="variable">bmp</span> <span class="operator">=</span> PhotoUtil.getScaleBitmap(image_path);</span><br><span class="line">    <span class="type">float</span>[] inputData = PhotoUtil.getScaledMatrix(bmp, ddims[<span class="number">2</span>], ddims[<span class="number">3</span>]);</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">        <span class="comment">// get predict result</span></span><br><span class="line">        <span class="type">float</span>[] result = ImageRecognition.predictImage(inputData, ddims);</span><br><span class="line">        Log.d(TAG, <span class="string">"origin predict result:"</span> + Arrays.toString(result));</span><br><span class="line">        <span class="type">long</span> <span class="variable">end</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">        <span class="type">long</span> <span class="variable">time</span> <span class="operator">=</span> end - start;</span><br><span class="line">        Log.d(<span class="string">"result length"</span>, String.valueOf(result.length));</span><br><span class="line">        <span class="comment">// show predict result and time</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">r</span> <span class="operator">=</span> get_max_result(result);</span><br><span class="line">        <span class="type">String</span> <span class="variable">show_text</span> <span class="operator">=</span> <span class="string">"result："</span> + r + <span class="string">"\nprobability："</span> + result[r] + <span class="string">"\ntime："</span> + time + <span class="string">"ms"</span>;</span><br><span class="line">        result_text.setText(show_text);</span><br><span class="line">    } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="type">int</span> <span class="title function_">get_max_result</span><span class="params">(<span class="type">float</span>[] result)</span> {</span><br><span class="line">    <span class="type">float</span> <span class="variable">probability</span> <span class="operator">=</span> result[<span class="number">0</span>];</span><br><span class="line">    <span class="type">int</span> <span class="variable">r</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; result.length; i++) {</span><br><span class="line">        <span class="keyword">if</span> (probability &lt; result[i]) {</span><br><span class="line">            probability = result[i];</span><br><span class="line">            r = i;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> r;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// request permissions</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">request_permissions</span><span class="params">()</span> {</span><br><span class="line"></span><br><span class="line">    List&lt;String&gt; permissionList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    <span class="keyword">if</span> (ContextCompat.checkSelfPermission(<span class="built_in">this</span>, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {</span><br><span class="line">        permissionList.add(Manifest.permission.CAMERA);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ContextCompat.checkSelfPermission(<span class="built_in">this</span>, Manifest.permission.WRITE_EXTERNAL_STORAGE) != PackageManager.PERMISSION_GRANTED) {</span><br><span class="line">        permissionList.add(Manifest.permission.WRITE_EXTERNAL_STORAGE);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ContextCompat.checkSelfPermission(<span class="built_in">this</span>, Manifest.permission.READ_EXTERNAL_STORAGE) != PackageManager.PERMISSION_GRANTED) {</span><br><span class="line">        permissionList.add(Manifest.permission.READ_EXTERNAL_STORAGE);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if list is not empty will request permissions</span></span><br><span class="line">    <span class="keyword">if</span> (!permissionList.isEmpty()) {</span><br><span class="line">        ActivityCompat.requestPermissions(<span class="built_in">this</span>, permissionList.toArray(<span class="keyword">new</span> <span class="title class_">String</span>[permissionList.size()]), <span class="number">1</span>);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onRequestPermissionsResult</span><span class="params">(<span class="type">int</span> requestCode, <span class="meta">@NonNull</span> String[] permissions, <span class="meta">@NonNull</span> <span class="type">int</span>[] grantResults)</span> {</span><br><span class="line">    <span class="built_in">super</span>.onRequestPermissionsResult(requestCode, permissions, grantResults);</span><br><span class="line">    <span class="keyword">switch</span> (requestCode) {</span><br><span class="line">        <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> (grantResults.length &gt; <span class="number">0</span>) {</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; grantResults.length; i++) {</span><br><span class="line"></span><br><span class="line">                    <span class="type">int</span> <span class="variable">grantResult</span> <span class="operator">=</span> grantResults[i];</span><br><span class="line">                    <span class="keyword">if</span> (grantResult == PackageManager.PERMISSION_DENIED) {</span><br><span class="line">                        <span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> permissions[i];</span><br><span class="line">                        Toast.makeText(<span class="built_in">this</span>, s + <span class="string">" permission was denied"</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">                    }</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">onDestroy</span><span class="params">()</span> {</span><br><span class="line">    <span class="comment">// clear model before destroy app</span></span><br><span class="line">    clear_model();</span><br><span class="line">    <span class="built_in">super</span>.onDestroy();</span><br><span class="line">}</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>load_model()方法是加载预测模型的。</li>
<li>clear_model()方法是清空预测模型的。</li>
<li>copy_file_from_asset()方法是把预测模型复制到内存卡上。</li>
<li>predict_image()方法是预测图片的。</li>
<li>get_max_result()方法是获取概率最大的预测结果。</li>
<li>request_permissions()方法是动态请求权限的。</li>
</ul>
<p>因为使用到图像加载框架Glide，所以要在<code>build.gradle</code>加入以下的引用。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">implementation 'com.github.bumptech.glide:glide:4.3.1'</span><br></pre></td></tr></tbody></table></figure>
<p>8、最后运行项目，选择图片预测就会得到结果。</p>
<h2 id="移动端开源框架部署疑难"><a href="#移动端开源框架部署疑难" class="headerlink" title="移动端开源框架部署疑难"></a>移动端开源框架部署疑难</h2><p>增加常见的几个问题</p>
<p>知识蒸馏（Distillation）相关论文阅读（1）——Distilling the Knowledge in a Neural Network（以及代码复现）</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
</search>
