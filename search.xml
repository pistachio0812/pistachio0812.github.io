<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CNN参数计算</title>
    <url>/CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在CNN的前向传播过程中，我们通常需要参数量，那么参数量的计算又当如何计算呢，不急，听我慢慢道来。</p>
<p>实例：</p>
<p>1.定义一个简单网络</p>
<pre><code class="lang-python"># 该文件为test.py
import torch
import torch.nn as nn
import math


def conv_out_size_same(size, stride):
    return int(math.ceil(float(size) / float(stride)))


class discriminator(nn.Module):
    def __init__(self, d=128, input_shape=[64, 64]):
        super(discriminator, self).__init__()
        s_h, s_w = input_shape[0], input_shape[1]
        s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)
        s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)
        s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)
        self.s_h16, self.s_w16 = conv_out_size_same(
            s_h8, 2), conv_out_size_same(s_w8, 2)

        # 64,64,3 -&gt; 32,32,128
        self.conv1 = nn.Conv2d(3, d, 4, 2, 1)

        # 32,32,128 -&gt; 16,16,256
        self.conv2 = nn.Conv2d(d, d * 2, 4, 2, 1)
        self.conv2_bn = nn.BatchNorm2d(d * 2)

        # 16,16,256 -&gt; 8,8,512
        self.conv3 = nn.Conv2d(d * 2, d * 4, 4, 2, 1)
        self.conv3_bn = nn.BatchNorm2d(d * 4)

        # 8,8,512 -&gt; 4,4,1024
        self.conv4 = nn.Conv2d(d * 4, d * 8, 4, 2, 1)
        self.conv4_bn = nn.BatchNorm2d(d * 8)

        # 4,4,1024 -&gt; 1
        self.linear = nn.Linear(self.s_h16 * self.s_w16 * d * 8, 1)

        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)
        self.sigmoid = nn.Sigmoid()

    def weight_init(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                m.weight.data.normal_(0.0, 0.02)
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.normal_(0.1, 0.02)
                m.bias.data.fill_(0)
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0.0, 0.02)
                m.bias.data.fill_(0)

    def forward(self, x):
        bs, _, _, _ = x.size()
        # (3, 64, 64)-&gt;(128, 32, 32)
        x = self.leaky_relu(self.conv1(x))
        # (128, 32, 32)-&gt;(256, 16, 16)
        x = self.leaky_relu(self.conv2_bn(self.conv2(x)))
        # (256, 16, 16)-&gt;(512, 8, 8)
        x = self.leaky_relu(self.conv3_bn(self.conv3(x)))
        # (512, 8, 8)-&gt;(1024, 4, 4)
        x = self.leaky_relu(self.conv4_bn(self.conv4(x)))
        # (1024, 4, 4)-&gt;(bs, 16*1024)
        x = x.view([bs, -1])
        # (bs, 16*1024)-&gt;(bs, 1)
        x = self.sigmoid(self.linear(x))

        return x.squeeze()
</code></pre>
<p>2.调用summary()函数计算参数以及输出</p>
<pre><code class="lang-python">from test import discriminator
from torchsummary import summary
import torch
if __name__ == &#39;__main__&#39;:
    device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
    m = discriminator(d=128, input_shape=[64, 64]).to(device)
    summary(m, input_size=(3, 64, 64))
</code></pre>
<p>结果如下：</p>
<pre><code class="lang-markdown">----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1          [-1, 128, 32, 32]           6,272
         LeakyReLU-2          [-1, 128, 32, 32]               0
            Conv2d-3          [-1, 256, 16, 16]         524,544
       BatchNorm2d-4          [-1, 256, 16, 16]             512
         LeakyReLU-5          [-1, 256, 16, 16]               0
            Conv2d-6            [-1, 512, 8, 8]       2,097,664
       BatchNorm2d-7            [-1, 512, 8, 8]           1,024
         LeakyReLU-8            [-1, 512, 8, 8]               0
            Conv2d-9           [-1, 1024, 4, 4]       8,389,632
      BatchNorm2d-10           [-1, 1024, 4, 4]           2,048
        LeakyReLU-11           [-1, 1024, 4, 4]               0
           Linear-12                    [-1, 1]          16,385
          Sigmoid-13                    [-1, 1]               0
================================================================
Total params: 11,038,081
Trainable params: 11,038,081
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.05
Forward/backward pass size (MB): 4.63
Params size (MB): 42.11
Estimated Total Size (MB): 46.78
----------------------------------------------------------------

Process finished with exit code 0
</code></pre>
<p>1.卷积（Conv)</p>
<p>以Conv2d-1为例：</p>
<p>变化：（3, 64, 64)-&gt;(128, 32, 32)</p>
<p>计算方法：</p>
<p><img src="/CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/image-20220421212236498.png" class="lazyload" data-srcset="/CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/image-20220421212236498.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>其中bias等于输出通道数</p>
<p>因此：params=128×(4×4×3)+128=6272</p>
<p>2.激活函数（Activation)</p>
<p>不产生参数</p>
<p>3.正则化（BN)</p>
<p>以BatchNorm2d-4为例：</p>
<p>（256，16， 16)-&gt;(256， 16， 16 )</p>
<p>计算方法：</p>
<p><img src="/CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/image-20220421212929933.png" class="lazyload" data-srcset="/CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/image-20220421212929933.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>因此：params=256×2=512</p>
<p>4.全连接（FC)</p>
<p>以Linear-12为例：</p>
<p>（1024， 4， 4）-&gt;(1)</p>
<p>计算方法：</p>
<p><img src="/CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/image-20220421213603964.png" class="lazyload" data-srcset="/CN/CNN%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97/image-20220421213603964.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>其中：bias等于输出通道数</p>
<p>因此：params=1024×4×4×1+1=16385</p>
<p>5.池化层（pooling)</p>
<p>不产生参数</p>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>参数计算</tag>
      </tags>
  </entry>
  <entry>
    <title>DCGAN详解</title>
    <url>/CN/DCGAN/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/weixin_44791964/article/details/110475425">pytorch搭建DCGAN</a></p>
<p>论文地址：<a href="https://arxiv.53yu.com/pdf/1511.06434.pdf">https://arxiv.53yu.com/pdf/1511.06434.pdf</a></p>
<p>论文源码：略</p>
<p>文章引用源码：<a href="https://github.com/bubbliiiing/dcgan-pytorch">https://github.com/bubbliiiing/dcgan-pytorch</a></p>
<h2 id="网络构建"><a href="#网络构建" class="headerlink" title="网络构建"></a>网络构建</h2><h3 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h3><p>DCGAN的全称是Deep Convolutional Generative Adversarial Networks，即深度卷积对抗生成网络。</p>
<p>它是由Alec Radford在论文Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks中提出的。</p>
<p>实际上它就是在GAN的基础上增加深度卷积网络结构。</p>
<p>论文中给出的DCGAN结构如图所示。其使用反卷积将特征层的高宽不断扩大，整体结构看起来像普通神经网络的逆过程。<br><img src="/CN/DCGAN/image-20220421201415525.png" class="lazyload" data-srcset="/CN/DCGAN/image-20220421201415525.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h3 id="生成网络的构建"><a href="#生成网络的构建" class="headerlink" title="生成网络的构建"></a>生成网络的构建</h3><p>对于生成网络来讲，它的目的是生成假图片，它的输入是正态分布随机数。输出是假图片。</p>
<p>在GAN当中，我们将这个正态分布随机数长度定义为100，在经过处理后，我们会得到一个(64,64,3)的假图片。</p>
<p>在处理过程中，我们会使用到反卷积，反卷积的概念是相对于正常卷积的，在正常卷积下，我们的特征层的高宽会不断被压缩；在反卷积下，我们的特征层的高宽会不断变大。<br><img src="/CN/DCGAN/20201202124131662.png" class="lazyload" data-srcset="/CN/DCGAN/20201202124131662.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>在DCGAN的生成网络中，我们首先利用一个全连接，将输入长条全连接到16,384（4x4x1024）这样一个长度上，这样我们才可以对这个全连接的结果进行reshape，使它变成(4,4,1024)的特征层。</p>
<p>在获得这个特征层之后，我们就可以利用反卷积进行上采样了。</p>
<p>在每次反卷积后，特征层的高和宽会变为原来的两倍，在四次反卷积后，我们特征层的shape变化是这样的：( 4 , 4 , 1024 ) − &gt; ( 8 , 8 , 512 ) − &gt; ( 16 , 16 , 256 ) − &gt; ( 32 , 32 , 128 ) − &gt; ( 64 , 64 , 3 )</p>
<p>此时我们再进行一次tanh激活函数，我们就可以获得一张假图片了。<br>实现代码：</p>
<pre><code class="lang-python">import math
import torch
import torch.nn as nn

# 如果stride=2,就是宽高减半，下采样操作
def conv_out_size_same(size, stride):
    return int(math.ceil(float(size) / float(stride)))


# 反卷积公式：H_out=(H_in −1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1
class generator(nn.Module):
    def __init__(self, d=128, input_shape=[64, 64]):
        super(generator, self).__init__()
        # 64, 64
        s_h, s_w = input_shape[0], input_shape[1]
        # 32, 32
        s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)
        # 16, 16
        s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)
        # 8, 8
        s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)
        # 4, 4
        self.s_h16, self.s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)
        # (bs, 100)-&gt; (bs, 4*4*128*8)
        self.linear = nn.Linear(100, self.s_h16 * self.s_w16 * d * 8)
        self.linear_bn = nn.BatchNorm2d(d * 8)

        # (bs, 1024, 4, 4)-&gt;(bs, 512, 8, 8)
        self.deconv1 = nn.ConvTranspose2d(d * 8, d * 4, 4, 2, 1)
        self.deconv1_bn = nn.BatchNorm2d(d * 4)

        # (bs, 512, 8, 8)-&gt;(bs, 256, 16, 16)
        self.deconv2 = nn.ConvTranspose2d(d * 4, d * 2, 4, 2, 1)
        self.deconv2_bn = nn.BatchNorm2d(d * 2)

        # (bs, 256, 16, 16)-&gt;(bs, 128, 32, 32)
        self.deconv3 = nn.ConvTranspose2d(d * 2, d, 4, 2, 1)
        self.deconv3_bn = nn.BatchNorm2d(d)

        # (bs, 128, 8, 8)-&gt;(bs, 3, 64, 64)
        self.deconv4 = nn.ConvTranspose2d(d, 3, 4, 2, 1)

        self.relu = nn.ReLU()

    def weight_init(self):
        for m in self.modules():
            if isinstance(m, nn.ConvTranspose2d):
                m.weight.data.normal_(0.0, 0.02)
                m.bias.data.fill_(0)
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.normal_(0.1, 0.02)
                m.bias.data.fill_(0)
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0.0, 0.02)
                m.bias.data.fill_(0)

    def forward(self, x):
        # (bs, 100)
        bs, _ = x.size()
        # (bs, 16*1024)
        x = self.linear(x)
        # (bs, 1024, 4, 4)
        x = x.view([bs, -1, self.s_h16, self.s_w16])
        x = self.relu(self.linear_bn(x))
        # (bs, 1024, 4, 4)-&gt;(bs, 512, 8, 8)
        x = self.relu(self.deconv1_bn(self.deconv1(x)))
        # (bs, 512, 8, 8)-&gt;(bs, 256, 16, 16)
        x = self.relu(self.deconv2_bn(self.deconv2(x)))
        # (bs, 256, 16, 16)-&gt;(bs, 128, 32, 32)
        x = self.relu(self.deconv3_bn(self.deconv3(x)))
        # (bs, 128, 32, 32)-&gt;(bs, 3, 64, 64)
        x = torch.tanh(self.deconv4(x))
        return x
</code></pre>
<h3 id="判别网络的构建"><a href="#判别网络的构建" class="headerlink" title="判别网络的构建"></a>判别网络的构建</h3><p>对于生成网络来讲，它的目的是生成假图片，它的输入是正态分布随机数。输出是假图片。</p>
<p>对于判别网络来讲，它的目的是判断输入图片的真假，它的输入是图片，输出是判断结果。</p>
<p>判断结果处于0-1之间，利用接近1代表判断为真图片，接近0代表判断为假图片。</p>
<p>判别网络的构建和普通卷积网络差距不大，都是不断的卷积对图片进行下采用，在多次卷积后，最终接一次全连接判断结果。</p>
<p>实现代码如下：</p>
<pre><code class="lang-python">import torch
import torch.nn as nn
import math


def conv_out_size_same(size, stride):
    return int(math.ceil(float(size) / float(stride)))


class discriminator(nn.Module):
    def __init__(self, d=128, input_shape=[64, 64]):
        super(discriminator, self).__init__()
        s_h, s_w = input_shape[0], input_shape[1]
        s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)
        s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)
        s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)
        self.s_h16, self.s_w16 = conv_out_size_same(
            s_h8, 2), conv_out_size_same(s_w8, 2)

        # 64,64,3 -&gt; 32,32,128
        self.conv1 = nn.Conv2d(3, d, 4, 2, 1)

        # 32,32,128 -&gt; 16,16,256
        self.conv2 = nn.Conv2d(d, d * 2, 4, 2, 1)
        self.conv2_bn = nn.BatchNorm2d(d * 2)

        # 16,16,256 -&gt; 8,8,512
        self.conv3 = nn.Conv2d(d * 2, d * 4, 4, 2, 1)
        self.conv3_bn = nn.BatchNorm2d(d * 4)

        # 8,8,512 -&gt; 4,4,1024
        self.conv4 = nn.Conv2d(d * 4, d * 8, 4, 2, 1)
        self.conv4_bn = nn.BatchNorm2d(d * 8)

        # 4,4,1024 -&gt; 1
        self.linear = nn.Linear(self.s_h16 * self.s_w16 * d * 8, 1)

        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)
        self.sigmoid = nn.Sigmoid()

    def weight_init(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                m.weight.data.normal_(0.0, 0.02)
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.normal_(0.1, 0.02)
                m.bias.data.fill_(0)
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0.0, 0.02)
                m.bias.data.fill_(0)

    def forward(self, x):
        bs, _, _, _ = x.size()
        # (3, 64, 64)-&gt;(128, 32, 32)
        x = self.leaky_relu(self.conv1(x))
        # (128, 32, 32)-&gt;(256, 16, 16)
        x = self.leaky_relu(self.conv2_bn(self.conv2(x)))
        # (256, 16, 16)-&gt;(512, 8, 8)
        x = self.leaky_relu(self.conv3_bn(self.conv3(x)))
        # (512, 8, 8)-&gt;(1024, 4, 4)
        x = self.leaky_relu(self.conv4_bn(self.conv4(x)))
        # (1024, 4, 4)-&gt;(bs, 16*1024)
        x = x.view([bs, -1])
        # (bs, 16*1024)-&gt;(bs, 1)
        x = self.sigmoid(self.linear(x))

        return x.squeeze()
</code></pre>
<h2 id="训练思路"><a href="#训练思路" class="headerlink" title="训练思路"></a>训练思路</h2><p>DCGAN的训练可以分为生成器训练和判别器训练，每一个step中一般先训练判别器，然后训练生成器</p>
<h3 id="判别器的训练"><a href="#判别器的训练" class="headerlink" title="判别器的训练"></a>判别器的训练</h3><p>在训练判别器的时候我们希望判别器可以判断输入图片的真伪，因此我们的输入就是真图片、假图片和它们对应的标签。</p>
<p>因此判别器的训练步骤如下：</p>
<p>1、随机选取batch_size个真实的图片。<br>2、随机生成batch_size个N维向量，传入到Generator中生成batch_size个虚假图片。<br>3、真实图片的label为1，虚假图片的label为0，将真实图片和虚假图片当作训练集传入到Discriminator中进行训练。<br><img src="/CN/DCGAN/20201203114355820.png" class="lazyload" data-srcset="/CN/DCGAN/20201203114355820.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" style="zoom: 25%;"></p>
<h3 id="生成器训练"><a href="#生成器训练" class="headerlink" title="生成器训练"></a>生成器训练</h3><p>在训练生成器的时候我们希望生成器可以生成极为真实的假图片。因此我们在训练生成器需要知道判别器认为什么图片是真图片。</p>
<p>因此生成器的训练步骤如下：</p>
<p>1、随机生成batch_size个N维向量，传入到Generator中生成batch_size个虚假图片。<br>2、将虚假图片的Discriminator预测结果与1的对比作为loss对Generator进行训练（与1对比的意思是，让生成器根据判别器判别的结果进行训练）<br><img src="/CN/DCGAN/20201203114415814.png" class="lazyload" data-srcset="/CN/DCGAN/20201203114415814.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" style="zoom: 50%;"></p>
<h2 id="利用DCGAN生成图片"><a href="#利用DCGAN生成图片" class="headerlink" title="利用DCGAN生成图片"></a>利用DCGAN生成图片</h2><p>详情见源码和参考博文</p>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>C语言基础学习</title>
    <url>/CN/C%E8%AF%AD%E8%A8%80/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="参考博文"><a href="#参考博文" class="headerlink" title="参考博文"></a>参考博文</h2><p>1.<a href="https://www.runoob.com/cprogramming">菜鸟教程C语言教程</a></p>
<h2 id="编译-执行C程序"><a href="#编译-执行C程序" class="headerlink" title="编译/执行C程序"></a>编译/执行C程序</h2><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><pre><code class="lang-c">#include &lt;stdio.h&gt;

int main()
&#123;
    /* 我的第一个 C 程序 */
    printf(&quot;Hello, World! \n&quot;);

    return 0;
&#125;
</code></pre>
<p>实例解析：</p>
<ul>
<li>所有的 C 语言程序都需要包含 <strong>main()</strong> 函数。 代码从 <strong>main()</strong> 函数开始执行。</li>
<li><strong>/* … */</strong> 用于注释说明。</li>
<li><strong>printf()</strong> 用于格式化输出到屏幕。<strong>printf()</strong> 函数在 <strong>“stdio.h”</strong> 头文件中声明。</li>
<li><strong>stdio.h</strong> 是一个头文件 (标准输入输出头文件) , <strong>#include</strong> 是一个预处理命令，用来引入头文件。 当编译器遇到 <strong>printf()</strong> 函数时，如果没有找到 <strong>stdio.h</strong> 头文件，会发生编译错误。</li>
<li><strong>return 0;</strong> 语句用于表示退出程序。</li>
</ul>
<h2 id="C简介"><a href="#C简介" class="headerlink" title="C简介"></a>C简介</h2><p>C 语言是一种通用的高级语言，最初是由丹尼斯·里奇在贝尔实验室为开发 UNIX 操作系统而设计的。C 语言最开始是于 1972 年在 DEC PDP-11 计算机上被首次实现。</p>
<p>在 1978 年，布莱恩·柯林汉（Brian Kernighan）和丹尼斯·里奇（Dennis Ritchie）制作了 C 的第一个公开可用的描述，现在被称为 K&amp;R 标准。</p>
<p>UNIX 操作系统，C编译器，和几乎所有的 UNIX 应用程序都是用 C 语言编写的。由于各种原因，C 语言现在已经成为一种广泛使用的专业语言。</p>
<ul>
<li>易于学习。</li>
<li>结构化语言。</li>
<li>它产生高效率的程序。</li>
<li>它可以处理底层的活动。</li>
</ul>
<h3 id="关于C"><a href="#关于C" class="headerlink" title="关于C"></a>关于C</h3><ul>
<li><p>C 语言是为了编写 UNIX 操作系统而被发明的。</p>
</li>
<li><p>C 语言是以 B 语言为基础的，B 语言大概是在 1970 年被引进的。</p>
</li>
<li><p>C 语言标准是于 1988 年由美国国家标准协会（ANSI，全称 American National Standard Institute）制定的。</p>
</li>
<li><p>截至 1973 年，UNIX 操作系统完全使用 C 语言编写。</p>
</li>
<li><p>目前，C 语言是最广泛使用的系统程序设计语言。</p>
</li>
<li><p>大多数先进的软件都是使用 C 语言实现的。</p>
</li>
<li><p>当今最流行的 Linux 操作系统和 RDBMS（Relational Database Management System：关系数据库管理系统） MySQL 都是使用 C 语言编写的。</p>
</li>
</ul>
<h3 id="为什么要使用C"><a href="#为什么要使用C" class="headerlink" title="为什么要使用C"></a>为什么要使用C</h3><p>  C 语言最初是用于系统开发工作，特别是组成操作系统的程序。由于 C 语言所产生的代码运行速度与汇编语言编写的代码运行速度几乎一样，所以采用 C 语言作为系统开发语言。下面列举几个使用 C 的实例：操作系统、语言编译器、汇编器、文本编辑器、打印机、网络驱动器、现代程序、数据库、语言解释器、实体工具。</p>
<h3 id="C程序"><a href="#C程序" class="headerlink" title="C程序"></a>C程序</h3><p>一个 C 语言程序，可以是 3 行，也可以是数百万行，它可以写在一个或多个扩展名为 <strong>“.c”</strong> 的文本文件中，例如，<em>hello.c</em>。您可以使用 <strong>“vi”</strong>、<strong>“vim”</strong> 或任何其他文本编辑器来编写您的 C 语言程序。</p>
<p>本教程假定您已经知道如何编辑一个文本文件，以及如何在程序文件中编写源代码。</p>
<h3 id="C11"><a href="#C11" class="headerlink" title="C11"></a>C11</h3><p>C11（也被称为C1X）指ISO标准ISO/IEC 9899:2011，是当前最新的C语言标准。在它之前的C语言标准为C99。</p>
<h3 id="新特性"><a href="#新特性" class="headerlink" title="新特性"></a>新特性</h3><ul>
<li>对齐处理（Alignment）的标准化（包括_Alignas标志符，alignof运算符，aligned_alloc函数以及<stdalign.h>头文件）。</stdalign.h></li>
<li><em>Noreturn 函数标记，类似于 gcc 的 <em>_attribute</em></em>((noreturn))。</li>
<li>_Generic 关键字。</li>
<li>多线程（Multithreading）支持，包括：<br>_Thread_local存储类型标识符，<threads.h>头文件，里面包含了线程的创建和管理函数。<br>_Atomic类型修饰符和<stdatomic.h>头文件。</stdatomic.h></threads.h></li>
<li>增强的Unicode的支持。基于C Unicode技术报告ISO/IEC TR 19769:2004，增强了对Unicode的支持。包括为UTF-16/UTF-32编码增加了char16_t和char32_t数据类型，提供了包含unicode字符串转换函数的头文件<uchar.h>。</uchar.h></li>
<li>删除了 gets() 函数，使用一个新的更安全的函数gets_s()替代。</li>
<li>增加了边界检查函数接口，定义了新的安全的函数，例如 fopen_s()，strcat_s() 等等。</li>
<li>增加了更多浮点处理宏(宏)。</li>
<li>匿名结构体/联合体支持。这个在gcc早已存在，C11将其引入标准。</li>
<li>静态断言（Static assertions），_Static_assert()，在解释 #if 和 #error 之后被处理。</li>
<li>新的 fopen() 模式，(“…x”)。类似 POSIX 中的 O_CREAT|O_EXCL，在文件锁中比较常用。</li>
<li>新增 quick_exit() 函数作为第三种终止程序的方式。当 exit()失败时可以做最少的清理工作。</li>
</ul>
<h2 id="C环境设置"><a href="#C环境设置" class="headerlink" title="C环境设置"></a>C环境设置</h2><h2 id><a href="#" class="headerlink" title="#"></a>#</h2>]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title>Faster-RCNN解读</title>
    <url>/CN/Faster-rcnn/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文地址：<a href="https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf">Faster R-CNN</a></p>
<p>源码地址：<a href="https://github.com/ShaoqingRen/faster_rcnn">ShaoqingRen/faster_rcnn: Faster R-CNN (github.com)</a></p>
<p>文章引用源码：<a href="https://github.com/bubbliiiing/faster-rcnn-pytorch">https://github.com/bubbliiiing/faster-rcnn-pytorch</a></p>
<p>文章出处：<a href="https://blog.csdn.net/weixin_44791964/article/details/105739918">https://blog.csdn.net/weixin_44791964/article/details/105739918</a></p>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><h3 id="预测部分"><a href="#预测部分" class="headerlink" title="预测部分"></a>预测部分</h3><h4 id="主干网络"><a href="#主干网络" class="headerlink" title="主干网络"></a>主干网络</h4><p><img src="/CN/Faster-rcnn/image-20220417152429746.png" class="lazyload" data-srcset="/CN/Faster-rcnn/image-20220417152429746.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>论文详解</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use Linux</title>
    <url>/CN/Linux%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="Linux简介"><a href="#Linux简介" class="headerlink" title="Linux简介"></a>Linux简介</h3><p>UNIX 是一个交互式系统，用于同时处理多进程和多用户同时在线。为什么要说 UNIX，那是因为 Linux 是由 UNIX 发展而来的，UNIX 是由程序员设计，它的主要服务对象也是程序员。Linux 继承了 UNIX 的设计目标。从智能手机到汽车，超级计算机和家用电器，从家用台式机到企业服务器，Linux 操作系统无处不在。</p>
<p>大多数程序员都喜欢让系统尽量简单，优雅并具有一致性。举个例子，从最底层的角度来讲，一个文件应该只是一个字节集合。为了实现顺序存取、随机存取、按键存取、远程存取只能是妨碍你的工作。相同的，如果命令</p>
<p>ls A*<br>意味着只列出以 A 为开头的所有文件，那么命令</p>
<p>rm A<em><br>应该会移除所有以 A 为开头的文件而不是只删除文件名是 A</em> 的文件。这个特性也是最小吃惊原则(principle of least surprise)</p>
<p>最小吃惊原则一般常用于用户界面和软件设计。它的原型是：该功能或者特征应该符合用户的预期，不应该使用户感到惊讶和震惊。</p>
<p>一些有经验的程序员通常希望系统具有较强的功能性和灵活性。设计 Linux 的一个基本目标是每个应用程序只做一件事情并把他做好。所以编译器只负责编译的工作，编译器不会产生列表，因为有其他应用比编译器做的更好。</p>
<p>很多人都不喜欢冗余，为什么在 cp 就能描述清楚你想干什么时候还使用 copy？这完全是在浪费宝贵的 hacking time。为了从文件中提取所有包含字符串 ard 的行，Linux 程序员应该输入</p>
<p>grep ard f</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Liunx system</tag>
      </tags>
  </entry>
  <entry>
    <title>fork me on github</title>
    <url>/CN/Fork_github/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/weiwosuoai/article/details/88312326">如何在博客园添加 Fork me on GitHub 彩带效果</a></p>
<p>2.<a href="https://github.blog/2008-12-19-github-ribbons/">GitHub Ribbons | The GitHub Blog</a></p>
<p>3.<a href="https://blog.csdn.net/weixin_44543463/article/details/119750964">Hexo添加Follow me on CSDN效果</a></p>
<h2 id="博客园添加fork"><a href="#博客园添加fork" class="headerlink" title="博客园添加fork"></a>博客园添加fork</h2><h3 id="进入博客园后台"><a href="#进入博客园后台" class="headerlink" title="进入博客园后台"></a>进入博客园后台</h3><p>进入博客园的管理界面，依次点击 <strong>管理 </strong> -&gt; <strong>设置</strong>，进入到设置页面后，将页面拖动到最下面，您会看到：<strong>页首Html代码</strong>一栏</p>
<p><img src="/CN/Fork_github/image-20220502215437444.png" class="lazyload" data-srcset="/CN/Fork_github/image-20220502215437444.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220502215437444" style="zoom:50%;"></p>
<h3 id="添加代码"><a href="#添加代码" class="headerlink" title="添加代码"></a>添加代码</h3><p>在页面Html代码框中输入如下代码：</p>
<pre><code class="lang-js">&lt;a href=&quot;https://github.com/pistachio&quot;&gt;  
&lt;img style=&quot;position: fixed; top: 0; right: 0; border: 0; z-index:9999;&quot; 
       src=&quot;https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png&quot; 
       alt=&quot;Fork me on GitHub&quot;&gt;
&lt;/a&gt;
</code></pre>
<p><strong>注意，您需要将 <code>&lt;a href=&quot;&quot;&gt;</code> 中的链接换成您自己的 GitHub 主页地址。</strong></p>
<p>保存后，随意打开一篇您自己的博客，就可以看见和教程开头展示的效果一样了。大功告成！</p>
<h3 id="更换彩带颜色"><a href="#更换彩带颜色" class="headerlink" title="更换彩带颜色"></a>更换彩带颜色</h3><p>我上面使用的是红色的彩带，如果您需要更换成其他颜色，只需将 <code>&lt;img&gt;</code> 标签中的 <code>src</code> 地址更换成您想要颜色的地址即可。</p>
<pre><code class="lang-js"># 绿色
https://github.blog/wp-content/uploads/2008/12/forkme_right_green_007200.png
# 黑色
https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png
# 橘黄色
https://github.blog/wp-content/uploads/2008/12/forkme_right_orange_ff7600.png
# 灰色
https://github.blog/wp-content/uploads/2008/12/forkme_right_gray_6d6d6d.png
# 白色
https://github.blog/wp-content/uploads/2008/12/forkme_right_white_ffffff.png
</code></pre>
<p>从上面挑选一款您喜欢的样式颜色吧！！</p>
<h2 id="hexo添加fork"><a href="#hexo添加fork" class="headerlink" title="hexo添加fork"></a>hexo添加fork</h2><h3 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h3><p>粘贴复制如下的代码到<code>themes\hexo-theme-next\layout\layout.ejs</code>文件中(放在<code>&lt;div id=&quot;l_body&quot;&gt;&lt;/div&gt;</code>的下面 如图)，并把href改为你的csdn主页,换成github同上，记得换上自己的链接。</p>
<pre><code class="lang-js"># 黑色版本
# 个人认为position属性值改为fixed好一点~
  &lt;!--Follow me on CSDN--&gt;
  &lt;a href=&quot;https://blog.csdn.net/qq_38452951&quot;&gt; &lt;img loading=&quot;lazy&quot; width=&quot;149&quot; height=&quot;149&quot; style=&quot;position: absolute; top: 0; right: 0; border: 0;&quot; src=&quot;https://img-blog.csdnimg.cn/abe3797b7d77419b81ecc02dd1bf8c34.png&quot; class=&quot;attachment-full size-full&quot; alt=&quot;Fork me on GitHub&quot; data-recalc-dims=&quot;1&quot;&gt;&lt;/a&gt;

# 白色版本
  &lt;!--Follow me on CSDN--&gt;
  &lt;a href=&quot;https://blog.csdn.net/qq_38452951&quot;&gt;&lt;img loading=&quot;lazy&quot; width=&quot;149&quot; height=&quot;149&quot; style=&quot;position: absolute; top: 0; right: 0; border: 0;&quot; src=&quot;https://img-blog.csdnimg.cn/1f8e1ef9be9f4f7db01fe3a2d57829de.png&quot; class=&quot;attachment-full size-full&quot; alt=&quot;Fork me on GitHub&quot; data-recalc-dims=&quot;1&quot;&gt;&lt;/a&gt;
</code></pre>
<p><img src="/CN/Fork_github/image-20220502220838865.png" class="lazyload" data-srcset="/CN/Fork_github/image-20220502220838865.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220502220838865" style="zoom:50%;"></p>
<h3 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h3><p>详情请见：</p>
<p>1.<a href="https://www.cnblogs.com/pistachio0812/">追风赶月的少年 - 博客园 (cnblogs.com)</a></p>
<p>2.<a href="https://pistachio0812.github.io/">相思似海深旧事如天远 (pistachio0812.github.io)</a></p>
]]></content>
      <categories>
        <category>Hexo博客搭建</category>
      </categories>
      <tags>
        <tag>博客搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>InceptionV3详解</title>
    <url>/CN/InceptionV3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文地址：<a href="https://arxiv.org/pdf/1512.00567.pdf">Inceptionv3 </a></p>
<p>源码地址：<a href="https://github.com/MasazI/InceptionV3_TensorFlow">InceptionV3_TensorFlow: Inception v3 </a></p>
<p>文章引用出处1：<a href="https://blog.csdn.net/weixin_44791964/article/details/102802866">https://blog.csdn.net/weixin_44791964/article/details/102802866</a></p>
<p>文章引用出处2：<a href="https://cloud.tencent.com/developer/article/1006032">InceptionV3 网络模型</a></p>
<h2 id="InceptionV3模型"><a href="#InceptionV3模型" class="headerlink" title="InceptionV3模型"></a>InceptionV3模型</h2><p>InceptionV3模型是谷歌Inception系列里面的第三代模型，其模型结构与InceptionV2模型放在了同一篇论文里，其实二者模型结构差距不大，相比于其它神经网络模型，Inception网络最大的特点在于将神经网络层与层之间的卷积运算进行了拓展。<br>如VGG，AlexNet网络，它就是一直卷积下来的，一层接着一层；<br>ResNet则是创新性的引入了残差网络的概念，使得靠前若干层的某一层数据输出直接跳过多层引入到后面数据层的输入部分，后面的特征层的内容会有一部分由其前面的某一层线性贡献。<br>而Inception网络则是采用不同大小的卷积核，使得存在不同大小的感受野，最后实现拼接达到不同尺度特征的融合。<br>对于InceptionV3而言，其网络中存在着如下的结构。<br>这个结构使用不同大小的卷积核对输入进行卷积（这个结构主要在代码中的block1使用）。<br><img src="/CN/InceptionV3/2019111309515588.png" class="lazyload" data-srcset="/CN/InceptionV3/2019111309515588.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="Inceptionv3_block1"></p>
<p>还存在着这样的结构，利用1x7的卷积和7x1的卷积代替7x7的卷积，这样可以只使用约（1x7 + 7x1) / (7x7) = 28.6%的计算开销；利用1x3的卷积和3x1的卷积代替3x3的卷积，这样可以只使用约（1x3 + 3x1) / (3x3) = 67%的计算开销。<br>下图利用1x7的卷积和7x1的卷积代替7x7的卷积（这个结构主要在代码中的block2使用）。</p>
<p><img src="/CN/InceptionV3/20191113095610269.png" class="lazyload" data-srcset="/CN/InceptionV3/20191113095610269.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="Inceptionv3_block2"></p>
<p>下图利用1x3的卷积和3x1的卷积代替3x3的卷积（这个结构主要在代码中的block3使用）。</p>
<p><img src="/CN/InceptionV3/2019111309572648.png" class="lazyload" data-srcset="/CN/InceptionV3/2019111309572648.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="inceptionv3_block3"></p>
<h2 id="网络部分实现代码"><a href="#网络部分实现代码" class="headerlink" title="网络部分实现代码"></a>网络部分实现代码</h2><p>一共将InceptionV3划分为3个block，对应着35x35、17x17，8x8维度大小的图像。每个block中间有许多的part，对应着不同的特征层深度，用于特征提取。</p>
<pre><code class="lang-python">#-------------------------------------------------------------#
#   InceptionV3的网络部分
#-------------------------------------------------------------#
from __future__ import print_function
from __future__ import absolute_import

import warnings
import numpy as np

from keras.models import Model
from keras import layers
from keras.layers import Activation,Dense,Input,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D
from keras.layers import GlobalAveragePooling2D,GlobalMaxPooling2D
from keras.engine.topology import get_source_inputs
from keras.utils.layer_utils import convert_all_kernels_in_model
from keras.utils.data_utils import get_file
from keras import backend as K
from keras.applications.imagenet_utils import decode_predictions
from keras.preprocessing import image


def conv2d_bn(x,
              filters,
              num_row,
              num_col,
              padding=&#39;same&#39;,
              strides=(1, 1),
              name=None):
    if name is not None:
        bn_name = name + &#39;_bn&#39;
        conv_name = name + &#39;_conv&#39;
    else:
        bn_name = None
        conv_name = None
    x = Conv2D(
        filters, (num_row, num_col),
        strides=strides,
        padding=padding,
        use_bias=False,
        name=conv_name)(x)
    x = BatchNormalization(scale=False, name=bn_name)(x)
    x = Activation(&#39;relu&#39;, name=name)(x)
    return x


def InceptionV3(input_shape=[299,299,3],
                classes=1000):


    img_input = Input(shape=input_shape)

    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding=&#39;valid&#39;)
    x = conv2d_bn(x, 32, 3, 3, padding=&#39;valid&#39;)
    x = conv2d_bn(x, 64, 3, 3)
    x = MaxPooling2D((3, 3), strides=(2, 2))(x)

    x = conv2d_bn(x, 80, 1, 1, padding=&#39;valid&#39;)
    x = conv2d_bn(x, 192, 3, 3, padding=&#39;valid&#39;)
    x = MaxPooling2D((3, 3), strides=(2, 2))(x)

    #--------------------------------#
    #   Block1 35x35
    #--------------------------------#
    # Block1 part1
    # 35 x 35 x 192 -&gt; 35 x 35 x 256
    branch1x1 = conv2d_bn(x, 64, 1, 1)

    branch5x5 = conv2d_bn(x, 48, 1, 1)
    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)

    branch3x3dbl = conv2d_bn(x, 64, 1, 1)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)

    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=&#39;same&#39;)(x)
    branch_pool = conv2d_bn(branch_pool, 32, 1, 1)
    x = layers.concatenate(
        [branch1x1, branch5x5, branch3x3dbl, branch_pool],
        axis=3,
        name=&#39;mixed0&#39;)

    # Block1 part2
    # 35 x 35 x 256 -&gt; 35 x 35 x 288
    branch1x1 = conv2d_bn(x, 64, 1, 1)

    branch5x5 = conv2d_bn(x, 48, 1, 1)
    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)

    branch3x3dbl = conv2d_bn(x, 64, 1, 1)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)

    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=&#39;same&#39;)(x)
    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)
    x = layers.concatenate(
        [branch1x1, branch5x5, branch3x3dbl, branch_pool],
        axis=3,
        name=&#39;mixed1&#39;)

    # Block1 part3
    # 35 x 35 x 288 -&gt; 35 x 35 x 288
    branch1x1 = conv2d_bn(x, 64, 1, 1)

    branch5x5 = conv2d_bn(x, 48, 1, 1)
    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)

    branch3x3dbl = conv2d_bn(x, 64, 1, 1)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)

    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=&#39;same&#39;)(x)
    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)
    x = layers.concatenate(
        [branch1x1, branch5x5, branch3x3dbl, branch_pool],
        axis=3,
        name=&#39;mixed2&#39;)

    #--------------------------------#
    #   Block2 17x17
    #--------------------------------#
    # Block2 part1
    # 35 x 35 x 288 -&gt; 17 x 17 x 768
    branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding=&#39;valid&#39;)

    branch3x3dbl = conv2d_bn(x, 64, 1, 1)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)
    branch3x3dbl = conv2d_bn(
        branch3x3dbl, 96, 3, 3, strides=(2, 2), padding=&#39;valid&#39;)

    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)
    x = layers.concatenate(
        [branch3x3, branch3x3dbl, branch_pool], axis=3, name=&#39;mixed3&#39;)

    # Block2 part2
    # 17 x 17 x 768 -&gt; 17 x 17 x 768
    branch1x1 = conv2d_bn(x, 192, 1, 1)

    branch7x7 = conv2d_bn(x, 128, 1, 1)
    branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)
    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)

    branch7x7dbl = conv2d_bn(x, 128, 1, 1)
    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)
    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)
    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)
    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)

    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=&#39;same&#39;)(x)
    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)
    x = layers.concatenate(
        [branch1x1, branch7x7, branch7x7dbl, branch_pool],
        axis=3,
        name=&#39;mixed4&#39;)

    # Block2 part3 and part4
    # 17 x 17 x 768 -&gt; 17 x 17 x 768 -&gt; 17 x 17 x 768
    for i in range(2):
        branch1x1 = conv2d_bn(x, 192, 1, 1)

        branch7x7 = conv2d_bn(x, 160, 1, 1)
        branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)
        branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)

        branch7x7dbl = conv2d_bn(x, 160, 1, 1)
        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)
        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)
        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)
        branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)

        branch_pool = AveragePooling2D(
            (3, 3), strides=(1, 1), padding=&#39;same&#39;)(x)
        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)
        x = layers.concatenate(
            [branch1x1, branch7x7, branch7x7dbl, branch_pool],
            axis=3,
            name=&#39;mixed&#39; + str(5 + i))

    # Block2 part5
    # 17 x 17 x 768 -&gt; 17 x 17 x 768
    branch1x1 = conv2d_bn(x, 192, 1, 1)

    branch7x7 = conv2d_bn(x, 192, 1, 1)
    branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)
    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)

    branch7x7dbl = conv2d_bn(x, 192, 1, 1)
    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)
    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)
    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)
    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)

    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=&#39;same&#39;)(x)
    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)
    x = layers.concatenate(
        [branch1x1, branch7x7, branch7x7dbl, branch_pool],
        axis=3,
        name=&#39;mixed7&#39;)

    #--------------------------------#
    #   Block3 8x8
    #--------------------------------#
    # Block3 part1
    # 17 x 17 x 768 -&gt; 8 x 8 x 1280
    branch3x3 = conv2d_bn(x, 192, 1, 1)
    branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,
                          strides=(2, 2), padding=&#39;valid&#39;)

    branch7x7x3 = conv2d_bn(x, 192, 1, 1)
    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)
    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)
    branch7x7x3 = conv2d_bn(
        branch7x7x3, 192, 3, 3, strides=(2, 2), padding=&#39;valid&#39;)

    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)
    x = layers.concatenate(
        [branch3x3, branch7x7x3, branch_pool], axis=3, name=&#39;mixed8&#39;)

    # Block3 part2 part3
    # 8 x 8 x 1280 -&gt; 8 x 8 x 2048 -&gt; 8 x 8 x 2048
    for i in range(2):
        branch1x1 = conv2d_bn(x, 320, 1, 1)

        branch3x3 = conv2d_bn(x, 384, 1, 1)
        branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)
        branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)
        branch3x3 = layers.concatenate(
            [branch3x3_1, branch3x3_2], axis=3, name=&#39;mixed9_&#39; + str(i))

        branch3x3dbl = conv2d_bn(x, 448, 1, 1)
        branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)
        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)
        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)
        branch3x3dbl = layers.concatenate(
            [branch3x3dbl_1, branch3x3dbl_2], axis=3)

        branch_pool = AveragePooling2D(
            (3, 3), strides=(1, 1), padding=&#39;same&#39;)(x)
        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)
        x = layers.concatenate(
            [branch1x1, branch3x3, branch3x3dbl, branch_pool],
            axis=3,
            name=&#39;mixed&#39; + str(9 + i))
    # 平均池化后全连接。
    x = GlobalAveragePooling2D(name=&#39;avg_pool&#39;)(x)
    x = Dense(classes, activation=&#39;softmax&#39;, name=&#39;predictions&#39;)(x)


    inputs = img_input

    model = Model(inputs, x, name=&#39;inception_v3&#39;)

    return model
</code></pre>
<h2 id="预测部分"><a href="#预测部分" class="headerlink" title="预测部分"></a>预测部分</h2><p>建立网络后就可以进行预测了，实现代码如下：</p>
<pre><code class="lang-python">def preprocess_input(x):
    x /= 255.
    x -= 0.5
    x *= 2.
    return x


if __name__ == &#39;__main__&#39;:
    model = InceptionV3()

    model.load_weights(&quot;inception_v3_weights_tf_dim_ordering_tf_kernels.h5&quot;)

    img_path = &#39;elephant.jpg&#39;
    img = image.load_img(img_path, target_size=(299, 299))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)

    x = preprocess_input(x)

    preds = model.predict(x)
    print(&#39;Predicted:&#39;, decode_predictions(preds))
</code></pre>
<p>预测所需的已经训练好的InceptionV3模型可以在<a href="https://github.com/fchollet/deep-learning-models/releases下载。非常方便。">https://github.com/fchollet/deep-learning-models/releases下载。非常方便。</a><br>预测结果为：</p>
<pre><code class="lang-markdown">Predicted: [[(&#39;n02504458&#39;, &#39;African_elephant&#39;, 0.50874853), (&#39;n01871265&#39;, &#39;tusker&#39;, 0.19524273), (&#39;n02504013&#39;, &#39;Indian_elephant&#39;, 0.1566972), (&#39;n01917289&#39;, &#39;brain_coral&#39;, 0.0008956835), (&#39;n01695060&#39;, &#39;Komodo_dragon&#39;, 0.0008260256)]]
</code></pre>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>论文详解</tag>
      </tags>
  </entry>
  <entry>
    <title>Matplotlib学习笔记</title>
    <url>/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Matplotlib 是 Python 的绘图库，它能让使用者很轻松地将数据图形化，并且提供多样化的输出格式。可以用来绘制各种静态，动态，交互式的图表。是一个非常强大的 Python 画图工具，我们可以使用该工具将很多数据通过图表的形式更直观的呈现出来。可以绘制线图、散点图、等高线图、条形图、柱状图、3D 图形、甚至是图形动画等等。</p>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>Matplotlib 通常与 NumPy 和 SciPy（Scientific Python）一起使用， 这种组合广泛用于替代 MatLab，是一个强大的科学计算环境，有助于我们通过 Python 学习数据科学或者机器学习。</p>
<p>SciPy 是一个开源的 Python 算法库和数学工具包。</p>
<p>SciPy 包含的模块有最优化、线性代数、积分、插值、特殊函数、快速傅里叶变换、信号处理和图像处理、常微分方程求解和其他科学与工程中常用的计算。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>本章节，我们使用 pip 工具来安装 <code>Matplotlib</code> 库，如果还未安装该工具，可以参考 <a href="https://www.runoob.com/w3cnote/python-pip-install-usage.html">Python pip 安装与使用</a>。</p>
<p>升级 pip：</p>
<pre><code class="lang-python">python3 -m pip install -U pip
</code></pre>
<p>安装 <code>matplotlib</code> 库：</p>
<pre><code class="lang-python">python3 -m pip install -U matplotlib
</code></pre>
<p>安装完成后，我们就可以通过 import 来导入<code>matplotlib</code> 库：</p>
<p><code>import matplotlib</code></p>
<p>以下实例，我们通过导入 <code>matplotlib</code> 库，然后查看 <code>matplotlib</code>库的版本号：</p>
<p>实例1:</p>
<p><code>import matplotlib</code></p>
<p><code>print(matplotlib.__version__)</code></p>
<p>执行以上代码，输出结果如下：</p>
<pre><code>3.4.2
</code></pre><h2 id="Matplotlib-Pyplot"><a href="#Matplotlib-Pyplot" class="headerlink" title="Matplotlib Pyplot"></a>Matplotlib Pyplot</h2><p>Pyplot 是 Matplotlib 的子库，提供了和 MATLAB 类似的绘图 API。</p>
<p>Pyplot 是常用的绘图模块，能很方便让用户绘制 2D 图表。</p>
<p>Pyplot 包含一系列绘图函数的相关函数，每个函数会对当前的图像进行一些修改，例如：给图像加上标记，生新的图像，在图像中产生新的绘图区域等等。</p>
<p>使用的时候，我们可以使用 import 导入 pyplot 库，并设置一个别名<code>plt</code>：</p>
<p><code>import matplotlib.pyplot as plt</code></p>
<p>这样我们就可以使用 <strong>plt</strong> 来引用 Pyplot 包的方法。</p>
<p>以下实例，我们通过两个坐标 <strong>(0,0)</strong> 到 <strong>(6,100)</strong> 来绘制一条线:</p>
<p>实例1:</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

xpoints = np.array([0, 6])
ypoints = np.array([0, 100])

plt.plot(xpoints, ypoints)
plt.show()
</code></pre>
<p>输出结果如下：</p>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403204022943.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403204022943.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>以上实例中我们使用了 Pyplot 的<code>plot()</code> 函数， <strong>plot()</strong> 函数是绘制二维图形的最基本函数。</p>
<p><strong>plot()</strong> 用于画图它可以绘制点和线，语法格式如下：</p>
<pre><code class="lang-python"># 画单条线
plot([x], y, [fmt], *, data=None, **kwargs)
# 画多条线
plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)
</code></pre>
<p>参数说明：</p>
<ul>
<li><strong>x, y：</strong>点或线的节点，x 为 x 轴数据，y 为 y 轴数据，数据可以列表或数组。</li>
<li><strong>fmt：</strong>可选，定义基本格式（如颜色、标记和线条样式）。</li>
<li><strong><em>\</em>kwargs：</strong>可选，用在二维平面图上，设置指定属性，如标签，线的宽度等。</li>
</ul>
<pre><code class="lang-python">&gt;&gt;&gt; plot(x, y)        # 创建 y 中数据与 x 中对应值的二维线图，使用默认样式
&gt;&gt;&gt; plot(x, y, &#39;bo&#39;)  # 创建 y 中数据与 x 中对应值的二维线图，使用蓝色实心圈绘制
&gt;&gt;&gt; plot(y)           # x 的值为 0..N-1
&gt;&gt;&gt; plot(y, &#39;r+&#39;)     # 使用红色 + 号
</code></pre>
<p><strong>颜色字符：</strong>‘b’ 蓝色，’m’ 洋红色，’g’ 绿色，’y’ 黄色，’r’ 红色，’k’ 黑色，’w’ 白色，’c’ 青绿色，’#008000’ RGB 颜色符串。多条曲线不指定颜色时，会自动选择不同颜色。</p>
<p><strong>线型参数：</strong>‘‐’ 实线，’‐‐’ 破折线，’‐.’ 点划线，’:’ 虚线。</p>
<p><strong>标记字符：</strong>‘.’ 点标记，’,’ 像素标记(极小点)，’o’ 实心圈标记，’v’ 倒三角标记，’^’ 上三角标记，’&gt;’ 右三角标记，’&lt;’ 左三角标记…等等。</p>
<p>如果我们只想绘制两个坐标点，而不是一条线，可以使用 <strong>o</strong> 参数，表示一个实心圈的标记。</p>
<p>实例2：绘制坐标 (1, 3) 和 (8, 10) 的两个点</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

xpoints = np.array([1, 8])
ypoints = np.array([3, 10])

plt.plot(xpoints, ypoints, &#39;o&#39;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403205635489.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403205635489.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>我们也可以绘制任意数量的点，只需确保两个轴上的点数相同即可。</p>
<p>实例3：绘制一条不规则线，坐标为 (1, 3) 、 (2, 8) 、(6, 1) 、(8, 10)，对应的两个数组为：[1, 2, 6, 8] 与 [3, 8, 1, 10]。</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

xpoints = np.array([1, 2, 6, 8])
ypoints = np.array([3, 8, 1, 10])

plt.plot(xpoints, ypoints)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403205827906.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403205827906.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例4：如果我们不指定 x 轴上的点，y只限定范围。</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([3, 10])

plt.plot(ypoints)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403210031623.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403210031623.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>从上图可以看出 x 的值默认设置为 <strong>[0, 1]</strong>。</p>
<p>实例5：如果我们不指定 x 轴上的点，y表明具体的点，则 x 会根据 y 的值来设置为 <strong>0, 1, 2, 3..N-1</strong></p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([3, 8, 1, 10, 5, 7])

plt.plot(ypoints)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403210331668.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403210331668.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例6：以下实例我们绘制一个正弦和余弦图，在 plt.plot() 参数中包含两对 <strong>x,y</strong> 值，第一对是 <strong>x,y</strong>，这对应于正弦函数，第二对是 <strong>x,z</strong>，这对应于余弦函数。</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.arange(0,4*np.pi,0.1)   # start,stop,step
y = np.sin(x)
z = np.cos(x)
plt.plot(x,y,x,z)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403210514868.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403210514868.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h2 id="Matplotlib-绘图标记"><a href="#Matplotlib-绘图标记" class="headerlink" title="Matplotlib 绘图标记"></a>Matplotlib 绘图标记</h2><p>绘图过程如果我们想要给坐标自定义一些不一样的标记，就可以使用 <strong>plot()</strong> 方法的 <code>marker</code>参数来定义。</p>
<p>实例1:定义实心圆标记</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([1,3,4,5,8,9,6,1,3,4,5,2,4])

plt.plot(ypoints, marker = &#39;o&#39;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403212010830.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403212010830.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>marker可以定义的符号如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">标记</th>
<th style="text-align:left">符号</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">“.”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m00.png" class="lazyload" data-srcset="https://www.runoob.com/images/m00.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m00"></td>
<td style="text-align:left">点</td>
</tr>
<tr>
<td style="text-align:left">“,”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m01.png" class="lazyload" data-srcset="https://www.runoob.com/images/m01.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m01"></td>
<td style="text-align:left">像素点</td>
</tr>
<tr>
<td style="text-align:left">“o”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m02.png" class="lazyload" data-srcset="https://www.runoob.com/images/m02.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m02"></td>
<td style="text-align:left">实心圆</td>
</tr>
<tr>
<td style="text-align:left">“v”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m03.png" class="lazyload" data-srcset="https://www.runoob.com/images/m03.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m03"></td>
<td style="text-align:left">下三角</td>
</tr>
<tr>
<td style="text-align:left">“^”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m04.png" class="lazyload" data-srcset="https://www.runoob.com/images/m04.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m04"></td>
<td style="text-align:left">上三角</td>
</tr>
<tr>
<td style="text-align:left">“&lt;”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m05.png" class="lazyload" data-srcset="https://www.runoob.com/images/m05.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m05"></td>
<td style="text-align:left">左三角</td>
</tr>
<tr>
<td style="text-align:left">“&gt;”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m06.png" class="lazyload" data-srcset="https://www.runoob.com/images/m06.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m06"></td>
<td style="text-align:left">右三角</td>
</tr>
<tr>
<td style="text-align:left">“1”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m07.png" class="lazyload" data-srcset="https://www.runoob.com/images/m07.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m07"></td>
<td style="text-align:left">下三叉</td>
</tr>
<tr>
<td style="text-align:left">“2”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m08.png" class="lazyload" data-srcset="https://www.runoob.com/images/m08.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m08"></td>
<td style="text-align:left">上三叉</td>
</tr>
<tr>
<td style="text-align:left">“3”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m09.png" class="lazyload" data-srcset="https://www.runoob.com/images/m09.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m09"></td>
<td style="text-align:left">左三叉</td>
</tr>
<tr>
<td style="text-align:left">“4”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m10.png" class="lazyload" data-srcset="https://www.runoob.com/images/m10.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m10"></td>
<td style="text-align:left">右三叉</td>
</tr>
<tr>
<td style="text-align:left">“8”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m11.png" class="lazyload" data-srcset="https://www.runoob.com/images/m11.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m11"></td>
<td style="text-align:left">八角形</td>
</tr>
<tr>
<td style="text-align:left">“s”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m12.png" class="lazyload" data-srcset="https://www.runoob.com/images/m12.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m12"></td>
<td style="text-align:left">正方形</td>
</tr>
<tr>
<td style="text-align:left">“p”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m13.png" class="lazyload" data-srcset="https://www.runoob.com/images/m13.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m13"></td>
<td style="text-align:left">五边形</td>
</tr>
<tr>
<td style="text-align:left">“P”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m23.png" class="lazyload" data-srcset="https://www.runoob.com/images/m23.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m23"></td>
<td style="text-align:left">加号（填充）</td>
</tr>
<tr>
<td style="text-align:left">“*”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m14.png" class="lazyload" data-srcset="https://www.runoob.com/images/m14.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m14"></td>
<td style="text-align:left">星号</td>
</tr>
<tr>
<td style="text-align:left">“h”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m15.png" class="lazyload" data-srcset="https://www.runoob.com/images/m15.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m15"></td>
<td style="text-align:left">六边形 1</td>
</tr>
<tr>
<td style="text-align:left">“H”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m16.png" class="lazyload" data-srcset="https://www.runoob.com/images/m16.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m16"></td>
<td style="text-align:left">六边形 2</td>
</tr>
<tr>
<td style="text-align:left">“+”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m17.png" class="lazyload" data-srcset="https://www.runoob.com/images/m17.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m17"></td>
<td style="text-align:left">加号</td>
</tr>
<tr>
<td style="text-align:left">“x”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m18.png" class="lazyload" data-srcset="https://www.runoob.com/images/m18.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m18"></td>
<td style="text-align:left">乘号 x</td>
</tr>
<tr>
<td style="text-align:left">“X”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m24.png" class="lazyload" data-srcset="https://www.runoob.com/images/m24.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m24"></td>
<td style="text-align:left">乘号 x (填充)</td>
</tr>
<tr>
<td style="text-align:left">“D”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m19.png" class="lazyload" data-srcset="https://www.runoob.com/images/m19.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m19"></td>
<td style="text-align:left">菱形</td>
</tr>
<tr>
<td style="text-align:left">“d”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m20.png" class="lazyload" data-srcset="https://www.runoob.com/images/m20.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m20"></td>
<td style="text-align:left">瘦菱形</td>
</tr>
<tr>
<td style="text-align:left">“\</td>
<td style="text-align:left">“</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m21.png" class="lazyload" data-srcset="https://www.runoob.com/images/m21.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m21"></td>
<td>竖线</td>
</tr>
<tr>
<td style="text-align:left">“_”</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m22.png" class="lazyload" data-srcset="https://www.runoob.com/images/m22.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m22"></td>
<td style="text-align:left">横线</td>
</tr>
<tr>
<td style="text-align:left">0 (TICKLEFT)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m25.png" class="lazyload" data-srcset="https://www.runoob.com/images/m25.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m25"></td>
<td style="text-align:left">左横线</td>
</tr>
<tr>
<td style="text-align:left">1 (TICKRIGHT)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m26.png" class="lazyload" data-srcset="https://www.runoob.com/images/m26.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m26"></td>
<td style="text-align:left">右横线</td>
</tr>
<tr>
<td style="text-align:left">2 (TICKUP)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m27.png" class="lazyload" data-srcset="https://www.runoob.com/images/m27.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m27"></td>
<td style="text-align:left">上竖线</td>
</tr>
<tr>
<td style="text-align:left">3 (TICKDOWN)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m28.png" class="lazyload" data-srcset="https://www.runoob.com/images/m28.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m28"></td>
<td style="text-align:left">下竖线</td>
</tr>
<tr>
<td style="text-align:left">4 (CARETLEFT)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m29.png" class="lazyload" data-srcset="https://www.runoob.com/images/m29.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m29"></td>
<td style="text-align:left">左箭头</td>
</tr>
<tr>
<td style="text-align:left">5 (CARETRIGHT)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m30.png" class="lazyload" data-srcset="https://www.runoob.com/images/m30.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m30"></td>
<td style="text-align:left">右箭头</td>
</tr>
<tr>
<td style="text-align:left">6 (CARETUP)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m31.png" class="lazyload" data-srcset="https://www.runoob.com/images/m31.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m31"></td>
<td style="text-align:left">上箭头</td>
</tr>
<tr>
<td style="text-align:left">7 (CARETDOWN)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m32.png" class="lazyload" data-srcset="https://www.runoob.com/images/m32.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m32"></td>
<td style="text-align:left">下箭头</td>
</tr>
<tr>
<td style="text-align:left">8 (CARETLEFTBASE)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m33.png" class="lazyload" data-srcset="https://www.runoob.com/images/m33.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m33"></td>
<td style="text-align:left">左箭头 (中间点为基准)</td>
</tr>
<tr>
<td style="text-align:left">9 (CARETRIGHTBASE)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m34.png" class="lazyload" data-srcset="https://www.runoob.com/images/m34.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m34"></td>
<td style="text-align:left">右箭头 (中间点为基准)</td>
</tr>
<tr>
<td style="text-align:left">10 (CARETUPBASE)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m35.png" class="lazyload" data-srcset="https://www.runoob.com/images/m35.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m35"></td>
<td style="text-align:left">上箭头 (中间点为基准)</td>
</tr>
<tr>
<td style="text-align:left">11 (CARETDOWNBASE)</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m36.png" class="lazyload" data-srcset="https://www.runoob.com/images/m36.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m36"></td>
<td style="text-align:left">下箭头 (中间点为基准)</td>
</tr>
<tr>
<td style="text-align:left">“None”, “ “ or “”</td>
<td style="text-align:left"></td>
<td style="text-align:left">没有任何标记</td>
</tr>
<tr>
<td style="text-align:left">‘$…$’</td>
<td style="text-align:left"><img src="https://www.runoob.com/images/m37.png" class="lazyload" data-srcset="https://www.runoob.com/images/m37.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="m37"></td>
<td style="text-align:left">渲染指定的字符。例如 “$f$” 以字母 f 为标记。</td>
</tr>
</tbody>
</table>
</div>
<p>实例2:定义了 <strong>*</strong> 标记</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([1,3,4,5,8,9,6,1,3,4,5,2,4])

plt.plot(ypoints, marker = &#39;*&#39;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403212950411.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403212950411.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例3:定义下箭头</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import matplotlib.markers

plt.plot([1, 2, 3], marker=matplotlib.markers.CARETDOWNBASE)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403213418027.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403213418027.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h3 id="fmt-参数"><a href="#fmt-参数" class="headerlink" title="fmt 参数"></a>fmt 参数</h3><p>fmt 参数定义了基本格式，如标记、线条样式和颜色。</p>
<pre><code class="lang-python">fmt = &#39;[marker][line][color]&#39;
</code></pre>
<p>例如 <strong>o:r</strong>，<strong>o</strong> 表示实心圆标记，<strong>:</strong> 表示虚线，<strong>r</strong> 表示颜色为红色。</p>
<p>实例4：</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([6, 2, 13, 10])

plt.plot(ypoints, &#39;o:r&#39;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403213911622.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403213911622.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>线类型：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">线类型标记</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">‘-‘</td>
<td style="text-align:left">实线</td>
</tr>
<tr>
<td style="text-align:left">‘:’</td>
<td style="text-align:left">虚线</td>
</tr>
<tr>
<td style="text-align:left">‘—‘</td>
<td style="text-align:left">破折线</td>
</tr>
<tr>
<td style="text-align:left">‘-.’</td>
<td style="text-align:left">点划线</td>
</tr>
</tbody>
</table>
</div>
<p>颜色类型：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">颜色标记</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">‘r’</td>
<td style="text-align:left">红色</td>
</tr>
<tr>
<td style="text-align:left">‘g’</td>
<td style="text-align:left">绿色</td>
</tr>
<tr>
<td style="text-align:left">‘b’</td>
<td style="text-align:left">蓝色</td>
</tr>
<tr>
<td style="text-align:left">‘c’</td>
<td style="text-align:left">青色</td>
</tr>
<tr>
<td style="text-align:left">‘m’</td>
<td style="text-align:left">品红</td>
</tr>
<tr>
<td style="text-align:left">‘y’</td>
<td style="text-align:left">黄色</td>
</tr>
<tr>
<td style="text-align:left">‘k’</td>
<td style="text-align:left">黑色</td>
</tr>
<tr>
<td style="text-align:left">‘w’</td>
<td style="text-align:left">白色</td>
</tr>
</tbody>
</table>
</div>
<h3 id="标记大小和颜色"><a href="#标记大小和颜色" class="headerlink" title="标记大小和颜色"></a>标记大小和颜色</h3><p>我们可以自定义标记的大小与颜色，使用的参数分别是：</p>
<ul>
<li>markersize，简写为 <strong>ms</strong>：定义标记的大小。</li>
<li>markerfacecolor，简写为 <strong>mfc</strong>：定义标记内部的颜色。</li>
<li>markeredgecolor，简写为 <strong>mec</strong>：定义标记边框的颜色。</li>
</ul>
<p>实例5：设置标记大小</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([6, 2, 13, 10])

plt.plot(ypoints, marker = &#39;o&#39;, ms = 20)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403215102017.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403215102017.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例6：设置标记外边框颜色</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([6, 2, 13, 10])

plt.plot(ypoints, marker = &#39;o&#39;, ms = 20, mec = &#39;r&#39;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403215856505.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403215856505.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例7：设置标记内部颜色</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([6, 2, 13, 10])

plt.plot(ypoints, marker = &#39;o&#39;, ms = 20, mfc = &#39;r&#39;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403220017186.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403220017186.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例8：自定义标记内部与边框的颜色</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([6, 2, 13, 10])
plt.plot(ypoints, marker = &#39;o&#39;, ms = 20, mec = &#39;#4CAF50&#39;, mfc = &#39;#4CAF50&#39;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403220233003.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403220233003.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h2 id="Matplotlib-绘图线"><a href="#Matplotlib-绘图线" class="headerlink" title="Matplotlib 绘图线"></a>Matplotlib 绘图线</h2><p>绘图过程如果我们自定义线的样式，包括线的类型、颜色和大小等。</p>
<h3 id="线的类型"><a href="#线的类型" class="headerlink" title="线的类型"></a>线的类型</h3><p>线的类型可以使用 <strong>linestyle</strong> 参数来定义，简写为 <strong>ls</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">类型</th>
<th style="text-align:left">简写</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">‘solid’ (默认)</td>
<td style="text-align:left">‘-‘</td>
<td style="text-align:left">实线</td>
</tr>
<tr>
<td style="text-align:left">‘dotted’</td>
<td style="text-align:left">‘:’</td>
<td style="text-align:left">点虚线</td>
</tr>
<tr>
<td style="text-align:left">‘dashed’</td>
<td style="text-align:left">‘—‘</td>
<td style="text-align:left">破折线</td>
</tr>
<tr>
<td style="text-align:left">‘dashdot’</td>
<td style="text-align:left">‘-.’</td>
<td style="text-align:left">点划线</td>
</tr>
<tr>
<td style="text-align:left">‘None’</td>
<td style="text-align:left">‘’ 或 ‘ ‘</td>
<td style="text-align:left">不画线</td>
</tr>
</tbody>
</table>
</div>
<p>实例1：</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([6, 2, 13, 10])

plt.plot(ypoints, linestyle = &#39;dotted&#39;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403221432174.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403221432174.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例2：使用简写</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([6, 2, 13, 10])

plt.plot(ypoints, ls = &#39;-.&#39;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403221604494.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403221604494.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h3 id="线的颜色"><a href="#线的颜色" class="headerlink" title="线的颜色"></a>线的颜色</h3><p>线的颜色可以使用 <strong>color</strong> 参数来定义，简写为 <strong>c</strong>。</p>
<p>颜色类型：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">颜色标记</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">‘r’</td>
<td style="text-align:left">红色</td>
</tr>
<tr>
<td style="text-align:left">‘g’</td>
<td style="text-align:left">绿色</td>
</tr>
<tr>
<td style="text-align:left">‘b’</td>
<td style="text-align:left">蓝色</td>
</tr>
<tr>
<td style="text-align:left">‘c’</td>
<td style="text-align:left">青色</td>
</tr>
<tr>
<td style="text-align:left">‘m’</td>
<td style="text-align:left">品红</td>
</tr>
<tr>
<td style="text-align:left">‘y’</td>
<td style="text-align:left">黄色</td>
</tr>
<tr>
<td style="text-align:left">‘k’</td>
<td style="text-align:left">黑色</td>
</tr>
<tr>
<td style="text-align:left">‘w’</td>
<td style="text-align:left">白色</td>
</tr>
</tbody>
</table>
</div>
<p>当然也可以自定义颜色类型，例如：<strong>SeaGreen、#8FBC8F</strong> 等，完整样式可以参考 <a href="https://www.runoob.com/html/html-colorvalues.html">HTML 颜色值</a>。</p>
<p>实例3：</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([6, 2, 13, 10])

plt.plot(ypoints, color = &#39;r&#39;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403221822295.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403221822295.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例4：</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([6, 2, 13, 10])

plt.plot(ypoints, c = &#39;#8FBC8F&#39;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222000966.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222000966.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例5：</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([6, 2, 13, 10])

plt.plot(ypoints, c = &#39;SeaGreen&#39;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222117925.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222117925.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h3 id="线的宽度"><a href="#线的宽度" class="headerlink" title="线的宽度"></a>线的宽度</h3><p>线的宽度可以使用 <strong>linewidth</strong> 参数来定义，简写为 <strong>lw</strong>，值可以是浮点数，如：<strong>1</strong>、<strong>2.0</strong>、<strong>5.67</strong> 等</p>
<p>实例6：</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

ypoints = np.array([6, 2, 13, 10])

plt.plot(ypoints, linewidth = &#39;12.5&#39;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222318825.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222318825.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h3 id="多条线"><a href="#多条线" class="headerlink" title="多条线"></a>多条线</h3><p>plot() 方法中可以包含多对 x,y 值来绘制多条线。</p>
<p>实例7：</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

y1 = np.array([3, 7, 5, 9])
y2 = np.array([6, 2, 13, 10])

plt.plot(y1)
plt.plot(y2)

plt.show()
</code></pre>
<p>从上图可以看出 <strong>x</strong> 的值默认设置为 <strong>[0, 1, 2, 3]</strong>。</p>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222526935.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222526935.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例8：</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x1 = np.array([0, 1, 2, 3])
y1 = np.array([3, 7, 5, 9])
x2 = np.array([0, 1, 2, 3])
y2 = np.array([6, 2, 13, 10])

plt.plot(x1, y1, x2, y2)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222628227.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220403222628227.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h2 id="Matplotlib-轴标签和标题"><a href="#Matplotlib-轴标签和标题" class="headerlink" title="Matplotlib 轴标签和标题"></a>Matplotlib 轴标签和标题</h2><p>我们可以使用 <strong>xlabel()</strong> 和 <strong>ylabel()</strong> 方法来设置 x 轴和 y 轴的标签。</p>
<p>实例1：</p>
<pre><code class="lang-python">import numpy as np
import matplotlib.pyplot as plt

x = np.array([1, 2, 3, 4])
y = np.array([1, 4, 9, 16])
plt.plot(x, y)

plt.xlabel(&quot;x - label&quot;)
plt.ylabel(&quot;y - label&quot;)

plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404095612577.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404095612577.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><p>实例2：使用 <strong>title()</strong> 方法来设置标题:</p>
<pre><code class="lang-python">import numpy as np
import matplotlib.pyplot as plt

x = np.array([1, 2, 3, 4])
y = np.array([1, 4, 9, 16])
plt.plot(x, y)

plt.title(&quot;line chart&quot;)
plt.xlabel(&quot;x - label&quot;)
plt.ylabel(&quot;y - label&quot;)

plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404100123540.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404100123540.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h3 id="图像中文显示"><a href="#图像中文显示" class="headerlink" title="图像中文显示"></a>图像中文显示</h3><p>Matplotlib 默认情况不支持中文，我们可以使用以下简单的方法来解决。</p>
<p>这里我们使用思源黑体，思源黑体是 Adobe 与 Google 推出的一款开源字体。</p>
<p>官网：<a href="https://source.typekit.com/source-han-serif/cn/">https://source.typekit.com/source-han-serif/cn/</a></p>
<p>GitHub 地址：<a href="https://github.com/adobe-fonts/source-han-sans/tree/release/OTF/SimplifiedChinese">https://github.com/adobe-fonts/source-han-sans/tree/release/OTF/SimplifiedChinese</a></p>
<p>打开链接后，在里面选一个就好了：</p>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104121709.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104121709.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>你也可以在网盘下载: <a href="https://pan.baidu.com/s/10-w1JbXZSnx3Tm6uGpPGOw，提取码：**yxqu**。">https://pan.baidu.com/s/10-w1JbXZSnx3Tm6uGpPGOw，提取码：**yxqu**。</a></p>
<p>可以下载个 OTF 字体，比如 SourceHanSansSC-Bold.otf，将该文件文件放在当前执行的代码文件中：</p>
<p><code>SourceHanSansSC-Bold.otf</code> 文件放在当前执行的代码文件中。</p>
<p>实例3：</p>
<pre><code class="lang-python">import numpy as np 
from matplotlib import pyplot as plt 
import matplotlib

# fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径
zhfont1 = matplotlib.font_manager.FontProperties(fname=&quot;SourceHanSansSC-Bold.otf&quot;) 

x = np.arange(1,11) 
y =  2  * x +  5 
plt.title(&quot;测试&quot;, fontproperties=zhfont1) 

# fontproperties 设置中文显示，fontsize 设置字体大小
plt.xlabel(&quot;x标签&quot;, fontproperties=zhfont1)
plt.ylabel(&quot;y标签&quot;, fontproperties=zhfont1)
plt.plot(x,y) 
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104616081.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104616081.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p><em>此外，我们还可以使用系统的字体：</em></p>
<pre><code class="lang-python">from matplotlib import pyplot as plt
import matplotlib
a=sorted([f.name for f in matplotlib.font_manager.fontManager.ttflist])

for i in a:
    print(i)
</code></pre>
<p><em>打印出你的 font_manager 的 ttflist 中所有注册的名字，找一个看中文字体例如：STFangsong(仿宋）,然后添加以下代码即可：</em></p>
<pre><code class="lang-python">plt.rcParams[&#39;font.family&#39;]=[&#39;STFangsong&#39;]
</code></pre>
<p>实例4：:自定义字体的样式</p>
<pre><code class="lang-python">import numpy as np
from matplotlib import pyplot as plt
import matplotlib

# fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径，size 参数设置字体大小
zhfont1 = matplotlib.font_manager.FontProperties(fname=&quot;SourceHanSansSC-Bold.otf&quot;, size=18)
font1 = &#123;&#39;color&#39;:&#39;blue&#39;,&#39;size&#39;:20&#125;
font2 = &#123;&#39;color&#39;:&#39;darkred&#39;,&#39;size&#39;:15&#125;
x = np.arange(1,11)
y =  2  * x +  5

# fontdict 可以使用 css 来设置字体样式
plt.title(&quot;菜鸟教程 - 测试&quot;, fontproperties=zhfont1, fontdict = font1)

# fontproperties 设置中文显示，fontsize 设置字体大小
plt.xlabel(&quot;x 轴&quot;, fontproperties=zhfont1)
plt.ylabel(&quot;y 轴&quot;, fontproperties=zhfont1)
plt.plot(x,y)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104841496.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104841496.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h3 id="标题与标签的定位"><a href="#标题与标签的定位" class="headerlink" title="标题与标签的定位"></a>标题与标签的定位</h3><p><strong>title()</strong> 方法提供了 <strong>loc</strong> 参数来设置标题显示的位置，可以设置为: <strong>‘left’, ‘right’, 和 ‘center’， 默认值为 ‘center’</strong>。</p>
<p><strong>xlabel()</strong> 方法提供了 <strong>loc</strong> 参数来设置 x 轴显示的位置，可以设置为: <strong>‘left’, ‘right’, 和 ‘center’， 默认值为 ‘center’</strong>。</p>
<p><strong>ylabel()</strong> 方法提供了 <strong>loc</strong> 参数来设置 y 轴显示的位置，可以设置为: <strong>‘bottom’, ‘top’, 和 ‘center’， 默认值为 ‘center’</strong>。</p>
<p>实例5：</p>
<pre><code class="lang-python">import numpy as np
from matplotlib import pyplot as plt
import matplotlib

# fname 为 你下载的字体库路径，注意 SourceHanSansSC-Bold.otf 字体的路径，size 参数设置字体大小
zhfont1 = matplotlib.font_manager.FontProperties(fname=&quot;SourceHanSansSC-Bold.otf&quot;, size=18)
font1 = &#123;&#39;color&#39;:&#39;blue&#39;,&#39;size&#39;:20&#125;
font2 = &#123;&#39;color&#39;:&#39;darkred&#39;,&#39;size&#39;:15&#125;
x = np.arange(1,11)
y =  2  * x +  5

# fontdict 可以使用 css 来设置字体样式
plt.title(&quot;菜鸟教程 - 测试&quot;, fontproperties=zhfont1, fontdict = font1, loc=&quot;left&quot;)

# fontproperties 设置中文显示，fontsize 设置字体大小
plt.xlabel(&quot;x 轴&quot;, fontproperties=zhfont1, loc=&quot;left&quot;)
plt.ylabel(&quot;y 轴&quot;, fontproperties=zhfont1, loc=&quot;top&quot;)
plt.plot(x,y)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104916266.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404104916266.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h2 id="Matplotlib-网格线"><a href="#Matplotlib-网格线" class="headerlink" title="Matplotlib 网格线"></a>Matplotlib 网格线</h2><p>我们可以使用 pyplot 中的 grid() 方法来设置图表中的网格线。</p>
<p>grid() 方法语法格式如下：</p>
<pre><code>matplotlib.pyplot.grid(b=None, which=&#39;major&#39;, axis=&#39;both&#39;, )
</code></pre><p><strong>参数说明：</strong></p>
<ul>
<li><p><strong>b</strong>：可选，默认为 None，可以设置布尔值，true 为显示网格线，false 为不显示，如果设置 **kwargs 参数，则值为 true。</p>
</li>
<li><p><strong>which</strong>：可选，可选值有 ‘major’、’minor’ 和 ‘both’，默认为 ‘major’，表示应用更改的网格线。</p>
</li>
<li><p><strong>axis</strong>：可选，设置显示哪个方向的网格线，可以是取 ‘both’（默认），’x’ 或 ‘y’，分别表示两个方向，x 轴方向或 y 轴方向。</p>
</li>
<li><p><strong><em>\</em>kwargs</strong>：可选，设置网格样式，可以是 color=’r’, linestyle=’-‘ 和 linewidth=2，分别表示网格线的颜色，样式和宽度。</p>
</li>
</ul>
<p>实例1：添加一个简单的网格线，参数使用默认值</p>
<pre><code class="lang-python">import numpy as np
import matplotlib.pyplot as plt

x = np.array([1, 2, 3, 4])
y = np.array([1, 4, 9, 16])


plt.title(&quot;RUNOOB grid() Test&quot;)
plt.xlabel(&quot;x - label&quot;)
plt.ylabel(&quot;y - label&quot;)

plt.plot(x, y)

plt.grid()

plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152016310.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152016310.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例2:添加一个简单的网格线，axis 参数使用 x，设置 x 轴方向显示网格线</p>
<pre><code class="lang-python">import numpy as np
import matplotlib.pyplot as plt

x = np.array([1, 2, 3, 4])
y = np.array([1, 4, 9, 16])


plt.title(&quot;RUNOOB grid() Test&quot;)
plt.xlabel(&quot;x - label&quot;)
plt.ylabel(&quot;y - label&quot;)

plt.plot(x, y)

plt.grid(axis=&#39;x&#39;) # 设置 y 就在轴方向显示网格线

plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152129680.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152129680.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>以下实例添加一个简单的网格线，并设置网格线的样式，格式如下：</p>
<pre><code>grid(color = &#39;color&#39;, linestyle = &#39;linestyle&#39;, linewidth = number)
</code></pre><p><strong>参数说明：</strong></p>
<p><strong>color：</strong>‘b’ 蓝色，’m’ 洋红色，’g’ 绿色，’y’ 黄色，’r’ 红色，’k’ 黑色，’w’ 白色，’c’ 青绿色，’#008000’ RGB 颜色符串。</p>
<p><strong>linestyle：</strong>‘‐’ 实线，’‐‐’ 破折线，’‐.’ 点划线，’:’ 虚线。</p>
<p><strong>linewidth</strong>：设置线的宽度，可以设置一个数字。</p>
<p>实例3：</p>
<pre><code class="lang-python">import numpy as np
import matplotlib.pyplot as plt

x = np.array([1, 2, 3, 4])
y = np.array([1, 4, 9, 16])


plt.title(&quot;RUNOOB grid() Test&quot;)
plt.xlabel(&quot;x - label&quot;)
plt.ylabel(&quot;y - label&quot;)

plt.plot(x, y)

plt.grid(color = &#39;r&#39;, linestyle = &#39;--&#39;, linewidth = 0.5)

plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152316633.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152316633.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h2 id="Matplotlib绘制多图"><a href="#Matplotlib绘制多图" class="headerlink" title="Matplotlib绘制多图"></a>Matplotlib绘制多图</h2><p>我们可以使用 pyplot 中的 <strong>subplot()</strong> 和 <strong>subplots()</strong> 方法来绘制多个子图。</p>
<p><strong>subplot()</strong> 方法在绘图时需要指定位置，<strong>subplots()</strong> 方法可以一次生成多个，在调用时只需要调用生成对象的 ax 即可。</p>
<h3 id="subplot"><a href="#subplot" class="headerlink" title="subplot"></a>subplot</h3><pre><code class="lang-python">subplot(nrows, ncols, index, **kwargs)
subplot(pos, **kwargs)
subplot(**kwargs)
subplot(ax)
</code></pre>
<p>以上函数将整个绘图区域分成 nrows 行和 ncols 列，然后从左到右，从上到下的顺序对每个子区域进行编号 <strong>1…N</strong> ，左上的子区域的编号为 1、右下的区域编号为 N，编号可以通过参数 <strong>index</strong> 来设置。</p>
<p>设置 numRows ＝ 1，numCols ＝ 2，就是将图表绘制成 1x2 的图片区域, 对应的坐标为：</p>
<pre><code>(1, 1), (1, 2)
</code></pre><p><strong>plotNum ＝ 1</strong>, 表示的坐标为(1, 1), 即第一行第一列的子图。</p>
<p><strong>plotNum ＝ 2</strong>, 表示的坐标为(1, 2), 即第一行第二列的子图。</p>
<p>实例1：</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

#plot 1:
xpoints = np.array([0, 6])
ypoints = np.array([0, 100])

plt.subplot(1, 2, 1)
plt.plot(xpoints,ypoints)
plt.title(&quot;plot 1&quot;)

#plot 2:
x = np.array([1, 2, 3, 4])
y = np.array([1, 4, 9, 16])

plt.subplot(1, 2, 2)
plt.plot(x,y)
plt.title(&quot;plot 2&quot;)

plt.suptitle(&quot;RUNOOB subplot Test&quot;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152756053.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152756053.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>设置 numRows ＝ 2，numCols ＝ 2，就是将图表绘制成 2x2 的图片区域, 对应的坐标为：</p>
<pre><code>(1, 1), (1, 2)
(2, 1), (2, 2)
</code></pre><p><strong>plotNum ＝ 1</strong>, 表示的坐标为(1, 1), 即第一行第一列的子图。</p>
<p><strong>plotNum ＝ 2</strong>, 表示的坐标为(1, 2), 即第一行第二列的子图。</p>
<p><strong>plotNum ＝ 3</strong>, 表示的坐标为(2, 1), 即第二行第一列的子图。</p>
<p><strong>plotNum ＝ 4</strong>, 表示的坐标为(2, 2), 即第二行第二列的子图。</p>
<p>实例2：</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

#plot 1:
x = np.array([0, 6])
y = np.array([0, 100])

plt.subplot(2, 2, 1)
plt.plot(x,y)
plt.title(&quot;plot 1&quot;)

#plot 2:
x = np.array([1, 2, 3, 4])
y = np.array([1, 4, 9, 16])

plt.subplot(2, 2, 2)
plt.plot(x,y)
plt.title(&quot;plot 2&quot;)

#plot 3:
x = np.array([1, 2, 3, 4])
y = np.array([3, 5, 7, 9])

plt.subplot(2, 2, 3)
plt.plot(x,y)
plt.title(&quot;plot 3&quot;)

#plot 4:
x = np.array([1, 2, 3, 4])
y = np.array([4, 5, 6, 7])

plt.subplot(2, 2, 4)
plt.plot(x,y)
plt.title(&quot;plot 4&quot;)

plt.suptitle(&quot;RUNOOB subplot Test&quot;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152958366.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404152958366.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h3 id="subplots"><a href="#subplots" class="headerlink" title="subplots()"></a>subplots()</h3><p>subplots() 方法语法格式如下：</p>
<pre><code class="lang-python">matplotlib.pyplot.subplots(nrows=1, ncols=1, *, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None, **fig_kw)
</code></pre>
<p><strong>参数说明：</strong></p>
<ul>
<li><strong>nrows</strong>：默认为 1，设置图表的行数。</li>
<li><strong>ncols</strong>：默认为 1，设置图表的列数。</li>
<li><strong>sharex、sharey</strong>：设置 x、y 轴是否共享属性，默认为 false，可设置为 ‘none’、’all’、’row’ 或 ‘col’。 False 或 none 每个子图的 x 轴或 y 轴都是独立的，True 或 ‘all’：所有子图共享 x 轴或 y 轴，’row’ 设置每个子图行共享一个 x 轴或 y 轴，’col’：设置每个子图列共享一个 x 轴或 y 轴。</li>
<li><strong>squeeze</strong>：布尔值，默认为 True，表示额外的维度从返回的 Axes(轴)对象中挤出，对于 N<em>1 或 1</em>N 个子图，返回一个 1 维数组，对于 N*M，N&gt;1 和 M&gt;1 返回一个 2 维数组。如果设置为 False，则不进行挤压操作，返回一个元素为 Axes 实例的2维数组，即使它最终是1x1。</li>
<li><strong>subplot_kw</strong>：可选，字典类型。把字典的关键字传递给 add_subplot() 来创建每个子图。</li>
<li><strong>gridspec_kw</strong>：可选，字典类型。把字典的关键字传递给 GridSpec 构造函数创建子图放在网格里(grid)。</li>
<li><strong><em>\</em>fig_kw</strong>：把详细的关键字参数传给 figure() 函数。</li>
</ul>
<p>实例3：</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

# 创建一些测试数据 -- 图1
x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

# 创建一个画像和子图 -- 图2
fig, ax = plt.subplots()
ax.plot(x, y)
ax.set_title(&#39;Simple plot&#39;)

# 创建两个子图 -- 图3
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title(&#39;Sharing Y axis&#39;)
ax2.scatter(x, y)

# 创建四个子图 -- 图4
fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=&quot;polar&quot;))
axs[0, 0].plot(x, y)
axs[1, 1].scatter(x, y)

# 共享 x 轴
plt.subplots(2, 2, sharex=&#39;col&#39;)

# 共享 y 轴
plt.subplots(2, 2, sharey=&#39;row&#39;)

# 共享 x 轴和 y 轴
plt.subplots(2, 2, sharex=&#39;all&#39;, sharey=&#39;all&#39;)

# 这个也是共享 x 轴和 y 轴
plt.subplots(2, 2, sharex=True, sharey=True)

# 创建10 张图，已经存在的则删除
fig, ax = plt.subplots(num=10, clear=True)

plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153408476.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153408476.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153431942.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153431942.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153453565.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153453565.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153511622.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404153511622.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h2 id="Matplotlib散点图"><a href="#Matplotlib散点图" class="headerlink" title="Matplotlib散点图"></a>Matplotlib散点图</h2><p>我们可以使用 pyplot 中的 scatter() 方法来绘制散点图。</p>
<p>scatter() 方法语法格式如下：</p>
<pre><code class="lang-python">matplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)
</code></pre>
<p><strong>参数说明：</strong></p>
<p><strong>x，y</strong>：长度相同的数组，也就是我们即将绘制散点图的数据点，输入数据。</p>
<p><strong>s</strong>：点的大小，默认 20，也可以是个数组，数组每个参数为对应点的大小。</p>
<p><strong>c</strong>：点的颜色，默认蓝色 ‘b’，也可以是个 RGB 或 RGBA 二维行数组。</p>
<p><strong>marker</strong>：点的样式，默认小圆圈 ‘o’。</p>
<p><strong>cmap</strong>：Colormap，默认 None，标量或者是一个 colormap 的名字，只有 c 是一个浮点数数组的时才使用。如果没有申明就是 image.cmap。</p>
<p><strong>norm</strong>：Normalize，默认 None，数据亮度在 0-1 之间，只有 c 是一个浮点数的数组的时才使用。</p>
<p><strong>vmin，vmax：</strong>：亮度设置，在 norm 参数存在时会忽略。</p>
<p><strong>alpha：</strong>：透明度设置，0-1 之间，默认 None，即不透明。</p>
<p><strong>linewidths：</strong>：标记点的长度。</p>
<p><strong>edgecolors：</strong>：颜色或颜色序列，默认为 ‘face’，可选值有 ‘face’, ‘none’, None。</p>
<p><strong>plotnonfinite：</strong>：布尔值，设置是否使用非限定的 c ( inf, -inf 或 nan) 绘制点。</p>
<p><strong><em>\</em>kwargs：</strong>：其他参数。</p>
<p>以下实例 scatter() 函数接收长度相同的数组参数，一个用于 x 轴的值，另一个用于 y 轴上的值：</p>
<p>实例1：</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.array([1, 2, 3, 4, 5, 6, 7, 8])
y = np.array([1, 4, 9, 16, 7, 11, 23, 18])

plt.scatter(x, y)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155337371.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155337371.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例2：设置图标大小</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.array([1, 2, 3, 4, 5, 6, 7, 8])
y = np.array([1, 4, 9, 16, 7, 11, 23, 18])
sizes = np.array([20,50,100,200,500,1000,60,90])
plt.scatter(x, y, s=sizes)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155550197.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155550197.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例3：自定义点的颜色</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.array([1, 2, 3, 4, 5, 6, 7, 8])
y = np.array([1, 4, 9, 16, 7, 11, 23, 18])
colors = np.array([&quot;red&quot;,&quot;green&quot;,&quot;black&quot;,&quot;orange&quot;,&quot;purple&quot;,&quot;beige&quot;,&quot;cyan&quot;,&quot;magenta&quot;])

plt.scatter(x, y, c=colors)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155740263.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155740263.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例4：设置两组散点图</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.array([5,7,8,7,2,17,2,9,4,11,12,9,6])
y = np.array([99,86,87,88,111,86,103,87,94,78,77,85,86])
plt.scatter(x, y, color = &#39;hotpink&#39;)

x = np.array([2,2,8,1,15,8,12,9,7,3,11,4,7,14,12])
y = np.array([100,105,84,105,90,99,90,95,94,100,79,112,91,80,85])
plt.scatter(x, y, color = &#39;#88c999&#39;)

plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155923033.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404155923033.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例5：使用随机数来设置散点图</p>
<pre><code class="lang-python">import numpy as np
import matplotlib.pyplot as plt

# 随机数生成器的种子
np.random.seed(19680801)


N = 50
x = np.random.rand(N)
y = np.random.rand(N)
colors = np.random.rand(N)
area = (30 * np.random.rand(N))**2  # 0 to 15 point radii

plt.scatter(x, y, s=area, c=colors, alpha=0.5) # 设置颜色及透明度

plt.title(&quot;RUNOOB Scatter Test&quot;) # 设置标题

plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160218777.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160218777.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h3 id="颜色条Colormap"><a href="#颜色条Colormap" class="headerlink" title="颜色条Colormap"></a>颜色条Colormap</h3><p>Matplotlib 模块提供了很多可用的颜色条。</p>
<p>颜色条就像一个颜色列表，其中每种颜色都有一个范围从 0 到 100 的值。</p>
<p>下面是一个颜色条的例子：</p>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160411035.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160411035.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例6：设置颜色条需要使用 cmap 参数，默认值为 ‘viridis’，之后颜色值设置为 0 到 100 的数组</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.array([5,7,8,7,2,17,2,9,4,11,12,9,6])
y = np.array([99,86,87,88,111,86,103,87,94,78,77,85,86])
colors = np.array([0, 10, 20, 30, 40, 45, 50, 55, 60, 70, 80, 90, 100])

plt.scatter(x, y, c=colors, cmap=&#39;viridis&#39;)

plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160555429.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160555429.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例7：如果要显示颜色条，需要使用 <strong>plt.colorbar()</strong> 方法</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.array([5,7,8,7,2,17,2,9,4,11,12,9,6])
y = np.array([99,86,87,88,111,86,103,87,94,78,77,85,86])
colors = np.array([0, 10, 20, 30, 40, 45, 50, 55, 60, 70, 80, 90, 100])

plt.scatter(x, y, c=colors, cmap=&#39;viridis&#39;)

plt.colorbar()

plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160751739.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160751739.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例8：换个颜色条参数， cmap 设置为 <strong>afmhot_r</strong></p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.array([5,7,8,7,2,17,2,9,4,11,12,9,6])
y = np.array([99,86,87,88,111,86,103,87,94,78,77,85,86])
colors = np.array([0, 10, 20, 30, 40, 45, 50, 55, 60, 70, 80, 90, 100])

plt.scatter(x, y, c=colors, cmap=&#39;afmhot_r&#39;)
plt.colorbar()
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160958004.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404160958004.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>颜色条参数值可以是以下值：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">颜色名称</th>
<th style="text-align:left">保留关键字</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Accent</td>
<td style="text-align:left">Accent_r</td>
</tr>
<tr>
<td style="text-align:left">Blues</td>
<td style="text-align:left">Blues_r</td>
</tr>
<tr>
<td style="text-align:left">BrBG</td>
<td style="text-align:left">BrBG_r</td>
</tr>
<tr>
<td style="text-align:left">BuGn</td>
<td style="text-align:left">BuGn_r</td>
</tr>
<tr>
<td style="text-align:left">BuPu</td>
<td style="text-align:left">BuPu_r</td>
</tr>
<tr>
<td style="text-align:left">CMRmap</td>
<td style="text-align:left">CMRmap_r</td>
</tr>
<tr>
<td style="text-align:left">Dark2</td>
<td style="text-align:left">Dark2_r</td>
</tr>
<tr>
<td style="text-align:left">GnBu</td>
<td style="text-align:left">GnBu_r</td>
</tr>
<tr>
<td style="text-align:left">Greens</td>
<td style="text-align:left">Greens_r</td>
</tr>
<tr>
<td style="text-align:left">Greys</td>
<td style="text-align:left">Greys_r</td>
</tr>
<tr>
<td style="text-align:left">OrRd</td>
<td style="text-align:left">OrRd_r</td>
</tr>
<tr>
<td style="text-align:left">Oranges</td>
<td style="text-align:left">Oranges_r</td>
</tr>
<tr>
<td style="text-align:left">PRGn</td>
<td style="text-align:left">PRGn_r</td>
</tr>
<tr>
<td style="text-align:left">Paired</td>
<td style="text-align:left">Paired_r</td>
</tr>
<tr>
<td style="text-align:left">Pastel1</td>
<td style="text-align:left">Pastel1_r</td>
</tr>
<tr>
<td style="text-align:left">Pastel2</td>
<td style="text-align:left">Pastel2_r</td>
</tr>
<tr>
<td style="text-align:left">PiYG</td>
<td style="text-align:left">PiYG_r</td>
</tr>
<tr>
<td style="text-align:left">PuBu</td>
<td style="text-align:left">PuBu_r</td>
</tr>
<tr>
<td style="text-align:left">PuBuGn</td>
<td style="text-align:left">PuBuGn_r</td>
</tr>
<tr>
<td style="text-align:left">PuOr</td>
<td style="text-align:left">PuOr_r</td>
</tr>
<tr>
<td style="text-align:left">PuRd</td>
<td style="text-align:left">PuRd_r</td>
</tr>
<tr>
<td style="text-align:left">Purples</td>
<td style="text-align:left">Purples_r</td>
</tr>
<tr>
<td style="text-align:left">RdBu</td>
<td style="text-align:left">RdBu_r</td>
</tr>
<tr>
<td style="text-align:left">RdGy</td>
<td style="text-align:left">RdGy_r</td>
</tr>
<tr>
<td style="text-align:left">RdPu</td>
<td style="text-align:left">RdPu_r</td>
</tr>
<tr>
<td style="text-align:left">RdYlBu</td>
<td style="text-align:left">RdYlBu_r</td>
</tr>
<tr>
<td style="text-align:left">RdYlGn</td>
<td style="text-align:left">RdYlGn_r</td>
</tr>
<tr>
<td style="text-align:left">Reds</td>
<td style="text-align:left">Reds_r</td>
</tr>
<tr>
<td style="text-align:left">Set1</td>
<td style="text-align:left">Set1_r</td>
</tr>
<tr>
<td style="text-align:left">Set2</td>
<td style="text-align:left">Set2_r</td>
</tr>
<tr>
<td style="text-align:left">Set3</td>
<td style="text-align:left">Set3_r</td>
</tr>
<tr>
<td style="text-align:left">Spectral</td>
<td style="text-align:left">Spectral_r</td>
</tr>
<tr>
<td style="text-align:left">Wistia</td>
<td style="text-align:left">Wistia_r</td>
</tr>
<tr>
<td style="text-align:left">YlGn</td>
<td style="text-align:left">YlGn_r</td>
</tr>
<tr>
<td style="text-align:left">YlGnBu</td>
<td style="text-align:left">YlGnBu_r</td>
</tr>
<tr>
<td style="text-align:left">YlOrBr</td>
<td style="text-align:left">YlOrBr_r</td>
</tr>
<tr>
<td style="text-align:left">YlOrRd</td>
<td style="text-align:left">YlOrRd_r</td>
</tr>
<tr>
<td style="text-align:left">afmhot</td>
<td style="text-align:left">afmhot_r</td>
</tr>
<tr>
<td style="text-align:left">autumn</td>
<td style="text-align:left">autumn_r</td>
</tr>
<tr>
<td style="text-align:left">binary</td>
<td style="text-align:left">binary_r</td>
</tr>
<tr>
<td style="text-align:left">bone</td>
<td style="text-align:left">bone_r</td>
</tr>
<tr>
<td style="text-align:left">brg</td>
<td style="text-align:left">brg_r</td>
</tr>
<tr>
<td style="text-align:left">bwr</td>
<td style="text-align:left">bwr_r</td>
</tr>
<tr>
<td style="text-align:left">cividis</td>
<td style="text-align:left">cividis_r</td>
</tr>
<tr>
<td style="text-align:left">cool</td>
<td style="text-align:left">cool_r</td>
</tr>
<tr>
<td style="text-align:left">coolwarm</td>
<td style="text-align:left">coolwarm_r</td>
</tr>
<tr>
<td style="text-align:left">copper</td>
<td style="text-align:left">copper_r</td>
</tr>
<tr>
<td style="text-align:left">cubehelix</td>
<td style="text-align:left">cubehelix_r</td>
</tr>
<tr>
<td style="text-align:left">flag</td>
<td style="text-align:left">flag_r</td>
</tr>
<tr>
<td style="text-align:left">gist_earth</td>
<td style="text-align:left">gist_earth_r</td>
</tr>
<tr>
<td style="text-align:left">gist_gray</td>
<td style="text-align:left">gist_gray_r</td>
</tr>
<tr>
<td style="text-align:left">gist_heat</td>
<td style="text-align:left">gist_heat_r</td>
</tr>
<tr>
<td style="text-align:left">gist_ncar</td>
<td style="text-align:left">gist_ncar_r</td>
</tr>
<tr>
<td style="text-align:left">gist_rainbow</td>
<td style="text-align:left">gist_rainbow_r</td>
</tr>
<tr>
<td style="text-align:left">gist_stern</td>
<td style="text-align:left">gist_stern_r</td>
</tr>
<tr>
<td style="text-align:left">gist_yarg</td>
<td style="text-align:left">gist_yarg_r</td>
</tr>
<tr>
<td style="text-align:left">gnuplot</td>
<td style="text-align:left">gnuplot_r</td>
</tr>
<tr>
<td style="text-align:left">gnuplot2</td>
<td style="text-align:left">gnuplot2_r</td>
</tr>
<tr>
<td style="text-align:left">gray</td>
<td style="text-align:left">gray_r</td>
</tr>
<tr>
<td style="text-align:left">hot</td>
<td style="text-align:left">hot_r</td>
</tr>
<tr>
<td style="text-align:left">hsv</td>
<td style="text-align:left">hsv_r</td>
</tr>
<tr>
<td style="text-align:left">inferno</td>
<td style="text-align:left">inferno_r</td>
</tr>
<tr>
<td style="text-align:left">jet</td>
<td style="text-align:left">jet_r</td>
</tr>
<tr>
<td style="text-align:left">magma</td>
<td style="text-align:left">magma_r</td>
</tr>
<tr>
<td style="text-align:left">nipy_spectral</td>
<td style="text-align:left">nipy_spectral_r</td>
</tr>
<tr>
<td style="text-align:left">ocean</td>
<td style="text-align:left">ocean_r</td>
</tr>
<tr>
<td style="text-align:left">pink</td>
<td style="text-align:left">pink_r</td>
</tr>
<tr>
<td style="text-align:left">plasma</td>
<td style="text-align:left">plasma_r</td>
</tr>
<tr>
<td style="text-align:left">prism</td>
<td style="text-align:left">prism_r</td>
</tr>
<tr>
<td style="text-align:left">rainbow</td>
<td style="text-align:left">rainbow_r</td>
</tr>
<tr>
<td style="text-align:left">seismic</td>
<td style="text-align:left">seismic_r</td>
</tr>
<tr>
<td style="text-align:left">spring</td>
<td style="text-align:left">spring_r</td>
</tr>
<tr>
<td style="text-align:left">summer</td>
<td style="text-align:left">summer_r</td>
</tr>
<tr>
<td style="text-align:left">tab10</td>
<td style="text-align:left">tab10_r</td>
</tr>
<tr>
<td style="text-align:left">tab20</td>
<td style="text-align:left">tab20_r</td>
</tr>
<tr>
<td style="text-align:left">tab20b</td>
<td style="text-align:left">tab20b_r</td>
</tr>
<tr>
<td style="text-align:left">tab20c</td>
<td style="text-align:left">tab20c_r</td>
</tr>
<tr>
<td style="text-align:left">terrain</td>
<td style="text-align:left">terrain_r</td>
</tr>
<tr>
<td style="text-align:left">twilight</td>
<td style="text-align:left">twilight_r</td>
</tr>
<tr>
<td style="text-align:left">twilight_shifted</td>
<td style="text-align:left">twilight_shifted_r</td>
</tr>
<tr>
<td style="text-align:left">viridis</td>
<td style="text-align:left">viridis_r</td>
</tr>
<tr>
<td style="text-align:left">winter</td>
<td style="text-align:left">winter_r</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404161117677.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404161117677.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404161139138.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404161139138.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404161157754.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404161157754.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h2 id="Matplotlib柱形图"><a href="#Matplotlib柱形图" class="headerlink" title="Matplotlib柱形图"></a>Matplotlib柱形图</h2><p>我们可以使用 pyplot 中的 bar() 方法来绘制柱形图。</p>
<p>bar() 方法语法格式如下：</p>
<pre><code class="lang-python">matplotlib.pyplot.bar(x, height, width=0.8, bottom=None, *, align=&#39;center&#39;, data=None, **kwargs)
</code></pre>
<p><strong>参数说明：</strong></p>
<p><strong>x</strong>：浮点型数组，柱形图的 x 轴数据。</p>
<p><strong>height</strong>：浮点型数组，柱形图的高度。</p>
<p><strong>width</strong>：浮点型数组，柱形图的宽度。</p>
<p><strong>bottom</strong>：浮点型数组，底座的 y 坐标，默认 0。</p>
<p><strong>align</strong>：柱形图与 x 坐标的对齐方式，’center’ 以 x 位置为中心，这是默认值。 ‘edge’：将柱形图的左边缘与 x 位置对齐。要对齐右边缘的条形，可以传递负数的宽度值及 align=’edge’。</p>
<p><strong><em>\</em>kwargs：</strong>：其他参数。</p>
<p>实例1：简单实用 bar() 来创建一个柱形图</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.array([&quot;Runoob-1&quot;, &quot;Runoob-2&quot;, &quot;Runoob-3&quot;, &quot;C-RUNOOB&quot;])
y = np.array([12, 22, 6, 18])

plt.bar(x,y)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162147295.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162147295.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例2：垂直方向的柱形图可以使用 <strong>barh()</strong> 方法来设置</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.array([&quot;Runoob-1&quot;, &quot;Runoob-2&quot;, &quot;Runoob-3&quot;, &quot;C-RUNOOB&quot;])
y = np.array([12, 22, 6, 18])

plt.barh(x,y)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162341451.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162341451.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例3：设置柱形图颜色</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.array([&quot;Runoob-1&quot;, &quot;Runoob-2&quot;, &quot;Runoob-3&quot;, &quot;C-RUNOOB&quot;])
y = np.array([12, 22, 6, 18])

plt.bar(x, y, color = &quot;#4CAF50&quot;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162449487.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162449487.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例4：自定义各个柱形的颜色</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.array([&quot;Runoob-1&quot;, &quot;Runoob-2&quot;, &quot;Runoob-3&quot;, &quot;C-RUNOOB&quot;])
y = np.array([12, 22, 6, 18])

plt.bar(x, y,  color = [&quot;#4CAF50&quot;,&quot;red&quot;,&quot;hotpink&quot;,&quot;#556B2F&quot;])
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162554129.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162554129.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例5：设置柱形图宽度，<strong>bar()</strong> 方法使用 <strong>width</strong> 设置</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.array([&quot;Runoob-1&quot;, &quot;Runoob-2&quot;, &quot;Runoob-3&quot;, &quot;C-RUNOOB&quot;])
y = np.array([12, 22, 6, 18])

plt.bar(x, y, width = 0.1)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162717588.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162717588.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例6：<strong>barh()</strong> 方法使用 <strong>height</strong> 设置 height</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

x = np.array([&quot;Runoob-1&quot;, &quot;Runoob-2&quot;, &quot;Runoob-3&quot;, &quot;C-RUNOOB&quot;])
y = np.array([12, 22, 6, 18])

plt.barh(x, y, height = 0.1)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162753714.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404162753714.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h2 id="Matplotlib饼图"><a href="#Matplotlib饼图" class="headerlink" title="Matplotlib饼图"></a>Matplotlib饼图</h2><p>我们可以使用 pyplot 中的 pie() 方法来绘制饼图。</p>
<p>pie() 方法语法格式如下：</p>
<pre><code class="lang-python">matplotlib.pyplot.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=0, radius=1, counterclock=True, wedgeprops=None, textprops=None, center=0, 0, frame=False, rotatelabels=False, *, normalize=None, data=None)[source]
</code></pre>
<p><strong>参数说明：</strong></p>
<p><strong>x</strong>：浮点型数组，表示每个扇形的面积。</p>
<p><strong>explode</strong>：数组，表示各个扇形之间的间隔，默认值为0。</p>
<p><strong>labels</strong>：列表，各个扇形的标签，默认值为 None。</p>
<p><strong>colors</strong>：数组，表示各个扇形的颜色，默认值为 None。</p>
<p><strong>autopct</strong>：设置饼图内各个扇形百分比显示格式，<strong>%d%%</strong> 整数百分比，<strong>%0.1f</strong> 一位小数， <strong>%0.1f%%</strong> 一位小数百分比， <strong>%0.2f%%</strong> 两位小数百分比。</p>
<p><strong>labeldistance</strong>：标签标记的绘制位置，相对于半径的比例，默认值为 1.1，如 <strong>&lt;1</strong>则绘制在饼图内侧。</p>
<p><strong>pctdistance：</strong>：类似于 labeldistance，指定 autopct 的位置刻度，默认值为 0.6。</p>
<p><strong>shadow：</strong>：布尔值 True 或 False，设置饼图的阴影，默认为 False，不设置阴影。</p>
<p><strong>radius：</strong>：设置饼图的半径，默认为 1。</p>
<p><strong>startangle：</strong>：起始绘制饼图的角度，默认为从 x 轴正方向逆时针画起，如设定 =90 则从 y 轴正方向画起。</p>
<p><strong>counterclock</strong>：布尔值，设置指针方向，默认为 True，即逆时针，False 为顺时针。</p>
<p><strong>wedgeprops</strong> ：字典类型，默认值 None。参数字典传递给 wedge 对象用来画一个饼图。例如：wedgeprops={‘linewidth’:5} 设置 wedge 线宽为5。</p>
<p><strong>textprops</strong> ：字典类型，默认值为：None。传递给 text 对象的字典参数，用于设置标签（labels）和比例文字的格式。</p>
<p><strong>center</strong> ：浮点类型的列表，默认值：(0,0)。用于设置图标中心位置。</p>
<p><strong>frame</strong> ：布尔类型，默认值：False。如果是 True，绘制带有表的轴框架。</p>
<p><strong>rotatelabels</strong> ：布尔类型，默认为 False。如果为 True，旋转每个 label 到指定的角度。</p>
<p>实例1：简单实用 pie() 来创建一个柱形图</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

y = np.array([35, 25, 25, 15])

plt.pie(y)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404165657025.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404165657025.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例2：设置饼图各个扇形的标签与颜色</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

y = np.array([35, 25, 25, 15])

plt.pie(y,
        labels=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;], # 设置饼图标签
        colors=[&quot;#d5695d&quot;, &quot;#5d8ca8&quot;, &quot;#65a479&quot;, &quot;#a564c9&quot;], # 设置饼图颜色
       )
plt.title(&quot;RUNOOB Pie Test&quot;) # 设置标题
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404165759838.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404165759838.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实例3：突出显示第二个扇形，并格式化输出百分比</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

y = np.array([35, 25, 25, 15])

plt.pie(y,
        labels=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;], # 设置饼图标签
        colors=[&quot;#d5695d&quot;, &quot;#5d8ca8&quot;, &quot;#65a479&quot;, &quot;#a564c9&quot;], # 设置饼图颜色
        explode=(0, 0.2, 0, 0), # 第二部分突出显示，值越大，距离中心越远
        autopct=&#39;%.2f%%&#39;, # 格式化输出百分比
       )
plt.title(&quot;RUNOOB Pie Test&quot;)
plt.show()
</code></pre>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404170000561.png" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20220404170000561.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p><strong>注意：</strong>默认情况下，第一个扇形的绘制是从 x 轴开始并逆时针移动：</p>
<p><img src="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/03D59143-B345-4B36-A7CD-53F698AB5284.jpg" class="lazyload" data-srcset="/CN/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/03D59143-B345-4B36-A7CD-53F698AB5284.jpg" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" style="zoom:50%;"></p>
]]></content>
  </entry>
  <entry>
    <title>MySQL学习笔记</title>
    <url>/CN/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="MySQL入门教程"><a href="#MySQL入门教程" class="headerlink" title="MySQL入门教程"></a>MySQL入门教程</h2><h3 id="什么是数据库"><a href="#什么是数据库" class="headerlink" title="什么是数据库"></a>什么是数据库</h3><p>数据库是按照数据结构来组织、存储和管理数据的仓库</p>
<p>关系型数据库：建立在关系模型基础上的数据库，借助于集合代数等数学概念和方法来处理数据库中的数据</p>
<p>RDBMS即关系数据库管理系统的特点：</p>
<p>1.数据以表格的形式出现</p>
<p>2.每行为各种记录名称</p>
<p>3.每列为记录名称所对应的数据域</p>
<p>4.许多行和列组成一张表单</p>
<p>5.若干的表单组成数据库</p>
<h3 id="RDBMS相关概念"><a href="#RDBMS相关概念" class="headerlink" title="RDBMS相关概念"></a>RDBMS相关概念</h3><p>·</p>
<h3 id="MySQL创建数据表"><a href="#MySQL创建数据表" class="headerlink" title="MySQL创建数据表"></a>MySQL创建数据表</h3><h4 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h4><p><code>CREATE TABLE table_name (column_name column_type)</code></p>
<p>举例：在W3CSCHOOL数据库中创建数据表w3cschool_tbl:</p>
<pre><code class="lang-mysql">CREATE TABLE IF NOT EXISTS tutorials_tbl(
    tutorial_id INT NOT NULL AUTO_INCREMENT,
    tutorial_title VARCHAR(100) NOT NULL,
    tutorial_author VARCHAR(40) NOT NULL,
    submission_date DATE,
    PRIMARY KEY (tutorial_id)
    );
</code></pre>
<p>注：</p>
<p>·如果你不想字段为NULL可以设置字段的属性为NOT NULL,在操作数据库如果输入该字段的数据为NULL,则会报错。</p>
<p>·AUTO_INCREMENT定义列为自增的属性，一般用于主键，数值会自动加1。</p>
<p>·PRIMARY KEY关键字用于定义列为主键。你可以使用多列来定义主键，列间以逗号分隔。</p>
<h3 id="MySQL删除数据表"><a href="#MySQL删除数据表" class="headerlink" title="MySQL删除数据表"></a>MySQL删除数据表</h3><h4 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h4><p><code>DROP TABLE table_name</code></p>
<h3 id="MySQL插入数据"><a href="#MySQL插入数据" class="headerlink" title="MySQL插入数据"></a>MySQL插入数据</h3><h4 id="语法-2"><a href="#语法-2" class="headerlink" title="语法"></a>语法</h4><pre><code class="lang-mysql">INSERT INTO table_name(field1, field2, ...fieldN)
                      VALUES
                      (value1, value2, ...valueN);
</code></pre>
<p>举例：使用SQL INSERT INTO语句向MySQL数据表w3cschool_tbl插入数据：</p>
<pre><code class="lang-mysql">INSERT INTO w3cschool_tbl
(w3cschool_title, w3cschool_author, submission_date)
VALUES
(&quot;Learn PHP&quot;, &quot;John Poul&quot;, NOW());
</code></pre>
<h3 id="MySQL查询数据"><a href="#MySQL查询数据" class="headerlink" title="MySQL查询数据"></a>MySQL查询数据</h3><h4 id="语法-3"><a href="#语法-3" class="headerlink" title="语法"></a>语法</h4><pre><code class="lang-mysql">SELECT column_name, column_name
FROM table_name
[WHERE Clause]
[OFFSET M][LIMIT N]
</code></pre>
<p>注：</p>
<p>·查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分隔，并使用WHERE语句来设定查询条件。</p>
<p>·SELECT命令可以读取一条或者多条记录。</p>
<p>·你可以使用星号(*)来代替其他字段，SELECT语句会返回表的所有字段数据。</p>
<p>·你可以使用WHERE语句来包含任何条件。</p>
<p>·你可以通过OFFSET指定SELECT语句开始查询的数据偏移量，默认偏移量为0。</p>
<p>·你可以使用LIMIT属性来设定返回的记录数。</p>
<p>举例：通过SQL SELECT命令来获取MySQL数据表w3cschool_tbl的数据：</p>
<p><code>SELECT * from w3cschool_tbl;</code></p>
<h3 id="MySQL-where子句"><a href="#MySQL-where子句" class="headerlink" title="MySQL where子句"></a>MySQL where子句</h3><h4 id="语法-4"><a href="#语法-4" class="headerlink" title="语法"></a>语法</h4><pre><code class="lang-mysql">SELECT field1, field2, ...fieldN
FROM table_name1, table_name2...
[WHERE condition1 [[AND][OR]] condition2...]
</code></pre>
<p>注：</p>
<p>·查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分隔，并使用WHERE语句来设定查询条件。</p>
<p>·你可以在WHERE子句中指定任何条件。</p>
<p>·你可以使用AND或者OR指定一个或多个条件。</p>
<p>·WHERE子句也可以运用于SQL的DELETE或UPDATE命令。</p>
<p>·WHERE子句类似于程序中的if条件，根据MySQL表中的字段值来读取指定的数据。</p>
<p>操作符列表，实例假定A=10,B=20:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">操作符</th>
<th>描述</th>
<th>实例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">=</td>
<td>等号，检测两个值是否相等，如果相等返回True</td>
<td>(A=B)返回false</td>
</tr>
<tr>
<td style="text-align:center">&lt;&gt;或！=</td>
<td>不等于，检测两个值是否相等，如果不相等返回True</td>
<td>（A!=B)返归true</td>
</tr>
<tr>
<td style="text-align:center">&gt;</td>
<td>大于号，检测左边的值是否大于右边的值，如果左边的值大于右边的值返回True</td>
<td>(A&gt;B)返回false</td>
</tr>
<tr>
<td style="text-align:center">&lt;</td>
<td>小于号，检测左边的值是否小于右边的值，如果左边的值小于右边的值返回True</td>
<td>(A&lt;B)返回true</td>
</tr>
<tr>
<td style="text-align:center">&gt;=</td>
<td>大于等于号，检测左边的值是否大于等于右边的值，如果左边的值大于或等于右边的值返回True</td>
<td>(A&gt;=B)返回false</td>
</tr>
<tr>
<td style="text-align:center">&lt;=</td>
<td>小于等于号，检测左边的值是否小于或等于右边的值，如果左边的值小于或等于右边的值返回True</td>
<td>(A&lt;=B)返回true</td>
</tr>
</tbody>
</table>
</div>
<p>举例：读取w3cschool_tbl表中w3cschool_author字段值为Sanjay的所有记录：</p>
<p><code>SELECT * from w3cschool_tbl WHERE w3cschool_author=&#39;Sanjay&#39;;</code></p>
<p>除非你使用LIKE来比较字符串，否则MySQL的WHERE子句的字符串比较是不区分大小写的。你可以使用BINARY关键字来设定WHERE子句的紫福春是区分大小写的。</p>
<p><code>SELECT * from w3cschool_tbl WHERE BINARY w3cschool_author=&#39;sanjay&#39;;</code></p>
<h3 id="MySQL-UPDATE查询"><a href="#MySQL-UPDATE查询" class="headerlink" title="MySQL UPDATE查询"></a>MySQL UPDATE查询</h3><h4 id="语法-5"><a href="#语法-5" class="headerlink" title="语法"></a>语法</h4><pre><code class="lang-mysql">UPDATE table_name SET field1=new-value1, field2=new-value2

[WHERE Clause]
</code></pre>
<p>注：</p>
<p>·你可以同时更新一个或多个字段</p>
<p>·你可以在WHERE字句中指定任何条件</p>
<p>·你可以在一个单独表中同时更新数据</p>
<p>当你需要更新表中指定行的数据时，WHERE子句是非常有用的。</p>
<p>举例：更新数据表中w3cschool_id为3的w3cschool_title字段值：</p>
<pre><code class="lang-mysql">UPDATE w3cschool_tbl
SET w3cschool_title=&#39;Learning JAVA&#39;
WHERE w3cschool_id=3;
</code></pre>
<h3 id="MySQL-DELETE语句"><a href="#MySQL-DELETE语句" class="headerlink" title="MySQL DELETE语句"></a>MySQL DELETE语句</h3><h4 id="语法-6"><a href="#语法-6" class="headerlink" title="语法"></a>语法</h4><p><code>DELETE FROM table_name [WHERE Clause]</code></p>
<p>注：</p>
<p>·如果没有指定WHERE子句，MySQL表中的所有记录将被删除</p>
<p>·你可以在WHERE字句中指定任何条件</p>
<p>·你可以在单个表中一次性删除记录</p>
<p>当你想删除数据表中的指定记录时，WHERE子句是非常有用的</p>
<p>举例：删除w3cschool_tbl表中w3cschool_id为3的记录：</p>
<pre><code class="lang-mysql">DELETE FROM w3cschool_tbl WHERE w3cschool_id=3;
</code></pre>
<h3 id="MySQL-LIKE子句"><a href="#MySQL-LIKE子句" class="headerlink" title="MySQL LIKE子句"></a>MySQL LIKE子句</h3><p>SQL LIKE子句中使用百分号（%）字符来表示任意字符，类似于UNIX或者正则表达式中的星号（*）。</p>
<h4 id="语法-7"><a href="#语法-7" class="headerlink" title="语法"></a>语法</h4><pre><code class="lang-mysql">SELECT field1, field2, ...fieldN
FROM table_name1, table_name2...
WHERE field1 LIKE condition [[AND][OR]] field2=&#39;somevalue&#39;
</code></pre>
<p>注：</p>
<p>·你可以在WHERE子句中指定任何条件</p>
<p>·你可以在WHERE字句中使用LIKE子句</p>
<p>·你可以使用LIKE子句代替等号</p>
<p>·LIKE通常与%一同使用，类似于一个元字符的搜索</p>
<p>·你可以使用AND或OR指定一个或多个条件</p>
<p>·你可以在DELETE或UPDATE命令中使用WHERE…LIKE子句来指定条件</p>
<p>举例：查询w3cschool_tbl表中的w3cschool_author字段中以’jay’为结尾的所有记录：</p>
<pre><code class="lang-mysql">SELECT * from w3cschool_tbl
WHERE w3cschool_author LIKE &#39;%jay&#39;;
</code></pre>
<h3 id="MySQL排序"><a href="#MySQL排序" class="headerlink" title="MySQL排序"></a>MySQL排序</h3><h4 id="语法-8"><a href="#语法-8" class="headerlink" title="语法"></a>语法</h4><pre><code class="lang-mysql">SELECT field1, field2,...fieldN FROM table_name1, table_name2...
ORDER BY field1, [field2...] [ASC[DESC]]
</code></pre>
<p>注：</p>
<p>·你可以使用任何字段来作为排序的条件，从而返回排序后的查询结果</p>
<p>·你可以设定多个字段来排序</p>
<p>·你可以使用ASC或DESC关键字来设置查询结果是按升序或者降序排列。默认情况下，它是按升序排列</p>
<p>·你可以添加WHERE…LIKE子句来设置条件</p>
<p>举例：使用ORDER BY子句来读取MySQL数据表w3cschool_tbl中的数据：</p>
<p><code>SELECT * from w3cschool_tbl ORDER BY w3cschool_author ASC;</code></p>
<p><code>SELECT * from w3cschool_tbl ORDER BY w3cschool_author DESC;</code></p>
<h3 id="MySQL-分组"><a href="#MySQL-分组" class="headerlink" title="MySQL 分组"></a>MySQL 分组</h3><p>GROUP BY语句根据一个或者多个列对结果集进行分组，在分组的列上我们可以使用COUNT,SUM,AVG等函数。</p>
<h4 id="语法-9"><a href="#语法-9" class="headerlink" title="语法"></a>语法</h4><pre><code class="lang-mysql">SELECT column_name, function(column_name)
FROM table_name
WHERE column_name operator value
GROUP BY column_name;
</code></pre>
<p>employee_tbl表格信息如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">id</th>
<th>name</th>
<th>date</th>
<th>singin</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td>小明</td>
<td>2016-04-22 15:25:33</td>
<td>1</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td>小王</td>
<td>2016-04-20 15:25:47</td>
<td>3</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td>小丽</td>
<td>2016-04-19 15:26:02</td>
<td>2</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td>小王</td>
<td>2016-04-07 15:26:14</td>
<td>4</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td>小明</td>
<td>2016-04-11 15:26:40</td>
<td>4</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td>小明</td>
<td>2016-04-04 15:26:54</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p>举例：将数据表按名字进行分组，并统计每个人有多少条记录：</p>
<p><code>SELECT name, COUNT(*) FROM employ_tbl GROUP BY name;</code></p>
<h3 id="使用WITH-ROLLUP"><a href="#使用WITH-ROLLUP" class="headerlink" title="使用WITH ROLLUP"></a>使用WITH ROLLUP</h3><p>WITH ROLLUP可以实现在分组统计数据基础上再进行相同的统计（SUM.AVG,COUNT).</p>
<p>举例：将以上的数据表按名字进行分组，再统计每个人登录的次数：</p>
<pre><code class="lang-mysql">SELECT name, SUM(singin) as singin_count
FROM employee_tbl
GROUP BY name WITH ROLLUP;
</code></pre>
<p>其中结果如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>name</th>
<th>singin_out</th>
</tr>
</thead>
<tbody>
<tr>
<td>小丽</td>
<td>2</td>
</tr>
<tr>
<td>小明</td>
<td>7</td>
</tr>
<tr>
<td>小王</td>
<td>7</td>
</tr>
<tr>
<td>NULL</td>
<td>16</td>
</tr>
</tbody>
</table>
</div>
<p>其中记录NULL表示所有人的登录次数，我们可以使用coalesce来设置一个可以取代NULL的名称。</p>
<h4 id="语法-10"><a href="#语法-10" class="headerlink" title="语法"></a>语法</h4><p><code>select coalesce(a, b, c);</code></p>
<p>参数说明：如果a==null,则选择b;如果b==null,则选择c;如果a!=null,则选择a;如果abc都为null,则返回null(没意义)。</p>
<p>举例：如果名字为空，使用总数代替：</p>
<pre><code class="lang-mysql">SELECT coalesce(name, &#39;总数&#39;), SUM(singin) as singin_out
FROM employee_tbl
GROUP BY name WITH ROLLUP;
</code></pre>
<h3 id="MySQL连接的使用"><a href="#MySQL连接的使用" class="headerlink" title="MySQL连接的使用"></a>MySQL连接的使用</h3><p>JOIN按照功能大致分为如下三类：</p>
<p>·INNER JOIN(内连接，或等值连接)：获取两个表中字段匹配关系的记录</p>
<p>·LEFT JOIN（左连接）：获取左表所有记录，即使右表没有对应匹配的记录</p>
<p>·RIGHT JOIN(右连接)：用于获取右表所有记录，即使左表没有对应匹配的记录</p>
<p><a href="https://raw.githubusercontent.com/pistachio0812/pistachio0812.github.io/main/images/SQL_JOINS.png">SQL JOINS</a></p>
<p>假设W3CSCHOOL数据库中有两张表tcount_tbl和w3cschool_tbl,两张数据表数据如下：</p>
<p>1.tcount_tbl</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">w3cschool_author</th>
<th>w3cschool_count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">mahran</td>
<td>20</td>
</tr>
<tr>
<td style="text-align:center">mahnaz</td>
<td>NULL</td>
</tr>
<tr>
<td style="text-align:center">Jen</td>
<td>NULL</td>
</tr>
<tr>
<td style="text-align:center">Gill</td>
<td>20</td>
</tr>
<tr>
<td style="text-align:center">John Poul</td>
<td>1</td>
</tr>
<tr>
<td style="text-align:center">Sanjay</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>2.w3cschool_tbl</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>w3cschool_id</th>
<th>w3cschool_title</th>
<th>w3cschool_author</th>
<th>submission_date</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Learn PHP</td>
<td>John Poul</td>
<td>2007-05-24</td>
</tr>
<tr>
<td>2</td>
<td>Learn MyQL</td>
<td>Abdul S</td>
<td>2007-05-24</td>
</tr>
<tr>
<td>3</td>
<td>JAVA Tutorial</td>
<td>Sanjay</td>
<td>2007-05-06</td>
</tr>
</tbody>
</table>
</div>
<p>举例：使用INNER JOIN来连接以上两张表来读取w3cschool_tbl表中所有w3cschool_author字段在tcount_tbl表中对应的w3cschool_count字段值。</p>
<pre><code class="lang-mysql">SELECT a.w3cschool_id, a.w3cschool_author, b.w3cschool_count
FROM w3cschool_tbl a INNER JOIN tcount_tbl b
ON a.w3cschool_author = b.w3cschool_author
</code></pre>
<p>等价于：</p>
<pre><code class="lang-mysql">SELECT a.w3cschool_id, a_w3cschool_author, b.w3cschool_count
FROM w3cschool_tbl a, tcount_tbl b
WHERE a.w3cschool_author = b.w3cschool_author;
</code></pre>
<p>举例：以w3cschool_tbl为左表， t_count_tbl为右表，理解MySQL LEFT JOIN的应用：</p>
<pre><code class="lang-mysql">SELECT a.w3cschool_id, a.w3cschool_author, b.w3cschool_count
FROM w3cschool_tbl a LEFT JOIN tcount_tbl b
ON a.w3cschool_author = b.w3cschool_author;
</code></pre>
<p>举例：以tcount_tbl为左表， w3cschool_tbl为右表，理解MySQL RIGHT JOIN的应用：</p>
<pre><code class="lang-mysql">SELECT b.w3cschool_id, b.w3cschool_author, a.w3cschool_count
FROM tcount_tbl a RIGHT JOIN w3cschool_tbl b
ON a.w3cschool_author = b.w3cschool_author;
</code></pre>
<h3 id="MySQL-NULL值处理"><a href="#MySQL-NULL值处理" class="headerlink" title="MySQL NULL值处理"></a>MySQL NULL值处理</h3><p>查询条件字段为NULL时，该命令可能无法正常工作，为了处理这种情况，MySQL提供了三大运算符：</p>
<p>·IS NULL:当列的值为NULL,此运算符返回True</p>
<p>·IS NOT NULL:当列的值不为NULL,运算符返回True</p>
<p>·&lt;=&gt;:比较运算符，当比较的两个值为NULL时返回True</p>
<p>关于NULL的条件比较运算是比较特殊的，你不能够使用=NULL或！=NULL在列中查找NULL值，在MySQL中，NULL值与任何其他值的比较永远返回false,即NULL=NULL返回false。</p>
<h4 id="在命令提示符中使用NULL值"><a href="#在命令提示符中使用NULL值" class="headerlink" title="在命令提示符中使用NULL值"></a>在命令提示符中使用NULL值</h4><p>假设数据库W3CSCHOOL中的表tcount_tbl含有两列w3cschool_author和w3cschool_count, w3cschool_count中设置插入NULL值。</p>
<p>假设表如下所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>w3cschool_author</th>
<th>w3cschool_count</th>
</tr>
</thead>
<tbody>
<tr>
<td>mahran</td>
<td>20</td>
</tr>
<tr>
<td>mahnaz</td>
<td>NULL</td>
</tr>
<tr>
<td>Jen</td>
<td>NULL</td>
</tr>
<tr>
<td>Gill</td>
<td>20</td>
</tr>
</tbody>
</table>
</div>
<p>查询数据表中w3cschool_count列是否为NULL,必须使用IS NULL和IS NOT NULL,如下实例：</p>
<pre><code class="lang-mysql">SELECT * FROM tcount_tbl
WHERE w3cschool_count IS NULL;
</code></pre>
<pre><code class="lang-mysql">SELECT * FROM tcount_tbl
WHERE w3cschool_count IS NOT NULL;
</code></pre>
<h3 id="MySQL正则表达式"><a href="#MySQL正则表达式" class="headerlink" title="MySQL正则表达式"></a>MySQL正则表达式</h3><p>下表中的正则模式可应用于REGEXP操作符中。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>^</th>
<th>匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配’\n’或’\r’之后的位置。</th>
</tr>
</thead>
<tbody>
<tr>
<td>$</td>
<td>匹配输入字符串的结束位置。如果设置了RegExp对象的Multiline属性，^也匹配’\n’或’\r’之前的位置。</td>
</tr>
<tr>
<td>.</td>
<td>匹配除’\n’之外的任何单个字符。要匹配包括’\n’在内的任何字符，请使用’[.\n]’的模式。</td>
</tr>
<tr>
<td>[…]</td>
<td>字符集合。匹配所包含的任何一个字符。例如’[abc]’可以匹配’plain’中的’a’。</td>
</tr>
<tr>
<td><sup><a href="#fn_..." id="reffn_...">...</a></sup></td>
<td>负值字符集合。匹配未包含的任意字符。例如，’<sup><a href="#fn_abc" id="reffn_abc">abc</a></sup>‘可以匹配’plain’中的’p’。</td>
</tr>
<tr>
<td>p1\</td>
<td>p2\</td>
<td>p3</td>
<td>匹配p1或p2或p3.例如，’z\</td>
<td>food’能匹配’z’或’food’。，’(z\</td>
<td>f)food’能匹配’zood’或’food’</td>
</tr>
<tr>
<td>*</td>
<td>匹配前面的子表达式零次或多次。例如，zo能匹配’z’以及’zoo’.*等价于{0，}。</td>
</tr>
<tr>
<td>+</td>
<td>匹配前面的子表达式一次或多次。例如，’zo+’能匹配’zo’以及’zoo’,但不能匹配’z’。+等价于{1，}</td>
</tr>
<tr>
<td>{n}</td>
<td>n是一个非负整数。匹配确定的n次。例如，’o{2}’不能匹配’Bob’中的’o’,但是能匹配’food’中的两个o。</td>
</tr>
<tr>
<td>{n, m}</td>
<td>m和n均为非负整数，其中n&lt;=m。最少匹配n次且最多匹配m次。</td>
</tr>
</tbody>
</table>
</div>
<p>举例：</p>
<p>1.查找name字段中以’st’为开头的所有数据：</p>
<pre><code class="lang-mysql">SELECT name FROM person_tbl WHERE name REGEXP &#39;^st&#39;;
</code></pre>
<p>2.查找name字段中以’ok’为结尾的所有数据：</p>
<pre><code class="lang-mysql">SELECT name FROM person_tbl WHERE name REGEXP &#39;ok$&#39;;
</code></pre>
<p>3.查找name字段中包含’mar’字符串的所有数据：</p>
<pre><code class="lang-mysql">SELECT name FROM person_tbl WHERE name REGEXP &#39;mar&#39;
</code></pre>
<p>4.查找name字段中以元音字符开头或以’ok’字符串结尾的所有数据：</p>
<pre><code class="lang-mysql">SELECT name FROM person_tbl WHERE name REGEXP &#39;^[aeiou]|ok$&#39;
</code></pre>
<h3 id="MySQL事务"><a href="#MySQL事务" class="headerlink" title="MySQL事务"></a>MySQL事务</h3><p>MySQL 事务主要用于处理操作量大，复杂度高的数据。比如说，在人员管理系统中，你删除一个人员，你即需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等，这样，这些数据库操作语句就构成一个事务！</p>
<ul>
<li>在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。</li>
<li>事务处理可以用来维护数据库的完整性，保证成批的SQL语句要么全部执行，要么全部不执行。</li>
<li>事务用来管理 insert , update , delete 语句。</li>
</ul>
<p>一般来说，事务是必须满足4个条件（ACID）： Atomicity（原子性或不可分割性）、Consistency（一致性）、Isolation（隔离性或独立性）、Durability（持久性）</p>
<ul>
<li>1、<strong>原子性：</strong>一组事务，要么成功；要么撤回，即事务在执行过程中出错会回滚到事务开始前的状态。</li>
<li>2、<strong>一致性</strong> ： 一个事务不论是开始前还是结束后，数据库的完整性都没有被破坏。因此写入的数据必须完全符合所有预设规则（资料精确度、串联性以及后续数据库能够自发完成预定工作）。</li>
<li>3、<strong>隔离性：</strong>数据库允许多个事务并发的同时对其数据进行读写修改等操作，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离可分为：Read uncommitted（读未提交）、Read committed（读提交）、Repeatable read（可重复读）、Serializable（串行化）。</li>
<li>4、持久<strong>性：</strong>事务在处理结束后对数据做出的修改是永久的，无法丢失</li>
</ul>
<h4 id="事务控制语句"><a href="#事务控制语句" class="headerlink" title="事务控制语句"></a>事务控制语句</h4><p>1.显式的开始一个事务：</p>
<p><code>start transaction</code>或者<code>begin</code></p>
<p>2.做保存点，一个事务中可以有多个保存点：</p>
<p><code>savepoint  [savepoint_name]</code></p>
<p>3.提交事务，并使数据库中进行的修改成为永久性的：</p>
<p><code>commit</code>或<code>commit work</code></p>
<p>4.回滚结束用户的事务，并撤销正在进行的所有未提交的修改：</p>
<p><code>rollback</code>或<code>rollback work</code></p>
<p>5.删除一个事务的保存点，若没有指定保存点，执行该语句操作则会抛错：</p>
<p><code>release savepoint [savepoint_name]</code></p>
<p>6.将事务滚回标记点：</p>
<p><code>rollback to 标记点</code></p>
<p>7.设置事务的隔离级别。InnoDB 存储引擎提供事务的隔离级别有READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ 和 SERIALIZABLE。</p>
<pre><code>set transaction
</code></pre><h4 id="事务处理方法"><a href="#事务处理方法" class="headerlink" title="事务处理方法"></a>事务处理方法</h4><p>1.用 begin ， rollback ， commit 来实现事务处理。</p>
<p>2.用 set 来改变 MySQL 的自动提交模式。</p>
<ul>
<li>set autocommit = 0 （禁止自动提交）。</li>
<li>set autocommit = 1 （开启自动提交）。</li>
</ul>
<h3 id="MySQL-ALTER命令"><a href="#MySQL-ALTER命令" class="headerlink" title="MySQL ALTER命令"></a>MySQL ALTER命令</h3><p>当我们需要修改数据表名或者修改数据表字段时，就需要使用到MySQL ALTER命令。</p>
<p>开始本章教程前让我们先创建一张表，表名为：testalter_tbl。</p>
<pre><code class="lang-mysql">create table testalter_tbl(
    i INT,
    c CHAR(1)
    );
</code></pre>
<p><code>SHOW COLUMNS FROM testalter_tbl;</code></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Null</th>
<th>Key</th>
<th>Default</th>
<th>Extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>i</td>
<td>int(11)</td>
<td>YES</td>
<td></td>
<td>NULL</td>
<td></td>
</tr>
<tr>
<td>c</td>
<td>char(1)</td>
<td>YES</td>
<td></td>
<td>NULL</td>
</tr>
</tbody>
</table>
</div>
<h4 id="删除、添加或修改表字段"><a href="#删除、添加或修改表字段" class="headerlink" title="删除、添加或修改表字段"></a>删除、添加或修改表字段</h4><p><em>如下命令使用了 ALTER 命令及 DROP 子句来删除以上创建表的 i 字段：</em></p>
<p><em><code>mysql&gt; ALTER TABLE testalter_tbl  DROP i;</code></em></p>
<p><em>如果数据表中只剩余一个字段则无法使用DROP来删除字段。</em></p>
<p><em>MySQL 中使用 ADD 子句来想数据表中添加列，如下实例在表 testalter_tbl 中添加 i 字段，并定义数据类型:</em></p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl ADD i INT;</code></p>
<p>执行以上命令后，i 字段会自动添加到数据表字段的末尾。</p>
<p>`mysql&gt; SHOW COLUMNS FROM testalter_tbl; </p>
<p> <code>+-------+---------+------+-----+---------+-------+</code></p>
<p> <code>| Field | Type    | Null | Key | Default | Extra |</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code></p>
<p> <code>| c     | char(1) | YES  |     | NULL    |       |</code></p>
<p> <code>| i     | int(11) | YES  |     | NULL    |       |</code> </p>
<p> <code>+-------+---------+------+-----+---------+-------+</code> </p>
<p> <code>2 rows in set (0.00 sec)</code></p>
<p>如果你需要指定新增字段的位置，可以使用MySQL提供的关键字 FIRST (设定位第一列)， AFTER 字段名（设定位于某个字段之后）。</p>
<p>尝试以下 ALTER TABLE 语句, 在执行成功后，使用 SHOW COLUMNS 查看表结构的变化：</p>
<p><code>ALTER TABLE testalter_tbl DROP i;</code></p>
<p> <code>ALTER TABLE testalter_tbl ADD i INT FIRST;</code></p>
<p> <code>ALTER TABLE testalter_tbl DROP i;</code> </p>
<p> <code>ALTER TABLE testalter_tbl ADD i INT AFTER c;</code></p>
<p>FIRST 和 AFTER 关键字只占用于 ADD 子句，所以如果你想重置数据表字段的位置就需要先使用 DROP 删除字段然后使用 ADD 来添加字段并设置位置。</p>
<h4 id="修改字段类型及名称"><a href="#修改字段类型及名称" class="headerlink" title="修改字段类型及名称"></a>修改字段类型及名称</h4><p>如果需要修改字段类型及名称, 你可以在ALTER命令中使用 MODIFY 或 CHANGE 子句 。</p>
<p>例如，把字段 c 的类型从 CHAR(1) 改为 CHAR(10)，可以执行以下命令:</p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl MODIFY c CHAR(10);</code></p>
<p> 使用 CHANGE 子句, 语法有很大的不同。 在 CHANGE 关键字之后，紧跟着的是你要修改的字段名，然后指定新字段的类型及名称。尝试如下实例：</p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl CHANGE i j BIGINT;</code></p>
<p>如果你现在想把字段 j 从 BIGINT 修改为 INT，SQL语句如下：</p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl CHANGE j j INT;</code></p>
<h4 id="ALTER-TABLE-对-Null-值和默认值的影响"><a href="#ALTER-TABLE-对-Null-值和默认值的影响" class="headerlink" title="ALTER TABLE 对 Null 值和默认值的影响"></a>ALTER TABLE 对 Null 值和默认值的影响</h4><p>当你修改字段时，你可以指定是否包含只或者是否设置默认值。以下实例，指定字段 j 为 NOT NULL 且默认值为100 。</p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl</code></p>
<p>​        <code>-&gt; MODIFY j BIGINT NOT NULL DEFAULT 100;</code></p>
<p>如果你不设置默认值，MySQL会自动设置该字段默认为 NULL。</p>
<h4 id="修改字段默认值"><a href="#修改字段默认值" class="headerlink" title="修改字段默认值"></a>修改字段默认值</h4><p>你可以使用 ALTER 来修改字段的默认值，尝试以下实例：</p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl ALTER i SET DEFAULT 1000;</code></p>
<p><code>mysql&gt; SHOW COLUMNS FROM testalter_tbl;</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code></p>
<p> <code>| Field | Type    | Null | Key | Default | Extra |</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code> </p>
<p> <code>| c     | char(1) | YES  |     | NULL    |       |</code></p>
<p> <code>| i     | int(11) | YES  |     | 1000    |       |</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code></p>
<p> <code>2 rows in set (0.00 sec)</code></p>
<p>你也可以使用 ALTER 命令及 DROP子句来删除字段的默认值，如下实例：</p>
<p><code>mysql&gt; ALTER TABLE testalter_tbl ALTER i DROP DEFAULT;</code></p>
<p> <code>mysql&gt; SHOW COLUMNS FROM testalter_tbl;</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code></p>
<p> <code>| Field | Type    | Null | Key | Default | Extra |</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code> </p>
<p> <code>| c     | char(1) | YES  |     | NULL    |       |</code></p>
<p> <code>| i     | int(11) | YES  |     | NULL    |       |</code></p>
<p> <code>+-------+---------+------+-----+---------+-------+</code></p>
<p> <code>2 rows in set (0.00 sec) Changing a Table Type:</code></p>
<p>修改数据表类型，可以使用 ALTER 命令及 TYPE 子句来完成。尝试以下实例，我们将表 testalter_tbl 的类型修改为 MYISAM ：<strong>注意：</strong>查看数据表类型可以使用 SHOW TABLE STATUS 语句。*</p>
<pre><code>mysql&gt; ALTER TABLE testalter_tbl TYPE = MYISAM;
mysql&gt;  SHOW TABLE STATUS LIKE &#39;testalter_tbl&#39;\G
 1. row **
           Name: testalter_tbl
           Type: MyISAM
     Row_format: Fixed
           Rows: 0
 Avg_row_length: 0
    Data_length: 0
Max_data_length: 25769803775
   Index_length: 1024
      Data_free: 0
 Auto_increment: NULL
    Create_time: 2007-06-03 08:04:36
    Update_time: 2007-06-03 08:04:36
     Check_time: NULL
 Create_options:
        Comment:
1 row in set (0.00 sec)
</code></pre><h4 id="修改表名"><a href="#修改表名" class="headerlink" title="修改表名"></a>修改表名</h4><p>如果需要修改数据表的名称，可以在 ALTER TABLE 语句中使用 RENAME 子句来实现。</p>
<p>尝试以下实例将数据表 testalter_tbl 重命名为 alter_tbl：</p>
<pre><code>mysql&gt; ALTER TABLE testalter_tbl RENAME TO alter_tbl;
</code></pre><h3 id="MySQL索引"><a href="#MySQL索引" class="headerlink" title="MySQL索引"></a>MySQL索引</h3><p>MySQL索引的建立对于MySQL的高效运行是很重要的，索引可以大大提高MySQL的检索速度。</p>
<p>打个比方，如果合理的设计且使用索引的MySQL是一辆兰博基尼的话，那么没有设计和使用索引的MySQL就是一个人力三轮车。</p>
<p>拿汉语字典的目录页（索引）打比方，我们可以按拼音、笔画、偏旁部首等排序的目录（索引）快速查找到需要的字。</p>
<p>索引分单列索引和组合索引。单列索引，即一个索引只包含单个列，一个表可以有多个单列索引，但这不是组合索引。组合索引，即一个索引包含多个列。</p>
<p>创建索引时，你需要确保该索引是应用在 SQL 查询语句的条件(一般作为 WHERE 子句的条件)。</p>
<p>实际上，索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录。</p>
<p>上面都在说使用索引的好处，但过多的使用索引将会造成滥用。因此索引也会有它的缺点：虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。</p>
<p>建立索引会占用磁盘空间的索引文件。</p>
<h4 id="普通索引"><a href="#普通索引" class="headerlink" title="普通索引"></a>普通索引</h4><p>创建索引</p>
<p>这是最基本的索引，它没有任何限制。它有以下几种创建方式：</p>
<pre><code>CREATE INDEX indexName ON mytable(username(length));
</code></pre><p>如果是CHAR，VARCHAR类型，length可以小于字段实际长度；如果是BLOB和TEXT类型，必须指定 length。</p>
<p>修改表结构(添加索引)</p>
<pre><code>ALTER table tableName ADD INDEX indexName(columnName)
</code></pre><p>创建表的时候直接指定</p>
<pre><code>CREATE TABLE mytable(  

ID INT NOT NULL,   

username VARCHAR(16) NOT NULL,  

INDEX [indexName] (username(length))  

);
</code></pre><p>删除索引的语法</p>
<pre><code>DROP INDEX [indexName] ON mytable;
</code></pre><h4 id="唯一索引"><a href="#唯一索引" class="headerlink" title="唯一索引"></a>唯一索引</h4><p>它与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。它有以下几种创建方式：</p>
<p>创建索引</p>
<pre><code>CREATE UNIQUE INDEX indexName ON mytable(username(length))
</code></pre><p>修改表结构</p>
<pre><code>ALTER table mytable ADD UNIQUE [indexName] (username(length))
</code></pre><p>创建表的时候直接指定</p>
<pre><code>CREATE TABLE mytable(  

ID INT NOT NULL,   

username VARCHAR(16) NOT NULL,  

UNIQUE [indexName] (username(length))  

);
</code></pre><p>使用ALTER 命令添加和删除索引</p>
<p>有四种方式来添加数据表的索引：</p>
<ul>
<li>ALTER TABLE tbl_name ADD PRIMARY KEY (column_list): 该语句添加一个主键，这意味着索引值必须是唯一的，且不能为NULL。</li>
<li>ALTER TABLE tbl_name ADD UNIQUE index_name (column_list): 这条语句创建索引的值必须是唯一的（除了NULL外，NULL可能会出现多次）。</li>
<li>ALTER TABLE tbl_name ADD INDEX index_name (column_list): 添加普通索引，索引值可出现多次。</li>
<li>ALTER TABLE tbl_name ADD FULLTEXT index_name (column_list):该语句指定了索引为 FULLTEXT ，用于全文索引。</li>
</ul>
<p>以下实例为在表中添加索引。</p>
<pre><code>mysql&gt; ALTER TABLE testalter_tbl ADD INDEX (c);
</code></pre><p>你还可以在 ALTER 命令中使用 DROP 子句来删除索引。尝试以下实例删除索引:</p>
<pre><code>mysql&gt; ALTER TABLE testalter_tbl DROP INDEX c;
</code></pre><p>使用 ALTER 命令添加和删除主键</p>
<p>主键只能作用于一个列上，添加主键索引时，你需要确保该主键默认不为空（NOT NULL）。实例如下：</p>
<pre><code>mysql&gt; ALTER TABLE testalter_tbl MODIFY i INT NOT NULL;
mysql&gt; ALTER TABLE testalter_tbl ADD PRIMARY KEY (i);
</code></pre><p>你也可以使用 ALTER 命令删除主键：</p>
<pre><code>mysql&gt; ALTER TABLE testalter_tbl DROP PRIMARY KEY;
</code></pre><p>删除主键时只需指定PRIMARY KEY，但在删除索引时，你必须知道索引名。</p>
<p>显示索引信息</p>
<p>你可以使用 SHOW INDEX 命令来列出表中的相关的索引信息。可以通过添加 \G 来格式化输出信息。</p>
<p>尝试以下实例:</p>
<pre><code>mysql&gt; SHOW INDEX FROM table_name; \G
........
</code></pre><h3 id="MySQL临时表"><a href="#MySQL临时表" class="headerlink" title="MySQL临时表"></a>MySQL临时表</h3><p>MySQL 临时表在我们需要保存一些临时数据时是非常有用的。临时表只在当前连接可见，当关闭连接时，MySQL会自动删除表并释放所有空间。</p>
<p>临时表在MySQL 3.23版本中添加，如果你的MySQL版本低于 3.23版本就无法使用MySQL的临时表。不过现在一般很少有再使用这么低版本的MySQL数据库服务了。</p>
<p>MySQL临时表只在当前连接可见，如果你使用PHP脚本来创建MySQL临时表，那每当PHP脚本执行完成后，该临时表也会自动销毁。</p>
<p>如果你使用了其他MySQL客户端程序连接MySQL数据库服务器来创建临时表，那么只有在关闭客户端程序时才会销毁临时表，当然你也可以手动销毁。</p>
<p>实例</p>
<p>以下展示了使用MySQL 临时表的简单实例，以下的SQL代码可以适用于PHP脚本的mysql_query()函数。</p>
<pre><code class="lang-mysql">mysql&gt; CREATE TEMPORARY TABLE SalesSummary (
    -&gt; product_name VARCHAR(50) NOT NULL
    -&gt; , total_sales DECIMAL(12,2) NOT NULL DEFAULT 0.00
    -&gt; , avg_unit_price DECIMAL(7,2) NOT NULL DEFAULT 0.00
    -&gt; , total_units_sold INT UNSIGNED NOT NULL DEFAULT 0
);
Query OK, 0 rows affected (0.00 sec)

mysql&gt; INSERT INTO SalesSummary
    -&gt; (product_name, total_sales, avg_unit_price, total_units_sold)
    -&gt; VALUES
    -&gt; (&#39;cucumber&#39;, 100.25, 90, 2);

mysql&gt; SELECT * FROM SalesSummary;
+--------------+-------------+----------------+------------------+
| product_name | total_sales | avg_unit_price | total_units_sold |
+--------------+-------------+----------------+------------------+
| cucumber     |      100.25 |          90.00 |                2 |
+--------------+-------------+----------------+------------------+
1 row in set (0.00 sec)
</code></pre>
<p>当你使用 <strong>SHOW TABLES</strong>命令显示数据表列表时，你将无法看到 SalesSummary表。</p>
<p>如果你退出当前MySQL会话，再使用 <strong>SELECT</strong>命令来读取原先创建的临时表数据，那你会发现数据库中没有该表的存在，因为在你退出时该临时表已经被销毁了。</p>
<p>删除MySQL 临时表</p>
<p>默认情况下，当你断开与数据库的连接后，临时表就会自动被销毁。当然你也可以在当前MySQL会话使用 <strong>DROP TABLE</strong> 命令来手动删除临时表。</p>
<p>以下是手动删除临时表的实例：</p>
<pre><code class="lang-mysql">mysql&gt; CREATE TEMPORARY TABLE SalesSummary (
    -&gt; product_name VARCHAR(50) NOT NULL
    -&gt; , total_sales DECIMAL(12,2) NOT NULL DEFAULT 0.00
    -&gt; , avg_unit_price DECIMAL(7,2) NOT NULL DEFAULT 0.00
    -&gt; , total_units_sold INT UNSIGNED NOT NULL DEFAULT 0
);
Query OK, 0 rows affected (0.00 sec)

mysql&gt; INSERT INTO SalesSummary
    -&gt; (product_name, total_sales, avg_unit_price, total_units_sold)
    -&gt; VALUES
    -&gt; (&#39;cucumber&#39;, 100.25, 90, 2);

mysql&gt; SELECT * FROM SalesSummary;
+--------------+-------------+----------------+------------------+
| product_name | total_sales | avg_unit_price | total_units_sold |
+--------------+-------------+----------------+------------------+
| cucumber     |      100.25 |          90.00 |                2 |
+--------------+-------------+----------------+------------------+
1 row in set (0.00 sec)
mysql&gt; DROP TABLE SalesSummary;
mysql&gt;  SELECT * FROM SalesSummary;
ERROR 1146: Table &#39;W3CSCHOOL.SalesSummary&#39; doesn&#39;t exist
</code></pre>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>Resnet</title>
    <url>/CN/Resnet/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://gitcode.net/mirrors/bubbliiiing/faster-rcnn-pytorch">faster-rcnn-pytorch代码</a></p>
<p>2.<a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">Deep Residual Learning for Image Recognition (thecvf.com)</a></p>
<p>3.<a href="https://www.cnblogs.com/wujianming-110117/p/14975136.html">ResNet50结构 </a></p>
<p>4.<a href="https://blog.csdn.net/weixin_44791964/article/details/105739918?ops_request_misc=%7B%22request%5Fid%22%3A%22165156747616781685390350%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fblog.%22%7D&amp;request_id=165156747616781685390350&amp;biz_id=0&amp;spm=1018.2226.3001.4450">Pytorch搭建Faster R-CNN目标检测平台</a></p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>该网络是从原论文搬过来的，也就是参考博文2里的内容。有兴趣的可以读读原论文。</p>
<p><img src="/CN/Resnet/image-20220503185916137.png" class="lazyload" data-srcset="/CN/Resnet/image-20220503185916137.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220503185916137"></p>
<h2 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h2><p>具体网络结构：</p>
<p>1.Bottleneck</p>
<p><img src="/CN/Resnet/image-20220503193754428.png" class="lazyload" data-srcset="/CN/Resnet/image-20220503193754428.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220503193754428" style="zoom:50%;"></p>
<p>2.Resnet50中50的含义：（3+4+6+3)*3+2=50（卷积全连接层数之和）</p>
<p>2.resnet101同理，只要把<code>model = ResNet(Bottleneck, [3, 4, 6, 3])</code>改成<code>model=ResNet(Bottleneck, [3, 4, 23, 3])</code>,101=(3+4+23+3)*3+2</p>
<pre><code class="lang-python">import math

import torch.nn as nn
from torch.hub import load_state_dict_from_url

# c0表示输入特征图通道数，c1表示输出特征图通道数，由此可见，该类作用为通道扩张4倍
class Bottleneck(nn.Module):
    expansion = 4
    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        # (N, C0, H, W)-&gt;(N, C1, H, W)
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)
        # (N, C1, H, W)-&gt;(N, C1, H, W)
        self.bn1 = nn.BatchNorm2d(planes)

        # (N, C1, H, W)-&gt;(N, C1, H, W)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        # (N, C1, H, W)-&gt;(N, C1, H, W)
        self.bn2 = nn.BatchNorm2d(planes)
        # (N, C1, H, W)-&gt;(N, 4*C1, H, W)
        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
        # (N, 4*C1, H, W)-&gt;(N, 4*C1, H, W)
        self.bn3 = nn.BatchNorm2d(planes * 4)

        # (N, 4*C1, H, W)-&gt;(N, 4*C1, H, W)
        self.relu = nn.ReLU(inplace=True)
        # defalt is None
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out

class ResNet(nn.Module):
    def __init__(self, block, layers, num_classes=1000):
        #-----------------------------------#
        #   假设输入进来的图片是600,600,3
        #-----------------------------------#
        self.inplanes = 64
        super(ResNet, self).__init__()

        # 600,600,3 -&gt; 300,300,64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)

        # 300,300,64 -&gt; 150,150,64
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)

        # 150,150,64 -&gt; 150,150,256
        # 此时stride=1,但通道和原来一样，进行下采样，通道变为4*64=256
        self.layer1 = self._make_layer(block, 64, layers[0])

        # 150,150,256 -&gt; 75,75,512
        # 此时stride=2,进行下采样，宽高变为原来的1/2，通道变为4*128=512
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)

        # 75,75,512 -&gt; 38,38,1024 到这里可以获得一个38,38,1024的共享特征层
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        # self.layer4被用在classifier模型中
        # 38,38,1024 -&gt; 19,19,2048
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)

        # 19,19,2048 -&gt; 2,2,2048
        self.avgpool = nn.AvgPool2d(7)
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    # 定义每个层的操作，比如在resnet50里conv2_x重复了三次，那么就是block=Bottleck,blocks=3
    # 其实conv3_x,conv4_x,conv5_x中block都是Bottlenecck,就是重复的次数不一样
    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        #-------------------------------------------------------------------#
        # 当模型需要进行高和宽的压缩的时候，或者通道不为原来的4倍，就需要用到残差边的downsample
        # 个人认为这里通道变化不大（相比于2倍上层特征图通道数）也需要downsample是因为通道变化不
        # 大，信息不够细致
        #-------------------------------------------------------------------#
        if stride != 1 or self.inplanes != planes * block.expansion:
            # (N, C0, H, W)-&gt;(N, 4*C1, H, W)
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )
        # layers记录块结果
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        # 接上面假设，此时x=(N, 2, 2, 2048)
        x = self.avgpool(x)
        # (N, 2, 2, 2048)-&gt;(N, 4096)
        x = x.view(x.size(0), -1)
        # (N,4096)-&gt;(N, 1000)
        x = self.fc(x)
        return x

def resnet50(pretrained = False):
    model = ResNet(Bottleneck, [3, 4, 6, 3])
    if pretrained:
        state_dict = load_state_dict_from_url(&quot;https://download.pytorch.org/models/resnet50-19c8e357.pth&quot;, model_dir=&quot;./model_data&quot;)
        model.load_state_dict(state_dict)
    #----------------------------------------------------------------------------#
    #   获取特征提取部分，从conv1到model.layer3，最终获得一个38,38,1024的特征层
    #----------------------------------------------------------------------------#
    features    = list([model.conv1, model.bn1, model.relu, model.maxpool, model.layer1, model.layer2, model.layer3])
    #----------------------------------------------------------------------------#
    #   获取分类部分，从model.layer4到model.avgpool
    #----------------------------------------------------------------------------#
    classifier  = list([model.layer4, model.avgpool])

    features    = nn.Sequential(*features)
    classifier  = nn.Sequential(*classifier)
    return features, classifier
</code></pre>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>代码复现</tag>
      </tags>
  </entry>
  <entry>
    <title>SSD损失函数详解</title>
    <url>/CN/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://www.freesion.com/article/9127332548/">目标识别：SSD pytorch代码学习笔记</a></p>
<p>2.<a href="https://linkspringer.53yu.com/content/pdf/10.1007/978-3-319-46448-0_2.pdf">SSD论文</a></p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>SSD损失函数包括两部分：位置损失函数<script type="math/tex">L_{loc}</script>和置信度损失函数<script type="math/tex">L_{conf}</script></p>
<p>整个损失函数为：</p>
<p><img src="/CN/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/image-20220506200608617.png" class="lazyload" data-srcset="/CN/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/image-20220506200608617.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220506200608617" style="zoom:80%;"></p>
<p>说明：</p>
<p>·N是先验框的正样本数量</p>
<p>·c是类别置信度预测值</p>
<p>·l为先验框所对应边界框的位置预测值</p>
<p>·g为ground truth的位置参数</p>
<h4 id="位置损失函数"><a href="#位置损失函数" class="headerlink" title="位置损失函数"></a>位置损失函数</h4><p>针对所有正样本，采用Smooth L1损失，位置信息都是编码之后的信息。</p>
<p><img src="/CN/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/390ed4e1f5fc0a137b65dea937b907a5.png" class="lazyload" data-srcset="/CN/SSD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/390ed4e1f5fc0a137b65dea937b907a5.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>代码如下：</p>
<pre><code class="lang-python">    def _l1_smooth_loss(self, y_true, y_pred):
        abs_loss = torch.abs(y_true - y_pred)
        sq_loss = 0.5 * (y_true - y_pred)**2
        l1_loss = torch.where(abs_loss &lt; 1.0, sq_loss, abs_loss - 0.5)
        return torch.sum(l1_loss, -1)
</code></pre>
<h4 id="置信度损失函数"><a href="#置信度损失函数" class="headerlink" title="置信度损失函数"></a>置信度损失函数</h4><p>首先需要使用 <strong>hard negative mining</strong> 将正负样本按照 <strong>1:3</strong> 的比例把负样本抽样出来，抽样的方法是：</p>
<p><strong>思想</strong>： 针对所有batch的confidence，按照置信度误差进行降序排列，取出前<strong>top_k</strong>个负样本。</p>
<p>编程:</p>
<p>·reshape所有batch中的conf</p>
<p><code>batch_conf = conf_data.view(-1, self.num_classes)</code></p>
<p>·置信度误差越大，实际上就是预测背景的置信度越小</p>
<p>·把所有置信度进行log_softmax处理（均为负值），预测的置信度越小，则log_softmax越小，取绝对值，则|log_softmax|越大，降序排列后，取前top_k的负样本。</p>
<p>softmax代码如下：</p>
<pre><code class="lang-python">    def _softmax_loss(self, y_true, y_pred):
        y_pred = torch.clamp(y_pred, min=1e-7)
        softmax_loss = -torch.sum(y_true * torch.log(y_pred),
                                  axis=-1)
        return softmax_loss
</code></pre>
<h4 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h4><pre><code class="lang-python">import math
from functools import partial

import torch
import torch.nn as nn


class MultiboxLoss(nn.Module):
    def __init__(self, num_classes, alpha=1.0, neg_pos_ratio=3.0,
                 background_label_id=0, negatives_for_hard=100.0):
        self.num_classes = num_classes
        self.alpha = alpha
        self.neg_pos_ratio = neg_pos_ratio
        if background_label_id != 0:
            raise Exception(&#39;Only 0 as background label id is supported&#39;)
        self.background_label_id = background_label_id
        self.negatives_for_hard = torch.FloatTensor([negatives_for_hard])[0]

    def _l1_smooth_loss(self, y_true, y_pred):
        abs_loss = torch.abs(y_true - y_pred)
        sq_loss = 0.5 * (y_true - y_pred)**2
        l1_loss = torch.where(abs_loss &lt; 1.0, sq_loss, abs_loss - 0.5)
        return torch.sum(l1_loss, -1)

    def _softmax_loss(self, y_true, y_pred):
        y_pred = torch.clamp(y_pred, min=1e-7)
        softmax_loss = -torch.sum(y_true * torch.log(y_pred),
                                  axis=-1)
        return softmax_loss

    def forward(self, y_true, y_pred):
        # --------------------------------------------- #
        #   y_true batch_size, 8732, 4 + self.num_classes + 1
        #   y_pred batch_size, 8732, 4 + self.num_classes
        # --------------------------------------------- #
        num_boxes = y_true.size()[1]
        y_pred = torch.cat([y_pred[0], nn.Softmax(-1)(y_pred[1])], dim=-1)

        # --------------------------------------------- #
        #   分类的loss
        #   batch_size,8732,21 -&gt; batch_size,8732
        # --------------------------------------------- #
        conf_loss = self._softmax_loss(y_true[:, :, 4:-1], y_pred[:, :, 4:])

        # --------------------------------------------- #
        #   框的位置的loss
        #   batch_size,8732,4 -&gt; batch_size,8732
        # --------------------------------------------- #
        loc_loss = self._l1_smooth_loss(y_true[:, :, :4],
                                        y_pred[:, :, :4])

        # --------------------------------------------- #
        #   获取所有的正标签的loss
        # --------------------------------------------- #
        pos_loc_loss = torch.sum(loc_loss * y_true[:, :, -1],
                                 axis=1)
        pos_conf_loss = torch.sum(conf_loss * y_true[:, :, -1],
                                  axis=1)

        # --------------------------------------------- #
        #   每一张图的正样本的个数
        #   num_pos     [batch_size,]
        # --------------------------------------------- #
        num_pos = torch.sum(y_true[:, :, -1], axis=-1)

        # --------------------------------------------- #
        #   每一张图的负样本的个数
        #   num_neg     [batch_size,]
        # --------------------------------------------- #
        num_neg = torch.min(self.neg_pos_ratio * num_pos, num_boxes - num_pos)
        # 找到了哪些值是大于0的
        pos_num_neg_mask = num_neg &gt; 0
        # --------------------------------------------- #
        #   如果所有的图，正样本的数量均为0
        #   那么则默认选取100个先验框作为负样本
        # --------------------------------------------- #
        has_min = torch.sum(pos_num_neg_mask)

        # --------------------------------------------- #
        #   从这里往后，与视频中看到的代码有些许不同。
        #   由于以前的负样本选取方式存在一些问题，
        #   我对该部分代码进行重构。
        #   求整个batch应该的负样本数量总和
        # --------------------------------------------- #
        num_neg_batch = torch.sum(
            num_neg) if has_min &gt; 0 else self.negatives_for_hard

        # --------------------------------------------- #
        #   对预测结果进行判断，如果该先验框没有包含物体
        #   那么它的不属于背景的预测概率过大的话
        #   就是难分类样本
        # --------------------------------------------- #
        confs_start = 4 + self.background_label_id + 1
        confs_end = confs_start + self.num_classes - 1

        # --------------------------------------------- #
        #   batch_size,8732
        #   把不是背景的概率求和，求和后的概率越大
        #   代表越难分类。
        # --------------------------------------------- #
        max_confs = torch.sum(y_pred[:, :, confs_start:confs_end], dim=2)

        # --------------------------------------------------- #
        #   只有没有包含物体的先验框才得到保留
        #   我们在整个batch里面选取最难分类的num_neg_batch个
        #   先验框作为负样本。
        # --------------------------------------------------- #
        max_confs = (max_confs * (1 - y_true[:, :, -1])).view([-1])

        _, indices = torch.topk(max_confs, k=int(
            num_neg_batch.cpu().numpy().tolist()))

        neg_conf_loss = torch.gather(conf_loss.view([-1]), 0, indices)

        # 进行归一化
        num_pos = torch.where(num_pos != 0, num_pos, torch.ones_like(num_pos))
        total_loss = torch.sum(
            pos_conf_loss) + torch.sum(neg_conf_loss) + torch.sum(self.alpha * pos_loc_loss)
        total_loss = total_loss / torch.sum(num_pos)
        return total_loss


def weights_init(net, init_type=&#39;normal&#39;, init_gain=0.02):
    def init_func(m):
        classname = m.__class__.__name__
        if hasattr(m, &#39;weight&#39;) and classname.find(&#39;Conv&#39;) != -1:
            if init_type == &#39;normal&#39;:
                torch.nn.init.normal_(m.weight.data, 0.0, init_gain)
            elif init_type == &#39;xavier&#39;:
                torch.nn.init.xavier_normal_(m.weight.data, gain=init_gain)
            elif init_type == &#39;kaiming&#39;:
                torch.nn.init.kaiming_normal_(
                    m.weight.data, a=0, mode=&#39;fan_in&#39;)
            elif init_type == &#39;orthogonal&#39;:
                torch.nn.init.orthogonal_(m.weight.data, gain=init_gain)
            else:
                raise NotImplementedError(
                    &#39;initialization method [%s] is not implemented&#39; % init_type)
        elif classname.find(&#39;BatchNorm2d&#39;) != -1:
            torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
            torch.nn.init.constant_(m.bias.data, 0.0)
    print(&#39;initialize network with %s type&#39; % init_type)
    net.apply(init_func)


def get_lr_scheduler(lr_decay_type, lr, min_lr, total_iters, warmup_iters_ratio=0.1, warmup_lr_ratio=0.1, no_aug_iter_ratio=0.3, step_num=10):
    def yolox_warm_cos_lr(lr, min_lr, total_iters, warmup_total_iters, warmup_lr_start, no_aug_iter, iters):
        if iters &lt;= warmup_total_iters:
            # lr = (lr - warmup_lr_start) * iters / float(warmup_total_iters) + warmup_lr_start
            lr = (lr - warmup_lr_start) * pow(iters /
                                              float(warmup_total_iters), 2) + warmup_lr_start
        elif iters &gt;= total_iters - no_aug_iter:
            lr = min_lr
        else:
            lr = min_lr + 0.5 * (lr - min_lr) * (
                1.0 + math.cos(math.pi * (iters - warmup_total_iters) /
                               (total_iters - warmup_total_iters - no_aug_iter))
            )
        return lr

    def step_lr(lr, decay_rate, step_size, iters):
        if step_size &lt; 1:
            raise ValueError(&quot;step_size must above 1.&quot;)
        n = iters // step_size
        out_lr = lr * decay_rate ** n
        return out_lr

    if lr_decay_type == &quot;cos&quot;:
        warmup_total_iters = min(max(warmup_iters_ratio * total_iters, 1), 3)
        warmup_lr_start = max(warmup_lr_ratio * lr, 1e-6)
        no_aug_iter = min(max(no_aug_iter_ratio * total_iters, 1), 15)
        func = partial(yolox_warm_cos_lr, lr, min_lr, total_iters,
                       warmup_total_iters, warmup_lr_start, no_aug_iter)
    else:
        decay_rate = (min_lr / lr) ** (1 / (step_num - 1))
        step_size = total_iters / step_num
        func = partial(step_lr, lr, decay_rate, step_size)

    return func


def set_optimizer_lr(optimizer, lr_scheduler_func, epoch):
    lr = lr_scheduler_func(epoch)
    for param_group in optimizer.param_groups:
        param_group[&#39;lr&#39;] = lr
</code></pre>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>SSD</tag>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title>RetinaNet详解</title>
    <url>/CN/RetinaNet/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文地址：<a href="https://arxiv.org/pdf/1708.02002.pdf">Focal loss for dense object detection</a></p>
<p>源码地址：<a href="https://github.com/facebookresearch/Detectron">RetinaNet</a></p>
<p>文章引用代码地址：<a href="https://github.com/bubbliiiing/retinanet-pytorch">https://github.com/bubbliiiing/retinanet-pytorch</a></p>
<p>文章出处：<a href="https://blog.csdn.net/weixin_44791964/article/details/108319189">https://blog.csdn.net/weixin_44791964/article/details/108319189</a></p>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>Retinanet是在何凯明大神提出Focal loss同时提出的一种新的目标检测方案，来验证Focal Loss的有效性。</p>
<p>One-Stage目标检测方法常常使用先验框提高预测性能，一张图像可能生成成千上万的候选框，但是其中只有很少一部分是包含目标的的，有目标的就是正样本，没有目标的就是负样本。这种情况造成了One-Stage目标检测方法的正负样本不平衡，也使得One-Stage目标检测方法的检测效果比不上Two-Stage目标检测方法。</p>
<p>Focal Loss是一种新的用于平衡One-Stage目标检测方法正负样本的Loss方案。</p>
<p>Retinane的结构非常简单，但是其存在非常多的先验框，以输入600x600x3的图片为例，就存在着67995个先验框，这些先验框里面大多包含的是背景，存在非常多的负样本。以Focal Loss训练的Retinanet可以有效的平衡正负样本，实现有效的训练。</p>
<h3 id="预测部分"><a href="#预测部分" class="headerlink" title="预测部分"></a>预测部分</h3><h4 id="主干网络"><a href="#主干网络" class="headerlink" title="主干网络"></a>主干网络</h4><p><img src="/CN/RetinaNet/image-20220418210603737.png" class="lazyload" data-srcset="/CN/RetinaNet/image-20220418210603737.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>假设输入的图片大小为600x600x3。</p>
<p>ResNet50有两个基本的块，分别名为Conv Block和Identity Block，其中Conv Block输入和输出的维度是不一样的，所以不能连续串联，它的作用是改变网络的维度；Identity Block输入维度和输出维度相同，可以串联，用于加深网络的。</p>
<p><img src="/CN/RetinaNet/image-20220419095619530.png" class="lazyload" data-srcset="/CN/RetinaNet/image-20220419095619530.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>当输入的图片为600x600x3的时候，shape变化与总的网络结构如下：</p>
<p><img src="/CN/RetinaNet/20200215151533258.png" class="lazyload" data-srcset="/CN/RetinaNet/20200215151533258.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="img" style="zoom:50%;"></p>
<p>我们取出长宽压缩了三次、四次、五次的结果来进行网络金字塔结构的构造</p>
<p>实现代码：</p>
<pre><code class="lang-python">from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import math
import torch.utils.model_zoo as model_zoo
import pdb


model_urls = &#123;
&#39;resnet18&#39;: &#39;https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth&#39;,
&#39;resnet34&#39;: &#39;https://s3.amazonaws.com/pytorch/models/resnet34-333f7ec4.pth&#39;,
&#39;resnet50&#39;: &#39;https://s3.amazonaws.com/pytorch/models/resnet50-19c8e357.pth&#39;,
&#39;resnet101&#39;: &#39;https://s3.amazonaws.com/pytorch/models/resnet101-5d3b4d8f.pth&#39;,
&#39;resnet152&#39;: &#39;https://s3.amazonaws.com/pytorch/models/resnet152-b121ed2d.pth&#39;,
&#125;

def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):
    &quot;&quot;&quot;3x3 convolution with padding&quot;&quot;&quot;
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=dilation, groups=groups, bias=False, dilation=dilation)


def conv1x1(in_planes, out_planes, stride=1):
    &quot;&quot;&quot;1x1 convolution&quot;&quot;&quot;
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None):
        super(BasicBlock, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError(&#39;BasicBlock only supports groups=1 and base_width=64&#39;)
        if dilation &gt; 1:
            raise NotImplementedError(&quot;Dilation &gt; 1 not supported in BasicBlock&quot;)
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None):
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.)) * groups
        # Both self.conv2 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv1x1(inplanes, width)
        self.bn1 = norm_layer(width)
        self.conv2 = conv3x3(width, width, stride, groups, dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = conv1x1(width, planes * self.expansion)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class ResNet(nn.Module):
    def __init__(self, block, layers, num_classes=1000):
        self.inplanes = 64
        super(ResNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,
                    bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True) # change
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)

        self.avgpool = nn.AvgPool2d(7)
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                    kernel_size=1, stride=stride, bias=False),
            nn.BatchNorm2d(planes * block.expansion),
        )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x




def resnet18(pretrained=False, **kwargs):
    &quot;&quot;&quot;Constructs a ResNet-18 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    &quot;&quot;&quot;
    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet18&#39;], model_dir=&#39;model_data&#39;), strict=False)
    return model


def resnet34(pretrained=False, **kwargs):
    &quot;&quot;&quot;Constructs a ResNet-34 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    &quot;&quot;&quot;
    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet34&#39;], model_dir=&#39;model_data&#39;), strict=False)
    return model


def resnet50(pretrained=False, **kwargs):
    &quot;&quot;&quot;Constructs a ResNet-50 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    &quot;&quot;&quot;
    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet50&#39;], model_dir=&#39;model_data&#39;), strict=False)
    return model


def resnet101(pretrained=False, **kwargs):
    &quot;&quot;&quot;Constructs a ResNet-101 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    &quot;&quot;&quot;
    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet101&#39;], model_dir=&#39;model_data&#39;), strict=False)
    return model


def resnet152(pretrained=False, **kwargs):
    &quot;&quot;&quot;Constructs a ResNet-152 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    &quot;&quot;&quot;
    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls[&#39;resnet152&#39;], model_dir=&#39;model_data&#39;), strict=False)
    return model
</code></pre>
<h4 id="从特征获取预测结果"><a href="#从特征获取预测结果" class="headerlink" title="从特征获取预测结果"></a>从特征获取预测结果</h4><p><img src="/CN/RetinaNet/image-20220419100322079.png" class="lazyload" data-srcset="/CN/RetinaNet/image-20220419100322079.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>由抽象的结构图可知，获得到的特征还需要经过图像金字塔的处理，这样的结构可以融合多尺度的特征，实现更有效的预测。</p>
<p>图像金字塔的具体结构如下：</p>
<p><img src="/CN/RetinaNet/2020021613520193.png" class="lazyload" data-srcset="/CN/RetinaNet/2020021613520193.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>通过图像金字塔我们可以获得五个有效的特征层，分别是P3、P4、P5、P6、P7，<br>为了和普通特征层区分，我们称之为有效特征层，将这五个有效的特征层传输过class+box subnets就可以获得预测结果了。</p>
<p>class subnet采用4次256通道的卷积和1次num_anchors x num_classes的卷积，num_anchors指的是该特征层所拥有的先验框数量，num_classes指的是网络一共对多少类的目标进行检测。</p>
<p>box subnet采用4次256通道的卷积和1次num_anchors x 4的卷积，num_anchors指的是该特征层所拥有的先验框数量，4指的是先验框的调整情况。</p>
<p>需要注意的是，每个特征层所用的class subnet是同一个class subnet；每个特征层所用的box subnet是同一个box subnet。<br><img src="/CN/RetinaNet/image-20220419100832971.png" class="lazyload" data-srcset="/CN/RetinaNet/image-20220419100832971.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>其中：<br>1.num_anchors x 4的卷积 用于预测 该特征层上 每一个网格点上 每一个先验框的变化情况。（为什么说是变化情况呢，这是因为ssd的预测结果需要结合先验框获得预测框，预测结果就是先验框的变化情况。）</p>
<p>2.num_anchors x num_classes的卷积 用于预测 该特征层上 每一个网格点上 每一个预测框对应的种类。<br>实现代码：</p>
<pre><code class="lang-python">import torch.nn as nn
import torch.nn.functional as F  
import torch
import math
from nets.resnet import resnet18,resnet34,resnet50,resnet101,resnet152
from utils.anchors import Anchors

class PyramidFeatures(nn.Module):
    def __init__(self, C3_size, C4_size, C5_size, feature_size=256):
        super(PyramidFeatures, self).__init__()

        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)
        self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)

        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)
        self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)

        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)
        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)

        self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1)

        self.P7_1 = nn.ReLU()
        self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)

    def forward(self, inputs):
        C3, C4, C5 = inputs
        _, _, h4, w4 = C4.size()
        _, _, h3, w3 = C3.size()

        P5_x = self.P5_1(C5)
        P5_upsampled_x = F.interpolate(P5_x, size=(h4, w4))
        P5_x = self.P5_2(P5_x)

        P4_x = self.P4_1(C4)
        P4_x = P5_upsampled_x + P4_x
        P4_upsampled_x = F.interpolate(P4_x, size=(h3, w3))
        P4_x = self.P4_2(P4_x)

        P3_x = self.P3_1(C3)
        P3_x = P3_x + P4_upsampled_x
        P3_x = self.P3_2(P3_x)

        P6_x = self.P6(C5)

        P7_x = self.P7_1(P6_x)
        P7_x = self.P7_2(P7_x)

        return [P3_x, P4_x, P5_x, P6_x, P7_x]


class RegressionModel(nn.Module):
    def __init__(self, num_features_in, num_anchors=9, feature_size=256):
        super(RegressionModel, self).__init__()

        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)
        self.act1 = nn.ReLU()

        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)
        self.act2 = nn.ReLU()

        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)
        self.act3 = nn.ReLU()

        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)
        self.act4 = nn.ReLU()

        self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, padding=1)

    def forward(self, x):
        out = self.conv1(x)
        out = self.act1(out)

        out = self.conv2(out)
        out = self.act2(out)

        out = self.conv3(out)
        out = self.act3(out)

        out = self.conv4(out)
        out = self.act4(out)

        out = self.output(out)

        # out is B x C x W x H, with C = 4*num_anchors
        out = out.permute(0, 2, 3, 1)

        return out.contiguous().view(out.shape[0], -1, 4)


class ClassificationModel(nn.Module):
    def __init__(self, num_features_in, num_anchors=9, num_classes=80, anchor=0.01, feature_size=256):
        super(ClassificationModel, self).__init__()

        self.num_classes = num_classes
        self.num_anchors = num_anchors

        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)
        self.act1 = nn.ReLU()

        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)
        self.act2 = nn.ReLU()

        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)
        self.act3 = nn.ReLU()

        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)
        self.act4 = nn.ReLU()

        self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, padding=1)
        self.output_act = nn.Sigmoid()

    def forward(self, x):
        out = self.conv1(x)
        out = self.act1(out)

        out = self.conv2(out)
        out = self.act2(out)

        out = self.conv3(out)
        out = self.act3(out)

        out = self.conv4(out)
        out = self.act4(out)

        out = self.output(out)
        out = self.output_act(out)

        # out is B x C x W x H, with C = n_classes + n_anchors
        out1 = out.permute(0, 2, 3, 1)

        batch_size, width, height, channels = out1.shape

        out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes)

        return out2.contiguous().view(x.shape[0], -1, self.num_classes)

class Resnet(nn.Module):
    def __init__(self, phi, load_weights=False):
        super(Resnet, self).__init__()
        self.edition = [resnet18,resnet34,resnet50,resnet101,resnet152]
        model = self.edition[phi](load_weights)
        del model.avgpool
        del model.fc
        self.model = model

    def forward(self, x):
        x = self.model.conv1(x)
        x = self.model.bn1(x)
        x = self.model.relu(x)
        x = self.model.maxpool(x)

        x = self.model.layer1(x)
        feat1 = self.model.layer2(x)
        feat2 = self.model.layer3(feat1)
        feat3 = self.model.layer4(feat2)

        return [feat1,feat2,feat3]

class Retinanet(nn.Module):

    def __init__(self, num_classes, phi, pretrain_weights=False):
        super(Retinanet, self).__init__()
        self.pretrain_weights = pretrain_weights
        self.backbone_net = Resnet(phi,pretrain_weights)
        fpn_sizes = &#123;
            0: [128, 256, 512],
            1: [128, 256, 512],
            2: [512, 1024, 2048],
            3: [512, 1024, 2048],
            4: [512, 1024, 2048],
        &#125;[phi]

        self.fpn = PyramidFeatures(fpn_sizes[0], fpn_sizes[1], fpn_sizes[2])
        self.regressionModel = RegressionModel(256)
        self.classificationModel = ClassificationModel(256, num_classes=num_classes)
        self.anchors = Anchors()
        self._init_weights()

    def _init_weights(self):
        if not self.pretrain_weights:
            print(&quot;_init_weights&quot;)
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                    m.weight.data.normal_(0, math.sqrt(2. / n))
                elif isinstance(m, nn.BatchNorm2d):
                    m.weight.data.fill_(1)
                    m.bias.data.zero_()

        print(&quot;_init_classificationModel&quot;)
        anchor = 0.01
        self.classificationModel.output.weight.data.fill_(0)
        self.classificationModel.output.bias.data.fill_(-math.log((1.0 - anchor) / anchor))
        print(&quot;_init_regressionModel&quot;)
        self.regressionModel.output.weight.data.fill_(0)
        self.regressionModel.output.bias.data.fill_(0)


    def forward(self, inputs):

        p3, p4, p5 = self.backbone_net(inputs)

        features = self.fpn([p3, p4, p5])

        regression = torch.cat([self.regressionModel(feature) for feature in features], dim=1)

        classification = torch.cat([self.classificationModel(feature) for feature in features], dim=1)

        anchors = self.anchors(features)

        return features, regression, classification, anchors
</code></pre>
<h4 id="预测结果的解码"><a href="#预测结果的解码" class="headerlink" title="预测结果的解码"></a>预测结果的解码</h4><p>我们通过对每一个特征层的处理，可以获得三个内容，分别是：</p>
<p>num_anchors x 4的卷积 用于预测 该特征层上 每一个网格点上 每一个先验框的变化情况。</p>
<p>num_anchors x num_classes的卷积 用于预测 该特征层上 每一个网格点上 每一个预测框对应的种类。</p>
<p>每一个有效特征层对应的先验框对应着该特征层上 每一个网格点上 预先设定好的9个框。</p>
<p>我们利用 num_anchors x 4的卷积 与 每一个有效特征层对应的先验框 获得框的真实位置。</p>
<p>每一个有效特征层对应的先验框就是，如图所示的作用：<br>每一个有效特征层将整个图片分成与其长宽对应的网格，如P3的特征层就是将整个图像分成75x75个网格；然后从每个网格中心建立9个先验框，一共75x75x9个，50625个先验框<br><img src="/CN/RetinaNet/20200129210050147.png" class="lazyload" data-srcset="/CN/RetinaNet/20200129210050147.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>先验框虽然可以代表一定的框的位置信息与框的大小信息，但是其是有限的，无法表示任意情况，因此还需要调整，Retinanet利用4次256通道的卷积+num_anchors x 4的卷积的结果对先验框进行调整。</p>
<p>num_anchors x 4中的num_anchors表示了这个网格点所包含的先验框数量，其中的4表示了框的左上角xy轴，右下角xy的调整情况。</p>
<p>Retinanet解码过程就是将对应的先验框的左上角和右下角进行位置的调整，调整完的结果就是预测框的位置了。</p>
<p>当然得到最终的预测结构后还要进行得分排序与非极大抑制筛选这一部分基本上是所有目标检测通用的部分。<br>1、取出每一类得分大于confidence_threshold的框和得分。<br>2、利用框的位置和得分进行非极大抑制。<br>实现代码：</p>
<pre><code class="lang-python">def decodebox(regression, anchors, img):
    dtype = regression.dtype
    anchors = anchors.to(dtype)
    y_centers_a = (anchors[..., 0] + anchors[..., 2]) / 2
    x_centers_a = (anchors[..., 1] + anchors[..., 3]) / 2

    ha = anchors[..., 2] - anchors[..., 0]
    wa = anchors[..., 3] - anchors[..., 1]

    w = regression[..., 3].exp() * wa
    h = regression[..., 2].exp() * ha

    y_centers = regression[..., 0] * ha + y_centers_a
    x_centers = regression[..., 1] * wa + x_centers_a

    ymin = y_centers - h / 2.
    xmin = x_centers - w / 2.
    ymax = y_centers + h / 2.
    xmax = x_centers + w / 2.

    boxes = torch.stack([xmin, ymin, xmax, ymax], dim=2)

    _, _, height, width = np.shape(img)

    boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)
    boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)

    boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width - 1)
    boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height - 1)

    # fig = plt.figure()
    # ax = fig.add_subplot(121)
    # grid_x = x_centers_a[0,-4*4*9:]
    # grid_y = y_centers_a[0,-4*4*9:]
    # plt.ylim(-600,1200)
    # plt.xlim(-600,1200)
    # plt.gca().invert_yaxis()
    # plt.scatter(grid_x.cpu(),grid_y.cpu())

    # anchor_left = anchors[0,-4*4*9:,1]
    # anchor_top = anchors[0,-4*4*9:,0]
    # anchor_w = wa[0,-4*4*9:]
    # anchor_h = ha[0,-4*4*9:]

    # for i in range(9,18):
    #     rect1 = plt.Rectangle([anchor_left[i],anchor_top[i]],anchor_w[i],anchor_h[i],color=&quot;r&quot;,fill=False)
    #     ax.add_patch(rect1)

    # ax = fig.add_subplot(122)

    # grid_x = x_centers_a[0,-4*4*9:]
    # grid_y = y_centers_a[0,-4*4*9:]
    # plt.scatter(grid_x.cpu(),grid_y.cpu())
    # plt.ylim(-600,1200)
    # plt.xlim(-600,1200)
    # plt.gca().invert_yaxis()

    # y_centers = y_centers[0,-4*4*9:]
    # x_centers = x_centers[0,-4*4*9:]

    # pre_left = xmin[0,-4*4*9:]
    # pre_top = ymin[0,-4*4*9:]

    # pre_w = xmax[0,-4*4*9:]-xmin[0,-4*4*9:]
    # pre_h = ymax[0,-4*4*9:]-ymin[0,-4*4*9:]

    # for i in range(9,18):
    #     plt.scatter(x_centers[i].cpu(),y_centers[i].cpu(),c=&#39;r&#39;)
    #     rect1 = plt.Rectangle([pre_left[i],pre_top[i]],pre_w[i],pre_h[i],color=&quot;r&quot;,fill=False)
    #     ax.add_patch(rect1)

    # plt.show()
    return boxes

def retinanet_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image):
    #-----------------------------------------------------------------#
    #   把y轴放前面是因为方便预测框和图像的宽高进行相乘
    #-----------------------------------------------------------------#
    box_yx = box_xy[..., ::-1]
    box_hw = box_wh[..., ::-1]
    input_shape = np.array(input_shape)
    image_shape = np.array(image_shape)

    if letterbox_image:
        #-----------------------------------------------------------------#
        #   这里求出来的offset是图像有效区域相对于图像左上角的偏移情况
        #   new_shape指的是宽高缩放情况
        #-----------------------------------------------------------------#
        new_shape = np.round(image_shape * np.min(input_shape/image_shape))
        offset  = (input_shape - new_shape)/2./input_shape
        scale   = input_shape/new_shape

        box_yx  = (box_yx - offset) * scale
        box_hw *= scale

    box_mins    = box_yx - (box_hw / 2.)
    box_maxes   = box_yx + (box_hw / 2.)
    boxes  = np.concatenate([box_mins[..., 0:1], box_mins[..., 1:2], box_maxes[..., 0:1], box_maxes[..., 1:2]], axis=-1)
    boxes *= np.concatenate([image_shape, image_shape], axis=-1)
    return boxes

def non_max_suppression(prediction, input_shape, image_shape, letterbox_image, conf_thres=0.5, nms_thres=0.4):
    output = [None for _ in range(len(prediction))]

    #----------------------------------------------------------#
    #   预测只用一张图片，只会进行一次
    #----------------------------------------------------------#
    for i, image_pred in enumerate(prediction):
        #----------------------------------------------------------#
        #   对种类预测部分取max。
        #   class_conf  [num_anchors, 1]    种类置信度
        #   class_pred  [num_anchors, 1]    种类
        #----------------------------------------------------------#
        class_conf, class_pred = torch.max(image_pred[:, 4:], 1, keepdim=True)

        #----------------------------------------------------------#
        #   利用置信度进行第一轮筛选
        #----------------------------------------------------------#
        conf_mask = (class_conf[:, 0] &gt;= conf_thres).squeeze()

        #----------------------------------------------------------#
        #   根据置信度进行预测结果的筛选
        #----------------------------------------------------------#
        image_pred = image_pred[conf_mask]
        class_conf = class_conf[conf_mask]
        class_pred = class_pred[conf_mask]
        if not image_pred.size(0):
            continue
        #-------------------------------------------------------------------------#
        #   detections  [num_anchors, 6]
        #   6的内容为：x1, y1, x2, y2, class_conf, class_pred
        #-------------------------------------------------------------------------#
        detections = torch.cat((image_pred[:, :4], class_conf.float(), class_pred.float()), 1)

        #------------------------------------------#
        #   获得预测结果中包含的所有种类
        #------------------------------------------#
        unique_labels = detections[:, -1].cpu().unique()

        if prediction.is_cuda:
            unique_labels = unique_labels.cuda()
            detections = detections.cuda()

        for c in unique_labels:
            #------------------------------------------#
            #   获得某一类得分筛选后全部的预测结果
            #------------------------------------------#
            detections_class = detections[detections[:, -1] == c]

            #------------------------------------------#
            #   使用官方自带的非极大抑制会速度更快一些！
            #------------------------------------------#
            keep = nms(
                detections_class[:, :4],
                detections_class[:, 4],
                nms_thres
            )
            max_detections = detections_class[keep]

            # #------------------------------------------#
            # #   按照存在物体的置信度排序
            # #------------------------------------------#
            # _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)
            # detections_class = detections_class[conf_sort_index]
            # #------------------------------------------#
            # #   进行非极大抑制
            # #------------------------------------------#
            # max_detections = []
            # while detections_class.size(0):
            #     #---------------------------------------------------#
            #     #   取出这一类置信度最高的，一步一步往下判断。
            #     #   判断重合程度是否大于nms_thres，如果是则去除掉
            #     #---------------------------------------------------#
            #     max_detections.append(detections_class[0].unsqueeze(0))
            #     if len(detections_class) == 1:
            #         break
            #     ious = bbox_iou(max_detections[-1], detections_class[1:])
            #     detections_class = detections_class[1:][ious &lt; nms_thres]
            # #------------------------------------------#
            # #   堆叠
            # #------------------------------------------#
            # max_detections = torch.cat(max_detections).data

            output[i] = max_detections if output[i] is None else torch.cat((output[i], max_detections))

        if output[i] is not None:
            output[i]           = output[i].cpu().numpy()
            box_xy, box_wh      = (output[i][:, 0:2] + output[i][:, 2:4])/2, output[i][:, 2:4] - output[i][:, 0:2]
            output[i][:, :4]    = retinanet_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image)
    return output
</code></pre>
<h4 id="原图上进行绘制"><a href="#原图上进行绘制" class="headerlink" title="原图上进行绘制"></a>原图上进行绘制</h4><p>通过第三步，我们可以获得预测框在原图上的位置，而且这些预测框都是经过筛选的。这些筛选后的框可以直接绘制在图片上，就可以获得结果了。</p>
<h3 id="训练部分"><a href="#训练部分" class="headerlink" title="训练部分"></a>训练部分</h3><h4 id="真实框的处理"><a href="#真实框的处理" class="headerlink" title="真实框的处理"></a>真实框的处理</h4><p>从预测部分我们知道，每个特征层的预测结果，num_anchors x 4的卷积 用于预测 该特征层上 每一个网格点上 每一个先验框的变化情况。</p>
<p>也就是说，我们直接利用retinanet网络预测到的结果，并不是预测框在图片上的真实位置，需要解码才能得到真实位置。</p>
<p>而在训练的时候，我们需要计算loss函数，这个loss函数是相对于Retinanet网络的预测结果的。我们需要把图片输入到当前的Retinanet网络中，得到预测结果；同时还需要把真实框的信息，进行编码，这个编码是把真实框的位置信息格式转化为Retinanet预测结果的格式信息。</p>
<p>也就是，我们需要找到 每一张用于训练的图片的每一个真实框对应的先验框，并求出如果想要得到这样一个真实框，我们的预测结果应该是怎么样的。</p>
<p>从预测结果获得真实框的过程被称作解码，而从真实框获得预测结果的过程就是编码的过程。</p>
<p>因此我们只需要将解码过程逆过来就是编码过程了。</p>
<p>在进行编码的时候，我们需要找到每一个真实框对应的先验框，我们把和真实框重合程度在0.5以上的作为正样本，在0.4以下的作为负样本，在0.4和0.5之间的作为忽略样本。<br>实现代码：</p>
<pre><code class="lang-python">def get_target(anchor, bbox_annotation, classification, cuda):
    IoU = calc_iou(anchor[:, :], bbox_annotation[:, :4])

    IoU_max, IoU_argmax = torch.max(IoU, dim=1)

    # compute the loss for classification
    targets = torch.ones_like(classification) * -1
    if cuda:
        targets = targets.cuda()

    targets[torch.lt(IoU_max, 0.4), :] = 0

    positive_indices = torch.ge(IoU_max, 0.5)

    num_positive_anchors = positive_indices.sum()

    assigned_annotations = bbox_annotation[IoU_argmax, :]

    targets[positive_indices, :] = 0
    targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1
    return targets, num_positive_anchors, positive_indices, assigned_annotations

def encode_bbox(assigned_annotations, positive_indices, anchor_widths, anchor_heights, anchor_ctr_x, anchor_ctr_y):
    assigned_annotations = assigned_annotations[positive_indices, :]

    anchor_widths_pi = anchor_widths[positive_indices]
    anchor_heights_pi = anchor_heights[positive_indices]
    anchor_ctr_x_pi = anchor_ctr_x[positive_indices]
    anchor_ctr_y_pi = anchor_ctr_y[positive_indices]

    gt_widths = assigned_annotations[:, 2] - assigned_annotations[:, 0]
    gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1]
    gt_ctr_x = assigned_annotations[:, 0] + 0.5 * gt_widths
    gt_ctr_y = assigned_annotations[:, 1] + 0.5 * gt_heights

    # efficientdet style
    gt_widths = torch.clamp(gt_widths, min=1)
    gt_heights = torch.clamp(gt_heights, min=1)

    targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi
    targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi
    targets_dw = torch.log(gt_widths / anchor_widths_pi)
    targets_dh = torch.log(gt_heights / anchor_heights_pi)

    targets = torch.stack((targets_dy, targets_dx, targets_dh, targets_dw))
    targets = targets.t()
    return targets
</code></pre>
<h4 id="loss计算"><a href="#loss计算" class="headerlink" title="loss计算"></a>loss计算</h4><p>loss的计算分为两个部分：<br>1、Smooth Loss：获取所有正标签的框的预测结果的回归loss。<br>2、Focal Loss：获取所有未被忽略的种类的预测结果的交叉熵loss。</p>
<p>由于在Retinanet的训练过程中，正负样本极其不平衡，即 存在对应真实框的先验框可能只有若干个，但是不存在对应真实框的负样本却有上万个，这就会导致负样本的loss值极大，因此引入了Focal Loss进行正负样本的平衡。</p>
<p>Focal loss是何恺明大神提出的一种新的loss计算方案。其具有两个重要的特点。</p>
<p>a)控制正负样本的权重<br>控制容易分类和难分类样本的权重<br>正负样本的概念如下：<br>一张图像可能生成成千上万的候选框，但是其中只有很少一部分是包含目标的的，有目标的就是正样本，没有目标的就是负样本。</p>
<p>容易分类和难分类样本的概念如下：<br>假设存在一个二分类，样本1属于类别1的pt=0.9，样本2属于类别1的pt=0.6，显然前者更可能是类别1，其就是容易分类的样本；后者有可能是类别1，所以其为难分类样本。</p>
<p>如何实现权重控制呢：<br>以二分类为例，常用交叉熵loss:</p>
<p><img src="/CN/RetinaNet/20191101111610548.png" class="lazyload" data-srcset="/CN/RetinaNet/20191101111610548.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="img" style="zoom:50%;"></p>
<p>利用pt简化交叉熵损失：</p>
<p><img src="/CN/RetinaNet/20191101111700950.png" class="lazyload" data-srcset="/CN/RetinaNet/20191101111700950.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>因此得到：</p>
<p><img src="/CN/RetinaNet/20191101111712727.png" class="lazyload" data-srcset="/CN/RetinaNet/20191101111712727.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p><strong>想要降低负样本的影响，可以在常规的损失函数前增加一个系数αt。与Pt类似，当label=1的时候，αt=α；当label=otherwise的时候，αt=1 - α，a的范围也是0到1。此时我们便可以通过设置α实现控制正负样本对loss的贡献</strong><img src="/CN/RetinaNet/2019110111235956.png" class="lazyload" data-srcset="/CN/RetinaNet/2019110111235956.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>其中：</p>
<p><img src="/CN/RetinaNet/20200214164001867.png" class="lazyload" data-srcset="/CN/RetinaNet/20200214164001867.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>分解：</p>
<p><img src="/CN/RetinaNet/20200214164354541.jpg" class="lazyload" data-srcset="/CN/RetinaNet/20200214164354541.jpg" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>b)控制容易分类和难分类样本的权重</p>
<p>按照刚才的思路，一个二分类，样本1属于类别1的pt=0.9，样本2属于类别1的pt=0.6，也就是 <strong>是某个类的概率越大，其越容易分类</strong> 所以利用1-Pt就可以计算出其属于容易分类或者难分类。<br>具体实现方式如下。</p>
<p><img src="/CN/RetinaNet/20191101113143598.png" class="lazyload" data-srcset="/CN/RetinaNet/20191101113143598.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>其中调制系数为：</p>
<p><img src="/CN/RetinaNet/image-20220419103637923.png" class="lazyload" data-srcset="/CN/RetinaNet/image-20220419103637923.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220419103637923" style="zoom:50%;"></p>
<p>1、当pt趋于0的时候，调制系数趋于1，对于总的loss的贡献很大。当pt趋于1的时候，调制系数趋于0，也就是对于总的loss的贡献很小。<br>2、当γ=0的时候，focal loss就是传统的交叉熵损失，可以通过调整γ实现调制系数的改变。</p>
<p>c）两种权重控制方法合并<br>通过如下公式就可以实现控制正负样本的权重和控制容易分类和难分类样本的权重。</p>
<p><img src="/CN/RetinaNet/20191101114056207.png" class="lazyload" data-srcset="/CN/RetinaNet/20191101114056207.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>实现代码：</p>
<pre><code class="lang-python">class FocalLoss(nn.Module):
    def __init__(self):
        super(FocalLoss, self).__init__()

    def forward(self, classifications, regressions, anchors, annotations, alpha = 0.25, gamma = 2.0, cuda = True):
        # 设置
        dtype = regressions.dtype
        batch_size = classifications.shape[0]
        classification_losses = []
        regression_losses = []

        # 获得先验框，将先验框转换成中心宽高的形势
        anchor = anchors[0, :, :].to(dtype)
        # 转换成中心，宽高的形式
        anchor_widths = anchor[:, 3] - anchor[:, 1]
        anchor_heights = anchor[:, 2] - anchor[:, 0]
        anchor_ctr_x = anchor[:, 1] + 0.5 * anchor_widths
        anchor_ctr_y = anchor[:, 0] + 0.5 * anchor_heights

        for j in range(batch_size):
            # 取出真实框
            bbox_annotation = annotations[j]

            # 获得每张图片的分类结果和回归预测结果
            classification = classifications[j, :, :]
            regression = regressions[j, :, :]
            # 平滑标签
            classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4)

            if len(bbox_annotation) == 0:
                alpha_factor = torch.ones_like(classification) * alpha

                if cuda:
                    alpha_factor = alpha_factor.cuda()
                alpha_factor = 1. - alpha_factor
                focal_weight = classification
                focal_weight = alpha_factor * torch.pow(focal_weight, gamma)

                bce = -(torch.log(1.0 - classification))

                cls_loss = focal_weight * bce

                if cuda:
                    regression_losses.append(torch.tensor(0).to(dtype).cuda())
                else:
                    regression_losses.append(torch.tensor(0).to(dtype))
                classification_losses.append(cls_loss.sum())
                continue

            # 获得目标预测结果
            targets, num_positive_anchors, positive_indices, assigned_annotations = get_target(anchor, bbox_annotation, classification, cuda)

            alpha_factor = torch.ones_like(targets) * alpha
            if cuda:
                alpha_factor = alpha_factor.cuda()
            alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor)
            focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification)
            focal_weight = alpha_factor * torch.pow(focal_weight, gamma)

            bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification))

            cls_loss = focal_weight * bce

            zeros = torch.zeros_like(cls_loss)
            if cuda:
                zeros = zeros.cuda()
            cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, zeros)
            classification_losses.append(cls_loss.sum() / torch.clamp(num_positive_anchors.to(dtype), min=1.0))
            # smoooth_l1
            if positive_indices.sum() &gt; 0:
                targets = encode_bbox(assigned_annotations, positive_indices, anchor_widths, anchor_heights, anchor_ctr_x, anchor_ctr_y)

                regression_diff = torch.abs(targets - regression[positive_indices, :])

                regression_loss = torch.where(
                    torch.le(regression_diff, 1.0 / 9.0),
                    0.5 * 9.0 * torch.pow(regression_diff, 2),
                    regression_diff - 0.5 / 9.0
                )
                regression_losses.append(regression_loss.mean())
            else:
                if cuda:
                    regression_losses.append(torch.tensor(0).to(dtype).cuda())
                else:
                    regression_losses.append(torch.tensor(0).to(dtype))
        c_loss = torch.stack(classification_losses).mean()
        r_loss = torch.stack(regression_losses).mean()
        loss = c_loss + r_loss
        return loss, c_loss, r_loss
</code></pre>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title>vision transformer详解</title>
    <url>/CN/VIT/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文地址：<a href="https://arxiv.org/pdf/2010.11929.pdf">https://arxiv.org/pdf/2010.11929.pdf</a></p>
<p>源码地址：<a href="https://github.com/google-research/vision_transformer">google-research/vision_transformer (github.com)</a></p>
<p>文章引用源码：<a href="https://github.com/bubbliiiing/classification-pytorch">https://github.com/bubbliiiing/classification-pytorch</a></p>
<p>文章出处：<a href="https://blog.csdn.net/weixin_44791964/article/details/122637701">https://blog.csdn.net/weixin_44791964/article/details/122637701</a></p>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>Vision Transformer是Transformer的视觉版本，Transformer基本上已经成为了自然语言处理的标配，但是在视觉中的运用还受到限制。</p>
<p>Vision Transformer打破了这种NLP与CV的隔离，将Transformer应用于图像图块（patch）序列上，进一步完成图像分类任务。简单来理解，Vision Transformer就是将输入进来的图片，每隔一定的区域大小划分图片块。然后将划分后的图片块组合成序列，将组合后的结果传入Transformer特有的Multi-head Self-attention进行特征提取。最后利用Cls Token进行分类。</p>
<h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p><img src="/CN/VIT/image-20220418091306571.png" class="lazyload" data-srcset="/CN/VIT/image-20220418091306571.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>与寻常的分类网络类似，整个Vision Transformer可以分为两部分，一部分是特征提取部分，另一部分是分类部分。</p>
<p>在特征提取部分，VIT所做的工作是特征提取。特征提取部分在图片中的对应区域是Patch+Position Embedding和Transformer Encoder。Patch+Position Embedding的作用主要是对输入进来的图片进行分块处理，每隔一定的区域大小划分图片块。然后将划分后的图片块组合成序列。在获得序列信息后，传入Transformer Encoder进行特征提取，这是Transformer特有的Multi-head Self-attention结构，通过自注意力机制，关注每个图片块的重要程度。</p>
<p>在分类部分，VIT所做的工作是利用提取到的特征进行分类。在进行特征提取的时候，我们会在图片序列中添加上Cls Token，该Token会作为一个单位的序列信息一起进行特征提取，提取的过程中，该Cls Token会与其它的特征进行特征交互，融合其它图片序列的特征。最终，我们利用Multi-head Self-attention结构提取特征后的Cls Token进行全连接分类。</p>
<h3 id="网络结构详解"><a href="#网络结构详解" class="headerlink" title="网络结构详解"></a>网络结构详解</h3><h4 id="特征提取部分"><a href="#特征提取部分" class="headerlink" title="特征提取部分"></a>特征提取部分</h4><p>a）Patch+Position Embedding</p>
<p><img src="/CN/VIT/image-20220418091855460.png" class="lazyload" data-srcset="/CN/VIT/image-20220418091855460.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>该部分作用：对输入进来的图片进行分块处理，每隔一定的区域大小划分图片块。然后将划分后的图片块组合成序列。</p>
<p>该部分首先对输入进来的图片进行分块处理，处理方式其实很简单，使用的是现成的卷积。由于卷积使用的是滑动窗口的思想，我们只需要设定特定的步长，就可以输入进来的图片进行分块处理了。</p>
<p>在VIT中，我们常设置这个卷积的卷积核大小为16x16，步长也为16x16，此时卷积就会每隔16个像素点进行一次特征提取，由于卷积核大小为16x16，两个图片区域的特征提取过程就不会有重叠。当我们输入的图片是[224, 224, 3]的时候，我们可以获得一个[14, 14, 768]的特征层。<br><img src="/CN/VIT/58cc10deb7dc45ae90ae606966d7c724.gif" class="lazyload" data-srcset="/CN/VIT/58cc10deb7dc45ae90ae606966d7c724.gif" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>下一步就是将这个特征层组合成序列，组合的方式非常简单，就是将高宽维度进行平铺，[14, 14, 768]在高宽维度平铺后，获得一个196, 768的特征层。平铺完成后，我们会在图片序列中添加上Cls Token，该Token会作为一个单位的序列信息一起进行特征提取，图中的这个0*就是Cls Token，我们此时获得一个197, 768的特征层。<br><img src="/CN/VIT/image-20220418092240916.png" class="lazyload" data-srcset="/CN/VIT/image-20220418092240916.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>添加完成Cls Token后，再为所有特征添加上位置信息，这样网络才有区分不同区域的能力。添加方式其实也非常简单，我们生成一个197, 768的参数矩阵，这个参数矩阵是可训练的，把这个矩阵加上197, 768的特征层即可。</p>
<p>到这里，Patch+Position Embedding就构建完成了，构建代码如下：</p>
<pre><code class="lang-python"># [224, 224, 3]-&gt;[14, 14, 768]
class PatchEmbed(nn.Module):
    def __init__(self, input_shape=[224, 224], patch_size=16, in_chans=3, num_features=768, norm_layer=None, flatten=True):
        super().__init__()
        # 196 = 14 * 14
        self.num_patches    = (input_shape[0] // patch_size) * (input_shape[1] // patch_size)
        self.flatten        = flatten

        self.proj = nn.Conv2d(in_chans, num_features, kernel_size=patch_size, stride=patch_size)
        self.norm = norm_layer(num_features) if norm_layer else nn.Identity()

    def forward(self, x):
        x = self.proj(x)
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # BCHW -&gt; BNC
        x = self.norm(x)
        # x = [b, 196, 768]
        return x

class VisionTransformer(nn.Module):
    def __init__(
            self, input_shape=[224, 224], patch_size=16, in_chans=3, num_classes=1000, num_features=768,
            depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0.1, attn_drop_rate=0.1, drop_path_rate=0.1,
            norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=GELU
        ):
        super().__init__()
        #-----------------------------------------------#
        #   224, 224, 3 -&gt; 196, 768
        #-----------------------------------------------#
        self.patch_embed    = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features)
        num_patches         = (224 // patch_size) * (224 // patch_size)
        self.num_features   = num_features
        self.new_feature_shape = [int(input_shape[0] // patch_size), int(input_shape[1] // patch_size)]
        self.old_feature_shape = [int(224 // patch_size), int(224 // patch_size)]

        #--------------------------------------------------------------------------------------------------------------------#
        #   classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。
        #
        #   在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。
        #   此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。
        #   在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。
        #--------------------------------------------------------------------------------------------------------------------#
        #   196, 768 -&gt; 197, 768
        self.cls_token      = nn.Parameter(torch.zeros(1, 1, num_features))
        #--------------------------------------------------------------------------------------------------------------------#
        #   为网络提取到的特征添加上位置信息。
        #   以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768
        #   此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。
        #--------------------------------------------------------------------------------------------------------------------#
        #   197, 768 -&gt; 197, 768
        self.pos_embed      = nn.Parameter(torch.zeros(1, num_patches + 1, num_features))

    def forward_features(self, x):
        # x = [b, 196, 768]
        x = self.patch_embed(x)
        # cls_token = [b, 1, 768]
        cls_token = self.cls_token.expand(x.shape[0], -1, -1) 
        # x = [b, 197, 768]
        x = torch.cat((cls_token, x), dim=1)
        # [1, 1, 768]
        cls_token_pe = self.pos_embed[:, 0:1, :]
        # [1, 196, 768]
        img_token_pe = self.pos_embed[:, 1: , :]
        # [1, 196, 768]-&gt;[1, 14, 14, 768]-&gt;[1, 768, 14, 14]
        img_token_pe = img_token_pe.view(1, *self.old_feature_shape, -1).permute(0, 3, 1, 2)
        # 做插值，以防输入图片不是224*224
        img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode=&#39;bicubic&#39;, align_corners=False)
        # [1, 768, 14, 14]-&gt;[1, 14, 14, 768]-&gt;[1, 196, 768]
        img_token_pe = img_token_pe.permute(0, 2, 3, 1).flatten(1, 2)
        # [1, 197, 768]
        pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=1)

        x = self.pos_drop(x + pos_embed)
</code></pre>
<p>b）transformer encoder</p>
<p><img src="/CN/VIT/8ff82ad32b994a12bfc2356718ac9683.gif" class="lazyload" data-srcset="/CN/VIT/8ff82ad32b994a12bfc2356718ac9683.gif" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>在上一步<strong>获得shape为197, 768的序列信息</strong>后，将<strong>序列信息传入Transformer Encoder进行特征提取</strong>，这是Transformer特有的Multi-head Self-attention结构，<strong>通过自注意力机制，关注每个图片块的重要程度。</strong></p>
<p>1)self-attention结构解析</p>
<p>看懂Self-attention结构，其实看懂下面这个动图就可以了，动图中存在<strong>一个序列的三个单位输入</strong>，<strong>每一个序列单位的输入</strong>都可以通过<strong>三个处理（比如全连接）获得Query、Key、Value</strong>，Query是查询向量、Key是键向量、Value值向量。</p>
<p><img src="/CN/VIT/32c551decdb64331a1c4ec0471cc1f3d.gif" class="lazyload" data-srcset="/CN/VIT/32c551decdb64331a1c4ec0471cc1f3d.gif" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>如果我们想要获得input-1的输出，那么我们进行如下几步：<br>1、利用input-1的查询向量，分别乘上input-1、input-2、input-3的键向量，此时我们获得了三个score。<br>2、然后对这三个score取softmax，获得了input-1、input-2、input-3各自的重要程度。<br>3、然后将这个重要程度乘上input-1、input-2、input-3的值向量，求和。<br>4、此时我们获得了input-1的输出。</p>
<p>如图所示，我们进行如下几步：<br>1、input-1的查询向量为[1, 0, 2]，分别乘上input-1、input-2、input-3的键向量，获得三个score为2，4，4。<br>2、然后对这三个score取softmax，获得了input-1、input-2、input-3各自的重要程度，获得三个重要程度为0.0，0.5，0.5。<br>3、然后将这个重要程度乘上input-1、input-2、input-3的值向量，求和，即<br>0.0 ∗ [ 1 , 2 , 3 ] + 0.5 ∗ [ 2 , 8 , 0 ] + 0.5 ∗ [ 2 , 6 , 3 ] = [ 2.0 , 7.0 , 1.5 ]<br>4、此时我们获得了input-1的输出 [2.0, 7.0, 1.5]。</p>
<p>上述的例子中，序列长度仅为3，每个单位序列的特征长度仅为3，在VIT的Transformer Encoder中，序列长度为197，每个单位序列的特征长度为768 // num_heads。但计算过程是一样的。在实际运算时，我们采用矩阵进行运算。<br>2)self-attention的矩阵运算</p>
<p>实际的矩阵运算过程如下图所示。我以实际矩阵为例子给大家解析：</p>
<p><img src="/CN/VIT/19f323060f1f41ba99e743cea1fa5174.png" class="lazyload" data-srcset="/CN/VIT/19f323060f1f41ba99e743cea1fa5174.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>输入的Query、Key、Value如下图所示：</p>
<p><img src="/CN/VIT/2500484f29ae4671944a06543ad3e026.png" class="lazyload" data-srcset="/CN/VIT/2500484f29ae4671944a06543ad3e026.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>首先利用 查询向量query 叉乘 转置后的键向量key，这一步可以通俗的理解为，利用查询向量去查询序列的特征，获得序列每个部分的重要程度score。</p>
<p>输出的每一行，都代表input-1、input-2、input-3，对当前input的贡献，我们对这个贡献值取一个softmax。<br><img src="/CN/VIT/image-20220418104816862.png" class="lazyload" data-srcset="/CN/VIT/image-20220418104816862.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>然后利用 score 叉乘 value，<strong>这一步可以通俗的理解为，将序列每个部分的重要程度重新施加到序列的值上去。</strong></p>
<p><img src="/CN/VIT/c41d889912a64057ab571bdfd5458910.png" class="lazyload" data-srcset="/CN/VIT/c41d889912a64057ab571bdfd5458910.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>矩阵代码运算如下：</p>
<pre><code class="lang-python">import numpy as np

def soft_max(z):
    t = np.exp(z)
    a = np.exp(z) / np.expand_dims(np.sum(t, axis=1), 1)
    return a

Query = np.array([
    [1,0,2],
    [2,2,2],
    [2,1,3]
])

Key = np.array([
    [0,1,1],
    [4,4,0],
    [2,3,1]
])

Value = np.array([
    [1,2,3],
    [2,8,0],
    [2,6,3]
])

scores = Query @ Key.T
print(scores)
scores = soft_max(scores)
print(scores)
out = scores @ Value
print(out)
</code></pre>
<p>3）Multihead多头注意力机制</p>
<p>多头注意力机制的示意图如图所示：</p>
<p><img src="/CN/VIT/430e12e75fd44c82ac95e504b5da0d50.png" class="lazyload" data-srcset="/CN/VIT/430e12e75fd44c82ac95e504b5da0d50.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>这幅图给人的感觉略显迷茫，我们跳脱出这个图，直接从矩阵的shape入手会清晰很多。</p>
<p>在第一步进行图像的分割后，我们获得的特征层为197, 768。</p>
<p>在施加多头的时候，我们直接对196, 768的最后一维度进行分割，比如我们想分割成12个头，那么矩阵的shape就变成了196, 12, 64。</p>
<p>然后我们将196, 12, 64进行转置，将12放到前面去，获得的特征层为12, 196, 64。之后我们忽略这个12，把它和batch维度同等对待，只对196, 64进行处理，其实也就是上面的注意力机制的过程了。</p>
<p><img src="/CN/VIT/90787898063c45fe888c136ba4b32e64.png" class="lazyload" data-srcset="/CN/VIT/90787898063c45fe888c136ba4b32e64.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="img" style="zoom:50%;"></p>
<pre><code class="lang-python">#--------------------------------------------------------------------------#
#   Attention机制
#   将输入的特征qkv特征进行划分，首先生成query, key, value。query是查询向量、key是键向量、v是值向量。
#   然后利用 查询向量query 叉乘 转置后的键向量key，这一步可以通俗的理解为，利用查询向量去查询序列的特征，获得序列每个部分的重要程度score。
#   然后利用 score 叉乘 value，这一步可以通俗的理解为，将序列每个部分的重要程度重新施加到序列的值上去。
#--------------------------------------------------------------------------#
class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads  = num_heads
        self.scale      = (dim // num_heads) ** -0.5
        # 768-&gt;768*3
        self.qkv        = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop  = nn.Dropout(attn_drop)
        self.proj       = nn.Linear(dim, dim)
        self.proj_drop  = nn.Dropout(proj_drop)

    def forward(self, x):
        # batch, 196, 768
        B, N, C     = x.shape
        # batch, 196, 768 -&gt; batch, 196, 768*3 -&gt; batch, 196, 3, 8, 768/8=96 -&gt; 3, batch, 8, 196, 96
        qkv         = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        # 3 * 1, batch, 8, 196, 96  -&gt; q, k, v = batch: 16, head: 8, patch: 196, each_head_attention_channels: 96
        q, k, v     = qkv[0], qkv[1], qkv[2]
        # batch, 8, 196, 96 @ batch, 8, 96, 196 -&gt; batch, 8, 196, 196
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        # batch, 8, 196, 196 @ batch, 8, 196, 96 -&gt; batch, 8, 196, 96 -&gt; batch, 196, 8, 96 -&gt; batch, 196, 768
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        # batch, 196, 768 -&gt; batch, 196, 768
        x = self.proj(x)
        # Dropout(batch, 196, 768)
        x = self.proj_drop(x)
        return x
</code></pre>
<p>4）TransformerBlock的构建</p>
<p><img src="/CN/VIT/4036cdfc91a6477d91009d574788a78b.png" class="lazyload" data-srcset="/CN/VIT/4036cdfc91a6477d91009d574788a78b.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p><strong>在完成MultiHeadSelfAttention的构建后，我们需要在其后加上两个全连接。就构建了整个TransformerBlock</strong></p>
<p>block流程见下图：</p>
<p><img src="/CN/VIT/e3bf360d541c4eb1a243e100f17a48b6.png" class="lazyload" data-srcset="/CN/VIT/e3bf360d541c4eb1a243e100f17a48b6.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="img" style="zoom: 80%;"></p>
<pre><code class="lang-python">class Mlp(nn.Module):
    &quot;&quot;&quot; MLP as used in Vision Transformer, MLP-Mixer and related networks
    &quot;&quot;&quot;
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=GELU, drop=0.):
        super().__init__()
        out_features    = out_features or in_features
        hidden_features = hidden_features or in_features
        drop_probs      = (drop, drop)

        self.fc1    = nn.Linear(in_features, hidden_features)
        self.act    = act_layer()
        self.drop1  = nn.Dropout(drop_probs[0])
        self.fc2    = nn.Linear(hidden_features, out_features)
        self.drop2  = nn.Dropout(drop_probs[1])

    def forward(self, x):
        # batch, 196, 768 -&gt; batch, 196, 768
        x = self.fc1(x)
        # batch, 196, 768 -&gt; batch, 196, 768
        x = self.act(x)
        # batch, 196, 768 -&gt; batch, 196, 768
        x = self.drop1(x)
        # batch, 196, 768 -&gt; batch, 196, 768
        x = self.fc2(x)
        # batch, 196, 768 -&gt; batch, 196, 768
        x = self.drop2(x)
        return x

#  a transoformer encoder block
class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1      = norm_layer(dim)
        self.attn       = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
        self.norm2      = norm_layer(dim)
        self.mlp        = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
        self.drop_path  = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()

    def forward(self, x):
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x
</code></pre>
<p>c）VIT模型构建</p>
<p><img src="/CN/VIT/image-20220418105537648.png" class="lazyload" data-srcset="/CN/VIT/image-20220418105537648.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>整个VIT模型由一个Patch+Position Embedding加上多个TransformerBlock组成。典型的TransforerBlock的数量为12个</p>
<pre><code class="lang-python">class VisionTransformer(nn.Module):
    def __init__(
            self, input_shape=[224, 224], patch_size=16, in_chans=3, num_classes=1000, num_features=768,
            depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0.1, attn_drop_rate=0.1, drop_path_rate=0.1,
            norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=GELU
        ):
        super(VisionTransformer, self).__init__()
        #-----------------------------------------------#
        #   224, 224, 3 -&gt; batch, 196, 768
        #-----------------------------------------------#
        self.patch_embed    = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features)
        num_patches         = (224 // patch_size) * (224 // patch_size)
        self.num_features   = num_features
        self.new_feature_shape = [int(input_shape[0] // patch_size), int(input_shape[1] // patch_size)]
        self.old_feature_shape = [int(224 // patch_size), int(224 // patch_size)]

        #--------------------------------------------------------------------------------------------------------------------#
        #   classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。
        #
        #   在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。
        #   此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。
        #   在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。
        #--------------------------------------------------------------------------------------------------------------------#
        #   1, 1, 768
        self.cls_token      = nn.Parameter(torch.zeros(1, 1, num_features))
        #--------------------------------------------------------------------------------------------------------------------#
        #   为网络提取到的特征添加上位置信息。
        #   以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768
        #   此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。
        #--------------------------------------------------------------------------------------------------------------------#
        #   1, 197, 768
        self.pos_embed      = nn.Parameter(torch.zeros(1, num_patches + 1, num_features))
        # 1, 197, 768
        self.pos_drop       = nn.Dropout(p=drop_rate)

        #-----------------------------------------------#
        #   197, 768 -&gt; 197, 768  12次
        #-----------------------------------------------#
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        self.blocks = nn.Sequential(
            *[
                Block(
                    dim         = num_features, 
                    num_heads   = num_heads, 
                    mlp_ratio   = mlp_ratio, 
                    qkv_bias    = qkv_bias, 
                    drop        = drop_rate,
                    attn_drop   = attn_drop_rate, 
                    drop_path   = dpr[i], 
                    norm_layer  = norm_layer, 
                    act_layer   = act_layer
                )for i in range(depth)
            ]
        )
        self.norm = norm_layer(num_features)
        self.head = nn.Linear(num_features, num_classes) if num_classes &gt; 0 else nn.Identity()

    def forward_features(self, x):
        x = self.patch_embed(x)
        cls_token = self.cls_token.expand(x.shape[0], -1, -1) 
        x = torch.cat((cls_token, x), dim=1)

        cls_token_pe = self.pos_embed[:, 0:1, :]
        img_token_pe = self.pos_embed[:, 1: , :]

        img_token_pe = img_token_pe.view(1, *self.old_feature_shape, -1).permute(0, 3, 1, 2)
        img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode=&#39;bicubic&#39;, align_corners=False)
        img_token_pe = img_token_pe.permute(0, 2, 3, 1).flatten(1, 2)
        pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=1)

        x = self.pos_drop(x + pos_embed)
        x = self.blocks(x)
        x = self.norm(x)
        return x[:, 0]

    def forward(self, x):
        # # 整个Transformer Encoder = batch, 768
        x = self.forward_features(x)
        # 最后的MLP Header = batch, 768 -&gt; 768 -&gt; 1000 -&gt; batch, 1000
        x = self.head(x)
        return x

    def freeze_backbone(self):
        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]]
        for module in backbone:
            try:
                for param in module.parameters():
                    param.requires_grad = False
            except:
                module.requires_grad = False

    def Unfreeze_backbone(self):
        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]]
        for module in backbone:
            try:
                for param in module.parameters():
                    param.requires_grad = True
            except:
                module.requires_grad = True
</code></pre>
<h4 id="分类部分"><a href="#分类部分" class="headerlink" title="分类部分"></a>分类部分</h4><p><img src="/CN/VIT/image-20220418105537648.png" class="lazyload" data-srcset="/CN/VIT/image-20220418105537648.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>在分类部分，VIT所做的工作是利用提取到的特征进行分类。</p>
<p>在进行特征提取的时候，我们会在图片序列中添加上Cls Token，该Token会作为一个单位的序列信息一起进行特征提取，提取的过程中，该Cls Token会与其它的特征进行特征交互，融合其它图片序列的特征。</p>
<p>最终，我们利用Multi-head Self-attention结构提取特征后的Cls Token进行全连接分类。</p>
<pre><code class="lang-python">class VisionTransformer(nn.Module):
    def __init__(
            self, input_shape=[224, 224], patch_size=16, in_chans=3, num_classes=1000, num_features=768,
            depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0.1, attn_drop_rate=0.1, drop_path_rate=0.1,
            norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=GELU
        ):
        super().__init__()
        #-----------------------------------------------#
        #   224, 224, 3 -&gt; 196, 768
        #-----------------------------------------------#
        self.patch_embed    = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features)
        num_patches         = (224 // patch_size) * (224 // patch_size)
        self.num_features   = num_features
        self.new_feature_shape = [int(input_shape[0] // patch_size), int(input_shape[1] // patch_size)]
        self.old_feature_shape = [int(224 // patch_size), int(224 // patch_size)]

        #--------------------------------------------------------------------------------------------------------------------#
        #   classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。
        #
        #   在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。
        #   此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。
        #   在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。
        #--------------------------------------------------------------------------------------------------------------------#
        #   196, 768 -&gt; 197, 768
        self.cls_token      = nn.Parameter(torch.zeros(1, 1, num_features))
        #--------------------------------------------------------------------------------------------------------------------#
        #   为网络提取到的特征添加上位置信息。
        #   以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768
        #   此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。
        #--------------------------------------------------------------------------------------------------------------------#
        #   197, 768 -&gt; 197, 768
        self.pos_embed      = nn.Parameter(torch.zeros(1, num_patches + 1, num_features))
        self.pos_drop       = nn.Dropout(p=drop_rate)

        #-----------------------------------------------#
        #   197, 768 -&gt; 197, 768  12次
        #-----------------------------------------------#
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        self.blocks = nn.Sequential(
            *[
                Block(
                    dim         = num_features, 
                    num_heads   = num_heads, 
                    mlp_ratio   = mlp_ratio, 
                    qkv_bias    = qkv_bias, 
                    drop        = drop_rate,
                    attn_drop   = attn_drop_rate, 
                    drop_path   = dpr[i], 
                    norm_layer  = norm_layer, 
                    act_layer   = act_layer
                )for i in range(depth)
            ]
        )
        self.norm = norm_layer(num_features)
        self.head = nn.Linear(num_features, num_classes) if num_classes &gt; 0 else nn.Identity()

    def forward_features(self, x):
        x = self.patch_embed(x)
        cls_token = self.cls_token.expand(x.shape[0], -1, -1) 
        x = torch.cat((cls_token, x), dim=1)

        cls_token_pe = self.pos_embed[:, 0:1, :]
        img_token_pe = self.pos_embed[:, 1: , :]

        img_token_pe = img_token_pe.view(1, *self.old_feature_shape, -1).permute(0, 3, 1, 2)
        img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode=&#39;bicubic&#39;, align_corners=False)
        img_token_pe = img_token_pe.permute(0, 2, 3, 1).flatten(1, 2)
        pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=1)

        x = self.pos_drop(x + pos_embed)
        x = self.blocks(x)
        x = self.norm(x)
        return x[:, 0]

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x

    def freeze_backbone(self):
        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]]
        for module in backbone:
            try:
                for param in module.parameters():
                    param.requires_grad = False
            except:
                module.requires_grad = False

    def Unfreeze_backbone(self):
        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]]
        for module in backbone:
            try:
                for param in module.parameters():
                    param.requires_grad = True
            except:
                module.requires_grad = True
</code></pre>
<h2 id="VIT构建代码"><a href="#VIT构建代码" class="headerlink" title="VIT构建代码"></a>VIT构建代码</h2><pre><code class="lang-python">import math
from collections import OrderedDict
from functools import partial

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

#--------------------------------------#
#   Gelu激活函数的实现
#   利用近似的数学公式
#--------------------------------------#
class GELU(nn.Module):
    def __init__(self):
        super(GELU, self).__init__()

    def forward(self, x):
        return 0.5 * x * (1 + F.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * torch.pow(x,3))))

def drop_path(x, drop_prob: float = 0., training: bool = False):
    if drop_prob == 0. or not training:
        return x
    keep_prob       = 1 - drop_prob
    shape           = (x.shape[0],) + (1,) * (x.ndim - 1)
    random_tensor   = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_() 
    output          = x.div(keep_prob) * random_tensor
    return output

class DropPath(nn.Module):
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)

class PatchEmbed(nn.Module):
    def __init__(self, input_shape=[224, 224], patch_size=16, in_chans=3, num_features=768, norm_layer=None, flatten=True):
        super().__init__()
        self.num_patches    = (input_shape[0] // patch_size) * (input_shape[1] // patch_size)
        self.flatten        = flatten

        self.proj = nn.Conv2d(in_chans, num_features, kernel_size=patch_size, stride=patch_size)
        self.norm = norm_layer(num_features) if norm_layer else nn.Identity()

    def forward(self, x):
        x = self.proj(x)
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # BCHW -&gt; BNC
        x = self.norm(x)
        return x

#--------------------------------------------------------------------------------------------------------------------#
#   Attention机制
#   将输入的特征qkv特征进行划分，首先生成query, key, value。query是查询向量、key是键向量、v是值向量。
#   然后利用 查询向量query 叉乘 转置后的键向量key，这一步可以通俗的理解为，利用查询向量去查询序列的特征，获得序列每个部分的重要程度score。
#   然后利用 score 叉乘 value，这一步可以通俗的理解为，将序列每个部分的重要程度重新施加到序列的值上去。
#--------------------------------------------------------------------------------------------------------------------#
class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads  = num_heads
        self.scale      = (dim // num_heads) ** -0.5

        self.qkv        = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop  = nn.Dropout(attn_drop)
        self.proj       = nn.Linear(dim, dim)
        self.proj_drop  = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C     = x.shape
        qkv         = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v     = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

class Mlp(nn.Module):
    &quot;&quot;&quot; MLP as used in Vision Transformer, MLP-Mixer and related networks
    &quot;&quot;&quot;
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=GELU, drop=0.):
        super().__init__()
        out_features    = out_features or in_features
        hidden_features = hidden_features or in_features
        drop_probs      = (drop, drop)

        self.fc1    = nn.Linear(in_features, hidden_features)
        self.act    = act_layer()
        self.drop1  = nn.Dropout(drop_probs[0])
        self.fc2    = nn.Linear(hidden_features, out_features)
        self.drop2  = nn.Dropout(drop_probs[1])

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop1(x)
        x = self.fc2(x)
        x = self.drop2(x)
        return x

class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1      = norm_layer(dim)
        self.attn       = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
        self.norm2      = norm_layer(dim)
        self.mlp        = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
        self.drop_path  = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()

    def forward(self, x):
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x

class VisionTransformer(nn.Module):
    def __init__(
            self, input_shape=[224, 224], patch_size=16, in_chans=3, num_classes=1000, num_features=768,
            depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0.1, attn_drop_rate=0.1, drop_path_rate=0.1,
            norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=GELU
        ):
        super().__init__()
        #-----------------------------------------------#
        #   224, 224, 3 -&gt; 196, 768
        #-----------------------------------------------#
        self.patch_embed    = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features)
        num_patches         = (224 // patch_size) * (224 // patch_size)
        self.num_features   = num_features
        self.new_feature_shape = [int(input_shape[0] // patch_size), int(input_shape[1] // patch_size)]
        self.old_feature_shape = [int(224 // patch_size), int(224 // patch_size)]

        #--------------------------------------------------------------------------------------------------------------------#
        #   classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。
        #
        #   在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。
        #   此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。
        #   在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。
        #--------------------------------------------------------------------------------------------------------------------#
        #   196, 768 -&gt; 197, 768
        self.cls_token      = nn.Parameter(torch.zeros(1, 1, num_features))
        #--------------------------------------------------------------------------------------------------------------------#
        #   为网络提取到的特征添加上位置信息。
        #   以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768
        #   此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。
        #--------------------------------------------------------------------------------------------------------------------#
        #   197, 768 -&gt; 197, 768
        self.pos_embed      = nn.Parameter(torch.zeros(1, num_patches + 1, num_features))
        self.pos_drop       = nn.Dropout(p=drop_rate)

        #-----------------------------------------------#
        #   197, 768 -&gt; 197, 768  12次
        #-----------------------------------------------#
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        self.blocks = nn.Sequential(
            *[
                Block(
                    dim         = num_features, 
                    num_heads   = num_heads, 
                    mlp_ratio   = mlp_ratio, 
                    qkv_bias    = qkv_bias, 
                    drop        = drop_rate,
                    attn_drop   = attn_drop_rate, 
                    drop_path   = dpr[i], 
                    norm_layer  = norm_layer, 
                    act_layer   = act_layer
                )for i in range(depth)
            ]
        )
        self.norm = norm_layer(num_features)
        self.head = nn.Linear(num_features, num_classes) if num_classes &gt; 0 else nn.Identity()

    def forward_features(self, x):
        x = self.patch_embed(x)
        cls_token = self.cls_token.expand(x.shape[0], -1, -1) 
        x = torch.cat((cls_token, x), dim=1)

        cls_token_pe = self.pos_embed[:, 0:1, :]
        img_token_pe = self.pos_embed[:, 1: , :]

        img_token_pe = img_token_pe.view(1, *self.old_feature_shape, -1).permute(0, 3, 1, 2)
        img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode=&#39;bicubic&#39;, align_corners=False)
        img_token_pe = img_token_pe.permute(0, 2, 3, 1).flatten(1, 2)
        pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=1)

        x = self.pos_drop(x + pos_embed)
        x = self.blocks(x)
        x = self.norm(x)
        return x[:, 0]

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x

    def freeze_backbone(self):
        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]]
        for module in backbone:
            try:
                for param in module.parameters():
                    param.requires_grad = False
            except:
                module.requires_grad = False

    def Unfreeze_backbone(self):
        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]]
        for module in backbone:
            try:
                for param in module.parameters():
                    param.requires_grad = True
            except:
                module.requires_grad = True


def vit(input_shape=[224, 224], pretrained=False, num_classes=1000):
    model = VisionTransformer(input_shape)
    if pretrained:
        model.load_state_dict(torch.load(&quot;model_data/vit-patch_16.pth&quot;))

    if num_classes!=1000:
        model.head = nn.Linear(model.num_features, num_classes)
    return model
</code></pre>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>yolov3解读</title>
    <url>/CN/YOLOV3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文地址：<a href="https://arxiv.org/pdf/1804.02767.pdf">https://arxiv.org/pdf/1804.02767.pdf</a></p>
<p>源码地址：<a href="https://github.com/ultralytics/yolov3">ultralytics/yolov3</a></p>
<p>文章引用源码：<a href="https://github.com/bubbliiiing/yolo3-pytorch">https://github.com/bubbliiiing/yolo3-pytorch</a></p>
<p>文章出处：<a href="https://blog.csdn.net/weixin_44791964/article/details/105310627">https://blog.csdn.net/weixin_44791964/article/details/105310627</a></p>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><h3 id="预测部分"><a href="#预测部分" class="headerlink" title="预测部分"></a>预测部分</h3><h4 id="主干网络Darknet53"><a href="#主干网络Darknet53" class="headerlink" title="主干网络Darknet53"></a>主干网络Darknet53</h4><p><img src="/CN/YOLOV3/image-20220416194945035.png" class="lazyload" data-srcset="/CN/YOLOV3/image-20220416194945035.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>YoloV3所使用的主干特征提取网络为Darknet53，它具有两个重要特点：<br>1、Darknet53具有一个重要特点是使用了残差网络Residual，Darknet53中的残差卷积就是首先进行一次卷积核大小为3X3、步长为2的卷积，该卷积会压缩输入进来的特征层的宽和高，此时我们可以获得一个特征层，我们将该特征层命名为layer。之后我们再对该特征层进行一次1X1的卷积和一次3X3的卷积，并把这个结果加上layer，此时我们便构成了残差结构。通过不断的1X1卷积和3X3卷积以及残差边的叠加，我们便大幅度的加深了网络。残差网络的特点是容易优化，并且能够通过增加相当的深度来提高准确率。其内部的残差块使用了跳跃连接，缓解了在深度神经网络中增加深度带来的梯度消失问题。</p>
<p>2、Darknet53的每一个卷积部分使用了特有的DarknetConv2D结构，每一次卷积的时候进行l2正则化，完成卷积后进行BatchNormalization标准化与LeakyReLU。普通的ReLU是将所有的负值都设为零，Leaky ReLU则是给所有负值赋予一个非零斜率。以数学的方式我们可以表示为：</p>
<p><img src="/CN/YOLOV3/image-20220419102411716.png" class="lazyload" data-srcset="/CN/YOLOV3/image-20220419102411716.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>实现代码：</p>
<pre><code>python
# 详情见nets/darknet.py
#---------------------------------------------------------------------#
#   残差结构
#   利用一个1x1卷积下降通道数，然后利用一个3x3卷积提取特征并且上升通道数
#   最后接上一个残差边
#---------------------------------------------------------------------#
class BasicBlock(nn.Module):
    def __init__(self, inplanes, planes):
        super(BasicBlock, self).__init__()
        self.conv1  = nn.Conv2d(inplanes, planes[0], kernel_size=1, stride=1, padding=0, bias=False)
        self.bn1    = nn.BatchNorm2d(planes[0])
        self.relu1  = nn.LeakyReLU(0.1)

        self.conv2  = nn.Conv2d(planes[0], planes[1], kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2    = nn.BatchNorm2d(planes[1])
        self.relu2  = nn.LeakyReLU(0.1)

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu1(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu2(out)

        out += residual
        return out

class DarkNet(nn.Module):
    def __init__(self, layers):
        super(DarkNet, self).__init__()
        self.inplanes = 32
        # 416,416,3 -&gt; 416,416,32
        self.conv1  = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1    = nn.BatchNorm2d(self.inplanes)
        self.relu1  = nn.LeakyReLU(0.1)

        # 416,416,32 -&gt; 208,208,64
        self.layer1 = self._make_layer([32, 64], layers[0])
        # 208,208,64 -&gt; 104,104,128
        self.layer2 = self._make_layer([64, 128], layers[1])
        # 104,104,128 -&gt; 52,52,256
        self.layer3 = self._make_layer([128, 256], layers[2])
        # 52,52,256 -&gt; 26,26,512
        self.layer4 = self._make_layer([256, 512], layers[3])
        # 26,26,512 -&gt; 13,13,1024
        self.layer5 = self._make_layer([512, 1024], layers[4])

        self.layers_out_filters = [64, 128, 256, 512, 1024]

        # 进行权值初始化
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    #---------------------------------------------------------------------#
    #   在每一个layer里面，首先利用一个步长为2的3x3卷积进行下采样
    #   然后进行残差结构的堆叠
    #---------------------------------------------------------------------#
    def _make_layer(self, planes, blocks):
        layers = []
        # 下采样，步长为2，卷积核大小为3
        layers.append((&quot;ds_conv&quot;, nn.Conv2d(self.inplanes, planes[1], kernel_size=3, stride=2, padding=1, bias=False)))
        layers.append((&quot;ds_bn&quot;, nn.BatchNorm2d(planes[1])))
        layers.append((&quot;ds_relu&quot;, nn.LeakyReLU(0.1)))
        # 加入残差结构
        self.inplanes = planes[1]
        for i in range(0, blocks):
            layers.append((&quot;residual_&#123;&#125;&quot;.format(i), BasicBlock(self.inplanes, planes)))
        return nn.Sequential(OrderedDict(layers))

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu1(x)

        x = self.layer1(x)
        x = self.layer2(x)
        out3 = self.layer3(x)
        out4 = self.layer4(out3)
        out5 = self.layer5(out4)

        return out3, out4, out5

def darknet53():
    model = DarkNet([1, 2, 8, 8, 4])
    return model
</code></pre><h4 id="从特征层获取预测结果"><a href="#从特征层获取预测结果" class="headerlink" title="从特征层获取预测结果"></a>从特征层获取预测结果</h4><p><img src="/CN/YOLOV3/image-20220416194945035.png" class="lazyload" data-srcset="/CN/YOLOV3/image-20220416194945035.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>从特征获取预测结果的过程可以分为两个部分，分别是：</p>
<p>·构建FPN特征金字塔进行加强特征提取。<br>·利用Yolo Head对三个有效特征层进行预测。<br>a）构建FPN特征金字塔进行加强特征提取<br>在特征利用部分，YoloV3提取多特征层进行目标检测，一共提取三个特征层。三个特征层位于主干部分Darknet53的不同位置，分别位于中间层，中下层，底层，三个特征层的shape分别为(52,52,256)、(26,26,512)、(13,13,1024)。</p>
<p>在获得三个有效特征层后，我们利用这三个有效特征层进行FPN层的构建，构建方式为：</p>
<p>·13x13x1024的特征层进行5次卷积处理，处理完后利用YoloHead获得预测结果，一部分用于进行上采样UmSampling2d后与26x26x512特征层进行结合，结合特征层的shape为(26,26,768)。<br>·结合特征层再次进行5次卷积处理，处理完后利用YoloHead获得预测结果，一部分用于进行上采样UmSampling2d后与52x52x256特征层进行结合，结合特征层的shape为(52,52,384)。<br>·结合特征层再次进行5次卷积处理，处理完后利用YoloHead获得预测结果。<br>特征金字塔可以将不同shape的特征层进行特征融合，有利于提取出更好的特征。</p>
<p>b）利用Yolo Head获得预测结果<br>利用FPN特征金字塔，我们可以获得三个加强特征，这三个加强特征的shape分别为(13,13,512)、(26,26,256)、(52,52,128)，然后我们利用这三个shape的特征层传入Yolo Head获得预测结果。</p>
<p>Yolo Head本质上是一次3x3卷积加上一次1x1卷积，3x3卷积的作用是特征整合，1x1卷积的作用是调整通道数。</p>
<p>对三个特征层分别进行处理，假设我们预测是的VOC数据集，我们的输出层的shape分别为(13,13,75)，(26,26,75)，(52,52,75)，最后一个维度为75是因为该图是基于voc数据集的，它的类为20种，YoloV3针对每一个特征层的每一个特征点存在3个先验框，所以预测结果的通道数为3x25；<br>如果使用的是coco训练集，类则为80种，最后的维度应该为255 = 3x85，三个特征层的shape为(13,13,255)，(26,26,255)，(52,52,255)</p>
<p>其实际情况就是，输入N张416x416的图片，在经过多层的运算后，会输出三个shape分别为(N,13,13,255)，(N,26,26,255)，(N,52,52,255)的数据，对应每个图分为13x13、26x26、52x52的网格上3个先验框的位置。</p>
<p>实现代码如下：</p>
<pre><code class="lang-python"># 详情见nets/yolo.py
def conv2d(filter_in, filter_out, kernel_size):
    pad = (kernel_size - 1) // 2 if kernel_size else 0
    return nn.Sequential(OrderedDict([
        (&quot;conv&quot;, nn.Conv2d(filter_in, filter_out, kernel_size=kernel_size, stride=1, padding=pad, bias=False)),
        (&quot;bn&quot;, nn.BatchNorm2d(filter_out)),
        (&quot;relu&quot;, nn.LeakyReLU(0.1)),
    ]))

#------------------------------------------------------------------------#
#   make_last_layers里面一共有七个卷积，前五个用于提取特征。
#   后两个用于获得yolo网络的预测结果
#------------------------------------------------------------------------#
def make_last_layers(filters_list, in_filters, out_filter):
    m = nn.Sequential(
        conv2d(in_filters, filters_list[0], 1),
        conv2d(filters_list[0], filters_list[1], 3),
        conv2d(filters_list[1], filters_list[0], 1),
        conv2d(filters_list[0], filters_list[1], 3),
        conv2d(filters_list[1], filters_list[0], 1),
        conv2d(filters_list[0], filters_list[1], 3),
        nn.Conv2d(filters_list[1], out_filter, kernel_size=1, stride=1, padding=0, bias=True)
    )
    return m

class YoloBody(nn.Module):
    def __init__(self, anchors_mask, num_classes):
        super(YoloBody, self).__init__()
        #---------------------------------------------------#   
        #   生成darknet53的主干模型
        #   获得三个有效特征层，他们的shape分别是：
        #   52,52,256
        #   26,26,512
        #   13,13,1024
        #---------------------------------------------------#
        self.backbone = darknet53()

        #---------------------------------------------------#
        #   out_filters : [64, 128, 256, 512, 1024]
        #---------------------------------------------------#
        out_filters = self.backbone.layers_out_filters

        #------------------------------------------------------------------------#
        #   计算yolo_head的输出通道数，对于voc数据集而言
        #   final_out_filter0 = final_out_filter1 = final_out_filter2 = 75
        #------------------------------------------------------------------------#
        self.last_layer0            = make_last_layers([512, 1024], out_filters[-1], len(anchors_mask[0]) * (num_classes + 5))

        self.last_layer1_conv       = conv2d(512, 256, 1)
        self.last_layer1_upsample   = nn.Upsample(scale_factor=2, mode=&#39;nearest&#39;)
        self.last_layer1            = make_last_layers([256, 512], out_filters[-2] + 256, len(anchors_mask[1]) * (num_classes + 5))

        self.last_layer2_conv       = conv2d(256, 128, 1)
        self.last_layer2_upsample   = nn.Upsample(scale_factor=2, mode=&#39;nearest&#39;)
        self.last_layer2            = make_last_layers([128, 256], out_filters[-3] + 128, len(anchors_mask[2]) * (num_classes + 5))

    def forward(self, x):
        #---------------------------------------------------#   
        #   获得三个有效特征层，他们的shape分别是：
        #   52,52,256；26,26,512；13,13,1024
        #---------------------------------------------------#
        x2, x1, x0 = self.backbone(x)

        #---------------------------------------------------#
        #   第一个特征层
        #   out0 = (batch_size,255,13,13)
        #---------------------------------------------------#
        # 13,13,1024 -&gt; 13,13,512 -&gt; 13,13,1024 -&gt; 13,13,512 -&gt; 13,13,1024 -&gt; 13,13,512
        out0_branch = self.last_layer0[:5](x0)
        out0        = self.last_layer0[5:](out0_branch)

        # 13,13,512 -&gt; 13,13,256 -&gt; 26,26,256
        x1_in = self.last_layer1_conv(out0_branch)
        x1_in = self.last_layer1_upsample(x1_in)

        # 26,26,256 + 26,26,512 -&gt; 26,26,768
        x1_in = torch.cat([x1_in, x1], 1)
        #---------------------------------------------------#
        #   第二个特征层
        #   out1 = (batch_size,255,26,26)
        #---------------------------------------------------#
        # 26,26,768 -&gt; 26,26,256 -&gt; 26,26,512 -&gt; 26,26,256 -&gt; 26,26,512 -&gt; 26,26,256
        out1_branch = self.last_layer1[:5](x1_in)
        out1        = self.last_layer1[5:](out1_branch)

        # 26,26,256 -&gt; 26,26,128 -&gt; 52,52,128
        x2_in = self.last_layer2_conv(out1_branch)
        x2_in = self.last_layer2_upsample(x2_in)

        # 52,52,128 + 52,52,256 -&gt; 52,52,384
        x2_in = torch.cat([x2_in, x2], 1)
        #---------------------------------------------------#
        #   第一个特征层
        #   out3 = (batch_size,255,52,52)
        #---------------------------------------------------#
        # 52,52,384 -&gt; 52,52,128 -&gt; 52,52,256 -&gt; 52,52,128 -&gt; 52,52,256 -&gt; 52,52,128
        out2 = self.last_layer2(x2_in)
        return out0, out1, out2
</code></pre>
<h4 id="预测结果的解码"><a href="#预测结果的解码" class="headerlink" title="预测结果的解码"></a>预测结果的解码</h4><p>由第二步我们可以获得三个特征层的预测结果，shape分别为：</p>
<p>·(N,13,13,255)<br>·(N,26,26,255)<br>·(N,52,52,255)<br>在这里我们简单了解一下每个有效特征层到底做了什么：<br>每一个有效特征层将整个图片分成与其长宽对应的网格，如(N,13,13,255)的特征层就是将整个图像分成13x13个网格；然后从每个网格中心建立多个先验框，这些框是网络预先设定好的框，网络的预测结果会判断这些框内是否包含物体，以及这个物体的种类。</p>
<p>由于每一个网格点都具有三个先验框，所以上述的预测结果可以reshape为：</p>
<p>(N,13,13,3,85)<br>(N,26,26,3,85)<br>(N,52,52,3,85)<br>其中的85可以拆分为4+1+80，其中的4代表先验框的调整参数，1代表先验框内是否包含物体，80代表的是这个先验框的种类，由于coco分了80类，所以这里是80。如果YoloV3只检测两类物体，那么这个85就变为了4+1+2 = 7。</p>
<p>即85包含了4+1+80，分别代表x_offset、y_offset、h和w、置信度、分类结果。</p>
<p>但是这个预测结果并不对应着最终的预测框在图片上的位置，还需要解码才可以完成。</p>
<p>YoloV3的解码过程分为两步：</p>
<p>·先将每个网格点加上它对应的x_offset和y_offset，加完后的结果就是预测框的中心。<br>·然后再利用 先验框和h、w结合 计算出预测框的宽高。这样就能得到整个预测框的位置了。</p>
<p>得到最终的预测结果后还要进行<strong>得分排序与非极大抑制筛选</strong>。</p>
<p>这一部分基本上是所有目标检测通用的部分。其对于每一个类进行判别：<br><strong>1、取出每一类得分大于self.obj_threshold的框和得分。<br>2、利用框的位置和得分进行非极大抑制。</strong></p>
<p>实现代码如下：</p>
<pre><code class="lang-python"># 详情见utils/utils_bbox.py
class DecodeBox():
    def __init__(self, anchors, num_classes, input_shape, anchors_mask = [[6,7,8], [3,4,5], [0,1,2]]):
        super(DecodeBox, self).__init__()
        self.anchors        = anchors
        self.num_classes    = num_classes
        self.bbox_attrs     = 5 + num_classes
        self.input_shape    = input_shape
        #-----------------------------------------------------------#
        #   13x13的特征层对应的anchor是[116,90],[156,198],[373,326]
        #   26x26的特征层对应的anchor是[30,61],[62,45],[59,119]
        #   52x52的特征层对应的anchor是[10,13],[16,30],[33,23]
        #-----------------------------------------------------------#
        self.anchors_mask   = anchors_mask

    def decode_box(self, inputs):
        outputs = []
        for i, input in enumerate(inputs):
            #-----------------------------------------------#
            #   输入的input一共有三个，他们的shape分别是
            #   batch_size, 255, 13, 13
            #   batch_size, 255, 26, 26
            #   batch_size, 255, 52, 52
            #-----------------------------------------------#
            batch_size      = input.size(0)
            input_height    = input.size(2)
            input_width     = input.size(3)

            #-----------------------------------------------#
            #   输入为416x416时
            #   stride_h = stride_w = 32、16、8
            #-----------------------------------------------#
            stride_h = self.input_shape[0] / input_height
            stride_w = self.input_shape[1] / input_width
            #-------------------------------------------------#
            #   此时获得的scaled_anchors大小是相对于特征层的
            #-------------------------------------------------#
            scaled_anchors = [(anchor_width / stride_w, anchor_height / stride_h) for anchor_width, anchor_height in self.anchors[self.anchors_mask[i]]]

            #-----------------------------------------------#
            #   输入的input一共有三个，他们的shape分别是
            #   batch_size, 3, 13, 13, 85
            #   batch_size, 3, 26, 26, 85
            #   batch_size, 3, 52, 52, 85
            #-----------------------------------------------#
            prediction = input.view(batch_size, len(self.anchors_mask[i]),
                                    self.bbox_attrs, input_height, input_width).permute(0, 1, 3, 4, 2).contiguous()

            #-----------------------------------------------#
            #   先验框的中心位置的调整参数
            #-----------------------------------------------#
            x = torch.sigmoid(prediction[..., 0])  
            y = torch.sigmoid(prediction[..., 1])
            #-----------------------------------------------#
            #   先验框的宽高调整参数
            #-----------------------------------------------#
            w = prediction[..., 2]
            h = prediction[..., 3]
            #-----------------------------------------------#
            #   获得置信度，是否有物体
            #-----------------------------------------------#
            conf        = torch.sigmoid(prediction[..., 4])
            #-----------------------------------------------#
            #   种类置信度
            #-----------------------------------------------#
            pred_cls    = torch.sigmoid(prediction[..., 5:])

            FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor
            LongTensor  = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor

            #----------------------------------------------------------#
            #   生成网格，先验框中心，网格左上角 
            #   batch_size,3,13,13
            #----------------------------------------------------------#
            grid_x = torch.linspace(0, input_width - 1, input_width).repeat(input_height, 1).repeat(
                batch_size * len(self.anchors_mask[i]), 1, 1).view(x.shape).type(FloatTensor)
            grid_y = torch.linspace(0, input_height - 1, input_height).repeat(input_width, 1).t().repeat(
                batch_size * len(self.anchors_mask[i]), 1, 1).view(y.shape).type(FloatTensor)

            #----------------------------------------------------------#
            #   按照网格格式生成先验框的宽高
            #   batch_size,3,13,13
            #----------------------------------------------------------#
            anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))
            anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))
            anchor_w = anchor_w.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(w.shape)
            anchor_h = anchor_h.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(h.shape)

            #----------------------------------------------------------#
            #   利用预测结果对先验框进行调整
            #   首先调整先验框的中心，从先验框中心向右下角偏移
            #   再调整先验框的宽高。
            #----------------------------------------------------------#
            pred_boxes          = FloatTensor(prediction[..., :4].shape)
            pred_boxes[..., 0]  = x.data + grid_x
            pred_boxes[..., 1]  = y.data + grid_y
            pred_boxes[..., 2]  = torch.exp(w.data) * anchor_w
            pred_boxes[..., 3]  = torch.exp(h.data) * anchor_h

            #----------------------------------------------------------#
            #   将输出结果归一化成小数的形式
            #----------------------------------------------------------#
            _scale = torch.Tensor([input_width, input_height, input_width, input_height]).type(FloatTensor)
            output = torch.cat((pred_boxes.view(batch_size, -1, 4) / _scale,
                                conf.view(batch_size, -1, 1), pred_cls.view(batch_size, -1, self.num_classes)), -1)
            outputs.append(output.data)
        return outputs

    def yolo_correct_boxes(self, box_xy, box_wh, input_shape, image_shape, letterbox_image):
        #-----------------------------------------------------------------#
        #   把y轴放前面是因为方便预测框和图像的宽高进行相乘
        #-----------------------------------------------------------------#
        box_yx = box_xy[..., ::-1]
        box_hw = box_wh[..., ::-1]
        input_shape = np.array(input_shape)
        image_shape = np.array(image_shape)

        if letterbox_image:
            #-----------------------------------------------------------------#
            #   这里求出来的offset是图像有效区域相对于图像左上角的偏移情况
            #   new_shape指的是宽高缩放情况
            #-----------------------------------------------------------------#
            new_shape = np.round(image_shape * np.min(input_shape/image_shape))
            offset  = (input_shape - new_shape)/2./input_shape
            scale   = input_shape/new_shape

            box_yx  = (box_yx - offset) * scale
            box_hw *= scale

        box_mins    = box_yx - (box_hw / 2.)
        box_maxes   = box_yx + (box_hw / 2.)
        boxes  = np.concatenate([box_mins[..., 0:1], box_mins[..., 1:2], box_maxes[..., 0:1], box_maxes[..., 1:2]], axis=-1)
        boxes *= np.concatenate([image_shape, image_shape], axis=-1)
        return boxes

    def non_max_suppression(self, prediction, num_classes, input_shape, image_shape, letterbox_image, conf_thres=0.5, nms_thres=0.4):
        #----------------------------------------------------------#
        #   将预测结果的格式转换成左上角右下角的格式。
        #   prediction  [batch_size, num_anchors, 85]
        #----------------------------------------------------------#
        box_corner          = prediction.new(prediction.shape)
        box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2
        box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2
        box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2
        box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2
        prediction[:, :, :4] = box_corner[:, :, :4]

        output = [None for _ in range(len(prediction))]
        for i, image_pred in enumerate(prediction):
            #----------------------------------------------------------#
            #   对种类预测部分取max。
            #   class_conf  [num_anchors, 1]    种类置信度
            #   class_pred  [num_anchors, 1]    种类
            #----------------------------------------------------------#
            class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1, keepdim=True)

            #----------------------------------------------------------#
            #   利用置信度进行第一轮筛选
            #----------------------------------------------------------#
            conf_mask = (image_pred[:, 4] * class_conf[:, 0] &gt;= conf_thres).squeeze()

            #----------------------------------------------------------#
            #   根据置信度进行预测结果的筛选
            #----------------------------------------------------------#
            image_pred = image_pred[conf_mask]
            class_conf = class_conf[conf_mask]
            class_pred = class_pred[conf_mask]
            if not image_pred.size(0):
                continue
            #-------------------------------------------------------------------------#
            #   detections  [num_anchors, 7]
            #   7的内容为：x1, y1, x2, y2, obj_conf, class_conf, class_pred
            #-------------------------------------------------------------------------#
            detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)

            #------------------------------------------#
            #   获得预测结果中包含的所有种类
            #------------------------------------------#
            unique_labels = detections[:, -1].cpu().unique()

            if prediction.is_cuda:
                unique_labels = unique_labels.cuda()
                detections = detections.cuda()

            for c in unique_labels:
                #------------------------------------------#
                #   获得某一类得分筛选后全部的预测结果
                #------------------------------------------#
                detections_class = detections[detections[:, -1] == c]

                #------------------------------------------#
                #   使用官方自带的非极大抑制会速度更快一些！
                #------------------------------------------#
                keep = nms(
                    detections_class[:, :4],
                    detections_class[:, 4] * detections_class[:, 5],
                    nms_thres
                )
                max_detections = detections_class[keep]

                # # 按照存在物体的置信度排序
                # _, conf_sort_index = torch.sort(detections_class[:, 4]*detections_class[:, 5], descending=True)
                # detections_class = detections_class[conf_sort_index]
                # # 进行非极大抑制
                # max_detections = []
                # while detections_class.size(0):
                #     # 取出这一类置信度最高的，一步一步往下判断，判断重合程度是否大于nms_thres，如果是则去除掉
                #     max_detections.append(detections_class[0].unsqueeze(0))
                #     if len(detections_class) == 1:
                #         break
                #     ious = bbox_iou(max_detections[-1], detections_class[1:])
                #     detections_class = detections_class[1:][ious &lt; nms_thres]
                # # 堆叠
                # max_detections = torch.cat(max_detections).data

                # Add max detections to outputs
                output[i] = max_detections if output[i] is None else torch.cat((output[i], max_detections))

            if output[i] is not None:
                output[i]           = output[i].cpu().numpy()
                box_xy, box_wh      = (output[i][:, 0:2] + output[i][:, 2:4])/2, output[i][:, 2:4] - output[i][:, 0:2]
                output[i][:, :4]    = self.yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image)
        return output
</code></pre>
<h4 id="原图上进行绘制"><a href="#原图上进行绘制" class="headerlink" title="原图上进行绘制"></a>原图上进行绘制</h4><p>通过第三步，我们可以获得预测框在原图上的位置，而且这些预测框都是经过筛选的。这些筛选后的框可以直接绘制在图片上，就可以获得结果了。</p>
<h3 id="训练部分"><a href="#训练部分" class="headerlink" title="训练部分"></a>训练部分</h3><h4 id="计算loss所需参数"><a href="#计算loss所需参数" class="headerlink" title="计算loss所需参数"></a>计算loss所需参数</h4><p>在计算loss的时候，实际上是pred和target之间的对比：<br>pred就是网络的预测结果。<br>target就是网络的真实框情况。</p>
<h4 id="pred是什么"><a href="#pred是什么" class="headerlink" title="pred是什么"></a>pred是什么</h4><p>对于yolo3的模型来说，网络最后输出的内容就是三个特征层每个网格点对应的预测框及其种类，即三个特征层分别对应着图片被分为不同size的网格后，每个网格点上三个先验框对应的位置、置信度及其种类。</p>
<p>输出层的shape分别为(13,13,75)，(26,26,75)，(52,52,75)，最后一个维度为75是因为是基于voc数据集的，它的类为20种，yolo3只有针对每一个特征层存在3个先验框，所以最后维度为3x25；<br>如果使用的是coco训练集，类则为80种，最后的维度应该为255 = 3x85，三个特征层的shape为(13,13,255)，(26,26,255)，(52,52,255)</p>
<p>现在的y_pre还是没有解码的，解码了之后才是真实图像上的情况。</p>
<h4 id="target是什么。"><a href="#target是什么。" class="headerlink" title="target是什么。"></a>target是什么。</h4><p>target就是一个真实图像中，真实框的情况。<br>第一个维度是batch_size，第二个维度是每一张图片里面真实框的数量，第三个维度内部是真实框的信息，包括位置以及种类。</p>
<h4 id="loss的计算过程"><a href="#loss的计算过程" class="headerlink" title="loss的计算过程"></a>loss的计算过程</h4><p>拿到pred和target后，不可以简单的减一下作为对比，需要进行如下步骤。</p>
<p>判断真实框在图片中的位置，判断其属于哪一个网格点去检测。判断真实框和这个特征点的哪个先验框重合程度最高。计算该网格点应该有怎么样的预测结果才能获得真实框，与真实框重合度最高的先验框被用于作为正样本。<br>根据网络的预测结果获得预测框，计算预测框和所有真实框的重合程度，如果重合程度大于一定门限，则将该预测框对应的先验框忽略。其余作为负样本。<br>最终损失由三个部分组成：a、正样本，编码后的长宽与xy轴偏移量与预测值的差距。b、正样本，预测结果中置信度的值与1对比；负样本，预测结果中置信度的值与0对比。c、实际存在的框，种类预测结果与实际结果的对比。</p>
<pre><code class="lang-python"># 详情见nets/yolo_training.py
class YOLOLoss(nn.Module):
    def __init__(self, anchors, num_classes, input_shape, cuda, anchors_mask = [[6,7,8], [3,4,5], [0,1,2]]):
        super(YOLOLoss, self).__init__()
        #-----------------------------------------------------------#
        #   13x13的特征层对应的anchor是[116,90],[156,198],[373,326]
        #   26x26的特征层对应的anchor是[30,61],[62,45],[59,119]
        #   52x52的特征层对应的anchor是[10,13],[16,30],[33,23]
        #-----------------------------------------------------------#
        self.anchors        = anchors
        self.num_classes    = num_classes
        self.bbox_attrs     = 5 + num_classes
        self.input_shape    = input_shape
        self.anchors_mask   = anchors_mask

        self.ignore_threshold = 0.7
        self.cuda = cuda

    def clip_by_tensor(self, t, t_min, t_max):
        t = t.float()
        result = (t &gt;= t_min).float() * t + (t &lt; t_min).float() * t_min
        result = (result &lt;= t_max).float() * result + (result &gt; t_max).float() * t_max
        return result

    def MSELoss(self, pred, target):
        return torch.pow(pred - target, 2)

    def BCELoss(self, pred, target):
        epsilon = 1e-7
        pred    = self.clip_by_tensor(pred, epsilon, 1.0 - epsilon)
        output  = - target * torch.log(pred) - (1.0 - target) * torch.log(1.0 - pred)
        return output

    def forward(self, l, input, targets=None):
        #----------------------------------------------------#
        #   l代表的是，当前输入进来的有效特征层，是第几个有效特征层
        #   input的shape为  bs, 3*(5+num_classes), 13, 13
        #                   bs, 3*(5+num_classes), 26, 26
        #                   bs, 3*(5+num_classes), 52, 52
        #   targets代表的是真实框。
        #----------------------------------------------------#
        #--------------------------------#
        #   获得图片数量，特征层的高和宽
        #   13和13
        #--------------------------------#
        bs      = input.size(0)
        in_h    = input.size(2)
        in_w    = input.size(3)
        #-----------------------------------------------------------------------#
        #   计算步长
        #   每一个特征点对应原来的图片上多少个像素点
        #   如果特征层为13x13的话，一个特征点就对应原来的图片上的32个像素点
        #   如果特征层为26x26的话，一个特征点就对应原来的图片上的16个像素点
        #   如果特征层为52x52的话，一个特征点就对应原来的图片上的8个像素点
        #   stride_h = stride_w = 32、16、8
        #   stride_h和stride_w都是32。
        #-----------------------------------------------------------------------#
        stride_h = self.input_shape[0] / in_h
        stride_w = self.input_shape[1] / in_w
        #-------------------------------------------------#
        #   此时获得的scaled_anchors大小是相对于特征层的
        #-------------------------------------------------#
        scaled_anchors  = [(a_w / stride_w, a_h / stride_h) for a_w, a_h in self.anchors]
        #-----------------------------------------------#
        #   输入的input一共有三个，他们的shape分别是
        #   bs, 3*(5+num_classes), 13, 13 =&gt; batch_size, 3, 13, 13, 5 + num_classes
        #   batch_size, 3, 26, 26, 5 + num_classes
        #   batch_size, 3, 52, 52, 5 + num_classes
        #-----------------------------------------------#
        prediction = input.view(bs, len(self.anchors_mask[l]), self.bbox_attrs, in_h, in_w).permute(0, 1, 3, 4, 2).contiguous()

        #-----------------------------------------------#
        #   先验框的中心位置的调整参数
        #-----------------------------------------------#
        x = torch.sigmoid(prediction[..., 0])
        y = torch.sigmoid(prediction[..., 1])
        #-----------------------------------------------#
        #   先验框的宽高调整参数
        #-----------------------------------------------#
        w = prediction[..., 2]
        h = prediction[..., 3]
        #-----------------------------------------------#
        #   获得置信度，是否有物体
        #-----------------------------------------------#
        conf = torch.sigmoid(prediction[..., 4])
        #-----------------------------------------------#
        #   种类置信度
        #-----------------------------------------------#
        pred_cls = torch.sigmoid(prediction[..., 5:])

        #-----------------------------------------------#
        #   获得网络应该有的预测结果
        #-----------------------------------------------#
        y_true, noobj_mask, box_loss_scale = self.get_target(l, targets, scaled_anchors, in_h, in_w)

        #---------------------------------------------------------------#
        #   将预测结果进行解码，判断预测结果和真实值的重合程度
        #   如果重合程度过大则忽略，因为这些特征点属于预测比较准确的特征点
        #   作为负样本不合适
        #----------------------------------------------------------------#
        noobj_mask = self.get_ignore(l, x, y, h, w, targets, scaled_anchors, in_h, in_w, noobj_mask)

        if self.cuda:
            y_true          = y_true.cuda()
            noobj_mask      = noobj_mask.cuda()
            box_loss_scale  = box_loss_scale.cuda()
        #-----------------------------------------------------------#
        #   reshape_y_true[...,2:3]和reshape_y_true[...,3:4]
        #   表示真实框的宽高，二者均在0-1之间
        #   真实框越大，比重越小，小框的比重更大。
        #-----------------------------------------------------------#
        box_loss_scale = 2 - box_loss_scale
        #-----------------------------------------------------------#
        #   计算中心偏移情况的loss，使用BCELoss效果好一些
        #-----------------------------------------------------------#
        loss_x = torch.sum(self.BCELoss(x, y_true[..., 0]) * box_loss_scale * y_true[..., 4])
        loss_y = torch.sum(self.BCELoss(y, y_true[..., 1]) * box_loss_scale * y_true[..., 4])
        #-----------------------------------------------------------#
        #   计算宽高调整值的loss
        #-----------------------------------------------------------#
        loss_w = torch.sum(self.MSELoss(w, y_true[..., 2]) * 0.5 * box_loss_scale * y_true[..., 4])
        loss_h = torch.sum(self.MSELoss(h, y_true[..., 3]) * 0.5 * box_loss_scale * y_true[..., 4])
        #-----------------------------------------------------------#
        #   计算置信度的loss
        #-----------------------------------------------------------#
        loss_conf   = torch.sum(self.BCELoss(conf, y_true[..., 4]) * y_true[..., 4]) + \
                      torch.sum(self.BCELoss(conf, y_true[..., 4]) * noobj_mask)

        loss_cls    = torch.sum(self.BCELoss(pred_cls[y_true[..., 4] == 1], y_true[..., 5:][y_true[..., 4] == 1]))

        loss        = loss_x  + loss_y + loss_w + loss_h + loss_conf + loss_cls
        num_pos = torch.sum(y_true[..., 4])
        num_pos = torch.max(num_pos, torch.ones_like(num_pos))
        return loss, num_pos

    def calculate_iou(self, _box_a, _box_b):
        #-----------------------------------------------------------#
        #   计算真实框的左上角和右下角
        #-----------------------------------------------------------#
        b1_x1, b1_x2 = _box_a[:, 0] - _box_a[:, 2] / 2, _box_a[:, 0] + _box_a[:, 2] / 2
        b1_y1, b1_y2 = _box_a[:, 1] - _box_a[:, 3] / 2, _box_a[:, 1] + _box_a[:, 3] / 2
        #-----------------------------------------------------------#
        #   计算先验框获得的预测框的左上角和右下角
        #-----------------------------------------------------------#
        b2_x1, b2_x2 = _box_b[:, 0] - _box_b[:, 2] / 2, _box_b[:, 0] + _box_b[:, 2] / 2
        b2_y1, b2_y2 = _box_b[:, 1] - _box_b[:, 3] / 2, _box_b[:, 1] + _box_b[:, 3] / 2

        #-----------------------------------------------------------#
        #   将真实框和预测框都转化成左上角右下角的形式
        #-----------------------------------------------------------#
        box_a = torch.zeros_like(_box_a)
        box_b = torch.zeros_like(_box_b)
        box_a[:, 0], box_a[:, 1], box_a[:, 2], box_a[:, 3] = b1_x1, b1_y1, b1_x2, b1_y2
        box_b[:, 0], box_b[:, 1], box_b[:, 2], box_b[:, 3] = b2_x1, b2_y1, b2_x2, b2_y2

        #-----------------------------------------------------------#
        #   A为真实框的数量，B为先验框的数量
        #-----------------------------------------------------------#
        A = box_a.size(0)
        B = box_b.size(0)

        #-----------------------------------------------------------#
        #   计算交的面积
        #-----------------------------------------------------------#
        max_xy  = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2), box_b[:, 2:].unsqueeze(0).expand(A, B, 2))
        min_xy  = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2), box_b[:, :2].unsqueeze(0).expand(A, B, 2))
        inter   = torch.clamp((max_xy - min_xy), min=0)
        inter   = inter[:, :, 0] * inter[:, :, 1]
        #-----------------------------------------------------------#
        #   计算预测框和真实框各自的面积
        #-----------------------------------------------------------#
        area_a = ((box_a[:, 2]-box_a[:, 0]) * (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]
        area_b = ((box_b[:, 2]-box_b[:, 0]) * (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]
        #-----------------------------------------------------------#
        #   求IOU
        #-----------------------------------------------------------#
        union = area_a + area_b - inter
        return inter / union  # [A,B]

    def get_target(self, l, targets, anchors, in_h, in_w):
        #-----------------------------------------------------#
        #   计算一共有多少张图片
        #-----------------------------------------------------#
        bs              = len(targets)
        #-----------------------------------------------------#
        #   用于选取哪些先验框不包含物体
        #-----------------------------------------------------#
        noobj_mask      = torch.ones(bs, len(self.anchors_mask[l]), in_h, in_w, requires_grad = False)
        #-----------------------------------------------------#
        #   让网络更加去关注小目标
        #-----------------------------------------------------#
        box_loss_scale  = torch.zeros(bs, len(self.anchors_mask[l]), in_h, in_w, requires_grad = False)
        #-----------------------------------------------------#
        #   batch_size, 3, 13, 13, 5 + num_classes
        #-----------------------------------------------------#
        y_true          = torch.zeros(bs, len(self.anchors_mask[l]), in_h, in_w, self.bbox_attrs, requires_grad = False)
        for b in range(bs):            
            if len(targets[b])==0:
                continue
            batch_target = torch.zeros_like(targets[b])
            #-------------------------------------------------------#
            #   计算出正样本在特征层上的中心点
            #-------------------------------------------------------#
            batch_target[:, [0,2]] = targets[b][:, [0,2]] * in_w
            batch_target[:, [1,3]] = targets[b][:, [1,3]] * in_h
            batch_target[:, 4] = targets[b][:, 4]
            batch_target = batch_target.cpu()

            #-------------------------------------------------------#
            #   将真实框转换一个形式
            #   num_true_box, 4
            #-------------------------------------------------------#
            gt_box          = torch.FloatTensor(torch.cat((torch.zeros((batch_target.size(0), 2)), batch_target[:, 2:4]), 1))
            #-------------------------------------------------------#
            #   将先验框转换一个形式
            #   9, 4
            #-------------------------------------------------------#
            anchor_shapes   = torch.FloatTensor(torch.cat((torch.zeros((len(anchors), 2)), torch.FloatTensor(anchors)), 1))
            #-------------------------------------------------------#
            #   计算交并比
            #   self.calculate_iou(gt_box, anchor_shapes) = [num_true_box, 9]每一个真实框和9个先验框的重合情况
            #   best_ns:
            #   [每个真实框最大的重合度max_iou, 每一个真实框最重合的先验框的序号]
            #-------------------------------------------------------#
            best_ns = torch.argmax(self.calculate_iou(gt_box, anchor_shapes), dim=-1)

            for t, best_n in enumerate(best_ns):
                if best_n not in self.anchors_mask[l]:
                    continue
                #----------------------------------------#
                #   判断这个先验框是当前特征点的哪一个先验框
                #----------------------------------------#
                k = self.anchors_mask[l].index(best_n)
                #----------------------------------------#
                #   获得真实框属于哪个网格点
                #----------------------------------------#
                i = torch.floor(batch_target[t, 0]).long()
                j = torch.floor(batch_target[t, 1]).long()
                #----------------------------------------#
                #   取出真实框的种类
                #----------------------------------------#
                c = batch_target[t, 4].long()

                #----------------------------------------#
                #   noobj_mask代表无目标的特征点
                #----------------------------------------#
                noobj_mask[b, k, j, i] = 0
                #----------------------------------------#
                #   tx、ty代表中心调整参数的真实值
                #----------------------------------------#
                y_true[b, k, j, i, 0] = batch_target[t, 0] - i.float()
                y_true[b, k, j, i, 1] = batch_target[t, 1] - j.float()
                y_true[b, k, j, i, 2] = math.log(batch_target[t, 2] / anchors[best_n][0])
                y_true[b, k, j, i, 3] = math.log(batch_target[t, 3] / anchors[best_n][1])
                y_true[b, k, j, i, 4] = 1
                y_true[b, k, j, i, c + 5] = 1
                #----------------------------------------#
                #   用于获得xywh的比例
                #   大目标loss权重小，小目标loss权重大
                #----------------------------------------#
                box_loss_scale[b, k, j, i] = batch_target[t, 2] * batch_target[t, 3] / in_w / in_h
        return y_true, noobj_mask, box_loss_scale

    def get_ignore(self, l, x, y, h, w, targets, scaled_anchors, in_h, in_w, noobj_mask):
        #-----------------------------------------------------#
        #   计算一共有多少张图片
        #-----------------------------------------------------#
        bs = len(targets)

        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor
        LongTensor  = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor
        #-----------------------------------------------------#
        #   生成网格，先验框中心，网格左上角
        #-----------------------------------------------------#
        grid_x = torch.linspace(0, in_w - 1, in_w).repeat(in_h, 1).repeat(
            int(bs * len(self.anchors_mask[l])), 1, 1).view(x.shape).type(FloatTensor)
        grid_y = torch.linspace(0, in_h - 1, in_h).repeat(in_w, 1).t().repeat(
            int(bs * len(self.anchors_mask[l])), 1, 1).view(y.shape).type(FloatTensor)

        # 生成先验框的宽高
        scaled_anchors_l = np.array(scaled_anchors)[self.anchors_mask[l]]
        anchor_w = FloatTensor(scaled_anchors_l).index_select(1, LongTensor([0]))
        anchor_h = FloatTensor(scaled_anchors_l).index_select(1, LongTensor([1]))

        anchor_w = anchor_w.repeat(bs, 1).repeat(1, 1, in_h * in_w).view(w.shape)
        anchor_h = anchor_h.repeat(bs, 1).repeat(1, 1, in_h * in_w).view(h.shape)
        #-------------------------------------------------------#
        #   计算调整后的先验框中心与宽高
        #-------------------------------------------------------#
        pred_boxes_x    = torch.unsqueeze(x.data + grid_x, -1)
        pred_boxes_y    = torch.unsqueeze(y.data + grid_y, -1)
        pred_boxes_w    = torch.unsqueeze(torch.exp(w.data) * anchor_w, -1)
        pred_boxes_h    = torch.unsqueeze(torch.exp(h.data) * anchor_h, -1)
        pred_boxes      = torch.cat([pred_boxes_x, pred_boxes_y, pred_boxes_w, pred_boxes_h], dim = -1)

        for b in range(bs):           
            #-------------------------------------------------------#
            #   将预测结果转换一个形式
            #   pred_boxes_for_ignore      num_anchors, 4
            #-------------------------------------------------------#
            pred_boxes_for_ignore = pred_boxes[b].view(-1, 4)
            #-------------------------------------------------------#
            #   计算真实框，并把真实框转换成相对于特征层的大小
            #   gt_box      num_true_box, 4
            #-------------------------------------------------------#
            if len(targets[b]) &gt; 0:
                batch_target = torch.zeros_like(targets[b])
                #-------------------------------------------------------#
                #   计算出正样本在特征层上的中心点
                #-------------------------------------------------------#
                batch_target[:, [0,2]] = targets[b][:, [0,2]] * in_w
                batch_target[:, [1,3]] = targets[b][:, [1,3]] * in_h
                batch_target = batch_target[:, :4]
                #-------------------------------------------------------#
                #   计算交并比
                #   anch_ious       num_true_box, num_anchors
                #-------------------------------------------------------#
                anch_ious = self.calculate_iou(batch_target, pred_boxes_for_ignore)
                #-------------------------------------------------------#
                #   每个先验框对应真实框的最大重合度
                #   anch_ious_max   num_anchors
                #-------------------------------------------------------#
                anch_ious_max, _    = torch.max(anch_ious, dim = 0)
                anch_ious_max       = anch_ious_max.view(pred_boxes[b].size()[:3])
                noobj_mask[b][anch_ious_max &gt; self.ignore_threshold] = 0
        return noobj_mask
</code></pre>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOV5详解</title>
    <url>/CN/YOLOV5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/weixin_44791964/article/details/121626848">pytorch搭建yolov5</a></p>
<p>2.<a href="https://github.com/bubbliiiing/yolov5-pytorch">yolov5-pytorch</a></p>
<p>3.<a href="https://blog.csdn.net/zhuangyuan7838/article/details/122672465">以YOLOV4为例详解anchor_based目标检测训练过程</a></p>
<h2 id="整体结构解析"><a href="#整体结构解析" class="headerlink" title="整体结构解析"></a>整体结构解析</h2><p><img src="/CN/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" class="lazyload" data-srcset="/CN/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="img" style="zoom:50%;"></p>
<p>在学习YoloV5之前，我们需要对YoloV5所作的工作有一定的了解，这有助于我们后面去了解网络的细节。</p>
<p>和之前版本的Yolo类似，整个YoloV5可以依然可以分为三个部分，分别是Backbone，FPN以及Yolo Head。</p>
<p>Backbone可以被称作YoloV5的主干特征提取网络，根据它的结构以及之前Yolo主干的叫法，我一般叫它CSPDarknet，输入的图片首先会在CSPDarknet里面进行特征提取，提取到的特征可以被称作特征层，是输入图片的特征集合。在主干部分，我们获取了三个特征层进行下一步网络的构建，这三个特征层我称它为有效特征层。</p>
<p>FPN可以被称作YoloV5的加强特征提取网络，在主干部分获得的三个有效特征层会在这一部分进行特征融合，特征融合的目的是结合不同尺度的特征信息。在FPN部分，已经获得的有效特征层被用于继续提取特征。在YoloV5里依然使用到了Panet的结构，我们不仅会对特征进行上采样实现特征融合，还会对特征再次进行下采样实现特征融合。</p>
<p>Yolo Head是YoloV5的分类器与回归器，通过CSPDarknet和FPN，我们已经可以获得三个加强过的有效特征层。每一个特征层都有宽、高和通道数，此时我们可以将特征图看作一个又一个特征点的集合，每一个特征点都有通道数个特征。Yolo Head实际上所做的工作就是对特征点进行判断，判断特征点是否有物体与其对应。与以前版本的Yolo一样，YoloV5所用的解耦头是一起的，也就是分类和回归在一个1X1卷积里实现。</p>
<p>因此，整个YoloV5网络所作的工作就是 特征提取-特征加强-预测特征点对应的物体情况。</p>
<h2 id="网络结构解析"><a href="#网络结构解析" class="headerlink" title="网络结构解析"></a>网络结构解析</h2><h3 id="主干网络backbone"><a href="#主干网络backbone" class="headerlink" title="主干网络backbone"></a>主干网络backbone</h3><p><img src="/CN/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" class="lazyload" data-srcset="/CN/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>这部分在nets/CSPdarknet.py文件下，YoloV5所使用的主干特征提取网络为CSPDarknet，接下来就是各个组件实现:</p>
<h4 id="Conv2D-BN-SiLU"><a href="#Conv2D-BN-SiLU" class="headerlink" title="Conv2D_BN_SiLU"></a>Conv2D_BN_SiLU</h4><p>在代码里把这卷积、正则化和激活进行封装，封装成类Conv</p>
<pre><code class="lang-python">class SiLU(nn.Module):
    @staticmethod
    def forward(x):
        return x * torch.sigmoid(x)

# 自动填充,padding=1/2*kernel_size,根据计算卷积计算宽高公式自然懂的
def autopad(k, p=None):
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k] 
    return p

class Conv(nn.Module):
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):
        super(Conv, self).__init__()
        self.conv   = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)
        self.bn     = nn.BatchNorm2d(c2, eps=0.001, momentum=0.03)
        self.act    = SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())

    def forward(self, x):
        return self.act(self.bn(self.conv(x)))

    def fuseforward(self, x):
        return self.act(self.conv(x))
</code></pre>
<h4 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h4><p>使用了<strong>残差网络Residual</strong>，CSPDarknet中的残差卷积可以分为两个部分，主干部分是一次1X1的卷积和一次3X3的卷积；残差边部分不做任何处理，直接将主干的输入与输出结合。整个YoloV5的主干部分都由残差卷积构成：</p>
<p>如下图所示：</p>
<p><img src="/CN/YOLOV5/d232d9ef19cb4b3bbb7b492aaf6ae097.png" class="lazyload" data-srcset="/CN/YOLOV5/d232d9ef19cb4b3bbb7b492aaf6ae097.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>代码如下：</p>
<pre><code class="lang-python">class Bottleneck(nn.Module):
    # Standard bottleneck
    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # 参数分别代表ch_in, ch_out, shortcut, groups, expansion
        super(Bottleneck, self).__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_, c2, 3, 1, g=g)
        self.add = shortcut and c1 == c2

    def forward(self, x):
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))
</code></pre>
<p>残差网络的特点是<strong>容易优化</strong>，并且能够通过增加相当的<strong>深度来提高准确率</strong>。其内部的<strong>残差块使用了跳跃连接，缓解了在深度神经网络中增加深度带来的梯度消失问题。</strong></p>
<h4 id="CSPnet"><a href="#CSPnet" class="headerlink" title="CSPnet"></a>CSPnet</h4><p>使用<strong>CSPnet</strong>网络结构，CSPnet结构并不算复杂，就是将原来的残差块的堆叠进行了一个拆分，拆成左右两部分：<strong>主干部分继续进行原来的残差块的堆叠</strong>；另一部分则像一个残差边一样，经过少量处理直接连接到最后。因此可以认为CSP中存在一个大的残差边。</p>
<p>如下图所示：</p>
<p><img src="/CN/YOLOV5/20200509113651540.png" class="lazyload" data-srcset="/CN/YOLOV5/20200509113651540.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="CSPLayer" style="zoom:50%;"></p>
<p>代码实现：</p>
<pre><code class="lang-python"># 这里是backbone里CSPLayer的实现
class C3(nn.Module):
    # CSP Bottleneck with 3 convolutions
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super(C3, self).__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)  # act=FReLU(c2)
        self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])
        # self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])

    def forward(self, x):
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))
</code></pre>
<h4 id="Focus网络结构"><a href="#Focus网络结构" class="headerlink" title="Focus网络结构"></a>Focus网络结构</h4><p>使用了Focus网络结构，这个网络结构是在YoloV5里面使用到比较有趣的网络结构，具体操作是在一张图片中每隔一个像素拿到一个值，这个时候获得了四个独立的特征层，然后将四个独立的特征层进行堆叠，此时宽高信息就集中到了通道信息，输入通道扩充了四倍。拼接起来的特征层相对于原先的三通道变成了十二个通道，下图很好的展示了Focus结构，一看就能明白。<br><img src="/CN/YOLOV5/7d1f567fc97140d4b9492b5e28cd0ebc.png" class="lazyload" data-srcset="/CN/YOLOV5/7d1f567fc97140d4b9492b5e28cd0ebc.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述"></p>
<pre><code class="lang-python">class Focus(nn.Module):
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super(Focus, self).__init__()
        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)

    def forward(self, x):
        return self.conv(torch.cat(
            [x[..., ::2, ::2], # 1
            x[..., 1::2, ::2], # 2
            x[..., ::2, 1::2], # 3
            x[..., 1::2, 1::2] # 4
            ], 1))
</code></pre>
<h4 id="SiLU函数"><a href="#SiLU函数" class="headerlink" title="SiLU函数"></a>SiLU函数</h4><p>使用了SiLU激活函数，SiLU是Sigmoid和ReLU的改进版。SiLU具备无上界有下界、平滑、非单调的特性。SiLU在深层模型上的效果优于 ReLU。可以看做是平滑的ReLU激活函数。<br><img src="/CN/YOLOV5/image-20220505154442317.png" class="lazyload" data-srcset="/CN/YOLOV5/image-20220505154442317.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220505154442317" style="zoom: 80%;"></p>
<p><img src="/CN/YOLOV5/e6cb6d3c11db4510a034ca9b1a0ca339.png" class="lazyload" data-srcset="/CN/YOLOV5/e6cb6d3c11db4510a034ca9b1a0ca339.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<pre><code class="lang-python">class SiLU(nn.Module):
    @staticmethod
    def forward(x):
        return x * torch.sigmoid(x)
</code></pre>
<h4 id="SPP"><a href="#SPP" class="headerlink" title="SPP"></a>SPP</h4><p>使用了SPP结构，通过不同池化核大小的最大池化进行特征提取，提高网络的感受野。在YoloV4中，SPP是用在FPN里面的，在YoloV5中，SPP模块被用在了主干特征提取网络中。</p>
<p>如下图所示：</p>
<p><img src="/CN/YOLOV5/image-20220505154713232.png" class="lazyload" data-srcset="/CN/YOLOV5/image-20220505154713232.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220505154713232" style="zoom:50%;"></p>
<p>代码如下：</p>
<pre><code class="lang-python">class SPP(nn.Module):
    # Spatial pyramid pooling layer used in YOLOv3-SPP
    def __init__(self, c1, c2, k=(5, 9, 13)):
        super(SPP, self).__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)
        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])

    def forward(self, x):
        x = self.cv1(x)
        return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))
</code></pre>
<h4 id="CSPDarknet"><a href="#CSPDarknet" class="headerlink" title="CSPDarknet"></a>CSPDarknet</h4><p>在完成各个部件后，整个的CSPDarknet实现如下（左边部分）：</p>
<pre><code class="lang-python">class CSPDarknet(nn.Module):
    def __init__(self, base_channels, base_depth):
        super().__init__()
        #-----------------------------------------------#
        #   输入图片是640, 640, 3
        #   初始的基本通道是64
        #-----------------------------------------------#

        #-----------------------------------------------#
        #   利用focus网络结构进行特征提取
        #   640, 640, 3 -&gt; 320, 320, 12 -&gt; 320, 320, 64
        #-----------------------------------------------#
        self.stem       = Focus(3, base_channels, k=3)
        #-----------------------------------------------#
        #   完成卷积之后，320, 320, 64 -&gt; 160, 160, 128
        #   完成CSPlayer之后，160, 160, 128 -&gt; 160, 160, 128
        #-----------------------------------------------#
        self.dark2 = nn.Sequential(
            Conv(base_channels, base_channels * 2, 3, 2),
            C3(base_channels * 2, base_channels * 2, base_depth),
        )
        #-----------------------------------------------#
        #   完成卷积之后，160, 160, 128 -&gt; 80, 80, 256
        #   完成CSPlayer之后，80, 80, 256 -&gt; 80, 80, 256
        #-----------------------------------------------#
        self.dark3 = nn.Sequential(
            Conv(base_channels * 2, base_channels * 4, 3, 2),
            C3(base_channels * 4, base_channels * 4, base_depth * 3),
        )

        #-----------------------------------------------#
        #   完成卷积之后，80, 80, 256 -&gt; 40, 40, 512
        #   完成CSPlayer之后，40, 40, 512 -&gt; 40, 40, 512
        #-----------------------------------------------#
        self.dark4 = nn.Sequential(
            Conv(base_channels * 4, base_channels * 8, 3, 2),
            C3(base_channels * 8, base_channels * 8, base_depth * 3),
        )
        #-----------------------------------------------#
        #   完成卷积之后，40, 40, 512 -&gt; 20, 20, 1024
        #   完成SPP之后，20, 20, 1024 -&gt; 20, 20, 1024
        #   完成CSPlayer之后，20, 20, 1024 -&gt; 20, 20, 1024
        #-----------------------------------------------#
        self.dark5 = nn.Sequential(
            Conv(base_channels * 8, base_channels * 16, 3, 2),
            SPP(base_channels * 16, base_channels * 16),
            C3(base_channels * 16, base_channels * 16, base_depth, shortcut=False),
        )
    # feat1, feat2, feat3是用来目标检测的三个特征层，后续还要进行FPN层的构建，也就是中间那部分
    def forward(self, x):
        x = self.stem(x)
        x = self.dark2(x)
        #-----------------------------------------------#
        #   dark3的输出为80, 80, 256，是一个有效特征层
        #-----------------------------------------------#
        x = self.dark3(x)
        feat1 = x
        #-----------------------------------------------#
        #   dark4的输出为40, 40, 512，是一个有效特征层
        #-----------------------------------------------#
        x = self.dark4(x)
        feat2 = x
        #-----------------------------------------------#
        #   dark5的输出为20, 20, 1024，是一个有效特征层
        #-----------------------------------------------#
        x = self.dark5(x)
        feat3 = x
        return feat1, feat2, feat3
</code></pre>
<p>好了，我们现在已经提取到了构建FPN层的三个特征层，接下来就是构建FPN</p>
<h3 id="构建FPN加强特征提取"><a href="#构建FPN加强特征提取" class="headerlink" title="构建FPN加强特征提取"></a>构建FPN加强特征提取</h3><p><img src="/CN/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" class="lazyload" data-srcset="/CN/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>再次回到这幅图，现在我们需要进行中间部分的实现了,这部分在yolo.py文件。</p>
<p>在特征利用部分，YoloV5提取多特征层进行目标检测，一共提取三个特征层。<br>三个特征层位于主干部分CSPdarknet的不同位置，分别位于中间层，中下层，底层，当输入为(640,640,3)的时候，三个特征层的shape分别为feat1=(80,80,256)、feat2=(40,40,512)、feat3=(20,20,1024)。</p>
<p>在获得三个有效特征层后，我们利用这三个有效特征层进行FPN层的构建，构建方式为：</p>
<p>1.feat3=(20,20,1024)的特征层进行1次1X1卷积调整通道后获得P5，P5进行上采样UmSampling2d后与feat2=(40,40,512)特征层进行结合，然后使用CSPLayer进行特征提取获得P5_upsample，此时获得的特征层为(40,40,512)。</p>
<p>2.P5_upsample=(40,40,512)的特征层进行1次1X1卷积调整通道后获得P4，P4进行上采样UmSampling2d后与feat1=(80,80,256)特征层进行结合，然后使用CSPLayer进行特征提取<strong>P3_out</strong>，此时获得的特征层为(80,80,256)。</p>
<p>3.P3_out=(80,80,256)的特征层进行一次3x3卷积进行下采样，下采样后与P4堆叠，然后使用CSPLayer进行特征提取<strong>P4_out</strong>，此时获得的特征层为(40,40,512)。</p>
<p>4.P4_out=(40,40,512)的特征层进行一次3x3卷积进行下采样，下采样后与P5堆叠，然后使用CSPLayer进行特征提取<strong>P5_out</strong>，此时获得的特征层为(20,20,1024)。</p>
<p>注：p3_out, p4_out,p5_out即为经过FPN输出的三个特征层，用于检测。</p>
<p>特征金字塔可以将不同shape的特征层进行特征融合，有利于提取出更好的特征。</p>
<p>代码实现如下：</p>
<pre><code class="lang-python">import torch
import torch.nn as nn

from nets.ConvNext import ConvNeXt_Small, ConvNeXt_Tiny
from nets.CSPdarknet import C3, Conv, CSPDarknet
from nets.Swin_transformer import Swin_transformer_Tiny


#---------------------------------------------------#
#   yolo_body
#---------------------------------------------------#
class YoloBody(nn.Module):
    def __init__(self, anchors_mask, num_classes, phi, backbone=&#39;cspdarknet&#39;, pretrained=False, input_shape=[640, 640]):
        super(YoloBody, self).__init__()
        depth_dict          = &#123;&#39;s&#39; : 0.33, &#39;m&#39; : 0.67, &#39;l&#39; : 1.00, &#39;x&#39; : 1.33,&#125;
        width_dict          = &#123;&#39;s&#39; : 0.50, &#39;m&#39; : 0.75, &#39;l&#39; : 1.00, &#39;x&#39; : 1.25,&#125;
        dep_mul, wid_mul    = depth_dict[phi], width_dict[phi]

        base_channels       = int(wid_mul * 64)  # 64
        base_depth          = max(round(dep_mul * 3), 1)  # 3
        #-----------------------------------------------#
        #   输入图片是640, 640, 3
        #   初始的基本通道是64
        #-----------------------------------------------#
        self.backbone_name  = backbone
        if backbone == &quot;cspdarknet&quot;:
            #---------------------------------------------------#   
            #   生成CSPdarknet53的主干模型
            #   获得三个有效特征层，他们的shape分别是：
            #   80,80,256
            #   40,40,512
            #   20,20,1024
            #---------------------------------------------------#
            self.backbone   = CSPDarknet(base_channels, base_depth, phi, pretrained)
        else:
            #---------------------------------------------------#   
            #   如果输入不为cspdarknet，则调整通道数
            #   使其符合YoloV5的格式
            #---------------------------------------------------#
            self.backbone       = &#123;
                &#39;convnext_tiny&#39;         : ConvNeXt_Tiny,
                &#39;convnext_small&#39;        : ConvNeXt_Small,
                &#39;swin_transfomer_tiny&#39;  : Swin_transformer_Tiny,
            &#125;[backbone](pretrained=pretrained, input_shape=input_shape)
            in_channels         = &#123;
                &#39;convnext_tiny&#39;         : [192, 384, 768],
                &#39;convnext_small&#39;        : [192, 384, 768],
                &#39;swin_transfomer_tiny&#39;  : [192, 384, 768],
            &#125;[backbone]
            feat1_c, feat2_c, feat3_c = in_channels 
            self.conv_1x1_feat1 = Conv(feat1_c, base_channels * 4, 1, 1)
            self.conv_1x1_feat2 = Conv(feat2_c, base_channels * 8, 1, 1)
            self.conv_1x1_feat3 = Conv(feat3_c, base_channels * 16, 1, 1)

        # 上采样操作，采用最近邻插值法
        self.upsample   = nn.Upsample(scale_factor=2, mode=&quot;nearest&quot;)

        # Conv1×1,获得p5
        self.conv_for_feat3         = Conv(base_channels * 16, base_channels * 8, 1, 1)
        # CSPLayer操作，进行特征提取，获得p5_unsample
        self.conv3_for_upsample1    = C3(base_channels * 16, base_channels * 8, base_depth, shortcut=False)

        self.conv_for_feat2         = Conv(base_channels * 8, base_channels * 4, 1, 1)
        self.conv3_for_upsample2    = C3(base_channels * 8, base_channels * 4, base_depth, shortcut=False)

        # 下采样操作，采用卷积步长为2的方法，通道不变，宽高压缩一半
        self.down_sample1           = Conv(base_channels * 4, base_channels * 4, 3, 2)
        self.conv3_for_downsample1  = C3(base_channels * 8, base_channels * 8, base_depth, shortcut=False)

        self.down_sample2           = Conv(base_channels * 8, base_channels * 8, 3, 2)
        self.conv3_for_downsample2  = C3(base_channels * 16, base_channels * 16, base_depth, shortcut=False)

        # 80, 80, 256 =&gt; 80, 80, 3 * (5 + num_classes) =&gt; 80, 80, 3 * (4 + 1 + num_classes)
        self.yolo_head_P3 = nn.Conv2d(base_channels * 4, len(anchors_mask[2]) * (5 + num_classes), 1)
        # 40, 40, 512 =&gt; 40, 40, 3 * (5 + num_classes) =&gt; 40, 40, 3 * (4 + 1 + num_classes)
        self.yolo_head_P4 = nn.Conv2d(base_channels * 8, len(anchors_mask[1]) * (5 + num_classes), 1)
        # 20, 20, 1024 =&gt; 20, 20, 3 * (5 + num_classes) =&gt; 20, 20, 3 * (4 + 1 + num_classes)
        self.yolo_head_P5 = nn.Conv2d(base_channels * 16, len(anchors_mask[0]) * (5 + num_classes), 1)

    def forward(self, x):
        #  backbone
        feat1, feat2, feat3 = self.backbone(x)
        if self.backbone_name != &quot;cspdarknet&quot;:
            feat1 = self.conv_1x1_feat1(feat1)
            feat2 = self.conv_1x1_feat2(feat2)
            feat3 = self.conv_1x1_feat3(feat3)

        # 20, 20, 1024 -&gt; 20, 20, 512
        P5          = self.conv_for_feat3(feat3)
        # 20, 20, 512 -&gt; 40, 40, 512
        P5_upsample = self.upsample(P5)
        # 40, 40, 512 -&gt; 40, 40, 1024
        P4          = torch.cat([P5_upsample, feat2], 1)
        # 40, 40, 1024 -&gt; 40, 40, 512
        P4          = self.conv3_for_upsample1(P4)

        # 40, 40, 512 -&gt; 40, 40, 256
        P4          = self.conv_for_feat2(P4)
        # 40, 40, 256 -&gt; 80, 80, 256
        P4_upsample = self.upsample(P4)
        # 80, 80, 256 cat 80, 80, 256 -&gt; 80, 80, 512
        P3          = torch.cat([P4_upsample, feat1], 1)
        # 80, 80, 512 -&gt; 80, 80, 256
        P3          = self.conv3_for_upsample2(P3)

        # 80, 80, 256 -&gt; 40, 40, 256
        P3_downsample = self.down_sample1(P3)
        # 40, 40, 256 cat 40, 40, 256 -&gt; 40, 40, 512
        P4 = torch.cat([P3_downsample, P4], 1)
        # 40, 40, 512 -&gt; 40, 40, 512
        P4 = self.conv3_for_downsample1(P4)

        # 40, 40, 512 -&gt; 20, 20, 512
        P4_downsample = self.down_sample2(P4)
        # 20, 20, 512 cat 20, 20, 512 -&gt; 20, 20, 1024
        P5 = torch.cat([P4_downsample, P5], 1)
        # 20, 20, 1024 -&gt; 20, 20, 1024
        P5 = self.conv3_for_downsample2(P5)

        #---------------------------------------------------#
        #   第三个特征层
        #   y3=(batch_size,75,80,80)
        #---------------------------------------------------#
        out2 = self.yolo_head_P3(P3)
        #---------------------------------------------------#
        #   第二个特征层
        #   y2=(batch_size,75,40,40)
        #---------------------------------------------------#
        out1 = self.yolo_head_P4(P4)
        #---------------------------------------------------#
        #   第一个特征层
        #   y1=(batch_size,75,20,20)
        #---------------------------------------------------#
        out0 = self.yolo_head_P5(P5)
        return out0, out1, out2
</code></pre>
<p>注：特征金字塔输出来的是p3,p4,p5。代码里输出的是yolohead输出来的结果，即out0,out1,out2</p>
<h3 id="利用YoloHead获得预测结果"><a href="#利用YoloHead获得预测结果" class="headerlink" title="利用YoloHead获得预测结果"></a>利用YoloHead获得预测结果</h3><p><img src="/CN/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" class="lazyload" data-srcset="/CN/YOLOV5/0b33c17fb3ac47cfb2b79a80d5b2fbaa.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>利用FPN特征金字塔，我们可以获得三个加强特征，这三个加强特征的shape分别为(20,20,1024)、(40,40,512)、(80,80,256)，然后我们利用这三个shape的特征层传入Yolo Head获得预测结果。</p>
<p>对于每一个特征层，我们可以获得利用一个卷积调整通道数，最终的通道数和需要区分的种类个数相关，在YoloV5里，每一个特征层上每一个特征点存在3个先验框。</p>
<p>如果使用的是voc训练集，类则为20种，最后的维度应该为75 = 3x25，三个特征层的shape为(20,20,75)，(40,40,75)，(80,80,75)。<br>最后的75可以拆分成3个25，对应3个先验框的25个参数，25可以拆分成4+1+20。<br>前4个参数用于判断每一个特征点的回归参数，回归参数调整后可以获得预测框；<br>第5个参数用于判断每一个特征点是否包含物体；<br>最后20个参数用于判断每一个特征点所包含的物体种类。</p>
<p>如果使用的是coco训练集，类则为80种，最后的维度应该为255 = 3x85，三个特征层的shape为(20,20,255)，(40,40,255)，(80,80,255)<br>最后的255可以拆分成3个85，对应3个先验框的85个参数，85可以拆分成4+1+80。<br>前4个参数用于判断每一个特征点的回归参数，回归参数调整后可以获得预测框；<br>第5个参数用于判断每一个特征点是否包含物体；<br>最后80个参数用于判断每一个特征点所包含的物体种类。</p>
<p>在上一部分其实已经写到了预测部分的代码，现在我们把它单独拿出来看看,或者直接看上面FPN那里的代码。</p>
<pre><code class="lang-python">class YoloBody(nn.Module):
    def __init__(self, anchor_mask, num_classes, phi):
        super(YoloBody, self).__init__()
        # 80, 80, 256 =&gt; 80, 80, 3 * (5 + num_classes) =&gt; 80, 80, 3 * (4 + 1 + num_classes)
        self.yolo_head_P3 = nn.Conv2d(base_channels * 4, len(anchors_mask[2]) * (5 + num_classes), 1)
        # 40, 40, 512 =&gt; 40, 40, 3 * (5 + num_classes) =&gt; 40, 40, 3 * (4 + 1 + num_classes)
        self.yolo_head_P4 = nn.Conv2d(base_channels * 8, len(anchors_mask[1]) * (5 + num_classes), 1)
        # 20, 20, 1024 =&gt; 20, 20, 3 * (5 + num_classes) =&gt; 20, 20, 3 * (4 + 1 + num_classes)
        self.yolo_head_P5 = nn.Conv2d(base_channels * 16, len(anchors_mask[0]) * (5 + num_classes), 1)
    def forward(self, x):
        #---------------------------------------------------#
        #   第三个特征层
        #   y3=(batch_size,75,80,80)
        #---------------------------------------------------#
        out2 = self.yolo_head_P3(P3)
        #---------------------------------------------------#
        #   第二个特征层
        #   y2=(batch_size,75,40,40)
        #---------------------------------------------------#
        out1 = self.yolo_head_P4(P4)
        #---------------------------------------------------#
        #   第一个特征层
        #   y1=(batch_size,75,20,20)
        #---------------------------------------------------#
        out0 = self.yolo_head_P5(P5)
        return out0, out1, out2
</code></pre>
<p>注：</p>
<p><code>anchor_mask=[[6, 7, 8], [3, 4, 5], [0, 1, 2]]</code>, 表示三个特征图的9个先验框，因此每一个特征图上每一个特点上存在3个先验框。</p>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><h3 id="获得预测框和得分"><a href="#获得预测框和得分" class="headerlink" title="获得预测框和得分"></a>获得预测框和得分</h3><p>假设我们使用coco数据集进行训练，由第二步我们可以获得三个特征层的预测结果，shape分别为(N,20,20,255)，(N,40,40,255)，(N,80,80,255)的数据。</p>
<p>但是这个预测结果并不对应着最终的预测框在图片上的位置，还需要解码才可以完成。在YoloV5里，每一个特征层上每一个特征点存在3个先验框。</p>
<p>每个特征层最后的255可以拆分成3个85，对应3个先验框的85个参数，我们先将其reshape一下，其结果为(N,20,20,3,85)，(N,40.40,3,85)，(N,80,80,3,85)。</p>
<p>其中的85可以拆分成4+1+80。<br>前4个参数用于判断每一个特征点的回归参数，回归参数调整后可以获得预测框；<br>第5个参数用于判断每一个特征点是否包含物体；<br>最后80个参数用于判断每一个特征点所包含的物体种类。</p>
<p>以(N,20,20,3,85)这个特征层为例，该特征层相当于将图像划分成20x20个特征点，如果某个特征点落在物体的对应框内，就用于预测该物体。</p>
<p>如图所示，蓝色的点为20x20的特征点，此时我们对左图黑色点的三个先验框进行解码操作演示：<br>1、进行中心预测点的计算，利用Regression预测结果前两个序号的内容对特征点的三个先验框中心坐标进行偏移，偏移后是右图红色的三个点；<br>2、进行预测框宽高的计算，利用Regression预测结果后两个序号的内容求指数后获得预测框的宽高；<br>3、此时获得的预测框就可以绘制在图片上了。<br><img src="/CN/YOLOV5/93fc40f7a47f46bbb37819115826ec1a.png" class="lazyload" data-srcset="/CN/YOLOV5/93fc40f7a47f46bbb37819115826ec1a.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>除去这样的解码操作，还有非极大抑制的操作需要进行，防止同一种类的框的堆积。</p>
<p>代码如下：</p>
<pre><code class="lang-python"># utils/utils_bbox.py
def decode_box(self, inputs):
    outputs = []
    for i, input in enumerate(inputs):
        #-----------------------------------------------#
        #   输入的input一共有三个，他们的shape分别是
        #   batch_size, 255, 20, 20
        #   batch_size, 255, 40, 40
        #   batch_size, 255, 80, 80
        #-----------------------------------------------#
        batch_size      = input.size(0)
        input_height    = input.size(2)
        input_width     = input.size(3)

        #-----------------------------------------------#
        #   输入为416x416时
        #   stride_h = stride_w = 32、16、8
        #-----------------------------------------------#
        stride_h = self.input_shape[0] / input_height
        stride_w = self.input_shape[1] / input_width
        #-------------------------------------------------#
        #   此时获得的scaled_anchors大小是相对于特征层的
        #-------------------------------------------------#
        scaled_anchors = [(anchor_width / stride_w, anchor_height / stride_h) for anchor_width, anchor_height in self.anchors[self.anchors_mask[i]]]

        #-----------------------------------------------#
        #   输入的input一共有三个，他们的shape分别是
        #   batch_size, 3, 20, 20, 85
        #   batch_size, 3, 40, 40, 85
        #   batch_size, 3, 80, 80, 85
        #-----------------------------------------------#
        prediction = input.view(batch_size, len(self.anchors_mask[i]),
                                self.bbox_attrs, input_height, input_width).permute(0, 1, 3, 4, 2).contiguous()

        #-----------------------------------------------#
        #   先验框的中心位置的调整参数
        #-----------------------------------------------#
        x = torch.sigmoid(prediction[..., 0])  
        y = torch.sigmoid(prediction[..., 1])
        #-----------------------------------------------#
        #   先验框的宽高调整参数
        #-----------------------------------------------#
        w = torch.sigmoid(prediction[..., 2]) 
        h = torch.sigmoid(prediction[..., 3]) 
        #-----------------------------------------------#
        #   获得置信度，是否有物体
        #-----------------------------------------------#
        conf        = torch.sigmoid(prediction[..., 4])
        #-----------------------------------------------#
        #   种类置信度
        #-----------------------------------------------#
        pred_cls    = torch.sigmoid(prediction[..., 5:])

        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor
        LongTensor  = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor

        #----------------------------------------------------------#
        #   生成网格，先验框中心，网格左上角 
        #   batch_size,3,20,20
        #----------------------------------------------------------#
        grid_x = torch.linspace(0, input_width - 1, input_width).repeat(input_height, 1).repeat(
            batch_size * len(self.anchors_mask[i]), 1, 1).view(x.shape).type(FloatTensor)
        grid_y = torch.linspace(0, input_height - 1, input_height).repeat(input_width, 1).t().repeat(
            batch_size * len(self.anchors_mask[i]), 1, 1).view(y.shape).type(FloatTensor)

        #----------------------------------------------------------#
        #   按照网格格式生成先验框的宽高
        #   batch_size,3,20,20
        #----------------------------------------------------------#
        anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))
        anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))
        anchor_w = anchor_w.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(w.shape)
        anchor_h = anchor_h.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(h.shape)

        #----------------------------------------------------------#
        #   利用预测结果对先验框进行调整
        #   首先调整先验框的中心，从先验框中心向右下角偏移
        #   再调整先验框的宽高。
        #----------------------------------------------------------#
        pred_boxes          = FloatTensor(prediction[..., :4].shape)
        pred_boxes[..., 0]  = x.data * 2. - 0.5 + grid_x
        pred_boxes[..., 1]  = y.data * 2. - 0.5 + grid_y
        pred_boxes[..., 2]  = (w.data * 2) ** 2 * anchor_w
        pred_boxes[..., 3]  = (h.data * 2) ** 2 * anchor_h

        #----------------------------------------------------------#
        #   将输出结果归一化成小数的形式
        #----------------------------------------------------------#
        _scale = torch.Tensor([input_width, input_height, input_width, input_height]).type(FloatTensor)
        output = torch.cat((pred_boxes.view(batch_size, -1, 4) / _scale,
                            conf.view(batch_size, -1, 1), pred_cls.view(batch_size, -1, self.num_classes)), -1)
        outputs.append(output.data)
    return outputs
</code></pre>
<h3 id="得分筛选和非极大值抑制"><a href="#得分筛选和非极大值抑制" class="headerlink" title="得分筛选和非极大值抑制"></a>得分筛选和非极大值抑制</h3><p>得到最终的预测结果后还要进行得分排序与非极大抑制筛选。</p>
<p>得分筛选就是筛选出得分满足confidence置信度的预测框。<br>非极大抑制就是筛选出一定区域内属于同一种类得分最大的框。</p>
<p>得分筛选与非极大抑制的过程可以概括如下：<br>1、找出该图片中得分大于门限函数的框。在进行重合框筛选前就进行得分的筛选可以大幅度减少框的数量。<br>2、对种类进行循环，非极大抑制的作用是筛选出一定区域内属于同一种类得分最大的框，对种类进行循环可以帮助我们对每一个类分别进行非极大抑制。<br>3、根据得分对该种类进行从大到小排序。<br>4、每次取出得分最大的框，计算其与其它所有预测框的重合程度，重合程度过大的则剔除。</p>
<p>得分筛选与非极大抑制后的结果就可以用于绘制预测框了。</p>
<p>下图是经过非极大抑制的：</p>
<p><img src="/CN/YOLOV5/20200526140617608.png" class="lazyload" data-srcset="/CN/YOLOV5/20200526140617608.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>下图是未经过非极大值抑制的：</p>
<p><img src="/CN/YOLOV5/20200519211538419.png" class="lazyload" data-srcset="/CN/YOLOV5/20200519211538419.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>实现代码如下：</p>
<pre><code class="lang-python">def non_max_suppression(self, prediction, num_classes, input_shape, image_shape, letterbox_image, conf_thres=0.5, nms_thres=0.4):
    #----------------------------------------------------------#
    #   将预测结果的格式转换成左上角右下角的格式。
    #   prediction  [batch_size, num_anchors, 85]
    #----------------------------------------------------------#
    box_corner          = prediction.new(prediction.shape)
    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2
    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2
    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2
    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2
    prediction[:, :, :4] = box_corner[:, :, :4]

    output = [None for _ in range(len(prediction))]
    for i, image_pred in enumerate(prediction):
        #----------------------------------------------------------#
        #   对种类预测部分取max。
        #   class_conf  [num_anchors, 1]    种类置信度
        #   class_pred  [num_anchors, 1]    种类
        #----------------------------------------------------------#
        class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1, keepdim=True)

        #----------------------------------------------------------#
        #   利用置信度进行第一轮筛选
        #----------------------------------------------------------#
        conf_mask = (image_pred[:, 4] * class_conf[:, 0] &gt;= conf_thres).squeeze()

        #----------------------------------------------------------#
        #   根据置信度进行预测结果的筛选
        #----------------------------------------------------------#
        image_pred = image_pred[conf_mask]
        class_conf = class_conf[conf_mask]
        class_pred = class_pred[conf_mask]
        if not image_pred.size(0):
            continue
        #-------------------------------------------------------------------------#
        #   detections  [num_anchors, 7]
        #   7的内容为：x1, y1, x2, y2, obj_conf, class_conf, class_pred
        #-------------------------------------------------------------------------#
        detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)

        #------------------------------------------#
        #   获得预测结果中包含的所有种类
        #------------------------------------------#
        unique_labels = detections[:, -1].cpu().unique()

        if prediction.is_cuda:
            unique_labels = unique_labels.cuda()
            detections = detections.cuda()

        for c in unique_labels:
            #------------------------------------------#
            #   获得某一类得分筛选后全部的预测结果
            #------------------------------------------#
            detections_class = detections[detections[:, -1] == c]

            #------------------------------------------#
            #   使用官方自带的非极大抑制会速度更快一些！
            #------------------------------------------#
            keep = nms(
                detections_class[:, :4],
                detections_class[:, 4] * detections_class[:, 5],
                nms_thres
            )
            max_detections = detections_class[keep]

            # # 按照存在物体的置信度排序
            # _, conf_sort_index = torch.sort(detections_class[:, 4]*detections_class[:, 5], descending=True)
            # detections_class = detections_class[conf_sort_index]
            # # 进行非极大抑制
            # max_detections = []
            # while detections_class.size(0):
            #     # 取出这一类置信度最高的，一步一步往下判断，判断重合程度是否大于nms_thres，如果是则去除掉
            #     max_detections.append(detections_class[0].unsqueeze(0))
            #     if len(detections_class) == 1:
            #         break
            #     ious = bbox_iou(max_detections[-1], detections_class[1:])
            #     detections_class = detections_class[1:][ious &lt; nms_thres]
            # # 堆叠
            # max_detections = torch.cat(max_detections).data

            # Add max detections to outputs
            output[i] = max_detections if output[i] is None else torch.cat((output[i], max_detections))

        if output[i] is not None:
            output[i]           = output[i].cpu().numpy()
            box_xy, box_wh      = (output[i][:, 0:2] + output[i][:, 2:4])/2, output[i][:, 2:4] - output[i][:, 0:2]
            output[i][:, :4]    = self.yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image)
    return output
</code></pre>
<h2 id="训练部分"><a href="#训练部分" class="headerlink" title="训练部分"></a>训练部分</h2><h3 id="loss组成"><a href="#loss组成" class="headerlink" title="loss组成"></a>loss组成</h3><p>计算loss实际上是网络的预测结果和网络的真实结果的对比。<br>和网络的预测结果一样，网络的损失也由三个部分组成，分别是Reg部分、Obj部分、Cls部分。Reg部分是特征点的回归参数判断、Obj部分是特征点是否包含物体判断、Cls部分是特征点包含的物体的种类。</p>
<h3 id="正样本的匹配过程"><a href="#正样本的匹配过程" class="headerlink" title="正样本的匹配过程"></a>正样本的匹配过程</h3><p>在YoloV5中，训练时正样本的匹配过程可以分为两部分。<br>a、匹配先验框。<br>b、匹配特征点。</p>
<p>所谓<strong>正样本匹配</strong>，就是<strong>寻找哪些先验框被认为有对应的真实框，并且负责这个真实框的预测</strong>。</p>
<h4 id="匹配先验框"><a href="#匹配先验框" class="headerlink" title="匹配先验框"></a>匹配先验框</h4><p>在YoloV5网络中，一共设计了9个不同大小的先验框。每个输出的特征层对应3个先验框。</p>
<p>对于任何一个真实框gt，YoloV5不再使用iou进行正样本的匹配，而是直接采用高宽比进行匹配，即使用真实框和9个不同大小的先验框计算宽高比。</p>
<p>如果真实框与某个先验框的宽高比例大于设定阈值，则说明该真实框和该先验框匹配度不够，将该先验框认为是负样本。</p>
<p>比如此时有一个真实框，它的宽高为[200, 200]，是一个正方形。YoloV5默认设置的9个先验框为[10,13], [16,30], [33,23], [30,61], [62,45], [59,119], [116,90], [156,198], [373,326]。设定阈值门限为4。</p>
<p>此时我们需要计算该真实框和9个先验框的宽高比例。比较宽高时存在两个情况，一个是真实框的宽高比先验框大，一个是先验框的宽高比真实框大。因此我们需要同时计算：真实框的宽高/先验框的宽高；先验框的宽高/真实框的宽高。然后在这其中选取最大值。</p>
<p>下个列表就是比较结果，这是一个shape为[9, 4]的矩阵，9代表9个先验框，4代表真实框的宽高/先验框的宽高；先验框的宽高/真实框的宽高。</p>
<pre><code class="lang-python">[[20.         15.38461538  0.05        0.065     ]
 [12.5         6.66666667  0.08        0.15      ]
 [ 6.06060606  8.69565217  0.165       0.115     ]
 [ 6.66666667  3.27868852  0.15        0.305     ]
 [ 3.22580645  4.44444444  0.31        0.225     ]
 [ 3.38983051  1.68067227  0.295       0.595     ]
 [ 1.72413793  2.22222222  0.58        0.45      ]
 [ 1.28205128  1.01010101  0.78        0.99      ]
 [ 0.53619303  0.61349693  1.865       1.63      ]]
</code></pre>
<p>然后对每个先验框的比较结果取最大值。获得下述矩阵：</p>
<pre><code class="lang-python">[20.         12.5         8.69565217  6.66666667  4.44444444  3.38983051
   2.22222222  1.28205128  1.865     ]
</code></pre>
<p>之后我们判断，哪些先验框的比较结果的值小于门限。可以知道[59,119], [116,90], [156,198], [373,326]四个先验框均满足需求。</p>
<p>[116,90], [156,198], [373,326]属于20,20的特征层。<br>[59,119]属于40,40的特征层。</p>
<p>此时我们已经可以判断哪些大小的先验框可用于该真实框的预测。</p>
<h4 id="匹配特征点"><a href="#匹配特征点" class="headerlink" title="匹配特征点"></a>匹配特征点</h4><p>在过去的Yolo系列中，每个真实框由其中心点所在的网格内的左上角特征点来负责预测。</p>
<p>对于被选中的特征层，首先计算真实框落在哪个网格内，此时<strong>该网格左上角特征点便是一个负责预测的特征点。</strong></p>
<p><strong>同时利用四舍五入规则，找出最近的两个网格，将这三个网格</strong>都认为是负责预测该真实框的。</p>
<p><img src="/CN/YOLOV5/ddf3729aef4240b3aa6e5aa914ffff52.png" class="lazyload" data-srcset="/CN/YOLOV5/ddf3729aef4240b3aa6e5aa914ffff52.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p><strong>红色点表示该真实框的中心</strong>，除了当前所处的网格外，其2个最近的邻域网格也被选中。从这里就可以发现预测框的XY轴偏移部分的取值范围不再是0-1，而是0.5-1.5。</p>
<p><strong>找到对应特征点后，对应特征点在a中被选中的先验框负责该真实框的预测。</strong></p>
<h3 id="计算loss"><a href="#计算loss" class="headerlink" title="计算loss"></a>计算loss</h3><p>由第一部分可知，YoloV5的损失由三个部分组成：<br>1、Reg部分，由第2部分可知道每个真实框对应的先验框，获取到每个框对应的先验框后，取出该先验框对应的预测框，利用真实框和预测框计算CIOU损失，作为Reg部分的Loss组成。<br>2、Obj部分，由第2部分可知道每个真实框对应的先验框，所有真实框对应的先验框都是正样本，剩余的先验框均为负样本，根据正负样本和特征点的是否包含物体的预测结果计算交叉熵损失，作为Obj部分的Loss组成。<br>3、Cls部分，由第三部分可知道每个真实框对应的先验框，获取到每个框对应的先验框后，取出该先验框的种类预测结果，根据真实框的种类和先验框的种类预测结果计算交叉熵损失，作为Cls部分的Loss组成。<br>实现代码如下：</p>
<pre><code class="lang-python">import torch
import torch.nn as nn
import math
import numpy as np

class YOLOLoss(nn.Module):
    def __init__(self, anchors, num_classes, input_shape, cuda, anchors_mask = [[6,7,8], [3,4,5], [0,1,2]], label_smoothing = 0):
        super(YOLOLoss, self).__init__()
        #-----------------------------------------------------------#
        #   13x13的特征层对应的anchor是[142, 110],[192, 243],[459, 401]
        #   26x26的特征层对应的anchor是[36, 75],[76, 55],[72, 146]
        #   52x52的特征层对应的anchor是[12, 16],[19, 36],[40, 28]
        #-----------------------------------------------------------#
        self.anchors        = anchors
        self.num_classes    = num_classes
        self.bbox_attrs     = 5 + num_classes
        self.input_shape    = input_shape
        self.anchors_mask   = anchors_mask
        self.label_smoothing = label_smoothing

        self.threshold      = 4

        self.balance        = [0.4, 1.0, 4]
        self.box_ratio      = 5
        self.cls_ratio      = 0.5
        self.obj_ratio      = 1
        self.cuda = cuda

    def clip_by_tensor(self, t, t_min, t_max):
        t = t.float()
        result = (t &gt;= t_min).float() * t + (t &lt; t_min).float() * t_min
        result = (result &lt;= t_max).float() * result + (result &gt; t_max).float() * t_max
        return result

    def MSELoss(self, pred, target):
        return torch.pow(pred - target, 2)

    def BCELoss(self, pred, target):
        epsilon = 1e-7
        pred    = self.clip_by_tensor(pred, epsilon, 1.0 - epsilon)
        output  = - target * torch.log(pred) - (1.0 - target) * torch.log(1.0 - pred)
        return output

    def box_giou(self, b1, b2):
        &quot;&quot;&quot;
        输入为：
        ----------
        b1: tensor, shape=(batch, feat_w, feat_h, anchor_num, 4), xywh
        b2: tensor, shape=(batch, feat_w, feat_h, anchor_num, 4), xywh

        返回为：
        -------
        giou: tensor, shape=(batch, feat_w, feat_h, anchor_num, 1)
        &quot;&quot;&quot;
        #----------------------------------------------------#
        #   求出预测框左上角右下角
        #----------------------------------------------------#
        b1_xy       = b1[..., :2]
        b1_wh       = b1[..., 2:4]
        b1_wh_half  = b1_wh/2.
        b1_mins     = b1_xy - b1_wh_half
        b1_maxes    = b1_xy + b1_wh_half
        #----------------------------------------------------#
        #   求出真实框左上角右下角
        #----------------------------------------------------#
        b2_xy       = b2[..., :2]
        b2_wh       = b2[..., 2:4]
        b2_wh_half  = b2_wh/2.
        b2_mins     = b2_xy - b2_wh_half
        b2_maxes    = b2_xy + b2_wh_half

        #----------------------------------------------------#
        #   求真实框和预测框所有的iou
        #----------------------------------------------------#
        intersect_mins  = torch.max(b1_mins, b2_mins)
        intersect_maxes = torch.min(b1_maxes, b2_maxes)
        intersect_wh    = torch.max(intersect_maxes - intersect_mins, torch.zeros_like(intersect_maxes))
        intersect_area  = intersect_wh[..., 0] * intersect_wh[..., 1]
        b1_area         = b1_wh[..., 0] * b1_wh[..., 1]
        b2_area         = b2_wh[..., 0] * b2_wh[..., 1]
        union_area      = b1_area + b2_area - intersect_area
        iou             = intersect_area / union_area

        #----------------------------------------------------#
        #   找到包裹两个框的最小框的左上角和右下角
        #----------------------------------------------------#
        enclose_mins    = torch.min(b1_mins, b2_mins)
        enclose_maxes   = torch.max(b1_maxes, b2_maxes)
        enclose_wh      = torch.max(enclose_maxes - enclose_mins, torch.zeros_like(intersect_maxes))
        #----------------------------------------------------#
        #   计算对角线距离
        #----------------------------------------------------#
        enclose_area    = enclose_wh[..., 0] * enclose_wh[..., 1]
        giou            = iou - (enclose_area - union_area) / enclose_area

        return giou

    #---------------------------------------------------#
    #   平滑标签
    #---------------------------------------------------#
    def smooth_labels(self, y_true, label_smoothing, num_classes):
        return y_true * (1.0 - label_smoothing) + label_smoothing / num_classes

    def forward(self, l, input, targets=None):
        #----------------------------------------------------#
        #   l 代表使用的是第几个有效特征层
        #   input的shape为  bs, 3*(5+num_classes), 13, 13
        #                   bs, 3*(5+num_classes), 26, 26
        #                   bs, 3*(5+num_classes), 52, 52
        #   targets 真实框的标签情况 [batch_size, num_gt, 5]
        #----------------------------------------------------#
        #--------------------------------#
        #   获得图片数量，特征层的高和宽
        #--------------------------------#
        bs      = input.size(0)
        in_h    = input.size(2)
        in_w    = input.size(3)
        #-----------------------------------------------------------------------#
        #   计算步长
        #   每一个特征点对应原来的图片上多少个像素点
        #   
        #   如果特征层为13x13的话，一个特征点就对应原来的图片上的32个像素点
        #   如果特征层为26x26的话，一个特征点就对应原来的图片上的16个像素点
        #   如果特征层为52x52的话，一个特征点就对应原来的图片上的8个像素点
        #   stride_h = stride_w = 32、16、8
        #-----------------------------------------------------------------------#
        stride_h = self.input_shape[0] / in_h
        stride_w = self.input_shape[1] / in_w
        #-------------------------------------------------#
        #   此时获得的scaled_anchors大小是相对于特征层的
        #-------------------------------------------------#
        scaled_anchors  = [(a_w / stride_w, a_h / stride_h) for a_w, a_h in self.anchors]
        #-----------------------------------------------#
        #   输入的input一共有三个，他们的shape分别是
        #   bs, 3 * (5+num_classes), 13, 13 =&gt; bs, 3, 5 + num_classes, 13, 13 =&gt; batch_size, 3, 13, 13, 5 + num_classes

        #   batch_size, 3, 13, 13, 5 + num_classes
        #   batch_size, 3, 26, 26, 5 + num_classes
        #   batch_size, 3, 52, 52, 5 + num_classes
        #-----------------------------------------------#
        prediction = input.view(bs, len(self.anchors_mask[l]), self.bbox_attrs, in_h, in_w).permute(0, 1, 3, 4, 2).contiguous()

        #-----------------------------------------------#
        #   先验框的中心位置的调整参数
        #-----------------------------------------------#
        x = torch.sigmoid(prediction[..., 0])
        y = torch.sigmoid(prediction[..., 1])
        #-----------------------------------------------#
        #   先验框的宽高调整参数
        #-----------------------------------------------#
        w = torch.sigmoid(prediction[..., 2]) 
        h = torch.sigmoid(prediction[..., 3]) 
        #-----------------------------------------------#
        #   获得置信度，是否有物体
        #-----------------------------------------------#
        conf = torch.sigmoid(prediction[..., 4])
        #-----------------------------------------------#
        #   种类置信度
        #-----------------------------------------------#
        pred_cls = torch.sigmoid(prediction[..., 5:])
        #-----------------------------------------------#
        #   获得网络应该有的预测结果
        #-----------------------------------------------#
        y_true, noobj_mask, box_loss_scale = self.get_target(l, targets, scaled_anchors, in_h, in_w)

        #---------------------------------------------------------------#
        #   将预测结果进行解码，判断预测结果和真实值的重合程度
        #   如果重合程度过大则忽略，因为这些特征点属于预测比较准确的特征点
        #   作为负样本不合适
        #----------------------------------------------------------------#
        pred_boxes = self.get_pred_boxes(l, x, y, h, w, targets, scaled_anchors, in_h, in_w)

        if self.cuda:
            y_true          = y_true.cuda()
            noobj_mask      = noobj_mask.cuda()
            box_loss_scale  = box_loss_scale.cuda()
        #-----------------------------------------------------------#
        #   reshape_y_true[...,2:3]和reshape_y_true[...,3:4]
        #   表示真实框的宽高，二者均在0-1之间
        #   真实框越大，比重越小，小框的比重更大。
        #-----------------------------------------------------------#
        box_loss_scale = 2 - box_loss_scale

        #---------------------------------------------------------------#
        #   计算预测结果和真实结果的giou
        #----------------------------------------------------------------#
        giou        = self.box_giou(pred_boxes[y_true[..., 4] == 1], y_true[..., :4][y_true[..., 4] == 1])

        loss_loc    = torch.sum((1 - giou) * box_loss_scale[y_true[..., 4] == 1])
        #-----------------------------------------------------------#
        #   计算置信度的loss
        #-----------------------------------------------------------#
        loss_conf   = torch.sum(self.BCELoss(conf[y_true[..., 4] == 1], giou.detach().clamp(0))) + \
                      torch.sum(self.BCELoss(conf, y_true[..., 4]) * noobj_mask)
        loss_cls    = torch.sum(self.BCELoss(pred_cls[y_true[..., 4] == 1], self.smooth_labels(y_true[..., 5:][y_true[..., 4] == 1], self.label_smoothing, self.num_classes)))

        loss        = loss_loc * self.box_ratio + loss_conf * self.balance[l] * self.obj_ratio + loss_cls * self.cls_ratio
        num_pos = torch.sum(y_true[..., 4])
        num_pos = torch.max(num_pos, torch.ones_like(num_pos))
        return loss, num_pos

    def get_near_points(self, x, y, i, j):
        sub_x = x - i
        sub_y = y - j
        if sub_x &gt; 0.5 and sub_y &gt; 0.5:
            return [[0, 0], [1, 0], [0, 1]]
        elif sub_x &lt; 0.5 and sub_y &gt; 0.5:
            return [[0, 0], [-1, 0], [0, 1]]
        elif sub_x &lt; 0.5 and sub_y &lt; 0.5:
            return [[0, 0], [-1, 0], [0, -1]]
        else:
            return [[0, 0], [1, 0], [0, -1]]

    def get_target(self, l, targets, anchors, in_h, in_w):
        #-----------------------------------------------------#
        #   计算一共有多少张图片
        #-----------------------------------------------------#
        bs              = len(targets)
        #-----------------------------------------------------#
        #   用于选取哪些先验框不包含物体
        #-----------------------------------------------------#
        noobj_mask      = torch.ones(bs, len(self.anchors_mask[l]), in_h, in_w, requires_grad = False)
        #-----------------------------------------------------#
        #   让网络更加去关注小目标
        #-----------------------------------------------------#
        box_loss_scale  = torch.zeros(bs, len(self.anchors_mask[l]), in_h, in_w, requires_grad = False)
        #-----------------------------------------------------#
        #   anchors_best_ratio
        #-----------------------------------------------------#
        box_best_ratio = torch.zeros(bs, len(self.anchors_mask[l]), in_h, in_w, requires_grad = False)
        #-----------------------------------------------------#
        #   batch_size, 3, 13, 13, 5 + num_classes
        #-----------------------------------------------------#
        y_true          = torch.zeros(bs, len(self.anchors_mask[l]), in_h, in_w, self.bbox_attrs, requires_grad = False)
        for b in range(bs):            
            if len(targets[b])==0:
                continue
            batch_target = torch.zeros_like(targets[b])
            #-------------------------------------------------------#
            #   计算出正样本在特征层上的中心点
            #-------------------------------------------------------#
            batch_target[:, [0,2]] = targets[b][:, [0,2]] * in_w
            batch_target[:, [1,3]] = targets[b][:, [1,3]] * in_h
            batch_target[:, 4] = targets[b][:, 4]
            batch_target = batch_target.cpu()

            #-------------------------------------------------------#
            #   batch_target            : num_true_box, 4
            #   anchors                 : 9, 2
            #
            #   ratios_of_gt_anchors    : num_true_box, 9, 2
            #   ratios_of_anchors_gt    : num_true_box, 9, 2
            #
            #   ratios                  : num_true_box, 9, 4
            #   max_ratios              : num_true_box, 9
            #-------------------------------------------------------#
            ratios_of_gt_anchors = torch.unsqueeze(batch_target[:, 2:4], 1) / torch.unsqueeze(torch.FloatTensor(anchors), 0)
            ratios_of_anchors_gt = torch.unsqueeze(torch.FloatTensor(anchors), 0) /  torch.unsqueeze(batch_target[:, 2:4], 1)
            ratios               = torch.cat([ratios_of_gt_anchors, ratios_of_anchors_gt], dim = -1)
            max_ratios, _        = torch.max(ratios, dim = -1)

            for t, ratio in enumerate(max_ratios):
                #-------------------------------------------------------#
                #   ratio : 9
                #-------------------------------------------------------#
                over_threshold = ratio &lt; self.threshold
                over_threshold[torch.argmin(ratio)] = True
                for k, mask in enumerate(self.anchors_mask[l]):
                    if not over_threshold[mask]:
                        continue
                    #----------------------------------------#
                    #   获得真实框属于哪个网格点
                    #----------------------------------------#
                    i = torch.floor(batch_target[t, 0]).long()
                    j = torch.floor(batch_target[t, 1]).long()

                    offsets = self.get_near_points(batch_target[t, 0], batch_target[t, 1], i, j)
                    for offset in offsets:
                        local_i = i + offset[0]
                        local_j = j + offset[1]

                        if local_i &gt;= in_w or local_i &lt; 0 or local_j &gt;= in_h or local_j &lt; 0:
                            continue

                        if box_best_ratio[b, k, local_j, local_i] != 0:
                            if box_best_ratio[b, k, local_j, local_i] &gt; ratio[mask]:
                                y_true[b, k, local_j, local_i, :] = 0
                            else:
                                continue

                        #----------------------------------------#
                        #   取出真实框的种类
                        #----------------------------------------#
                        c = batch_target[t, 4].long()

                        #----------------------------------------#
                        #   noobj_mask代表无目标的特征点
                        #----------------------------------------#
                        noobj_mask[b, k, local_j, local_i] = 0
                        #----------------------------------------#
                        #   tx、ty代表中心调整参数的真实值
                        #----------------------------------------#
                        y_true[b, k, local_j, local_i, 0] = batch_target[t, 0]
                        y_true[b, k, local_j, local_i, 1] = batch_target[t, 1]
                        y_true[b, k, local_j, local_i, 2] = batch_target[t, 2]
                        y_true[b, k, local_j, local_i, 3] = batch_target[t, 3]
                        y_true[b, k, local_j, local_i, 4] = 1
                        y_true[b, k, local_j, local_i, c + 5] = 1
                        #----------------------------------------#
                        #   用于获得xywh的比例
                        #   大目标loss权重小，小目标loss权重大
                        #----------------------------------------#
                        box_loss_scale[b, k, local_j, local_i] = batch_target[t, 2] * batch_target[t, 3] / in_w / in_h
                        #----------------------------------------#
                        #   获得当前先验框最好的比例
                        #----------------------------------------#
                        box_best_ratio[b, k, local_j, local_i] = ratio[mask]

        return y_true, noobj_mask, box_loss_scale

    def get_pred_boxes(self, l, x, y, h, w, targets, scaled_anchors, in_h, in_w):
        #-----------------------------------------------------#
        #   计算一共有多少张图片
        #-----------------------------------------------------#
        bs = len(targets)

        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor
        LongTensor  = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor
        #-----------------------------------------------------#
        #   生成网格，先验框中心，网格左上角
        #-----------------------------------------------------#
        grid_x = torch.linspace(0, in_w - 1, in_w).repeat(in_h, 1).repeat(
            int(bs * len(self.anchors_mask[l])), 1, 1).view(x.shape).type(FloatTensor)
        grid_y = torch.linspace(0, in_h - 1, in_h).repeat(in_w, 1).t().repeat(
            int(bs * len(self.anchors_mask[l])), 1, 1).view(y.shape).type(FloatTensor)

        # 生成先验框的宽高
        scaled_anchors_l = np.array(scaled_anchors)[self.anchors_mask[l]]
        anchor_w = FloatTensor(scaled_anchors_l).index_select(1, LongTensor([0]))
        anchor_h = FloatTensor(scaled_anchors_l).index_select(1, LongTensor([1]))

        anchor_w = anchor_w.repeat(bs, 1).repeat(1, 1, in_h * in_w).view(w.shape)
        anchor_h = anchor_h.repeat(bs, 1).repeat(1, 1, in_h * in_w).view(h.shape)
        #-------------------------------------------------------#
        #   计算调整后的先验框中心与宽高
        #-------------------------------------------------------#
        pred_boxes_x    = torch.unsqueeze(x * 2. - 0.5 + grid_x, -1)
        pred_boxes_y    = torch.unsqueeze(y * 2. - 0.5 + grid_y, -1)
        pred_boxes_w    = torch.unsqueeze((w * 2) ** 2 * anchor_w, -1)
        pred_boxes_h    = torch.unsqueeze((h * 2) ** 2 * anchor_h, -1)
        pred_boxes      = torch.cat([pred_boxes_x, pred_boxes_y, pred_boxes_w, pred_boxes_h], dim = -1)
        return pred_boxes
</code></pre>
]]></content>
      <categories>
        <category>论文详解</category>
      </categories>
      <tags>
        <tag>yolov5</tag>
      </tags>
  </entry>
  <entry>
    <title>CenterNet详解</title>
    <url>/CN/centernet/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文地址：<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Duan_CenterNet_Keypoint_Triplets_for_Object_Detection_ICCV_2019_paper.pdf">CenterNet: Keypoint Triplets for Object Detection (thecvf.com)</a></p>
<p>源码地址： <a href="https://github.com/Duankaiwen/CenterNet">CenterNet: Keypoint Triplets for Object Detection</a></p>
<p>文章引用代码地址：</p>
<p>文章出处：</p>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>论文详解</tag>
      </tags>
  </entry>
  <entry>
    <title>Concat和add的区别</title>
    <url>/CN/concat_add/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://openreview.net/pdf?id=q2ZaVU6bEsT">https://openreview.net/pdf?id=q2ZaVU6bEsT</a></p>
<p>今天偶然看到了这篇论文，论文里有一部分讲到了concat,add还有自适应的关系，很感兴趣，记录下来。</p>
<p><img src="/CN/concat_add/image-20220508171742020.png" class="lazyload" data-srcset="/CN/concat_add/image-20220508171742020.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220508171742020" style="zoom:50%;"></p>
<p>方法（a）和（c）分别是加权融合和连接操作。即在空间和通道维度上直接添加特征图。方法（b）是一种自适应融合方法。具体来说，假设输入的大小可以表示为（bs，C，H，W），我们可以通过卷积运算得到（bs，3，H，W）的空间自适应权重，连接和 Softmax。三个通道与三个输入一一对应，通过计算加权和可以将上下文信息聚合到输出。</p>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>github使用指南</title>
    <url>/CN/github%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h4 id="git安装"><a href="#git安装" class="headerlink" title="git安装"></a>git安装</h4><p>1.安装git OSX版</p>
<p>下载地址：<a href="http://git-scm.com/download/mac">http://git-scm.com/download/mac</a></p>
<p>2.安装git Windows版</p>
<p>下载地址：<a href="http://book.git-scm.com/download/win">http://book.git-scm.com/download/win</a></p>
<p>3.安装git Linux版</p>
<p>下载地址：<a href="http://book.git-scm.com/download/linux">http://book.git-scm.com/download/linux</a></p>
<h4 id="创建新仓库"><a href="#创建新仓库" class="headerlink" title="创建新仓库"></a>创建新仓库</h4><p>创建新文件夹，打开，然后执行<code>git init</code>以创建新的git仓库</p>
<h4 id="检出仓库"><a href="#检出仓库" class="headerlink" title="检出仓库"></a>检出仓库</h4><p>执行如下命令以创建一个本地仓库的克隆版本：</p>
<p><code>git clone /path/to/repository</code></p>
<p>如果是远端服务器上的仓库，则使用如下命令：</p>
<p><code>git clone username@host:/path/to/repository</code></p>
<h4 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h4><p>你的本地仓库由git维护的三棵“树”组成。第一个是你的<strong>工作目录</strong>，它持有实际文件；第二个是<strong>暂存区（index)</strong>,它像个缓存区域，临时保存你的改动；最后是<strong>HEAD</strong>,它指向你最后一次提交的结果。</p>
<h4 id="添加和提交"><a href="#添加和提交" class="headerlink" title="添加和提交"></a>添加和提交</h4><p>你可以提出更改（把它们添加到暂存区），使用如下命令：</p>
<p><code>git add &lt;filename&gt;</code></p>
<p><code>git add *</code></p>
<p>这是git基本工作流程的第一步；使用如下命令以实际提交改动：</p>
<p><code>git commit -m &quot;代码提交信息&quot;</code></p>
<p>现在，你的改动已经提交到了HEAD,但是还没到你的远端仓库。</p>
<h4 id="推送改动"><a href="#推送改动" class="headerlink" title="推送改动"></a>推送改动</h4><p>你的改动现在已经在本地仓库的HEAD中了。执行如下命令以将这些改动提交到远端仓库：</p>
<p><code>git push origin master</code></p>
<p>可以把<em>master</em>换成你想要推送的任何分支。</p>
<p>如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加：</p>
<p><code>git remote add origin &lt;server&gt;</code></p>
<p>如此你就能够将你的改动推送到所添加的服务器上去了。</p>
<h4 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h4><p>分支是用来将特性开发绝缘开来的。在你创建仓库的时候，<em>master</em>是默认的分支。在其他分支上进行开发，完成后再将它们合并到主分支上。</p>
<p>创建一个叫做“feature_x”的分支，并切换过去：</p>
<p><code>git checkout -b feature_x</code></p>
<p>切换回主分支：</p>
<p><code>git checkout master</code></p>
<p>再把新建的分支删掉：</p>
<p><code>git branch -d feature_x</code></p>
<p>除非你将分支推送到远端仓库，不然该分支就是不为他人所见的：</p>
<p><code>git push origin &lt;branch&gt;</code></p>
<h4 id="更新与合并"><a href="#更新与合并" class="headerlink" title="更新与合并"></a>更新与合并</h4><p>要更新你的本地仓库至最新改动，执行：<br><code>git pull</code><br>以在你的工作目录中 <em>获取（fetch）</em> 并 <em>合并（merge）</em> 远端的改动。<br>要合并其他分支到你的当前分支（例如 master），执行：<br><code>git merge &lt;branch&gt;</code><br>在这两种情况下，git 都会尝试去自动合并改动。遗憾的是，这可能并非每次都成功，并可能出现<em>冲突（conflicts）</em>。 这时候就需要你修改这些文件来手动合并这些<em>冲突（conflicts）</em>。改完之后，你需要执行如下命令以将它们标记为合并成功：</p>
<p><code>git add &lt;filename&gt;</code><br>在合并改动之前，你可以使用如下命令预览差异：<br><code>git diff &lt;source_branch&gt; &lt;target_branch&gt;</code></p>
<h4 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h4><p>为软件发布创建标签是推荐的。这个概念早已存在，在 SVN 中也有。你可以执行如下命令创建一个叫做 <em>1.0.0</em> 的标签：<br><code>git tag 1.0.0 1b2e1d63ff</code><br><em>1b2e1d63ff</em> 是你想要标记的提交 ID 的前 10 位字符。可以使用下列命令获取提交 ID：<br><code>git log</code><br>你也可以使用少一点的提交 ID 前几位，只要它的指向具有唯一性。</p>
<h4 id="log"><a href="#log" class="headerlink" title="log"></a>log</h4><p>如果你想了解本地仓库的历史记录，最简单的命令就是使用:<br><code>git log</code><br>你可以添加一些参数来修改他的输出，从而得到自己想要的结果。 只看某一个人的提交记录:<br><code>git log --author=bob</code><br>一个压缩后的每一条提交记录只占一行的输出:<br><code>git log --pretty=oneline</code><br>或者你想通过 ASCII 艺术的树形结构来展示所有的分支, 每个分支都标示了他的名字和标签:<br><code>git log --graph --oneline --decorate --all</code><br>看看哪些文件改变了:<br><code>git log --name-status</code><br>这些只是你可以使用的参数中很小的一部分。更多的信息，参考：<br><code>git log --help</code></p>
<h4 id="替换本地改动"><a href="#替换本地改动" class="headerlink" title="替换本地改动"></a>替换本地改动</h4><p>假如你操作失误（当然，这最好永远不要发生），你可以使用如下命令替换掉本地改动：<br><code>git checkout -- &lt;filename&gt;</code><br>此命令会使用 HEAD 中的最新内容替换掉你的工作目录中的文件。已添加到暂存区的改动以及新文件都不会受到影响。</p>
<p>假如你想丢弃你在本地的所有改动与提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它：<br><code>git fetch origin</code><br><code>git reset --hard origin/master</code></p>
<h4 id="实用小贴士"><a href="#实用小贴士" class="headerlink" title="实用小贴士"></a>实用小贴士</h4><p>内建的图形化 git：<br><code>gitk</code><br>彩色的 git 输出：<br><code>git config color.ui true</code><br>显示历史记录时，每个提交的信息只显示一行：<br><code>git config format.pretty oneline</code><br>交互式添加文件到暂存区：<br><code>git add -i</code></p>
<p>更多内容请参考：<a href="http://rogerdudler.github.io/git-guide/index.zh.html">git - 简明指南</a></p>
]]></content>
      <categories>
        <category>github</category>
      </categories>
      <tags>
        <tag>使用指南</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo标签插件测试</title>
    <url>/CN/hexo%E6%A0%87%E7%AD%BE%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>本文语法参考<a href="https://volantis.js.org/v4/tag-plugins/">volantis4.0</a>,博客正在使用的版本为4.3.1</p>
<p>问题总结如下：</p>
<p>1.在复选框checkbox下，选中状态比不选中状态框提前，对不齐。</p>
<p>2.button基础按钮那里本文写的参考有点问题，不知道哪里出了问题，但源码没错。</p>
<h3 id="text"><a href="#text" class="headerlink" title="text"></a>text</h3><p>带 <u>下划线</u> 的文本；带 <emp>着重号</emp> 的文本；带 <wavy>波浪线</wavy> 的文本；带 <del>删除线</del> 的文本</p>
<p>键盘样式的文本：<kbd>⌘</kbd> + <kbd>D</kbd></p>
<p>密码样式的文本：<psw>这里没有验证码</psw></p>
<p>源码：</p>
<pre><code class="lang-markdown">带 &#123;% u 下划线 %&#125; 的文本；带 &#123;% emp 着重号 %&#125; 的文本；带 &#123;% wavy 波浪线 %&#125; 的文本；带 &#123;% del 删除线 %&#125; 的文本

键盘样式的文本：&#123;% kbd ⌘ %&#125; + &#123;% kbd D %&#125;

密码样式的文本：&#123;% psw 这里没有验证码 %&#125;
</code></pre>
<h3 id="span"><a href="#span" class="headerlink" title="span"></a>span</h3><p>语法：</p>
<pre><code class="lang-markdown">&#123;% span 样式参数, 文本内容 %&#125;
</code></pre>
<p>效果：</p>
<h4 id="彩色文字"><a href="#彩色文字" class="headerlink" title="彩色文字"></a>彩色文字</h4><p>在一段话中方便插入各种颜色的标签，包括：<span class="p red">红色</span>、<span class="p yellow">黄色</span>、<span class="p green">绿色</span>、<span class="p cyan">青色</span>、<span class="p blue">蓝色</span>、<span class="p gray">灰色</span>。</p>
<h4 id="超大号文字"><a href="#超大号文字" class="headerlink" title="超大号文字"></a>超大号文字</h4><p>文档「开始」页面中的标题部分就是超大号文字。</p>
<span class="p center logo large">Volantis</span>
<span class="p center small">A Wonderful Theme for Hexo</span>
<p>源码:</p>
<pre><code class="lang-markdown">#### 彩色文字

在一段话中方便插入各种颜色的标签，包括：&#123;% span red, 红色 %&#125;、&#123;% span yellow, 黄色 %&#125;、&#123;% span green, 绿色 %&#125;、&#123;% span cyan, 青色 %&#125;、&#123;% span blue, 蓝色 %&#125;、&#123;% span gray, 灰色 %&#125;。

#### 超大号文字

文档「开始」页面中的标题部分就是超大号文字。

&#123;% span center logo large, Volantis %&#125;
&#123;% span center small, A Wonderful Theme for Hexo %&#125;
</code></pre>
<p>参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>可选值</th>
</tr>
</thead>
<tbody>
<tr>
<td>字体</td>
<td><code>logo</code>, <code>code</code></td>
</tr>
<tr>
<td>颜色</td>
<td><code>red</code>, <code>yellow</code>, <code>green</code>, <code>cyan</code>, <code>blue</code>, <code>gray</code></td>
</tr>
<tr>
<td>大小</td>
<td><code>small</code>, <code>h4</code>, <code>h3</code>, <code>h2</code>, <code>h1</code>, <code>large</code>, <code>huge</code>, <code>ultra</code></td>
</tr>
<tr>
<td>对齐方向</td>
<td><code>left</code>, <code>center</code>, <code>right</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="p"><a href="#p" class="headerlink" title="p"></a>p</h3><p>语法：</p>
<pre><code class="lang-markdown">&#123;% p 样式参数, 文本内容 %&#125;
</code></pre>
<p>效果：</p>
<h4 id="彩色文字-1"><a href="#彩色文字-1" class="headerlink" title="彩色文字"></a>彩色文字</h4><p>在一段话中方便插入各种颜色的标签，包括：</p><p class="p red">红色</p><p class="p yellow">黄色</p><p class="p green">绿色</p><p class="p cyan">青色</p><p class="p blue">蓝色</p><p class="p gray">灰色</p><p></p>
<h4 id="超大号文字-1"><a href="#超大号文字-1" class="headerlink" title="超大号文字"></a>超大号文字</h4><p>文档「开始」页面中的标题部分就是超大号文字。</p>
<p class="p center logo large">Volantis</p>
<p class="p center small">A Wonderful Theme for Hexo</p>
<p>源码：</p>
<pre><code class="lang-markdown">#### 彩色文字

在一段话中方便插入各种颜色的标签，包括：&#123;% p red, 红色 %&#125;&#123;% p yellow, 黄色 %&#125;&#123;% p green, 绿色 %&#125;&#123;% p cyan, 青色 %&#125;&#123;% p blue, 蓝色 %&#125;&#123;% p gray, 灰色 %&#125;

#### 超大号文字

文档「开始」页面中的标题部分就是超大号文字。

&#123;% p center logo large, Volantis %&#125;
&#123;% p center small, A Wonderful Theme for Hexo %&#125;
</code></pre>
<p>参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>可选值</th>
</tr>
</thead>
<tbody>
<tr>
<td>字体</td>
<td><code>logo</code>, <code>code</code></td>
</tr>
<tr>
<td>颜色</td>
<td><code>red</code>, <code>yellow</code>, <code>green</code>, <code>cyan</code>, <code>blue</code>, <code>gray</code></td>
</tr>
<tr>
<td>大小</td>
<td><code>small</code>, <code>h4</code>, <code>h3</code>, <code>h2</code>, <code>h1</code>, <code>large</code>, <code>huge</code>, <code>ultra</code></td>
</tr>
<tr>
<td>对齐方向</td>
<td><code>left</code>, <code>center</code>, <code>right</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="note"><a href="#note" class="headerlink" title="note"></a>note</h3><p>NoteBlock 是 Blockquote 的增强版，在左边显示图标，并且可以自定颜色。而 Note 是 NoteBlock 的简便写法。</p>
<p>语法：</p>
<pre><code class="lang-markdown">&#123;% note 样式参数, 文本内容 %&#125;
```

效果：

#### 经典用法

&#123;% note, 可以在配置文件中设置默认样式，为简单的一句话提供最的简便写法。 %&#125;
&#123;% note quote, note quote 适合引用一段话 %&#125;
&#123;% note info, note info 默认主题色，适合中性的信息 %&#125;
&#123;% note warning, note warning 默认黄色，适合警告性的信息 %&#125;
&#123;% note danger, note error/danger 默认红色，适合危险性的信息 %&#125;
&#123;% note success, note done/success 默认绿色，适合正确操作的信息 %&#125;

#### 更多图标

这些都是默认样式，可以手动加上颜色：

&#123;% note radiation, note radiation 默认样式 %&#125;
&#123;% note radiation yellow, note radiation yellow 可以加上颜色 %&#125;
&#123;% note bug red, note bug red 说明还存在的一些故障 %&#125;
&#123;% note link green, note link green 可以放置一些链接 %&#125;
&#123;% note paperclip blue, note paperclip blue 放置一些附件链接 %&#125;
&#123;% note todo, note todo 待办事项 %&#125;
&#123;% note guide clear, note guide clear 可以加上一段向导 %&#125;
&#123;% note download, note download 可以放置下载链接 %&#125;
&#123;% note message gray, note message gray 一段消息 %&#125;
&#123;% note up, note up 可以说明如何进行更新 %&#125;
&#123;% note undo light, note undo light 可以说明如何撤销或者回退 %&#125;

源码：

```markdown
#### 经典用法

&#123;% note, 可以在配置文件中设置默认样式，为简单的一句话提供最的简便写法。 %&#125;
&#123;% note quote, note quote 适合引用一段话 %&#125;
&#123;% note info, note info 默认主题色，适合中性的信息 %&#125;
&#123;% note warning, note warning 默认黄色，适合警告性的信息 %&#125;
&#123;% note danger, note error/danger 默认红色，适合危险性的信息 %&#125;
&#123;% note success, note done/success 默认绿色，适合正确操作的信息 %&#125;

#### 更多图标

这些都是默认样式，可以手动加上颜色：

&#123;% note radiation, note radiation 默认样式 %&#125;
&#123;% note radiation yellow, note radiation yellow 可以加上颜色 %&#125;
&#123;% note bug red, note bug red 说明还存在的一些故障 %&#125;
&#123;% note link green, note link green 可以放置一些链接 %&#125;
&#123;% note paperclip blue, note paperclip blue 放置一些附件链接 %&#125;
&#123;% note todo, note todo 待办事项 %&#125;
&#123;% note guide clear, note guide clear 可以加上一段向导 %&#125;
&#123;% note download, note download 可以放置下载链接 %&#125;
&#123;% note message gray, note message gray 一段消息 %&#125;
&#123;% note up, note up 可以说明如何进行更新 %&#125;
&#123;% note undo light, note undo light 可以说明如何撤销或者回退 %&#125;
```

参数:

| 属性 | 可选值                                                       |
| ---- | :----------------------------------------------------------- |
| 图标 | # 彩色的      quote, info, warning, done/success, error/danger                                                                                                                                                                                                 # 灰色的，也可以指定颜色      radiation, bug, idea, link, paperclip, todo, message, guide, download, up, undo |
| 颜色 | clear, light, gray, red, yellow, green, cyan, blue           |

### noteblock

NoteBlock 是 Blockquote 的增强版，在左边显示图标，并且可以自定颜色。而 Note 是 NoteBlock 的简便写法。

语法：

```markdown
&#123;% noteblock 样式参数（可选）, 标题（可选） %&#125;
文本段落
&#123;% endnoteblock %&#125;
</code></pre>
<p>演示效果：</p>
<div class="note "><p><strong>标题（可选）</strong></p><p></p><p>Windows 10不是為所有人設計,而是為每個人設計</p><p></p><p><div class="note done"><p>嵌套测试： 请坐和放宽，我正在帮你搞定一切…</p></div></p><details yellow><summary> Folding 测试： 点击查看更多 </summary>              <div class="content">              <div class="note warning"><p>不要说我们没有警告过你</p></div><div class="note bug red"><p>我们都有不顺利的时候</p></div>              </div>            </details></div>
<p>源码：</p>
<pre><code class="lang-markdown">&#123;% noteblock, 标题（可选） %&#125;

Windows 10不是為所有人設計,而是為每個人設計

&#123;% noteblock done %&#125;
嵌套测试： 请坐和放宽，我正在帮你搞定一切...
&#123;% endnoteblock %&#125;

&#123;% folding yellow, Folding 测试： 点击查看更多 %&#125;

&#123;% note warning, 不要说我们没有警告过你 %&#125;
&#123;% noteblock bug red %&#125;
我们都有不顺利的时候
&#123;% endnoteblock %&#125;

&#123;% endfolding %&#125;
&#123;% endnoteblock %&#125;
</code></pre>
<p>参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th style="text-align:left">可选值</th>
</tr>
</thead>
<tbody>
<tr>
<td>图标</td>
<td style="text-align:left"># 彩色的      quote, info, warning, done/success, error/danger                                                                                                                                                                                                 # 灰色的，也可以指定颜色      radiation, bug, idea, link, paperclip, todo, message, guide, download, up, undo</td>
</tr>
<tr>
<td>颜色</td>
<td style="text-align:left">clear, light, gray, red, yellow, green, cyan, blue</td>
</tr>
</tbody>
</table>
</div>
<h3 id="checkbox"><a href="#checkbox" class="headerlink" title="checkbox"></a>checkbox</h3><p>语法：</p>
<pre><code class="lang-markdown">&#123;% checkbox 样式参数（可选）, 文本（支持简单md） %&#125;
</code></pre>
<p>演示效果：</p>
<div class="checkbox"><input type="checkbox">
            <p>纯文本测试</p>
            </div>
<div class="checkbox checked"><input type="checkbox" checked="checked">
            <p>支持简单的 <a href="https://guides.github.com/features/mastering-markdown/">markdown</a> 语法</p>
            </div>
<div class="checkbox red"><input type="checkbox">
            <p>支持自定义颜色</p>
            </div>
<div class="checkbox green checked"><input type="checkbox" checked="checked">
            <p>绿色 + 默认选中</p>
            </div>
<div class="checkbox yellow checked"><input type="checkbox" checked="checked">
            <p>黄色 + 默认选中</p>
            </div>
<div class="checkbox cyan checked"><input type="checkbox" checked="checked">
            <p>青色 + 默认选中</p>
            </div>
<div class="checkbox blue checked"><input type="checkbox" checked="checked">
            <p>蓝色 + 默认选中</p>
            </div>
<div class="checkbox plus green checked"><input type="checkbox" checked="checked">
            <p>增加</p>
            </div>
<div class="checkbox minus yellow checked"><input type="checkbox" checked="checked">
            <p>减少</p>
            </div>
<div class="checkbox times red checked"><input type="checkbox" checked="checked">
            <p>叉</p>
            </div>
<p>源码：</p>
<pre><code class="lang-markdown"># 复选框
&#123;% checkbox 纯文本测试 %&#125;
&#123;% checkbox checked, 支持简单的 [markdown](https://guides.github.com/features/mastering-markdown/) 语法 %&#125;
&#123;% checkbox red, 支持自定义颜色 %&#125;
&#123;% checkbox green checked, 绿色 + 默认选中 %&#125;
&#123;% checkbox yellow checked, 黄色 + 默认选中 %&#125;
&#123;% checkbox cyan checked, 青色 + 默认选中 %&#125;
&#123;% checkbox blue checked, 蓝色 + 默认选中 %&#125;
&#123;% checkbox plus green checked, 增加 %&#125;
&#123;% checkbox minus yellow checked, 减少 %&#125;
&#123;% checkbox times red checked, 叉 %&#125;
</code></pre>
<p>参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>可选值</th>
</tr>
</thead>
<tbody>
<tr>
<td>颜色</td>
<td>red, yellow, green, cyan, blue</td>
</tr>
<tr>
<td>样式</td>
<td>plus, minus, times</td>
</tr>
<tr>
<td>选中状态</td>
<td>checked</td>
</tr>
</tbody>
</table>
</div>
<h3 id="radio"><a href="#radio" class="headerlink" title="radio"></a>radio</h3><p>语法：</p>
<pre><code class="lang-markdown"># 单选框
&#123;% radio 样式参数（可选）, 文本（支持简单md） %&#125;
</code></pre>
<p>演示效果：</p>
<div class="checkbox"><input type="radio">
            <p>纯文本测试</p>
            </div>
<div class="checkbox checked"><input type="radio" checked="checked">
            <p>支持简单的 <a href="https://guides.github.com/features/mastering-markdown/">markdown</a> 语法</p>
            </div>
<div class="checkbox red"><input type="radio">
            <p>支持自定义颜色</p>
            </div>
<div class="checkbox green"><input type="radio">
            <p>绿色</p>
            </div>
<div class="checkbox yellow"><input type="radio">
            <p>黄色</p>
            </div>
<div class="checkbox cyan"><input type="radio">
            <p>青色</p>
            </div>
<div class="checkbox blue"><input type="radio">
            <p>蓝色</p>
            </div>
<p>源码：</p>
<pre><code class="lang-markdown">&#123;% radio 纯文本测试 %&#125;
&#123;% radio checked, 支持简单的 [markdown](https://guides.github.com/features/mastering-markdown/) 语法 %&#125;
&#123;% radio red, 支持自定义颜色 %&#125;
&#123;% radio green, 绿色 %&#125;
&#123;% radio yellow, 黄色 %&#125;
&#123;% radio cyan, 青色 %&#125;
&#123;% radio blue, 蓝色 %&#125;
</code></pre>
<p>参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>可选值</th>
</tr>
</thead>
<tbody>
<tr>
<td>颜色</td>
<td>red, yellow, green, cyan, blue</td>
</tr>
<tr>
<td>选中状态</td>
<td>checked</td>
</tr>
</tbody>
</table>
</div>
<h3 id="timeline"><a href="#timeline" class="headerlink" title="timeline"></a>timeline</h3><p>语法：</p>
<pre><code class="lang-markdown">&#123;% timeline 时间线标题（可选） %&#125;

&#123;% timenode 时间节点（标题） %&#125;

正文内容

&#123;% endtimenode %&#125;

&#123;% timenode 时间节点（标题） %&#125;

正文内容

&#123;% endtimenode %&#125;

&#123;% endtimeline %&#125;
</code></pre>
<p>效果：</p>
<div class="timeline">
<div class="timenode"><div class="meta"><p></p><p>2020-07-24 <a href="https://github.com/volantis-x/hexo-theme-volantis/releases">2.6.6 -&gt; 3.0</a></p>
<p></p></div><div class="body"><ol><li>如果有 <code>hexo-lazyload-image</code> 插件，需要删除并重新安装最新版本，设置 <code>lazyload.isSPA: true</code>。</li><li>2.x 版本的 css 和 js 不适用于 3.x 版本，如果使用了 <code>use_cdn: true</code> 则需要删除。</li><li>2.x 版本的 fancybox 标签在 3.x 版本中被重命名为 gallery 。</li><li>2.x 版本的置顶 <code>top: true</code> 改为了 <code>pin: true</code>，并且同样适用于 <code>layout: page</code> 的页面。</li><li>如果使用了 <code>hexo-offline</code> 插件，建议卸载，3.0 版本默认开启了 pjax 服务。</li></ol></div></div>

<div class="timenode"><div class="meta"><p></p><p>2020-05-15 <a href="https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.6">2.6.3 -&gt; 2.6.6</a></p>
<p></p></div><div class="body"><p>不需要额外处理。</p></div></div>

<div class="timenode"><div class="meta"><p></p><p>2020-04-20 <a href="https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.3">2.6.2 -&gt; 2.6.3</a></p>
<p></p></div><div class="body"><ol><li>全局搜索 <code>seotitle</code> 并替换为 <code>seo_title</code>。</li><li>group 组件的索引规则有变，使用 group 组件的文章内，<code>group: group_name</code> 对应的组件名必须是 <code>group_name</code>。</li><li>group 组件的列表名优先显示文章的 <code>short_title</code> 其次是 <code>title</code>。</li></ol></div></div>
</div>
<p>源码：</p>
<pre><code class="lang-markdown">&#123;% timeline %&#125;

&#123;% timenode 2020-07-24 [2.6.6 -> 3.0](https://github.com/volantis-x/hexo-theme-volantis/releases) %&#125;

1. 如果有 `hexo-lazyload-image` 插件，需要删除并重新安装最新版本，设置 `lazyload.isSPA: true`。
2. 2.x 版本的 css 和 js 不适用于 3.x 版本，如果使用了 `use_cdn: true` 则需要删除。
3. 2.x 版本的 fancybox 标签在 3.x 版本中被重命名为 gallery 。
4. 2.x 版本的置顶 `top: true` 改为了 `pin: true`，并且同样适用于 `layout: page` 的页面。
5. 如果使用了 `hexo-offline` 插件，建议卸载，3.0 版本默认开启了 pjax 服务。

&#123;% endtimenode %&#125;

&#123;% timenode 2020-05-15 [2.6.3 -> 2.6.6](https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.6) %&#125;

不需要额外处理。

&#123;% endtimenode %&#125;

&#123;% timenode 2020-04-20 [2.6.2 -> 2.6.3](https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.3) %&#125;

1. 全局搜索 `seotitle` 并替换为 `seo_title`。
2. group 组件的索引规则有变，使用 group 组件的文章内，`group: group_name` 对应的组件名必须是 `group_name`。
2. group 组件的列表名优先显示文章的 `short_title` 其次是 `title`。

&#123;% endtimenode %&#125;

&#123;% endtimeline %&#125;
</code></pre>
<h3 id="link"><a href="#link" class="headerlink" title="link"></a>link</h3><p>语法：</p>
<pre><code>&#123;% link 标题, 链接, 图片链接（可选） %&#125;
</code></pre><p>演示效果：</p>
<div class="tag link"><a class="link-card" title="如何参与项目" href="https://volantis.js.org/contributors/"><div class="left"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets@master/logo/256/safari.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets@master/logo/256/safari.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></div><div class="right"><p class="text">如何参与项目</p><p class="url">https://volantis.js.org/contributors/</p></div></a></div>
<p>源码：</p>
<pre><code class="lang-markdown">&#123;% link 如何参与项目, https://volantis.js.org/contributors/, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets@master/logo/256/safari.png %&#125;
</code></pre>
<h3 id="button"><a href="#button" class="headerlink" title="button"></a>button</h3><h4 id="基础按钮"><a href="#基础按钮" class="headerlink" title="基础按钮"></a>基础按钮</h4><p>语法：`<span class="btn 样式参数（可选）"><a class="button" href="链接" title="标题"><i class="图标（可选）"></i>标题</a></span>`

参数如下：

样式参数：

`regular, large, center`

图标：

第1个或者第2个参数包含 `fa-` 的那个被识别为图标。

效果：

不设置任何参数的 <span class="btn"><a class="button" href="/" title="按钮">按钮</a></span> 适合融入段落中。

regular 按钮适合独立于段落之外：

<span class="btn regular"><a class="button" href="https://xaoxuu.com" title="示例博客"><i class="fas fa-play-circle"></i>示例博客</a></span>

large 按钮更具有强调作用，建议搭配 center 使用：

<span class="btn center large"><a class="button" href="https://volantis.js.org/v3/getting-started/" title="开始使用"><i class="fas fa-download"></i>开始使用</a></span>

源码：

```markdown
不设置任何参数的 <span class="btn"><a class="button" href="/" title="按钮">按钮</a></span> 适合融入段落中。

regular 按钮适合独立于段落之外：

<span class="btn regular"><a class="button" href="https://xaoxuu.com" title="示例博客"><i class="fas fa-play-circle"></i>示例博客</a></span>

large 按钮更具有强调作用，建议搭配 center 使用：

<span class="btn center large"><a class="button" href="https://volantis.js.org/v3/getting-started/" title="开始使用"><i class="fas fa-download"></i>开始使用</a></span>
```

#### 富文本按钮

语法：

```markdown
<div class="btns 样式参数">
            <a class="button" href="链接" title="标题"><img src="/CN/hexo%E6%A0%87%E7%AD%BE%E6%8F%92%E4%BB%B6/图片或者图标" class="lazyload" data-srcset="/CN/hexo%E6%A0%87%E7%AD%BE%E6%8F%92%E4%BB%B6/图片或者图标" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">标题</a>
<a class="button" href="链接" title="标题"><img src="/CN/hexo%E6%A0%87%E7%AD%BE%E6%8F%92%E4%BB%B6/图片或者图标" class="lazyload" data-srcset="/CN/hexo%E6%A0%87%E7%AD%BE%E6%8F%92%E4%BB%B6/图片或者图标" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">标题</a>
          </div></p>
<pre><code>
参数列表：

样式参数位置可以写图片样式、布局方式，多个样式参数用空格隔开。

圆角样式：

```markdown
# 默认为方形
rounded, circle
</code></pre><p>布局方式：</p>
<p>默认为自动宽度，适合视野内只有一两个的情况。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>wide</td>
<td>宽一点的按钮</td>
</tr>
<tr>
<td>fill</td>
<td>填充布局，自动铺满至少一行，多了会换行。</td>
</tr>
<tr>
<td>center</td>
<td>居中，按钮之间是固定间距。</td>
</tr>
<tr>
<td>around</td>
<td>居中分散</td>
</tr>
<tr>
<td>grid2</td>
<td>等宽最多2列，屏幕变窄会适当减少列数。</td>
</tr>
<tr>
<td>grid3</td>
<td>等宽最多3列，屏幕变窄会适当减少列数。</td>
</tr>
<tr>
<td>grid4</td>
<td>等宽最多4列，屏幕变窄会适当减少列数。</td>
</tr>
<tr>
<td>grid5</td>
<td>等宽最多5列，屏幕变窄会适当减少列数。</td>
</tr>
</tbody>
</table>
</div>
<p>增加文字样式：</p>
<p>可以在容器内增加 <code>&lt;b&gt;标题&lt;/b&gt;</code> 和 <code>&lt;p&gt;描述文字&lt;/p&gt;</code></p>
<p>效果：</p>
<p>如果需要显示类似「团队成员」之类的一组含有头像的链接：</p>
<div class="btns circle grid5">
            <a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">xaoxuu</a>
<a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">xaoxuu</a>
<a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">xaoxuu</a>
<a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">xaoxuu</a>
<a class="button" href="https://xaoxuu.com" title="xaoxuu"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">xaoxuu</a>
          </div>
<p>或者含有图标的按钮：</p>
<div class="btns rounded grid5">
            <a class="button" href="/" title="下载源码"><i class="fas fa-download"></i>下载源码</a>
<a class="button" href="/" title="查看文档"><i class="fas fa-book-open"></i>查看文档</a>
          </div>
<p>圆形图标 + 标题 + 描述 + 图片 + 网格5列 + 居中</p>
<div class="btns circle center grid5">
            <a href="https://apps.apple.com/cn/app/heart-mate-pro-hrm-utility/id1463348922?ls=1">
  <i class="fab fa-apple"></i>
  <b>心率管家</b>
  <p class="p red">专业版</p>
  <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_pro.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_pro.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">
</a>
<a href="https://apps.apple.com/cn/app/heart-mate-lite-hrm-utility/id1475747930?ls=1">
  <i class="fab fa-apple"></i>
  <b>心率管家</b>
  <p class="p green">免费版</p>
  <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_lite.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_lite.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">
</a>
          </div>
<p>源码：</p>
<pre><code class="lang-markdown"># 如果需要显示类似「团队成员」之类的一组含有头像的链接：
&#123;% btns circle grid5 %&#125;
&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;
&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;
&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;
&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;
&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;
&#123;% endbtns %&#125;
# 或者含有图标的按钮：
&#123;% btns rounded grid5 %&#125;
&#123;% cell 下载源码, /, fas fa-download %&#125;
&#123;% cell 查看文档, /, fas fa-book-open %&#125;
&#123;% endbtns %&#125;
# 圆形图标 + 标题 + 描述 + 图片 + 网格5列 + 居中
&#123;% btns circle center grid5 %&#125;
<a href="https://apps.apple.com/cn/app/heart-mate-pro-hrm-utility/id1463348922?ls=1">
  <i class="fab fa-apple"></i>
  <b>心率管家</b>
  &#123;% p red, 专业版 %&#125;
  <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_pro.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_pro.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">
</a>
<a href="https://apps.apple.com/cn/app/heart-mate-lite-hrm-utility/id1475747930?ls=1">
  <i class="fab fa-apple"></i>
  <b>心率管家</b>
  &#123;% p green, 免费版 %&#125;
  <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_lite.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_lite.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">
</a>
&#123;% endbtns %&#125;
</code></pre>
<h3 id="ghcard"><a href="#ghcard" class="headerlink" title="ghcard"></a>ghcard</h3><p>样式：</p>
<pre><code class="lang-markdown">&#123;% ghcard 用户名, 其它参数（可选） %&#125;
&#123;% ghcard 用户名/仓库, 其它参数（可选） %&#125;
</code></pre>
<p>效果示例：</p>
<p>如下所示，其实就是一个4×2的表格，只不过每个格子里放的是用户信息卡片</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></th>
<th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistacho0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistacho0812&theme=vue&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistacho0812&theme=vue&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&theme=buefy&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&theme=buefy&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></td>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&theme=solarized-light&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&theme=solarized-light&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></td>
</tr>
<tr>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&theme=onedark&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&theme=onedark&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></td>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&theme=solarized-dark&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&theme=solarized-dark&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></td>
</tr>
<tr>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&theme=algolia&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&theme=algolia&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></td>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/pistachio0812"><img src="https://github-readme-stats.vercel.app/api/?username=pistachio0812&theme=calm&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/?username=pistachio0812&theme=calm&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></td>
</tr>
</tbody>
</table>
</div>
<p>源码：</p>
<pre><code class="lang-markdown"># 用户信息卡片
| &#123;% ghcard pistachio0812 %&#125; | &#123;% ghcard pistacho0812, theme=vue %&#125; |
| -- | -- |
| &#123;% ghcard pistachio0812, theme=buefy %&#125; | &#123;% ghcard pistachio0812, theme=solarized-light %&#125; |
| &#123;% ghcard pistachio0812, theme=onedark %&#125; | &#123;% ghcard pistachio0812, theme=solarized-dark %&#125; |
| &#123;% ghcard pistachio0812, theme=algolia %&#125; | &#123;% ghcard pistachio0812, theme=calm %&#125; |
</code></pre>
<p>效果示例：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></th>
<th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=vue&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=vue&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=buefy&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=buefy&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></td>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=solarized-light&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=solarized-light&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></td>
</tr>
<tr>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=onedark&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=onedark&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></td>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=solarized-dark&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=solarized-dark&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></td>
</tr>
<tr>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=algolia&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=algolia&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></td>
<td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=calm&show_owner=true" class="lazyload" data-srcset="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=calm&show_owner=true" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></a></td>
</tr>
</tbody>
</table>
</div>
<p>源码：</p>
<pre><code class="lang-markdown"># 仓库信息卡片
| &#123;% ghcard volantis-x/hexo-theme-volantis %&#125; | &#123;% ghcard volantis-x/hexo-theme-volantis, theme=vue %&#125; |
| -- | -- |
| &#123;% ghcard volantis-x/hexo-theme-volantis, theme=buefy %&#125; | &#123;% ghcard volantis-x/hexo-theme-volantis, theme=solarized-light %&#125; |
| &#123;% ghcard volantis-x/hexo-theme-volantis, theme=onedark %&#125; | &#123;% ghcard volantis-x/hexo-theme-volantis, theme=solarized-dark %&#125; |
| &#123;% ghcard volantis-x/hexo-theme-volantis, theme=algolia %&#125; | &#123;% ghcard volantis-x/hexo-theme-volantis, theme=calm %&#125; |
</code></pre>
<p>更多参数选择：</p>
<pre><code>

</code></pre><div class="tag link"><a class="link-card" title="GitHub卡片API参数" href="https://github.com/anuraghazra/github-readme-stats"><div class="left"><img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets@master/logo/256/safari.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets@master/logo/256/safari.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></div><div class="right"><p class="text">GitHub卡片API参数</p><p class="url">https://github.com/anuraghazra/github-readme-stats</p></div></a></div>
<h3 id="site"><a href="#site" class="headerlink" title="site"></a>site</h3><p>网站卡片可以显示网站截图、logo、标题、描述，使用方法和友链标签一模一样，唯一的区别是数据文件名称为 <code>sites.yml</code>，可以和友链数据混用，通过分组过滤实现不一样的效果。</p>
<p>样式：</p>
<pre><code class="lang-markdown">&#123;% sites only:community_team %&#125;
</code></pre>
<h3 id="dropmenu"><a href="#dropmenu" class="headerlink" title="dropmenu"></a>dropmenu</h3><h4 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h4><p>容器：</p>
<pre><code class="lang-markdown">&#123;% menu 前缀（可省略）, 标题, 后缀（可省略） %&#125;
菜单内容
&#123;% endmenu %&#125;
</code></pre>
<p>菜单内容：</p>
<p>1.菜单项</p>
<pre><code class="lang-markdown">&#123;% menuitem 文本, 链接, 图标 %&#125;
</code></pre>
<p>2.分割线</p>
<pre><code class="lang-markdown">&#123;% menuitem hr %&#125;
</code></pre>
<p>3.子菜单</p>
<pre><code class="lang-markdown">&#123;% submenu 嵌套菜单, 图标 %&#125;
菜单内容
&#123;% endsubmenu %&#125;
</code></pre>
<h4 id="示例效果"><a href="#示例效果" class="headerlink" title="示例效果"></a>示例效果</h4><p>示例1</p>
<div class="dropmenu-wrapper">
              <div class="dropmenu">
                <a>下拉菜单</a>
                <ul class="list-v">
                  <li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/" title="主题源码">
                  <i class="fas fa-file-code fa-fw"></i>
                  主题源码
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/releases/" title="更新日志">
                  <i class="fas fa-clipboard-list fa-fw"></i>
                  更新日志
                </a>
              </li>
<hr>
<li>
              <a class="menuitem">
                <i class=" fas fa-question-circle fa-fw"></i>
                有疑问？
              </a>
              <ul class="list-v">
                <li>
                <a class="menuitem" href="/faqs/" title="看 FAQ">
                  看 FAQ
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/volantis-docs/" title="看 本站源码">
                  看 本站源码
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/issues/" title="提 Issue">
                  提 Issue
                </a>
              </li>
              </ul>
            </li>
                </ul>
              </div>
            </div>
<p>示例2</p>
<div class="dropmenu-wrapper">
              <span>这个是</span>
              <div class="dropmenu">
                <a>下拉菜单</a>
                <ul class="list-v">
                  <li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/" title="主题源码">
                  <i class="fas fa-file-code fa-fw"></i>
                  主题源码
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/releases/" title="更新日志">
                  <i class="fas fa-clipboard-list fa-fw"></i>
                  更新日志
                </a>
              </li>
<hr>
<li>
              <a class="menuitem">
                <i class=" fas fa-question-circle fa-fw"></i>
                有疑问？
              </a>
              <ul class="list-v">
                <li>
                <a class="menuitem" href="/faqs/" title="看 FAQ">
                  看 FAQ
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/volantis-docs/" title="看 本站源码">
                  看 本站源码
                </a>
              </li>

              </ul>
            </li>

                </ul>
              </div>
            </div>
<p>示例3</p>
<div class="dropmenu-wrapper">
              <span>这个是</span>
              <div class="dropmenu">
                <a>下拉菜单</a>
                <ul class="list-v">
                  <li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/" title="主题源码">
                  <i class="fas fa-file-code fa-fw"></i>
                  主题源码
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/hexo-theme-volantis/releases/" title="更新日志">
                  <i class="fas fa-clipboard-list fa-fw"></i>
                  更新日志
                </a>
              </li>
<hr>
<li>
              <a class="menuitem">
                <i class=" fas fa-question-circle fa-fw"></i>
                有疑问？
              </a>
              <ul class="list-v">
                <li>
                <a class="menuitem" href="/faqs/" title="看 FAQ">
                  看 FAQ
                </a>
              </li>
<li>
                <a class="menuitem" href="https://github.com/volantis-x/volantis-docs/" title="看 本站源码">
                  看 本站源码
                </a>
              </li>

              </ul>
            </li>

                </ul>
              </div>
              <span>的示例效果。</span>
            </div>
<h4 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h4><pre><code class="lang-markdown">示例1
&#123;% menu 下拉菜单 %&#125;
&#123;% menuitem 主题源码, https://github.com/volantis-x/hexo-theme-volantis/, fas fa-file-code %&#125;
&#123;% menuitem 更新日志, https://github.com/volantis-x/hexo-theme-volantis/releases/, fas fa-clipboard-list %&#125;
&#123;% menuitem hr %&#125;
&#123;% submenu 有疑问？, fas fa-question-circle %&#125;
&#123;% menuitem 看 FAQ, /faqs/ %&#125;
&#123;% menuitem 看 本站源码, https://github.com/volantis-x/volantis-docs/ %&#125;
&#123;% menuitem 提 Issue, https://github.com/volantis-x/hexo-theme-volantis/issues/ %&#125;
&#123;% endsubmenu %&#125;
&#123;% endmenu %&#125;
示例2
&#123;% menu 这个是, 下拉菜单 %&#125;
（同上）
&#123;% endmenu %&#125;
示例3
&#123;% menu 这个是, 下拉菜单, 的示例效果。 %&#125;
（同上）
&#123;% endmenu %&#125;
</code></pre>
<h3 id="tab"><a href="#tab" class="headerlink" title="tab"></a>tab</h3><p>此插件移植自Next</p>
<p>语法：</p>
<pre><code class="lang-markdown">&#123;% tabs 页面内不重复的ID %&#125;
<!-- tab 栏目1 -->
内容
<!-- endtab -->
<!-- tab 栏目2 -->
内容
<!-- endtab -->
&#123;% endtabs %&#125;
</code></pre>
<p>演示效果：</p>
<div class="tabs" id="tab-id"><ul class="nav-tabs"><li class="tab active"><a class="#tab-id-1">栏目1</a></li><li class="tab"><a class="#tab-id-2">栏目2</a></li></ul><div class="tab-content"><div class="tab-pane active" id="tab-id-1"><p>这是栏目1</p></div><div class="tab-pane" id="tab-id-2"><p>这是栏目2</p></div></div></div>
<p>源码：</p>
<pre><code class="lang-markdown">&#123;% tabs tab-id %&#125;

<!-- tab 栏目1 -->

这是栏目1

<!-- endtab -->

<!-- tab 栏目2 -->

这是栏目2

<!-- endtab -->

&#123;% endtabs %&#125;
</code></pre>
]]></content>
      <categories>
        <category>Hexo博客搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/CN/hello-world/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="lang-bash">$ hexo new &quot;My New Post&quot;
</code></pre>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="lang-bash">$ hexo server
</code></pre>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="lang-bash">$ hexo generate
</code></pre>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="lang-bash">$ hexo deploy
</code></pre>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>Hexo博客搭建</category>
      </categories>
      <tags>
        <tag>博客搭建</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch官方文档中文版</title>
    <url>/CN/pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h1><h2 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h2><h3 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h3><p>CLASS    <code>torch.nn.Module</code>    <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">SOURCE</a></p>
<p>它是所有神经网络模型的基类，你的模块应该继承于该类。</p>
<p>模块还能包含其他模块，允许把它们嵌套在一个树结构中。你可以分配子模块作为常规属性：</p>
<pre><code class="lang-python">import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>用这种方式分配的模块将会显示，并且当你调用<code>to( )</code>等等方法，它们的参数也将被转换。</p>
<p>注：</p>
<p>正如上面的例子一样，一个<code>__init__()</code>调用父类必须在子类赋值之前完成。</p>
<hr>
<p>变量</p>
<p><code>training(bool)</code>-布尔值代表这个模块是训练模式还是评估模式</p>
<p><code>add_module(name,    module)</code>    <a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.add_module">SOURCE</a></p>
<p>​                添加一个子模块到当前模块</p>
<p>​                这个模块可以用给定的名称作为属性访问模块</p>
<p>​                参数：</p>
<p>​                        ·name(string)-子模块的名字，这个子模块可以用给定的名称作为属性访问。</p>
<p>​                        ·module(Module)-子模块添加到模块上</p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>文档</tag>
      </tags>
  </entry>
  <entry>
    <title>可视化技术</title>
    <url>/CN/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8A%80%E6%9C%AF/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://www.toutiao.com/article/6725276071358366215">CNN模型的可视化</a></p>
<h2 id="CNNVis"><a href="#CNNVis" class="headerlink" title="CNNVis"></a><a href="http://shixialiu.com/publications/cnnvis/demo/">CNNVis</a></h2><p>清华大学视觉分析组做的一个网站，目的是为了更好地分析深度卷积神经网络。大家可以在训练的时候采取不同的卷积核尺寸和个数对照来看训练的中间过程。</p>
<h2 id="PlotNeuralNet"><a href="#PlotNeuralNet" class="headerlink" title="PlotNeuralNet"></a><a href="https://github.com/HarisIqbal88/PlotNeuralNet">PlotNeuralNet</a></h2><p>1.安装texlive</p>
<p>ubuntu</p>
<pre><code>(1)下载texlive镜像 
https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/
(2)使用图形化安装界面，需要安装perl的tk组件 
sudo apt-get install perl-tk
(3)加载镜像文件安装
sudo mount -o loop texlive.iso /mnt
cd /mnt 
sudo ./install-tl -gui 
(4)安装texlive-latex-extra
sudo apt-get install texlive-latex-extra
</code></pre><p>windows</p>
<pre><code>(1)下载并安装[MikTex](https://miktex.org/download)
(2)下载并安装bash, 详情见 Git bash(https://git-scm.com/download/win)or Cygwin(https://www.cygwin.com/)
</code></pre>]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title>pip换源</title>
    <url>/CN/pip%E6%8D%A2%E6%BA%90/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>由于使用pip或pip3安装python第三方包时，经常出现read timed out问题，所以需要将pip的官方软件源服务器换成国内的镜像服务器，从而提升python软件包安装效率和成功率，pip 国内的一些镜像：</p>
<ul>
<li>阿里云 <a href="http://mirrors.aliyun.com/pypi/simple/">http://mirrors.aliyun.com/pypi/simple/</a></li>
<li>中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/">https://pypi.mirrors.ustc.edu.cn/simple/</a></li>
<li>豆瓣(douban) <a href="http://pypi.douban.com/simple/">http://pypi.douban.com/simple/</a></li>
<li>清华大学 <a href="https://pypi.tuna.tsinghua.edu.cn/simple/">https://pypi.tuna.tsinghua.edu.cn/simple/</a></li>
<li>中国科学技术大学 <a href="http://pypi.mirrors.ustc.edu.cn/simple/">http://pypi.mirrors.ustc.edu.cn/simple/</a></li>
</ul>
<h2 id="更换源"><a href="#更换源" class="headerlink" title="更换源"></a>更换源</h2><h3 id="临时使用"><a href="#临时使用" class="headerlink" title="临时使用"></a>临时使用</h3><p>可以在使用 pip 的时候在后面加上-i 参数，指定 pip 源</p>
<pre><code>eg: pip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple
</code></pre><h3 id="永久修改"><a href="#永久修改" class="headerlink" title="永久修改"></a>永久修改</h3><h4 id="linux"><a href="#linux" class="headerlink" title="linux"></a>linux</h4><p>修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件，文件夹要加“.”，表示是隐藏文件夹)， 内容如下：</p>
<pre><code class="lang-python">[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
[install]
trusted-host = https://pypi.tuna.tsinghua.edu.cn
</code></pre>
<h4 id="windows"><a href="#windows" class="headerlink" title="windows"></a>windows</h4><p>1.pip永久换源</p>
<pre><code class="lang-python">pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/
</code></pre>
<p>在cmd命令行中输入上述命令即可。</p>
<p>最后，升级 pip 到最新的版本</p>
<pre><code class="lang-python">pip install pip -U
</code></pre>
<pre><code class="lang-python">python -m pip install --user --upgrade pip
</code></pre>
<p>2.直接在 user 目录中创建一个 pip 目录，如：C:\Users\xx\pip，在 pip 目录下新建文件 pip.ini，即 %HOMEPATH%\pip\pip.ini，内容如下：</p>
<pre><code>[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
[install]
trusted-host = pypi.tuna.tsinghua.edu.cn
</code></pre><p>可以在开始运行里面输入三个点 <code>...</code>，敲回车即可打开用户目录。</p>
]]></content>
  </entry>
  <entry>
    <title>学习笔记</title>
    <url>/CN/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>tf中矩阵量存在形式常用有三种，具体如下：<br>1.tf.Variable()<br>表示神经网络中可变化的量（可以通过trainable=False设置成不可变),可在运行中赋值，可以通过constant或者其他方式进行初始化。<br>2.tf.constant()<br>可以通过numpy中的array或者list,还有给定的shape和数值进行赋值<br>3.tf.placeholder()<br>相当于占位符，也是有shape的量，因为训练过程中需要不断赋值和替换值，而整体计算的结构是不变的。<br>代码：<br>//导包</p>
<p><code>import tensorflow as tf</code>    </p>
<p>//定义变量<br><code>A = tf.Variable(tf.ones([4,4]))</code></p>
<p>//变量初始化<br><code>import numpy as np      
cst = tf.constant(np.ones([4,4]),dtype=tf.float32)</code></p>
<h1 id="需要指定类型dtype-tf-float32-tf中不能隐式转换浮点和整型"><a href="#需要指定类型dtype-tf-float32-tf中不能隐式转换浮点和整型" class="headerlink" title="需要指定类型dtype=tf.float32,tf中不能隐式转换浮点和整型"></a>需要指定类型dtype=tf.float32,tf中不能隐式转换浮点和整型</h1><h1 id="cst-tf-constant-1-0-shape-4-4-dtype-tf-float32-也是可以的"><a href="#cst-tf-constant-1-0-shape-4-4-dtype-tf-float32-也是可以的" class="headerlink" title="cst = tf.constant(1.0,shape=[4,4],dtype=tf.float32)也是可以的"></a>cst = tf.constant(1.0,shape=[4,4],dtype=tf.float32)也是可以的</h1><p><code>A = tf.Variable(cst)</code></p>
<p>//定义placeholder<br><code>X = tf.placeholder(dtype=tf.float32,shape=[4,1])</code></p>
<p>//矩阵相乘<br><code>C = tf.matmul(A,X)</code></p>
<p>//定义Session<br><code>sess = tf.Session()</code></p>
<p>//初始化变量<br><code>init = tf.global_variables_initializer()</code></p>
<p>//执行初始化<br><code>sess.run(init)</code></p>
<p>//运行矩阵相乘<br><code>sess.run(C,feed_dict=&#123;X:[[1],[1],[1],[1]]&#125;)</code></p>
<p>//获取变量值<br><code>A_val = A.value()  
Avalue = sess.run(A_val)</code></p>
<p>//完整矩形相乘代码<br><code>import tensorflow as tf  
import numpy as np</code></p>
<p><code>X = tf.placeholder(dtype=tf.float32,shape=[4,1])  
A = tf.Variable(tf.zeros([4,4]))  
C = tf.matmul(A,X)  
sess = tf.session()  
init = tf.global_variables_initializer()  
sess.run(init)  
print(sess.run(A))</code></p>
<p>//为了使计算图更加清晰，可以使用variable_scope()<br>//定义变量名称<br><code>with  tf.variable_scope(&quot;first-nn-layer&quot;):
    W = tf.Variable(tf.zeros([784,10]),name=&quot;W&quot;)
    b = tf.Variable(tf.zeros([10]),name=&quot;b&quot;)
    y = tf.matmul(x,W)+b
    variable_summaries(W)</code></p>
<p>//标识不同的变量<br>//不同作用域下的同名变量<br><code>with tf.variable_scope(&quot;first-nn-layer&quot;):
    W = tf.Variable(tf.zeros([784,10]),name=&quot;W&quot;)
    b = tf.Variable(tf.zeros([10]),name=&quot;b&quot;)
    W1 = tf.Variable(tf.zeros([784,10]),name=&quot;W&quot;)
print(W.name)
print(W1.name)</code></p>
<h1 id="w、w1虽然name一样，但是计算中依然当成不同的变量，让同一个scope的同一变量可以通过get-variable-函数"><a href="#w、w1虽然name一样，但是计算中依然当成不同的变量，让同一个scope的同一变量可以通过get-variable-函数" class="headerlink" title="w、w1虽然name一样，但是计算中依然当成不同的变量，让同一个scope的同一变量可以通过get_variable()函数"></a>w、w1虽然name一样，但是计算中依然当成不同的变量，让同一个scope的同一变量可以通过get_variable()函数</h1><p>//获取变量<br><code>with tf.variable_scope(&quot;first-nn-layer&quot;) as scope:
    W = tf.get_variable(&quot;W&quot;,[784, 10])
    b = tf.get_variable(&quot;b&quot;,[10])
    scope.reuse_variables()#缺少则会报错
    W1 = tf.get_variables(&quot;W&quot;,shape=[784,10])
print(W.name)
print(W1.name)</code></p>
<h1 id="w、w1属于同一个变量"><a href="#w、w1属于同一个变量" class="headerlink" title="w、w1属于同一个变量"></a>w、w1属于同一个变量</h1><p>注：若此时缺少了scope.reuse_variables()则会报错，因为同时引用了同一个变量，对于不同层的变量，可以利用variable_scope进行区分，在再次引入相关变量时，需要加上reuse=True,否则依然会报错。如果变量不存在时加上reuse=True,依然会报错，因为该变量不存在</p>
<p><code>with tf.variable_scope(&quot;first-nn-layer&quot;) as scope:
    W = tf.get_variable(&quot;W&quot;,[784, 10])
    b = tf.get_variable(&quot;b&quot;,[10])
with tf.variable_scope(&quot;second-nn-layer&quot;) as scope:
    W = tf.get_variable(&quot;W&quot;,[784, 10])
    b = tf.get_variable(&quot;b&quot;,[10])
with tf.variable_scope(&quot;second-nn-layer&quot;, reuse=True) as scope:
    W3 = tf.get_variable(&quot;W&quot;,[784, 10])
    b3 = tf.get_variable(&quot;b&quot;,[10])
print(W.name)
print(W3.name)</code></p>
<p>//保存模型<br>//定义saver<br><code>saver = tf.train.Saver()</code></p>
<p>//在训练过程中进行保存，保存为训练过程中的变量<br>//变量保存<br><code>for itr in range(1000):
    ...
    saver.save(sess,&quot;model/al&quot;,global_step=itr)</code></p>
<p>//加载计算<br>//变量载入<br><code>saver.restore(sess,&quot;model/v2-200&quot;)</code></p>
<p>3.4构建计算图<br>//前面在描述计算图，这里观察所建立的计算图<br>//定义summary<br><code>train_writer = tf.summary.FileWriter(&quot;logdir&quot;,sess.graph)</code></p>
<p>注：sess.graph就是描绘的计算图，”logdir”是log的存储文件夹。在Shell中运行Tensorboard,在浏览器中输入localhost:6006,然后点击graph就可以看到设计的网络模型了。</p>
<p>//问题：描绘的计算图非常杂乱无章，变量命名的可读性很差，需要进行整理。<br>//变量命名<br><code>x = tf.placeholder(tf.float32,[None,784],name=&quot;input_x&quot;)
label = tf.placeholder(tf.float32,[None,10],name=&quot;input_label&quot;)
W = tf.Variable(tf.zeros([874,10]),name=&quot;W&quot;)
b = tf.Variable(tf.zeros([10]),name=&quot;b&quot;)</code></p>
<p>//问题：依然不够清楚，可以将输入层的x和label归为一类<br>//定义作用域<br><code>with tf.variable_scope(&quot;input&quot;):
    x = tf.placeholder(tf.float32,[None,784],name=&quot;input_x&quot;)
    label = tf.placeholder(tf.float32,[None,10],name=&quot;input_label&quot;)
with tf.variable_scope(&quot;first-nn-layer&quot;):
    W = tf.Variable(tf.zeros([784,10]), name=&quot;W&quot;)
    b = tf.Variable(tf.zeros([10]),name=&quot;b&quot;)
    y = tf.matmul(x,W)+b
with tf.variable_scope(&quot;loss&quot;):
    loss = tf.reduce_mean(tf.square(y-label))</code></p>
<p>//同一作用域下的同名变量是相同的，涉及到变量复用的问题，以及后续变量的获取，为了观察变量的变化，需要观察的变量加入summary函数<br>//定义summary函数<br><code>def variable_summaries(var):
    with tf.name_scope(&#39;summaries&#39;):
        mean = tf.reduce_mean(var)
        tf.summary.scalar(&#39;mean&#39;,mean)
        with tf.name_scope(&#39;stddev&#39;):
            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))
        tf.summary.scalar(&#39;stddev&#39;,stddev)
        tf.summary.scalar(&#39;max&#39;,tf.reduce_max(var))
        tf.summary.scalar(&#39;min&#39;,tf.reduce_min(var))
        tf.summary.histogram(&#39;histogram&#39;,var)</code></p>
<p>//若要观测W的相关情况，调用summary函数<br>//调用summary函数<br><code>variable_summaries(W)</code></p>
<p>//再用merge_all函数收集summary信息<br>//获取summary信息<br><code>merged = tf.summary.merge_all()</code></p>
<p>//summary保存<br><code>summary = sess.run(merged, feed_dict=&#123;x:batch_xs,label:batch_ys&#125;)
train_writer.add_summary(summary,itr)</code></p>
<p>注：此时可以在网页中访问，观察变量随迭代变化的情况，可以通过不同的方式对变量进行观测，比如时序统计、histogram,这些统计信息对于分析训练过程是非常重要的</p>
<p>3.5全连接网络构建<br>//tf官方手写识别版本的简化版本<br>//单层全连接网络</p>
<h1 id="引入库"><a href="#引入库" class="headerlink" title="引入库"></a>引入库</h1><p><code>from tensorflow.examples.tutorials.mnist import input_data#产生数据，手写识别的图片和标签
import tensorflow as tf</code></p>
<h1 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h1><p><code>mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;,one_hot=True)</code></p>
<h1 id="构建网络模型"><a href="#构建网络模型" class="headerlink" title="构建网络模型"></a>构建网络模型</h1><h1 id="x-label分别为图形数据和标签数据"><a href="#x-label分别为图形数据和标签数据" class="headerlink" title="x,label分别为图形数据和标签数据"></a>x,label分别为图形数据和标签数据</h1><p><code>x = tf.placeholder(tf.float32,[None,784])
label = tf.placeholder(tf.float32,[None,10])</code></p>
<h1 id="构建单层网络中的权值和偏置"><a href="#构建单层网络中的权值和偏置" class="headerlink" title="构建单层网络中的权值和偏置"></a>构建单层网络中的权值和偏置</h1><p><code>W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10])</code></p>
<h1 id="本例中无非线性激活函数"><a href="#本例中无非线性激活函数" class="headerlink" title="本例中无非线性激活函数"></a>本例中无非线性激活函数</h1><p><code>y = tf.matmul(x,W)+b</code></p>
<h1 id="定义损失函数为欧氏距离，但这并不是最好的，多分类问题通常使用交叉熵"><a href="#定义损失函数为欧氏距离，但这并不是最好的，多分类问题通常使用交叉熵" class="headerlink" title="定义损失函数为欧氏距离，但这并不是最好的，多分类问题通常使用交叉熵"></a>定义损失函数为欧氏距离，但这并不是最好的，多分类问题通常使用交叉熵</h1><p><code>loss = tf.reduce_mean(tf.square(y-label))</code></p>
<h1 id="若使用交叉熵损失函数"><a href="#若使用交叉熵损失函数" class="headerlink" title="若使用交叉熵损失函数"></a>若使用交叉熵损失函数</h1><p><code>soft_max = tf.nn.softmax(logit, axis=1)
loss = tf.reduce_mean(-label*tf.log(soft_max))</code></p>
<h1 id="用梯度迭代算法"><a href="#用梯度迭代算法" class="headerlink" title="用梯度迭代算法"></a>用梯度迭代算法</h1><p><code>train_step = tf.train.GradientDescentOptimizer(0.005).minimize(loss)</code></p>
<h1 id="用于验证"><a href="#用于验证" class="headerlink" title="用于验证"></a>用于验证</h1><p><code>correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(label,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float.32))</code></p>
<h1 id="定义会话"><a href="#定义会话" class="headerlink" title="定义会话"></a>定义会话</h1><p><code>sess = tf.Session()</code></p>
<h1 id="初始化所有变量"><a href="#初始化所有变量" class="headerlink" title="初始化所有变量"></a>初始化所有变量</h1><p><code>sess.run(tf.global_variable_initializer())</code></p>
<h1 id="迭代过程"><a href="#迭代过程" class="headerlink" title="迭代过程"></a>迭代过程</h1><p><code>for itr in range(3000):
    batch_xs,batch_ys = mnist.train.next_batch(100)
    sess.run(train_step, feed_dict=&#123;x:batch_xs,label:batch_ys&#125;)
    if itr%10==0:
        print(&quot;step:%6d accuracy:&quot;%iter, sess.run(accuracy, feed_dict=&#123;x:mnist.test.images, label:mnist.test.labels&#125;))</code></p>
<h1 id="获取W取值"><a href="#获取W取值" class="headerlink" title="获取W取值"></a>获取W取值</h1><p><code>W_value = sess.run(W.value())</code></p>
<p>//定义一个单层全连接函数<br><code>def full_layer(input_tensor, out_dim, name=&quot;full&quot;):
    with tf.variable_scope(name):
        shape = input_tensor.get_shape()as_list()
        W = tf.get_variable(&#39;W&#39;,(shape[1],out_dim),dtype=tf.float32, initalizer=tf.truncated_normal_initializer(stddev=0.1))
        b = tf.get_variable(&#39;b&#39;,[out_dim], dtype=tf.float32, initializer=tf.constant_initializer(0))
        out = tf.matmul(input_tensor, W)+b
    return tf.nn.sigmoid(nn)</code></p>
<p>3.6CNN构建<br>//CNN手写识别</p>
<h1 id="预读取MNIST手写字库"><a href="#预读取MNIST手写字库" class="headerlink" title="预读取MNIST手写字库"></a>预读取MNIST手写字库</h1><p><code>from tensorflow.examples.tutorials.mnist import input_data
  mnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)</code></p>
<p><code>import tensorflow as tf</code></p>
<h1 id="用正态分布随机数初始化变量，本例中仅作为权值"><a href="#用正态分布随机数初始化变量，本例中仅作为权值" class="headerlink" title="用正态分布随机数初始化变量，本例中仅作为权值"></a>用正态分布随机数初始化变量，本例中仅作为权值</h1><p><code>def weight_variable(shape):
    initial=tf.truncated_normal(shape,stddev=0.1)</code></p>
<pre><code>#正态分布
`return tf.Variable(initial)`
</code></pre><h1 id="用常量方式初始化偏置"><a href="#用常量方式初始化偏置" class="headerlink" title="用常量方式初始化偏置"></a>用常量方式初始化偏置</h1><p><code>def bias_variable(shape):
    initial=tf.constant(0.1,shape=shape)</code></p>
<pre><code>#常数分布
   `return tf.Variable(initial)`
</code></pre><h1 id="定义二维卷积的过程"><a href="#定义二维卷积的过程" class="headerlink" title="定义二维卷积的过程"></a>定义二维卷积的过程</h1><p>def conv2d(x,W):<br>    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=’SAME’)</p>
<h1 id="定义池化层，简单地说就是选个最大的数，进一步降低自由参数的个数"><a href="#定义池化层，简单地说就是选个最大的数，进一步降低自由参数的个数" class="headerlink" title="定义池化层，简单地说就是选个最大的数，进一步降低自由参数的个数"></a>定义池化层，简单地说就是选个最大的数，进一步降低自由参数的个数</h1><p><code>def max_pool_2x2(x):
    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=&#39;SAME&#39;)</code></p>
<p><code>x = tf.placeholder(tf.float32,shape=[100,784])
y = tf.placeholder(tf.float32,shape=[100,10])</code></p>
<p><code>W_conv1 = weight_variable([5,5,1,32])
b_conv1 = bias_variable([32])
x_image = tf.reshape(x,[-1,28,28,1])
y_conv1 = tf.nn.relu(conv2d(x_iamge,W_conv1)+b_conv1)
y_pool1 = max_pool_2x2(y_conv1)</code></p>
<p><code>W_conv2 = weight_variable([5,5,32,64])
b_conv2 = weight_variable([64])
y_conv2 = tf.nn.relu(conv2d(y_pool1,W_conv2)+b_conv2)
y_pool2 = max_pool_2x2(y_conv2)</code></p>
<p><code>y_fc_flat = tf.reshape(y_pool2,[-1,7*7*64])
W_fc1 = weight_variable([7*7*64,10])
b_fc1 = bias_variable([10])
y_fc1 = tf.nn.relu(tf.matmul(y_fc_flat,W_fc1)+b_fc1)</code></p>
<p><code>cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_fc1))
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</code></p>
<p><code>sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)</code></p>
<p><code>for i in range(1000):
    bx,by = mnist.train.next_batch(100)
    sess.run(train_step,feed_dict=&#123;x:bx,y:by&#125;)</code></p>
<p><code>import numpy as np
import matplotlib.pyplot as plt</code></p>
<h1 id="设置输出风格，为画图美观"><a href="#设置输出风格，为画图美观" class="headerlink" title="设置输出风格，为画图美观"></a>设置输出风格，为画图美观</h1><p><code>import matplotlib  as mpl
mpl.style.use(&#39;seaborn-darkgrid&#39;)</code></p>
<p><code>val = W_conv1.value()
convVal = np.array(sess.run(val))
convVal = np.reshape(convVal,[5,5,32])
plt.imshow(convVal[:,:,6])
plt.show()</code></p>
<p>3.8多架构运行<br>//GPU使用<br>GPU可以加速深度学习作业的训练速度，如果服务器有多个GPU,那么tensorflow默认使用全部<br>使用部分GPU:</p>
<p>python程序启动时调用：<br><code>CUDA_VISIBLE_DEVICES=0.2.3 python script.py</code></p>
<p>python代码内进行调用：<br><code>import os
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;1&quot;</code></p>
<p>//配置GPU显存<br>某些情况下，多作业或者共享GPU的场景中，可以控制tf使用GPU显存大小<br><code>gpuOptions = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpuOptions))</code></p>
<p>//GPU运行代码</p>
<h1 id="将变量的定义和分配定义到GPU上进行"><a href="#将变量的定义和分配定义到GPU上进行" class="headerlink" title="将变量的定义和分配定义到GPU上进行"></a>将变量的定义和分配定义到GPU上进行</h1><p><code>with tf.device(&#39;/gpu:0&#39;):
    W = tf.get_variable(&#39;W&#39;,(in_dim,out_dim),dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.1))
    b = tf.get_variable(&#39;b&#39;),[out_dim],dtype=tf.float32,initializer=tf.constant_initializer(0))
    net = tf.matmul(input_tensor,W)+b</code></p>
<h1 id="在CPU上计算激活函数"><a href="#在CPU上计算激活函数" class="headerlink" title="在CPU上计算激活函数"></a>在CPU上计算激活函数</h1><p><code>with tf.device(&#39;/cpu:0&#39;):
    net = tf.nn.sigmoid(net)</code></p>
<p>//多CPU使用，多设备计算<br>//利用标号简单的区分并运行</p>
<h1 id="在CPU0上计算"><a href="#在CPU0上计算" class="headerlink" title="在CPU0上计算"></a>在CPU0上计算</h1><p><code>with tf.device(&#39;/cpu:0&#39;)
    ...
    net = tf.nn.sigmoid(net)</code></p>
<h1 id="在CPU1上计算"><a href="#在CPU1上计算" class="headerlink" title="在CPU1上计算"></a>在CPU1上计算</h1><p><code>with tf.device(&#39;/cpu:1&#39;)
    ...
    net = tf.nn.sigmoid(net)</code></p>
<p>//在集群上运行，需要在多个主机上准备多份代码，代码前面部分相同，后续有所不同<br>//定义多主机运行参数</p>
<h1 id="这里的地址形式为IP-Port"><a href="#这里的地址形式为IP-Port" class="headerlink" title="这里的地址形式为IP:Port"></a>这里的地址形式为IP:Port</h1><p><code>cluster = tf.train.ClusterSpectf.train.ClusterSpec(&#123;
    &quot;worker&quot;:[
        &quot;xx.xx.xx.xx:2222&quot;,    #/job:worker/task:0
        &quot;xx.xx.xx.xx:2222&quot;,    #这里job的名称为自定义
        &quot;xx.xx.xx.xx:2222&quot;    #task编号同样需在Server中定义
        ],
    &quot;ps&quot;:[
        &quot;xx.xx.xx.xx:2222&quot;,
        &quot;xx.xx.xx.xx:2222&quot;
        ]&#125;)
server = tf.train.Server(cluster, job_name=&quot;worker&quot;, task_index=0)</code></p>
<p>//定义第二个主机参数</p>
<h1 id="这里的地址形式为IP-Port-1"><a href="#这里的地址形式为IP-Port-1" class="headerlink" title="这里的地址形式为IP:Port"></a>这里的地址形式为IP:Port</h1><p><code>cluster = tf.train.ClusterSpectf.train.ClusterSpec(&#123;
    &quot;worker&quot;:[
        &quot;xx.xx.xx.xx:2222&quot;,    #/job:worker/task:0
        &quot;xx.xx.xx.xx:2222&quot;,    #这里job的名称为自定义
        &quot;xx.xx.xx.xx:2222&quot;    #task编号同样需在Server中定义
        ],
    &quot;ps&quot;:[
        &quot;xx.xx.xx.xx:2222&quot;,
        &quot;xx.xx.xx.xx:2222&quot;
        ]&#125;)
server = tf.train.Server(cluster, job_name=&quot;worker&quot;, task_index=1)</code></p>
<p>//不同设备的运行代码<br><code>with tf.device(&#39;/job:worker/task:0/cpu:0&#39;):
    ...</code></p>
<p>//将不同的任务分配到不同的计算节点上<br>//分配计算任务<br><code>with tf.device(tf.train.replica_device_setter(
    worker_device=&quot;/job:worker/task:%d&quot; %task_index,cluster=cluster)</code></p>
<p>//函数replica_device_setter会将变量参数的定义部分自动定义到ps服务中，后续需要定义Session,用于执行这个过程<br>//多主机运行</p>
<h1 id="定义句柄，迭代多少步后停止迭代"><a href="#定义句柄，迭代多少步后停止迭代" class="headerlink" title="定义句柄，迭代多少步后停止迭代"></a>定义句柄，迭代多少步后停止迭代</h1><p><code>hooks = [tf.train.StopAtStepHook(last_step=1000000)]</code></p>
<h1 id="MonitoredTrainingSession函数会完成会话初始化工作"><a href="#MonitoredTrainingSession函数会完成会话初始化工作" class="headerlink" title="MonitoredTrainingSession函数会完成会话初始化工作"></a>MonitoredTrainingSession函数会完成会话初始化工作</h1><h1 id="保存checkpoint-恢复checkpoint-异常判断等"><a href="#保存checkpoint-恢复checkpoint-异常判断等" class="headerlink" title="保存checkpoint,恢复checkpoint,异常判断等"></a>保存checkpoint,恢复checkpoint,异常判断等</h1><h1 id="这里需要定义master主机，定义保存、控制操作的master"><a href="#这里需要定义master主机，定义保存、控制操作的master" class="headerlink" title="这里需要定义master主机，定义保存、控制操作的master"></a>这里需要定义master主机，定义保存、控制操作的master</h1><p><code>with tf.train.MonitroedTrainingSession(
    master=server.target,
    is_chief=(task_index==0),
    checkpoint_dir=&quot;dir/to/cp&quot;,
    hooks=hooks) as mon_sess:
    ...</code></p>
<p>注：在程序运行过程中，需要认为将程序分配到各个主机上，依次运行各个主机</p>
<p>//队列用于数据读取和处理，队列可以是先进先出队列，也可以是随机队列，用于随机化输出<br>//tf中队列的操作是对于训练前的过程而言的，有以下作用<br>1.多线程数据预处理并将其推入队列<br>2.在执行过程中，队列不断提供训练数据<br>//简单实例说明队列使用<br><code>def simple_shuffle_batch(source,capacity,batch_size=10):</code></p>
<pre><code>#定义随机序列
`queue = tf.RandomShuffleQueue(
    capacity=capacity,
    min_after_dequeue=int(0.9*capacity),
    shapes=source.shape,
    dtypes=source.dtype)`
#定义enqueue过程
`enqueue = queue.enqueue(source)`
#定义执行进程个数
`num_threads = 4
qr = tf.train.QueueRunner(queue,[enqueue]*num_threads)`
#声明Queue runner,使得其可以被执行
`tf.train.add_queue_runner(qr)`
#获取数据
`return queue.dequeue_many(batch_size)`
</code></pre><h1 id="产生测试数据"><a href="#产生测试数据" class="headerlink" title="产生测试数据"></a>产生测试数据</h1><p><code>input = tf.constant(list(range(100)))
input = tf.data.Dataset.from_tensor_slices(input)
input = input.make_one_shot_iterator().get_next()</code></p>
<h1 id="定义函数"><a href="#定义函数" class="headerlink" title="定义函数"></a>定义函数</h1><p><code>get_batch = simple_shuffle_batch(input,capacity=20)</code></p>
<h1 id="定义session"><a href="#定义session" class="headerlink" title="定义session"></a>定义session</h1><p><code>with tf.train.MonitoredSession() as sess:
    while not sess.should_stop():
        print(sess.run(get_batch))</code></p>
<p>注：队列操作可以使得数据读取过程得到并行的优化，这对于提升程序的运行速度是很有利的。</p>
<p>//tf相关扩展<br>4.2.1 tf Layers<br>//全连接网络</p>
<h1 id="layers定义全连接网络"><a href="#layers定义全连接网络" class="headerlink" title="layers定义全连接网络"></a>layers定义全连接网络</h1><p><code>net = tf.layers.dense(inputs=net, units=units, activation=tf.nn.relu)</code></p>
<h1 id="卷积网络"><a href="#卷积网络" class="headerlink" title="卷积网络"></a>卷积网络</h1><p><code>net = tf.layers.conv2d(
    inputs=net, #输入
    filters=n_features, #输出特征数
    kernel-size=[5, 5], #卷积核心大小
    padding=&quot;same&quot;, #边界
    activation=tf.nn.relu #激活函数
    )</code></p>
<p>//前馈神经网络函数</p>
<h1 id="二维最大池化"><a href="#二维最大池化" class="headerlink" title="二维最大池化"></a>二维最大池化</h1><p><code>net = tf.layers.max_pooling2d(...)</code></p>
<h1 id="二维平均池化"><a href="#二维平均池化" class="headerlink" title="二维平均池化"></a>二维平均池化</h1><p><code>net = tf.layers.average_pooling2d(...)</code></p>
<h1 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h1><p><code>net = tf.layers.conv2d(...)</code></p>
<h1 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h1><p><code>net = tf.layers.dropout(...)</code></p>
<h1 id="展开"><a href="#展开" class="headerlink" title="展开"></a>展开</h1><p><code>net = tf.layers.flatten(...)</code></p>
<h1 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h1><p><code>net = tf.layers.batch_normalization(...)</code></p>
<p>4.2.2 tf Slim</p>
<h1 id="卷积函数"><a href="#卷积函数" class="headerlink" title="卷积函数"></a>卷积函数</h1><p><code>def conv2d_layer(input_tensor, size=1, feature=128, name=&#39;conv1d&#39;):
    with tf.variable_scope(name):
        shape = input_tensor.get_shape.as_list()
        kernel = tf.get_variable(&#39;kernel&#39;, (size, size, shape[-1], feature), dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1))
        b = tf.get_variable(&#39;b&#39;, [feature], dtype=tf.float32, initializer=tf.constant_initializer(0))
        out = tf.nn.conv2d(input_tensor, kernel, strides=[1,2,2,1],padding=&#39;SAME&#39;)+b
    return tf.nn.relu(out)</code></p>
<h1 id="全连接函数"><a href="#全连接函数" class="headerlink" title="全连接函数"></a>全连接函数</h1><p><code>def full_layer(input_tensor, out_dim, name=&#39;full&#39;):
    with tf.variable_scope(name):
        shape = input_tensor.get_shape.as_list()
        W = tf.get_variable(&#39;W&#39;, (shape[1], out_dim), dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1))
        b = tf.get_variabel(&#39;b&#39;, [out_dim], dtype=tf.float32, initializer=tf.constant_initializer(0))
        out = tf.matmul(input_tensor, W)+b
    return out</code></p>
<p>//slim实现卷积，tfv2取消该库</p>
<h1 id="引入slim库"><a href="#引入slim库" class="headerlink" title="引入slim库"></a>引入slim库</h1><p><code>import tensorflow.contrib.slim as slim</code></p>
<h1 id="定义卷积层"><a href="#定义卷积层" class="headerlink" title="定义卷积层"></a>定义卷积层</h1><p><code>net = slim.conv2d(inputs, 16, 4, strides=2, activation_fn=tf.nn.relu, scope=&#39;conv1&#39;)</code></p>
<h1 id="加入池化层"><a href="#加入池化层" class="headerlink" title="加入池化层"></a>加入池化层</h1><p><code>net = tf.nn.max_pool(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)</code></p>
<p><code>net = slim.conv2d(net, 32, 4, strides=2, activation_fn=tf.nn.relu, scope=&#39;conv2&#39;)
net = tf.nn.max_pool(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)</code></p>
<h1 id="flatten层，用于将三维的图形数据展开成一维数据，用于全连接层"><a href="#flatten层，用于将三维的图形数据展开成一维数据，用于全连接层" class="headerlink" title="flatten层，用于将三维的图形数据展开成一维数据，用于全连接层"></a>flatten层，用于将三维的图形数据展开成一维数据，用于全连接层</h1><p><code>net = slim.flatten(net)</code></p>
<h1 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h1><p><code>y = slim.fully_connected(net, 10, activation_fn=line, scope=&#39;full&#39;, reuse=False)</code></p>
<p>4.2.3 tfLearn<br>//tflearn抽象层次更高，代码可读性更好,其是一个完整的生态<br>//基础网络架构</p>
<h1 id="全连接"><a href="#全连接" class="headerlink" title="全连接"></a>全连接</h1><p><code>net = tflearn.fully_connected(...)</code></p>
<h1 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h1><p><code>net = tflearn.conv_2d(...)</code></p>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p><code>net = tflearn.lstm(...)</code></p>
<h1 id="dropout-1"><a href="#dropout-1" class="headerlink" title="dropout"></a>dropout</h1><p><code>net = tflearn.dropout(...)</code></p>
<p>//输入函数<br><code>network = tflearn.input_data(shape=[None, 28, 28, 1], name=&#39;input&#39;)</code></p>
<p>//优化部分</p>
<h1 id="定义优化过程"><a href="#定义优化过程" class="headerlink" title="定义优化过程"></a>定义优化过程</h1><p><code>network = tflearn.layers.estimator.regression(
    network,
    optimizer=&#39;adam&#39;, #优化方法
    learning_rate=0.01, #学习率
    loss=&#39;categorical_crossentropy&#39;, #损失函数
    name=&#39;target&#39;)</code></p>
<p>//利用tflearn完成手写数字的识别任务<br><code>import tflearn
from tflearn.layers.core import input_data,dropout, fully_connected
from tflearn.layers.conv import conv_2d, , max_pool_2d
from tflearn.layers.normalization import local_response_normalization
from tflearn.layers.estimator import regression</code></p>
<h1 id="载入并处理数据"><a href="#载入并处理数据" class="headerlink" title="载入并处理数据"></a>载入并处理数据</h1><p><code>import tflearn.datasets.mnist as mnist
X, Y, testX, testY = mnist.load_data(one_hot=True)</code></p>
<h1 id="转换为二维图形"><a href="#转换为二维图形" class="headerlink" title="转换为二维图形"></a>转换为二维图形</h1><p><code>X = X.reshape([-1, 28, 28, 1])
testX = testX.reshape([-1, 28, 28, 1])</code></p>
<h1 id="建立神经网络"><a href="#建立神经网络" class="headerlink" title="建立神经网络"></a>建立神经网络</h1><p><code>network = tflearn.input_data(shape=[None, 28, 28, 1], name=&#39;input&#39;)
network = conv_2d(network, 32, 3, activation=&#39;relu&#39;, regularizer=&#39;L2&#39;)
network = max_pool_2d(network)
network = local_response_normalization(network)
network = fully_connected(network, 128, activation=&#39;tanh&#39;)
network = dropout(network, 0.8)
network = fully_connected(network, 256, activation=&#39;tanh&#39;)
network = dropout(network, 0.8)
network = fully_connected(network, 10, activation=&#39;softmax&#39;)</code></p>
<h1 id="定义优化过程-1"><a href="#定义优化过程-1" class="headerlink" title="定义优化过程"></a>定义优化过程</h1><p><code>network = regression(
    network,
    optimizer=&#39;adam&#39;,
    learning_rate=0.01,
    loss=&#39;categorical_crossentropy&#39;,
    name=&#39;target&#39;)</code></p>
<h1 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h1><p><code>model = tflearn.DNN(network, tensorboard_verbose=0)
model.fit(&#123;&#39;input&#39;:X&#125;, &#123;&#39;target&#39;:Y&#125;, n_epoch=20,
    validation_set=(&#123;&#39;input&#39;:testX&#125;, &#123;&#39;target&#39;:testY&#125;),
    snapshot_step=100, show_metric=True, run_id=&#39;convnet_mnist&#39;)</code></p>
<p>//Keras代码可读性好，并且横跨多个机器学习框架，但其扩展性较差<br>//引入库<br><code>from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D</code></p>
<p>//Keras直接顺序加入模型，无需通过数据方式进行传递<br>//基础网络层<br><code>from keras.models import Sequential
model = Sequential()</code></p>
<h1 id="加入卷积层"><a href="#加入卷积层" class="headerlink" title="加入卷积层"></a>加入卷积层</h1><p><code>model.add(Conv2D(...))</code></p>
<h1 id="加入池化层-1"><a href="#加入池化层-1" class="headerlink" title="加入池化层"></a>加入池化层</h1><p><code>model.add(MaxPooling2D(...))</code></p>
<h1 id="加入全连接层"><a href="#加入全连接层" class="headerlink" title="加入全连接层"></a>加入全连接层</h1><p><code>model.add(Dense(...))</code></p>
<h1 id="dropout-2"><a href="#dropout-2" class="headerlink" title="dropout"></a>dropout</h1><p><code>model.add(Dropout(0.25))</code></p>
<p>//定义model后可直接加入多种层进行操作，同样其需要定义训练函数<br>//定义优化过程<br><code>from keras.optimizers import SGD</code></p>
<h1 id="定义迭代算法"><a href="#定义迭代算法" class="headerlink" title="定义迭代算法"></a>定义迭代算法</h1><p><code>sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=sgd)</code></p>
<h1 id="训练过程-1"><a href="#训练过程-1" class="headerlink" title="训练过程"></a>训练过程</h1><p><code>model.fit(x_train, y_train, batch_szie=32, epochs=10)</code></p>
<h1 id="评估训练效果"><a href="#评估训练效果" class="headerlink" title="评估训练效果"></a>评估训练效果</h1><p><code>score = model.evaluate(x_test, y_test, batch_size=32)</code></p>
<p>//完整代码<br><code>import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.optimizers import SGD</code></p>
<h1 id="这里utils为自己定义的库函数，用于载入数据"><a href="#这里utils为自己定义的库函数，用于载入数据" class="headerlink" title="这里utils为自己定义的库函数，用于载入数据"></a>这里utils为自己定义的库函数，用于载入数据</h1><p><code>import utils
X, Y, testX, testY = utils.load_data(one_hot=True)
model = Sequential()</code></p>
<h1 id="定义神经网络过程"><a href="#定义神经网络过程" class="headerlink" title="定义神经网络过程"></a>定义神经网络过程</h1><p><code>model.add(Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(100, 100, 3)))
model.add(Conv2D(32, (3, 3), activation=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))</code></p>
<p><code>model.add(Conv2D(64, (3, 3), activation=&#39;relu&#39;))
model.add(Conv2D(64, (3, 3), activatin=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))</code></p>
<h1 id="展开为一维数据用于全连接层"><a href="#展开为一维数据用于全连接层" class="headerlink" title="展开为一维数据用于全连接层"></a>展开为一维数据用于全连接层</h1><p><code>model.add(Flatten())
model.add(Dense(256, activation=&#39;relu&#39;))
model.add(Dropout(0.5))
model.add(Dense(10, activation=&#39;softmax&#39;))</code></p>
<h1 id="梯度迭代算法"><a href="#梯度迭代算法" class="headerlink" title="梯度迭代算法"></a>梯度迭代算法</h1><p><code>sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=sgd)</code></p>
<h1 id="训练过程-2"><a href="#训练过程-2" class="headerlink" title="训练过程"></a>训练过程</h1><p><code>model.fit(x_train, y_train, batch_size=32, epochs=10)</code></p>
<h1 id="效果评估"><a href="#效果评估" class="headerlink" title="效果评估"></a>效果评估</h1><p><code>score = model.evaluate(x_test, y_test, batch_size=32)</code></p>
<p>4.3 Tensorboard与问题监控<br>//tensorboard最重要的作用就在于观察训练过程中的各种问题并改善，包括梯度消失、过拟合等问题<br>//获取所有可训练的参数<br><code>var_list_w = [var for var in tf.trainable_variables() if &#39;w&#39; in var.name]
var_list_b = [var for var in tf.trainable_variables() if &#39;b&#39; in var.name]</code></p>
<p>//利用定义的梯度算法来计算梯度<br><code>gradient_w = optimizer.compute_gradients(loss, var_list=var_list_w)
gradient_b = optimizer.compute_gradients(loss, var_list=var_list_b)</code></p>
<p>//返回的梯度是一个列表，可对其进行各种列表操作<br>//加入summary操作<br><code>for idx, itr_g in enumerate(gradient_w):
    variable_summaries(itr_g[0], &#39;layer%d-w-grad&#39;%idx)
for idx, itr_g in enumerate(gradient_b):
    variable_summaries(itr_g[0], &#39;layer%d-b-grad&#39;%idx</code></p>
<p><code>for idx, itr_g in enumerate(var_list_w):
    variable_summaries(itr_g, &#39;layer%d-w-grad&#39;%idx)
for idx, itr_g in enumerate(var_list_b):
    variable_summaries(itr_g, &#39;layer%d-b-grad&#39;%idx)</code></p>
<p>4.4改善深度神经网络<br>//出现梯度消失一种最有效的方式就是进行BN操作<br>//batchnorm层<br><code>net = tf.contrib.layers.batch_norm(net)</code></p>
<p>//加入BN层的神经网络</p>
<h1 id="对于sigmoid激活函数来讲，BN操作效果可能不理想"><a href="#对于sigmoid激活函数来讲，BN操作效果可能不理想" class="headerlink" title="对于sigmoid激活函数来讲，BN操作效果可能不理想"></a>对于sigmoid激活函数来讲，BN操作效果可能不理想</h1><p><code>net = slim.fully_connected(x, 4, activation_fn=tf.nn.sigmoid, scope=&#39;full1&#39;, reuse=False)
net = tf.contrib.layers.batch_norm(net)</code></p>
<p><code>net = slim.fully_connected(net, 8, activation_fn=tf.nn.sigmoid, scope=&#39;full2&#39;, reuse=False)
net = tf.contrib.layers.batch_norm(net)</code></p>
<p><code>net = slim.fully_connected(net, 8, activation_fn=tf.nn.sigmoid, scope=&#39;full3&#39;, reuse=False)
net = tf.contrib.layers.batch_norm(net)</code></p>
<p><code>net = slim.fully_connected(net, 4, activation_fn=tf.nn.sigmoid, scope=&#39;full4&#39;, reuse=False)
net = tf.contrib.layers.batch_norm(net)</code></p>
<p><code>net = slim.fully_connected(net, 3, activation_fn=tf.nn.sigmoid, scope=&#39;full5&#39;, reuse=False)</code></p>
<p><code>loss = tf.reduce_mean(tf.square(y-label))</code></p>
<p>4.5性能优化建议<br>//训练前的优化技巧<br>1.网络结构优化<br>Relu和BN能够有效加快神经网络训练速度<br>卷积核心的选取可以从大的卷积核心修改为多个小的卷积核心<br>将nxn修改为nx1+1xn，减少参数量，不同的输出内容之间可以进行concat<br>引入跨层支路解决梯度问题（ResNet)<br>2.初始值的选取<br>不好的初始值对训练的影响非常大，有效的初始化方法包括xavier初始化方法和He初始化方法<br>3.数据预处理<br>包括去均值和方差均衡<br>//训练过程中的优化技巧<br>1）优化算法的选择<br>Adam<br>2）学习率的选取<br>从大的步长开始进行迭代，逐步减少学习率<br>3）Batchsize选择<br>4）model ensembles<br>使用不同初始值同时训练多个模型，预测过程中将多个模型输出结果做平均，有效提升结果精度<br>5)dropout选择<br>从0.5附近进行调整，调整步长为0.05左右</p>
<p>//物体检测<br>1.传统检测方法<br>2001年，基于Haar特征和Adaboost检测方法引起轰动<br>2012年之前，三方面不断创新与优化：特征的设计更新、检测窗口的选择、分类器的设计更新</p>
<p>2.深度学习的物体检测<br>1）基于分类的物体检测<br>处理过程：图像被分解成多个小区域，每个小区域将运行一个分类算法以确定区域是否包含待检测物体，之后再在这个小区域的周围确认物体的边界框。代表算法：R-CNN、Fast-RCNN、Faster-RCNN<br>2) 基于回归的物体检测<br>将问题建模为回归问题，通过深度神经网络直接预测出边界框和所属类别的置信度。代表算法：SSD、YOLO模型</p>
<p>//YOLO模型<br>官网：<a href="https://pjreddie.com/darknet/yolo/">https://pjreddie.com/darknet/yolo/</a><br>//选讲tiny YOLO v1模型，由9个卷积层和3个全连接层组成，每个卷积层都由卷积层、LeakyRelu和Max Pooling操作组成，前9个卷积层可被理解为特征提取器，最后三个全连接层可被理解为预测边界框的回归器。<br>参考论文：You Only Look Once:Unified, Real-Time Object Detection<br>参考实例：<a href="https://github.com/xslittlegrass/CarND-Vehicle-Detection">https://github.com/xslittlegrass/CarND-Vehicle-Detection</a><br>模型参数：45089374<br>深度学习框架：Keras 1.2.2</p>
<p>//构建YOLO模型网络结构<br><code>import keras
from keras.models import Sequential
from keras.layers.convolutional import Convlution2D, MaxPooling2D
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.core import Flatten, Dense, Activation, Reshape
from utils import load_weights, Box, yolo_net_out_to_car_boxes, draw_box
def construct_yolo_model():
    keras.backend.set_image_dim_ordering(&#39;th&#39;)
    model = Sequential()
    model.add(Convolution2D(16, 3, 3, input_shape=(3, 448, 448), border_mode=&#39;same&#39;, subsample=(1, 1)))
    model.add(LeakyReLU(alpha=0.1))
    model.add(MaxPooling2D(pool_size=(2, 2)))</code></p>
<pre><code>model.add(Convolution2D(32, 3, 3, border_mode=&#39;same&#39;))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(pool_size=(2, 2), border_mode=&#39;valid&#39;))

model.add(Convolution2D(32, 3, 3, border_mode=&#39;same&#39;))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(pool_size=(2, 2), border_mode=&#39;valid&#39;))

model.add(Convolution2D(64, 3, 3, border_mode=&#39;same&#39;))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(pool_size=(2, 2), border_mode=&#39;valid&#39;))

model.add(Convolution2D(128, 3, 3, border_mode=&#39;same&#39;))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(pool_size=(2, 2), border_mode=&#39;valid&#39;))

model.add(Convolution2D(256, 3, 3, border_mode=&#39;same&#39;))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(pool_size=(2, 2), border_mode=&#39;valid&#39;))

model.add(Convolution2D(512, 3, 3, border_mode=&#39;same&#39;))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(pool_size=(2, 2), border_mode=&#39;valid&#39;))

model.add(Convolution2D(1024, 3, 3, border_mode=&#39;same&#39;))
model.add(LeakyReLU(alpha=0.1))

model.add(Convolution2D(1024, 3, 3, border_mode=&#39;same&#39;))
model.add(LeakyReLU(alpha=0.1))

model.add(Convolution2D(1024, 3, 3, border_mode=&#39;same&#39;))
model.add(LeakyReLU(alpha=0.1

model.add(Flatten())
model.add(Dense(256))
model.add(Dense(4096))
model.add(LeakyReLU(alpha=0.1))
model.add(Dense(1470))
model.summary()
return model
</code></pre><p>注：网络的输入是形状为（3,448,448)的图像，其输出是一个1470维度的向量，它包含预测边界框、物体类别信息。1470矢量被分成三个部分，分别给出了所属类别概率、置信度和边框坐标。这三个部分进一步划分为49个小区域，与每个单元的预测相对应。<br>输出向量信息组织方式：<br><code>probability:49*20=980</code><br>判断类别，20个类别<br><code>confidence:49*2=98</code><br>是否包含物体（0,1）<br><code>box coordinates:49*8=392
(x_min,y_min,x_max,y_max),(c_x,c_y,w,h)</code></p>
<p>8.4.3车辆图像数据探索<br>1.车辆视频数据预处理<br>//预处理及可视化图像<br><code>def visualize_images():
    imagePath = &#39;./test_images/test1.jpg&#39;
    image = plt.imread(imagePath)</code></p>
<pre><code>#去除顶部和底部图片
`image_crop = image[300:650,500:,:]`
#将图片转换为模型所需要的输入格式
`resized = cv2.resize(image_crop, (448, 448))
f1,(ax11,ax22,ax33) = plt.subplot(1, 3, figsize=(16, 6))
ax11.imshow(image)
ax22.imshow(image_crop)
ax33.imshow(resized)
pylab.show()
return resized`
</code></pre><p>8.4.5迁移学习<br>通过迁移学习加载使用Pre-trained YOLO模型进行行车检测。具体做法是将pre-trained模型中的权重加载进之前构造的模型结构中，官网提供的权重，可以通过脚本解析成Keras能够加载的格式。<br>//加载YOLO模型权重<br>`def load_model_weights(model):</p>
<pre><code>#预训练权重网址：https://pjreddie.com/darknet/yolo/
load_weights(model, &#39;./yolo-tiny.weights&#39;)`
</code></pre><p>//加载模型权重的具体逻辑<br><code>def load_weights(model, yolo_weight_file):
    data = np.fromfile(yolo_weight_file, np.float32)
    data = data[4:]
    index = 0
    for layer in model.layers:
        shape = [w.shape for w in layer.get_weights()]
        if shape !=[]:
            kshape, bshape = shape
            bia = data[index:index+np.prod(bshape)].reshape(bshape)
            index += np.prod(bshape)
            ker = data[index:index:index+np.prod(kshape)].reshape(kshape)
            index += np.prod(kshape)
            layer.set_weights([ker, bia])</code></p>
<p>//模型推断<br>//使用模型进行在线推断，预测出车辆区域<br>`def inference_image(model, resized):</p>
<pre><code>#转置
batch = np.transpose(resized, (2, 0, 1))
#将像素值变换到-1~1
batch = 2*(batch/255.) - 1
#将一张图片转为数组
batch = np.expand_dims(batch, axis=0）
out = model.predict(batch)
return out`
</code></pre><p>//绘制检测结果<br>//将上述的预测结果转换为边框坐标，同时基于阈值进行预测<br><code>th = 0.17
boxes = yolo_net_to_out_to_car_boxes(out[0], threshold=th)</code></p>
<p>//定义box边框对象，判断是否保留预测的边框结果通过c,在图像上绘制车辆位置通过对象中的坐标信息</p>
<h1 id="定义box类，存储边框信息和物体检测类别等信息"><a href="#定义box类，存储边框信息和物体检测类别等信息" class="headerlink" title="定义box类，存储边框信息和物体检测类别等信息"></a>定义box类，存储边框信息和物体检测类别等信息</h1><p>`class Box:<br>def <strong>init</strong>(self):</p>
<pre><code>#x, y轴坐标
self.x, self.y = float(), float()
#边框宽度和长度
self.w, self.h = float(), float()
#置信度
self.c = float()
#所属类别概率
self.prob = float()`
</code></pre><p>//通过yolo_net_to_out_to_car_boxes方法，将预测出的Vector转换为Box对象信息。其核心逻辑是解析模型预测输出向量中的坐标、类别和置信度信息<br>//置信度大于阈值边界框则进行保留<br><code>class_num = 6 #yolo模型可以预测多种类别，6为车辆所属类别
p = probs[grid, :] *bx.c
if p[class_num]&gt;=threshold:
    bx.prob = p[class_num]
    boxes.append(bx)</code></p>
<p>//将结果绘制在图像上<br><code>def visualize_image_car_detection(boxes):
    imagePath = &#39;./test_images/test1.jpg&#39;
    image = plt.imread(imagePath)
    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    ax1.imshow(image)
    ax2.imshow(draw_box(boxes, plt.imread(imagePath), [[500, 1280],[300,650]]))
    pylab.show()</code></p>
<p>//将边框绘制在图像上<br><code>def draw_box(boxes, im, crop_dim):
    imgcv = im
    [xmin, xmax] = crop_dim[0]
    [ymin, ymax] = crop_dim[1]
    for b in boxes:
        h, w, _ = imgcv.shape
        left = int((b.x-b.w/2.)*w)
        right = int((b.x+b.w/2.)*w)
        top = int((b.y-b.h/2.)*h)
        bot = int((b.y+b.h/2.)*h)
        left = int(left*(xmax-xmin)/w+xmin)
        right = int(right*(xmax-xmin)/w+xmin)
        top = int(top*(ymax-ymin)/h+ymin)
        bot = int(bot*(ymax-ymin)/h+ymin)
        if left&lt;0 : left=0
        if right&gt;w-1 : right=w-1
        if top&lt;0 : top=0
        if bot&gt;h-1 : bot=h-1
        thick = int((h+w)//150)
        cv2.rectangle(imgcv, (left, top), (right, bot), (255,0,0), thick)
    return imgcv</code></p>
<p>8.5.1英伟达End to End模型<br>End to End的好处：通过缩减人工预处理和后续处理，尽可能使模型从原始输入到输出，使得其根据网络模型能够有足够多的空间进行自动调节，从而减少基于规则的复杂变化。<br>缺点：可解释性较差，准确度和精度不容易受控制。<br>//构建英伟达模型<br><code>import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten, Lambda
from keras.layers import Conv2D, Dropout
from keras import losses
def nvida_model():
    model = Sequential()
    model.add(Lambda(lambda x: x/127.5-1., input_shape=(img_height, img_width, img_channels)))
    model.add(Conv2D(24, kernel_size=(5, 5), strides=(2, 2), padding=&#39;valid&#39;, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;))
    model.add(Conv2D(36, kernel_size=(5, 5), strides=(2, 2), padding=&#39;valid&#39;, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;))
    model.add(Conv2D(48, kernel_size=(5, 5), strides=(2, 2), padding=&#39;valid&#39;, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;))
    model.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding=&#39;valid&#39;, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;))
    model.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding=&#39;valid&#39;, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu
    model.add(Flatten())
    model.add(Dense(1164, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;))
    model.add(Dense(100, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;))
    model.add(Dense(50, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;))
    model.add(Dense(10, kernel_initializer=&#39;he_normal&#39;, activation=&#39;elu&#39;))
    model.add(Dense(1, kernel_initializer=&#39;he_normal&#39;))
    model.compile(loss=&#39;mse&#39;, optimizer=&#39;Adadelta&#39;)
    return model</code></p>
<p>//8.5.3数据分析<br>1）转向控制数据分布</p>
<h1 id="绘制转向分布"><a href="#绘制转向分布" class="headerlink" title="绘制转向分布"></a>绘制转向分布</h1><p><code>def steering_distribution():
    wheel_sig = pd.read_csv(params.data_dir+&#39;/epoch01_steering.csv&#39;)
    wheel_sig.head()
wheel_sig.wheel.hist(bins=50)</code></p>
<p>2)数据变化幅度</p>
<h1 id="绘制转向变化幅度"><a href="#绘制转向变化幅度" class="headerlink" title="绘制转向变化幅度"></a>绘制转向变化幅度</h1><p><code>def angel_visualize():
    wheel_sig = pd.read_csv(params.data_dir+&#39;/epoch01_steering.csv&#39;)
    wheel_sig.plot(x=&#39;frame&#39;, y=&#39;wheel&#39;)
    plt.show()</code></p>
<p>8.5.4读入视频，并处理图像<br>//使用OpenCV从视频中提取图像，以及与其对应的转向角度并返回</p>
<h1 id="提取图像并处理"><a href="#提取图像并处理" class="headerlink" title="提取图像并处理"></a>提取图像并处理</h1><p>`imgs = []<br>wheels = []<br>epochs = [10]<br>for epoch in epochs:<br>    vid_path = utils.join_dir(params.data_dir, ‘epoch{:0&gt;2}_front.mp4’.format(epoch))<br>    assert os.path.isfile(vid_path)<br>    frame_count = frame_count_func(vid_path)<br>    cap = cv2.VideoCapture(vid_path)<br>    for frame_id in range(frame_count):<br>        while True:</p>
<pre><code>    #通过OpenCV中的VideoCapture进行视频中图像的提取
        ret, img = cap.read()
        if not ret:
            break
        #用户可以自定义对图像的处理、扩展和增强操作
        img = a_image_convert.img_preprocess(img, color_mode, flip=False)
        imgs.append(img)
    csv.path = os.path.join(data_dir, &#39;epoch&#123;:0&gt;2&#125;_steering.csv&#39;.format(epoch))
    rows = pd.read_csv(csv.path)
    yy = rows[&#39;wheel&#39;].values
    wheels.extend(yy)
    cap.release()
imgs = np.array(imgs)
wheels = np.array(wheels)
wheels = np.reshape(wheels, (len(wheels), 1)
return imgs, wheels`
</code></pre><p>8.5.5深度学习模型构建与训练<br>//训练模型<br>`def training(model, X_train_RGB, y_train_RGB):<br>    RGB_model = model<br>    time_start = time.time()</p>
<pre><code>#fit the model
RGB_history = RGB_model.fit(X_train_RGB, y_train_RGB, epochs=30, batch_size=batch_size)
return RGB_model, RGB_history`
</code></pre><p>//可视化结果</p>
<h1 id="将训练过程中的loss误差进行可视化"><a href="#将训练过程中的loss误差进行可视化" class="headerlink" title="将训练过程中的loss误差进行可视化"></a>将训练过程中的loss误差进行可视化</h1><p><code>def visualize_label(RGB_history):
    print(RGB_history.history[&#39;loss&#39;]
    plt.figure(figsize=(9, 6))
    plt.plot(RGB_history.history[&#39;loss&#39;])
    plt.title(&#39;model loss&#39;)
    plt.ylabel(&#39;Loss&#39;, fontsize=12)
    plt.xlabel(&#39;Epoch&#39;, fontsize=12)
    plt.legend([&#39;train RGB&#39;], loc=&#39;upper right&#39;)
    plt.grid()
    plt.show()</code></p>
<p>//可视化<br>//数据的绘图过程就是将前面所得到的一系列数据，通过静态、动态的二维、三维图形进行展示<br>1.Matplotlib<br>//绘制y=sinx图像<br><code>import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0, 4*np.pi, 1000)
y = np.sin(x)
plt.plot(x,y)</code></p>
<p>//利用API,绘制更加审美要求的图像<br>`import numpy as np<br>import matplotlib.pyplot as plt<br>import matplotlib as mpl</p>
<h1 id="设置图片风格"><a href="#设置图片风格" class="headerlink" title="设置图片风格"></a>设置图片风格</h1><p>mpl.style.use(‘seaborn-darkgrid’)</p>
<h1 id="定义曲线"><a href="#定义曲线" class="headerlink" title="定义曲线"></a>定义曲线</h1><p>x = np.linspace(0, 4*np.pi, 100)<br>y1 = np.sin(x)<br>y2 = np.sin(x+1)<br>y3 = np.sin(x+2)</p>
<h1 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a>绘图</h1><p>plt.plot(x, y1, color=’#009900’, lw=6, alpha=0.6)<br>plt.plot(x, y2, color=’#990000’, lw=6, alpha=0.6)<br>plt.plot(x, y3, color=’#000099’, lw=6, alpha=0.6)</p>
<h1 id="展示"><a href="#展示" class="headerlink" title="展示"></a>展示</h1><p>plt.show()`</p>
<p>9.4ECharts<br>//ECharts提供了常规的折线图、柱状图、散点图、饼图、K线图等等，功能强大。<br>//ECharts图形绘制<br>略</p>
<p>//文本向量化<br>//文本向量化函数</p>
<h1 id="文本TfIdf向量化"><a href="#文本TfIdf向量化" class="headerlink" title="文本TfIdf向量化"></a>文本TfIdf向量化</h1><p><code>from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidVectorizer()
vectors = vectorizer.fit_transform(datas)</code></p>
<p>//文本向量化的数据进行降维<br>//LDA降维<br><code>from sklearn.decomposition import LatentDirichletAllocation
lda = LatenDirichletAllocation(n_components=n_topic, max_iter=5,
    learning_method = &#39;online&#39;,
    learning_offset = 50.,
    radom_state = 0)</code></p>
<h1 id="用LDA方法降维数据"><a href="#用LDA方法降维数据" class="headerlink" title="用LDA方法降维数据"></a>用LDA方法降维数据</h1><p><code>dr_vectors = lad.fit_transform(vectors)</code></p>
<p>9.6三维可视化<br>//ECharts地图柱状图<br><code>myChart.setOption(&#123;
    visualMap: &#123;
        show: flase,
        calculable: true,
        realtime: false,
        inRange: &#123;
        color: [&#39;#313695&#39;, &#39;#4575b4&#39;, &#39;#74add1&#39;, &#39;#abd9e9&#39;,
                &#39;#e0f3f8&#39;, &#39;#ffffbf&#39;, &#39;#fee090&#39;, &#39;#fdae61&#39;,
                &#39;#f46d43&#39;, &#39;#d73027&#39;, &#39;#d73027&#39;, &#39;a50026&#39;]
                &#125;,
        outOfRange: &#123;
            colorAlpha: 0
            &#125;,
        max: linedata[1]
        &#125;,
        ...
        series: [&#123;
            type: &#39;bar3D&#39;,
            shading: &#39;realistic&#39;,
            coordinateSystem: &#39;mapbox&#39;,
            barSize: 0.2,
            silent: true,
            data: linedata[0]
            &#125;]
        &#125;);</code></p>
<p>//利用Matplotlib完成对三维数据的可视化任务<br><code>from mpl_toolkits.mplot3d import axes3d
import matplotlib.pyplot as plt
from matplotlib import cm
import matplotlib.style as style
style.use(&#39;seaborn-darkgrid&#39;)</code></p>
<h1 id="定义三维画布"><a href="#定义三维画布" class="headerlink" title="定义三维画布"></a>定义三维画布</h1><p><code>fig = plt.figure()
ax = fig.gca(projection=&#39;3d&#39;)</code></p>
<h1 id="获取数据-1"><a href="#获取数据-1" class="headerlink" title="获取数据"></a>获取数据</h1><p><code>X, Y, Z = axes3d.get_test_data(0.05)</code></p>
<h1 id="绘制surface"><a href="#绘制surface" class="headerlink" title="绘制surface"></a>绘制surface</h1><p><code>ax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.3)</code></p>
<h1 id="绘制等值线"><a href="#绘制等值线" class="headerlink" title="绘制等值线"></a>绘制等值线</h1><p><code>cst = ax.contourf(X, Y, Z, zdir=&#39;z&#39;, offset=-100, cmap=cm.coolwarm)
cst = ax.contourf(X, Y, Z, zdir=&#39;x&#39;, offset=-40, cmap=cm.coolwarm)
cst = ax.contourf(X, Y, Z, zdir=&#39;y&#39;, offset=40, cmap=cm.coolwarm)</code></p>
<p><code>plt.show()</code></p>
<p>9.7动态可视化<br>//Matplotlib中用于数据动态演示的方法为animation,其可以通过函数进行简单的调用，以进行动态图形的演示工作<br>//动画展示<br><code>import matplotlib.animation as animation
animation.FuncAniamtion(
    ...
    )</code></p>
<p>//动态可视化的展示方式是在普通的图表之上通过不断地修改数据并进行展示，这种修改可以通过setOption而得到的，在实现上可以通过函数递归的方式实现动态数据的可视化工作<br><code>function update()&#123;
    myChart.setOption(...);
        setTimeout(update, UPDATE_DURATION);
        &#125;
update();</code></p>
<p>//优化实践<br>10.1通用深度神经网络训练优化建议</p>
<p>1）通用的较为优化的训练过程<br>1.将问题转换为相似的经典问题场景，参照paper中的配置和调优技巧进行最初的实验与优化<br>2.优化算法：选用随机梯度下降（SGD)算法，虽然批量梯度下降（BGD)相比SGD有一些优势，但是在处理大规模数据时，SGD及其优化变种更加简单和快速。<br>3.随机Shuffle样本：应尽量避免连续处理的样本属于同一类别的情况。尽量选择当前样本能让模型产生较大的误差，而不是较小的误差<br>4.规范化数据：输入的每个变量均值最好趋近于0.变换输入变量，使其协方差相近，变量间尽量不要相关<br>5.激活函数的选取：相比Sigmoid函数，tanh和Relu有更好的收敛速度。<br>6.权重初始化：可以随机通过一种分布，均值为0.<br>7.选择学习率：每个权重都可以选取属于自己的学习率。处于低层的权重学习率最好大于高层的权重学习率。学习率最好正比于每个单元的输入数量。</p>
<p>2）CNN训练过程中通常关注的优化点和参数<br>一般比较关注：Learning Rate,Weight Decay,Momentum,Batchsize,Init Weights,数据增强<br>eg:在Resnet中，使用SGD优化算法优化方法训练，mini-batch的大小设置为256，学习率初始化为0.1.随着训练进行，当Loss不再下降，会每次自适应以10倍进行缩减学习率。模型训练用了60x10^4轮迭代。Weight Decay设置为0.0001，同时设置momentum为0.9</p>
<p>3)RNN训练过程中通常关注的优化点和参数<br>一般比较关注：SGD,正则化，规范化梯度，Pad Sentence,Init Weight, Batch Size, Embedding输入，输出控制，Vacabulary Size, Sampled Softmax<br>eg:Google发布的TTS模型TACOTRON为例</p>
<p>10.1.1 过拟合和欠拟合<br>欠拟合：若训练集和测试集的误差有收敛但很高时，则为高偏差<br>过拟合：若训练集和测试集的误差较大时，则为高方差</p>
<p>解决过拟合的方法：<br>正则化，数据增强，Early Stop, Dropout, Batch Normalization</p>
<p>解决欠拟合的方法：<br>1.使用更加复杂的深度学习网络架构<br>2.添加其他特征项，有时候模型出现欠拟合的情况是因为特征项不够导致的，可以添加其他特征项来很好的解决这个问题<br>3.减少正则化参数和组件，正则化的目的是用来防止过拟合。</p>
<p>10.1.2数据增强<br>//数据增强的根本原因在于机器在学习的过程中会在模型中遇到大量的参数，同时为了防止过拟合<br>1）对于图像数据，可采取：<br>1.图像平移：使得网络学习到平移不变的特性<br>2.图像旋转：使得网络学习到旋转不变的特性<br>3.图像亮度变化<br>4.裁剪<br>5.缩放<br>6.图像模糊:用不同的卷积模板产生模糊图像<br>2）语音识别中对输入数据添加随机噪声等方式<br>3）NLP中最常用的方式就是进行近义词替换等方式<br>4）噪声注入，可以对输入添加噪声，也可以对隐藏层或者输出层添加噪声</p>
<p>10.1.3梯度消失<br>//实验数据显示了深度神经网络在训练过程中，随着epoch的增加各隐藏层的学习率变化。前面隐藏层的学习速度要低于后面的隐藏层<br>//梯度消失的原因：根据链式法则，如果每一层神经元对上一层输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层的传播后，误差对输入层的偏导也会趋近于0<br>解决梯度消失的策略：<br>1.BN<br>2.RNN中使用LSTM:适用于RNN,门控制和长时记忆可缓解和解决梯度消失问题<br>3.激活函数Relu:新的激活函数解析性质更好，其在一定程度上克服了sigmoid函数和tanh函数的梯度消失问题。<br>4.在RNN反向传播过程中减少时间步长度。</p>
<p>10.1.4初始化权重<br>//在参数解空间内，好的权重初始化方式，意味着离全局最小值更近。<br>1.高斯初始化，为权重初始化较小的值，权重按照高斯分布随机进行初始化，固定均值和方差<br>2.Xaiver更新方法，使用tanh为激活函数，效果较好。进行梯度更新时，收敛速度较快，然而没有考虑Relu<br>3.MSRA方法，适用于从头训练深层深度神经网络的网络结构。权重以高斯分布随机进行初始化，方差需要考虑空间过滤器的大小和过滤器数量的影响。</p>
<p>10.1.5优化算法<br>近些年最常用的是采用Adam优化算法，也可以采用自适应学习率的方法实现快速收敛。</p>
<p>10.1.6超参数选择<br>一些实践经验：<br>1.在验证集上进行调参<br>2.优先调Learning Rate<br>3.通过初期设计卷积层尽量深、卷积核尽量多的模型，强行让模型拟合训练集，这时会出现过拟合，之后通过Dropout、正则化和Data Augument等等方式去改善模型结果<br>4.调整模型的层数和卷积核数量</p>
<p>//通过Scikit-learn的网格搜索库进行参数调优实例<br>1.常见搜索参数<br>学习率、Dropout、Epochs和神经元数量<br>2.数据集下载<br>数据集为Pima Indians Onset of Diabetes分类数据集<br>下载地址：<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/">https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/</a><br>3.搜索最优batchsize和epochs<br>//以20的步长，从10到100逐步评估不同的微型批尺寸，epochs分别设置为10、50、100<br><code>import numpy
from sklearn.grida_search import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier</code></p>
<h1 id="Function-to-create-model-required-for-KerasClassifier"><a href="#Function-to-create-model-required-for-KerasClassifier" class="headerlink" title="Function to create model, required for KerasClassifier"></a>Function to create model, required for KerasClassifier</h1><p>`def create_model():</p>
<pre><code>#create model
model = Sequential()
model.add(Dense(12, input_dim=8, activation=&#39;relu&#39;))
model.add(Dense(1, activation=&#39;sigmoid&#39;))`


#compile model
model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;])
return model
</code></pre><p>`#fix random seed for reproducibility<br>seed = 7<br>numpy.random.seed(seed)</p>
<h1 id="load-dataset"><a href="#load-dataset" class="headerlink" title="load dataset"></a>load dataset</h1><p>dataset = numpy.loadtxt(“pima-indians-diabetes.csv”, delimiter=’,’)</p>
<h1 id="split-into-input-x-and-output-Y-variables"><a href="#split-into-input-x-and-output-Y-variables" class="headerlink" title="split into input (x) and output (Y) variables"></a>split into input (x) and output (Y) variables</h1><p>X = [:, 0:8]<br>Y = [:, 8]</p>
<h1 id="create-model"><a href="#create-model" class="headerlink" title="create model"></a>create model</h1><p>model = KerasClassifier(build_fn=create_model, verbose=0)</p>
<h1 id="define-the-grid-search-parameters"><a href="#define-the-grid-search-parameters" class="headerlink" title="define the grid search parameters"></a>define the grid search parameters</h1><p>batch_size = [10, 20, 40, 60, 80, 100]<br>epochs = [10, 50, 100]<br>param_grid = dict(batch_size=batch_size, nb_epoch=epochs)<br>grid = GridSearchCV(estimator=model, param_grid=parm_grid, n_jobs=-1)<br>grid_result = grid_fit(X,Y)</p>
<h1 id="summarize-results"><a href="#summarize-results" class="headerlink" title="summarize results"></a>summarize results</h1><p>print(“Best: %f using %s” % (grid<em>result.best_score</em>, grid<em>result.best_params))<br>for params, mean_score, scores in grid_result.grid_scores</em>:<br>    print(“%f (%f) with: %r” % (scores.mean(), scores.std(), params))`</p>
<p>10.2深度学习系统性能优化建议<br>10.2.1输入及预处理流水线优化<br>输入流水线：从磁盘读取图像，将JPEG预处理为张量，进行数据预处理裁剪、翻转等，然后进行批处理操作<br>1.在CPU端进行预处理<br>//在CPU端上放置输入预处理操作可以显著提高性能，GPU专注训练<br>//控制代码在CPU端执行<br>`with tf.device(“/cpu:0”):</p>
<pre><code># function to get and process data.
</code></pre><p>​    distored_inputs = load_and_preprocess_images()`</p>
<p>2.使用大文件<br>读取大量的小文件会显著影响I/O性能<br>1）转换为TFRecord格式<br>2）小数据集加载到内存</p>
<p>10.2.2数据格式<br>NHWC的方存局部性更好（每三个输入像素即可得到一个输出像素），NCHW则必须等所有通道输入都准备好后才能得到最终的输出结果，需要占用较大的临时空间。<br>tf默认NHWC格式，Nvidia cuDNN默认NCHW格式<br>注：设计网络时充分考虑这两种格式，最好能够灵活切换，在GPU上训练时使用NCHW格式，在CPU上做预测时使用NHWC格式</p>
<p>10.2.3编译优化<br>//通过bazel命令对特定平台对tf进行编译<br><code>bazel build -c opt --copt=-march=&quot;brodewell&quot; --config=cuda
//tensorflow/tools/pip_package:build_pip_package</code></p>
<p>10.2.4GPU性能瓶颈诊断<br>//参考如下分析步骤对作业进行优化<br>1）对代码进行性能分析<br>2）找到运行慢的阶段<br>3）分析慢的原因<br>4）修改成更快的实现<br>5）再次对代码进行性能分析</p>
<p>//处理器有两个关键的性能瓶颈：浮点计算量和内存吞吐量。<br>//可通过以下工具进行深度学习作业的性能分析<br>1.Tensorflow性能分析工具Timeline(获取执行图中每个节点的执行时间）<br>1）创建metadata运行时对象<br>2）获取运行时信息创建Timeline对象<br>3）将Timeline对象写入json文件<br>4）Chrome加载trace的json文件</p>
<p>//tensorflow使用Timeline进行性能分析<br><code>import tensorflow as tf
from tensorflow.python.client import timeline</code></p>
<p><code>x = tf.random_normal([1000, 1000])
y = tf.random_normal([1000, 1000])
res = tf.matmul(x, y)</code></p>
<p><code>#run the graph with full trace option
with tf.Session() as sess:
    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
    run_metadata = tf.RunMetadata()
    sess.run(res, options=run_options, run_metadata=run_metadata)</code></p>
<pre><code>#create Timeline variable，then write it into json file
t1 = timeline.Timeline(run_metadata.step_stats)
ctf = t1.generate_chrome_trace_format()
with open(&#39;timeline.json&#39;, &#39;w&#39;) as f:
    f.write(ctf)
</code></pre><p>可以打开谷歌chrome浏览器，转到chrome://tracing页并加载timeline.json文件，接下来，可以进行程序的profiling</p>
<p>2.常用的GPU分析工具<br>1）nvprof是英伟达性能分析工具<br>2）nvvp则是带GUI的英伟达可视化性能分析工具</p>
<p>10.2.5CPU瓶颈优化<br>1）多线程方式优化<br>以下两个针对tensorflow的配置可以通过适配线程池进行CPU的性能优化<br>intra_op_parallelism_threads：对tf操作符内部的任务进行并行化<br>inter_op_parallelism_threads: 控制多个运算符之间的并行化运算<br>//多线程优化<br><code>config = tf.ConfigProto()
config.intra_op_parallelism_threads = 22
config.inter_op_parallelism_threads = 22
tf.session(config=config)</code></p>
<p>2)使用SIMD高级指令集<br>参考tf官方文档的”Performance Guide”章节</p>
<p>10.2.6模型压缩<br>模型小型化：从模型权重的角度进行压缩和从网络架构的角度进行压缩<br>网络架构角度：提出新的网络结构或卷积方法进行压缩优化，如SqueezeNet, MobileNets等<br>模型权重角度：一般是在已经训练好的模型上进行裁剪，然后fine-tuning到原有模型的准确率，一般的优化方式包括剪枝、权值共享、神经网络二值化等。</p>
<p>10.3工程实践建议<br>10.3.1Model格式转换<br>框架间的模型转换<br>参考链接：<br>1.<a href="https://github.com/ysh329/deep-learning-model-convertor">https://github.com/ysh329/deep-learning-model-convertor</a><br>2.<a href="https://github.com/Microsoft/MMdnn">https://github.com/Microsoft/MMdnn</a></p>
<p>10.3.2迁移学习（Transfer Learning)<br>其思想是将训练好的模型参数迁移到新的模型来帮助新模型的训练和预测。</p>
<p>//通过MNIST数据集0~4的数字训练一个模型，然后将模型迁移到5~9数据集上进行迁移学习<br>1）在MNIST数据集上训练一个简单的卷积神经网络，只预测0~4的数字<br>2）将训练好的预测0~4数据集的模型，应用到5~9数据集上。对模型冻结卷积层参数，Fine-Tuning全连接层。<br>//keras迁移学习实例<br><code>from __future__ import print_function
import datetime
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K</code></p>
<p><code>now = datetime.datetime.now
batch_size = 128
num_classes = 5
epochs = 5</code></p>
<p>`#input images dimensions<br>img_rows, img_cols = 28, 28</p>
<h1 id="number-of-convolutional-filters-to-use"><a href="#number-of-convolutional-filters-to-use" class="headerlink" title="number of convolutional filters  to use"></a>number of convolutional filters  to use</h1><p>filters = 32</p>
<h1 id="size-of-pooling-area-for-max-pooling"><a href="#size-of-pooling-area-for-max-pooling" class="headerlink" title="size of pooling area for max pooling"></a>size of pooling area for max pooling</h1><p>pool_size = 2</p>
<h1 id="convolution-kernel-size"><a href="#convolution-kernel-size" class="headerlink" title="convolution kernel size"></a>convolution kernel size</h1><p>kernel_size = 3`</p>
<p><code>if K.image_data_format()==&#39;channels_first&#39;:
    input_shape = (1, img_rows, img_cols)
else:
    input_shape = (img_rows, img_cols, 1)</code></p>
<p><code>def train_model(model, train, test, num_classes):
    x_train = train[0].reshape((train[0].shape[0],)+input_shape)
    x_test = test[0].reshape((test[0].shape[0],)+input_shape)
    x_train = x_train.astype(&#39;float32&#39;)
    x_test = x_test.astype(&#39;float32&#39;)
    x_train /= 255
    x_test /= 255
    print(&#39;x_train shape:&#39;, x_train.shape)
    print(x_train.shape[0], &#39;train samples&#39;)
    print(x_test.shape[0], &#39;test samples&#39;)</code></p>
<pre><code>#convert class vectors to binary class matrics
y_train = keras.utils.to_categorical(train[1], num_classes)
y_test = keras.utils.to_categorical(test[1], num_classes)

model.compile(
    loss = &#39;categorical_crossentropy&#39;,
    optimizer = &#39;adadelta&#39;,
    metrics = [&#39;accuracy&#39;]
    )

t = now()
model.fit(x_train, y_train,
    batch_size = batch_size,
    epochs = epochs,
    verbose = 1,
    validation_data = (x_test, y_test))
print(&#39;Training time: %s&#39; %(now() -t))
score = model.evaluate(x_test, y_test, verbose=0)
print(&#39;Test score:&#39;, score[0])
</code></pre><p>`#the data,shuffled and spilt between train and test sets<br>(x_train, y_train), (x_test, y_test) = mnist.load-data()</p>
<h1 id="create-two-datasets-one-with-digits-below-5-and-one-with-5-and-above"><a href="#create-two-datasets-one-with-digits-below-5-and-one-with-5-and-above" class="headerlink" title="create two datasets one with digits below 5 and one with 5 and above"></a>create two datasets one with digits below 5 and one with 5 and above</h1><p>x_train_lt5 = x_train[y_train&lt;5]<br>y_train_lt5 = x_train[y_train&lt;5]<br>x_test_lt5 = x_test[y_test&lt;5]<br>y_test_lt5 = y_test[y_test&lt;5]`</p>
<p><code>x_train_get5 = x_train[y_train&gt;=5]
y_train_get5 = y_train[y_train&gt;=5]-5
x_test_get5 = x_test[y_test&gt;=5]
y_test_get5 = y_test[y_test&gt;=5]-5</code></p>
<p><code>#define two groups of layers:feature(convolutions) and classification(dense)
feature_layers = [
        Conv2D(filters, kernel_size,
            padding=&#39;valid&#39;,
            input_shape=input_shape),
        Activation(&#39;relu&#39;),
        Conv2D(filters, kernel_size),
        Activation(&#39;relu&#39;),
        MaxPooling2D(pool_size=pool_size),
        Dropout(0.5),
        Flatten()]
classification_layers=[
    Dense(128),
    Activation(&#39;relu&#39;),
    Dropout(0.5),
    Dense(num_classes),
    Activation(&#39;softmax&#39;)]</code></p>
<p><code>#create complete model
model = Sequential(feature_layers+classification_layers)</code></p>
<p><code>#train model for 5-digit classification(0~4)
train_model(model,
    (x_train_lt5, y_train_lt5),
    (x_test_lt5, y_test_lt5), num_classes)</code></p>
<p><code>#freeze feature layers and rebuild model
for l in feature_layers:
    l.trainable = False</code></p>
<p><code>#transfer: train dense layers for new classification task(5~9)
train_model(model,
    (x_train_gte5, y_train_get5),
    (x_test_get5, y_test_get5), num_classes)</code></p>
<p>​<br>​<br>​    </p>
]]></content>
  </entry>
  <entry>
    <title>学术搜索网站集合</title>
    <url>/CN/%E5%AD%A6%E6%9C%AF%E6%90%9C%E7%B4%A2%E7%BD%91%E7%AB%99%E5%AF%BC%E8%88%AA/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>搞学术研究的必不可少的就是论文，而且是大量的论文。因此，在这里我特意把我平时用到的积累到的论文搜索网站集中到了这里，后续遇到了其他的还会继续补充。</p>
<h2 id="谷歌学术"><a href="#谷歌学术" class="headerlink" title="谷歌学术"></a>谷歌学术</h2><p><a href="https://xs.scqylaw.com/">谷歌学术</a>是一个可以免费搜索学术文章的Google网络应用。2004年11月，Google第一次发布了Google学术搜索的试用版。该项索引包括了世界上绝大部分出版的学术期刊， 可广泛搜索学术文献的简便方法。您可以从一个位置搜索众多学科和资料来源：来自学术著作出版商、专业性社团、预印本、各大学及其他学术组织的经同行评论的文章、论文、图书、摘要和文章。Google 学术搜索可帮助您在整个学术领域中确定相关性最强的研究。</p>
<p>相关页面如下：</p>
<p><img src="https://img.99lb.net/images1/logo.png" class="lazyload" data-srcset="https://img.99lb.net/images1/logo.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="谷歌学术搜索"></p>
<h2 id="github"><a href="#github" class="headerlink" title="github"></a>github</h2><p><a href="https://github.com">github</a>于2008年4月10日正式上线，除了代码仓库托管及基本的Web管理界面以外，还提供了订阅、讨论组、文本渲染、在线文件编辑器、协作图谱（报表）、代码片段分享（Gist）等功能。目前，其注册用户已经超过350万，托管版本数量也是非常之多，其中不乏知名开源项目Ruby on Rails、jQuery、python等。</p>
<p>相关页面如下：</p>
<p><img src="https://pic2.zhimg.com/v2-3224a8e2da3f76575baaa77e5768fcb2_1440w.jpg?source=172ae18b" class="lazyload" data-srcset="https://pic2.zhimg.com/v2-3224a8e2da3f76575baaa77e5768fcb2_1440w.jpg?source=172ae18b" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="github"></p>
<h2 id="CVPR"><a href="#CVPR" class="headerlink" title="CVPR"></a>CVPR</h2><p>国际计算机视觉与模式识别会议（<a href="http://www.cvpapers.com/">CVPR</a>）是IEEE一年一度的学术性会议，会议的主要内容是计算机视觉与模式识别技术。CVPR是世界顶级的计算机视觉会议（三大顶会之一，另外两个是ICCV和ECCV，近年来每年有约1500名参加者，收录的论文数量一般300篇左右。本会议每年都会有固定的研讨主题，而每一年都会有公司赞助该会议并获得在会场展示的机会。</p>
<p>相关页面如下：</p>
<p><img src="http://www.cvpapers.com/img/cvpapers.png" class="lazyload" data-srcset="http://www.cvpapers.com/img/cvpapers.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="CVPR"></p>
<h2 id="CVF"><a href="#CVF" class="headerlink" title="CVF"></a>CVF</h2><p><a href="https://openaccess.thecvf.com/">CVF</a>研究论文是由计算机视觉基金会提供的开放获取版本。除水印外，它们与接受的版本相同;最后发表的论文集可以在IEEE Xplore上找到。本材料的提出，以确保及时传播学术和技术工作。版权和其中的所有权利由作者或其他版权持有人保留。所有复制此信息的人都应遵守每个作者的版权所援引的条款和约束。</p>
<h2 id="arXiv"><a href="#arXiv" class="headerlink" title="arXiv"></a>arXiv</h2><p><a href="https://arxiv.org/">arXiv</a>是一个免费分发服务和开放获取的档案，涵盖物理、数学、计算机科学、定量生物学、定量金融学、统计学、电气工程和系统科学以及经济学领域的2,040,232篇学术文章。</p>
<h2 id="paperswithcode"><a href="#paperswithcode" class="headerlink" title="paperswithcode"></a>paperswithcode</h2><p><a href="https://paperswithcode.com/">Papers With Code</a>代码论文的任务是创建一个免费和开放的资源与机器学习论文，代码，数据集，方法和评估表。我们相信，在NLP和ML的支持下，与社区合作是最好的。这个网站上的所有内容都是公开许可的CC-BY-SA(与维基百科一样)，每个人都可以贡献——寻找“编辑”按钮!我们还运营专门的门户网站，提供天文学、物理学、计算机科学、数学和统计学的论文代码。</p>
]]></content>
  </entry>
  <entry>
    <title>学习网站集合</title>
    <url>/CN/%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99%E9%9B%86%E5%90%88/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="万门好课"><a href="#万门好课" class="headerlink" title="万门好课"></a>万门好课</h2><p><a href="https://www.wanmen.org/" title="加油吧少年">万门好课</a>是一家提供多品类原创精品课程的在线教育平台 ，课程覆盖IT与互联网类、职业成长类、经济金融类、本科学习类等领域 。 整体课程定位侧重于“用户刚需”类课程，如语言版块的出国英语考试类课程、小语种培训课程，本科学习版块的各学科基础大课，以及特色的万门通识课程包括PS、化妆等。</p>
<h2 id="网易云课堂"><a href="#网易云课堂" class="headerlink" title="网易云课堂"></a>网易云课堂</h2><p><a href="https://study.163.com/">网易云课堂</a>立足于实用性的要求，网易云课堂与多家教育、培训机构建立合作，课程数量已达4100+，课时总数超50000,涵盖实用软件、IT与互联网、外语学习、生活家居、兴趣爱好、职场技能、金融管理、考试认证、中小学、亲子教育等十余大门类。</p>
<h2 id="网易公开课"><a href="#网易公开课" class="headerlink" title="网易公开课"></a>网易公开课</h2><p><a href="https://open.163.com">网易公开课</a>首批1200集课程上线，其中有200多集配有中文字幕。用户可以在线免费观看来自于哈佛大学等世界级名校的公开课课程，可汗学院，TED等教育性组织的精彩视频，内容涵盖人文、社会、艺术、科学、金融等领域。 力求为爱学习的网友创造一个公开的免费课程平台，借此向外界公开招聘兼职字幕翻译。</p>
<h2 id="爱课程网"><a href="#爱课程网" class="headerlink" title="爱课程网"></a>爱课程网</h2><p><a href="https://www.icourses.cn/home">爱课程网</a>利用现代信息技术和网络技术， 面向高校师生和社会大众。提供优质教育资源共享和个性化教学资源服务，具有资源浏览、搜索、重组、评价、课程包的导入导出、发布、互动参与和“教”“学”兼备等功能。</p>
<h2 id="粉笔网"><a href="#粉笔网" class="headerlink" title="粉笔网"></a>粉笔网</h2><p><a href="https://www.fenbi.com">粉笔网</a>是一个互联网教育平台，业务包含：公务员考试，考研、教师资格、事业单位、英语、建造、财会等技能培训；利用技术手段实现智能批改功能，并提供免费题库，供用户查阅学习，利用网络直播，进行线上授课，同时提供实物图书、试卷以及客户服务。</p>
<h2 id="魔方英语"><a href="#魔方英语" class="headerlink" title="魔方英语"></a><a href="http://www.mofunenglish.com/">魔方英语</a></h2><h2 id="声同小语种论坛"><a href="#声同小语种论坛" class="headerlink" title="声同小语种论坛"></a><a href="http://www.somdom.com/">声同小语种论坛</a></h2>]]></content>
      <categories>
        <category>学习网站</category>
      </categories>
      <tags>
        <tag>学习网站</tag>
      </tags>
  </entry>
  <entry>
    <title>感受野的计算</title>
    <url>/CN/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://www.likecs.com/show-204875434.html">感受野的计算</a></p>
<p>2.<a href="https://www.jiqizhixin.com/graph/technologies/4821b1eb-34c3-4532-9dca-a97411441f23">感受野 | 机器之心 </a></p>
<p>计算机视觉中常常出现感受野的概念，我是在修改SSD网络中的过程中，发现增强感受野是非常有必要的。SSD的主干网络是VGG16,其中有个结论：低层特征图的感受野较小，高层特征图的感受野较大。</p>
<h2 id="感受野定义"><a href="#感受野定义" class="headerlink" title="感受野定义"></a>感受野定义</h2><p>神经网络中每一层输出特征图上的像素点在输入图片上的映射的区域大小，也就是特征图上的每一个点对应的输入图片的区域。</p>
<h2 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h2><p><img src="/CN/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/image-20220427214037594.png" class="lazyload" data-srcset="/CN/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/image-20220427214037594.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220427214037594" style="zoom:50%;"></p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>在VGG16中：pool5中<br>pool5:RF=2<br>conv5_3 ：RF=(2-1)<em>1+3=4<br>conv5_2 : RF=(4-1)</em>1+3=6<br>conv5_2: RF=(6-1)<em>1+3=8<br>conv5_1: RF=(8-1)</em>2+2=16<br>pool4 : RF=(8-1)<em>2+2=16<br>conv4_3: RF=(16-1)</em>1+3=18<br>conv4_2: RF=(18-1)<em>1+3=20<br>conv4_1 : RF=(20-1)</em>1+3=22<br>pool3: RF=(22-1)<em>2+2=44<br>conv3_3: RF=(44-1)</em>1+3=46<br>conv3_2: RF=(46-1)<em>1+3=48<br>conv3_1: RF=(48-1)</em>1+3=50<br>pool2: RF=(50-1)<em>2+2=100<br>conv2_2: RF=(150-1)</em>1+3=152<br>conv2_1: RF=（152-1)<em>1+3=154<br>pool1: RF=(154-1)</em>2+2=208<br>conv1_2: RF=(208-1)<em>1+3=210<br>conv1_1: RF=(210-1)</em>1+3=212<br>计算结果为：pool5输出的特征图在输入图片上的感受野为212*</p>
<p>结果如下图所示：</p>
<p><img src="/CN/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/image-20220427214256808.png" class="lazyload" data-srcset="/CN/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97/image-20220427214256808.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220427214256808"></p>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>感受野</tag>
      </tags>
  </entry>
  <entry>
    <title>扁平时代的写作·节选</title>
    <url>/CN/%E6%89%81%E5%B9%B3%E6%97%B6%E4%BB%A3%E7%9A%84%E5%86%99%E4%BD%9C/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>​        一个「扁平」的世界里众声喧沸。从原则上说，由编辑、审查、批准一类关卡所组成的文化权力体系几近瓦解，每一个IP地址自由发声，都可能成为强大的文化媒体。英才惨遭埋没的可能，伪学与赝品一手遮天的可能，在传统意义上都会减少。全民批评权的运用，也是一种有益的破坏性检验。不过问题的另一面，是胡说比深思容易，粗品比精品多产，优秀者至少没有数量上的优势。一旦优劣平权成了优劣俱放，文化产量中庸质与恶质的占比肯定大大攀升，低端文化产能不仅无法淘汰，还可能日益滚大和坐大。一些优秀作品即使生产出来，也可能在过量的文化淹没中，在受众们暴饮暴食式的阅读之后，在食欲不振的这些快餐者们那里，出现影响力的严重折扣。一旦肠胃已经吃坏了，再多的良药也都无济于事。</p>
<p>​        一个「扁平」的世界里多数为王。在一般的情况下，有些潮流可以修复民众良知，是真理的脱颖而出；有些潮流泯灭民众良知，是泡沫和垃圾的霸道横行。但不管是哪种情况，多数人的理解力构成潮流的边界，那么大众型和通俗化的真理尚有机会，而冷门的、偏僻的、艰险的、高难的——又常常是重要的文化探索，则可能缺氧。进一步说，市场总是嗅觉灵敏地跟踪多数，跟踪购买力的所在，以实现利润最大化。它们必然就低不就高，随众不随寡，视高深、高难、高雅为营销毒药，并有足够的本领使舆论、奖项、教育、权力等资源向低端集中，打造出泡沫霸权和垃圾霸权。一种品质趋下的文化诱导机制，在这种情况下几乎难以避免。</p>
<p>​        一个「扁平」的世界还有易破难立的特点。特别是自18世纪启蒙运动以来，敬畏感随着上帝一同消失。叛逆比服从更流行，权利比责任更动心，无论左右翼都造反成癖，在获得解构主义一番学术装备后更是见立必破打倒一切。这一过程削弱了上帝与王权，清算了教条与伪善，其功绩不可低估；但无政府式的激进狂飚若无解药，其结局必是相对性等同虚无性，民主化等同民粹化，任何共识难以成，真理永远缺位。真理也许还是有的，但在很多时候只剩下每个人那里「我」的真理，即自恋、自闭、自利的各种强辞，甚至是专职扒粪的哄客四起——这不过是社会沦入一片「原子化」散沙的文化表征。圣人、先知、导师一类从此不再，文化成了一地碎片和自由落体。一个个公权政府在这样的逐利时代也更像个总务处，无心也无力充当精神旗帜，无心也无力实施有效的社会调控。避骂自保的公关活动已够他们忙的了，讨好票源和收买民意已够他们累的了，他们哪还有建构民族与人类精神的远大抱负和坚定行动？</p>
<p>​        越来越多的迹象表明，一旦失去文化的约束和引导机制，一个扁平的世界就是没有方向的世界，是无深度和无高度的世界。即使有成打的托翁和莎翁再世，他们通常也形同刺猬而不是狮子，是暗燃而不是火炬——在生态、经济、政治等重大危机逼近之前，在民众的真理渴求大增之前，情况大体如此。</p>
<p>​        这个时代当然还有文化，有文化运动与文化冲突，也不乏轮番登台的文化偶像。不过，与传统意义上的圣人、先知、导师不同，很多现代文化偶像形式大于内容，迎合多于独行，公关造势优于埋头苦干，成功获利重于大道担当。这些人不过是营构一种虚假的方向，在无方向时代满足一种偶像消费，其中既包括对偶像的适时狂拜，也包括对偶像的适时狂毁。在这里，狂拜或狂毁只在一念，不需深思熟虑和身体力行，因此所需偶像不必经久耐用，隔数月或隔几天就更换一个，实为摊档上的寻常。正因为如此，很多偶像不得不焦灼难安，不得不到处奔走，拼命保持公众能见度成了他们的殊死搏斗，也成了他们与以往大师的明显区别之一。一个个豪华大片就这样火了，又冷了；一个个惊世的主义就这样火了，又冷了；一个个让人开心的狂生或浪女就这样火了，又冷了——到后来，很多人参与围观纯粹是为了有权开骂，争相点击只是赢来讥嘲和自秀高明的资格，于是火就是为了冷，或者说火本身就是冷。</p>
<p>​        中国互联网络信息中心2008年的统计报告显示，高达47％左右的公众已经不信任或不太信任网络。美国佩尤研究中心2004年的调查统计显示，媒体公信力一直下滑，比如对CNN信任值已跌至32％，即大多数人持怀疑态度。有意思的是，这一类文化产业不正是公众用高点击率、高收视率、高票房额等热心喂养起来的么？不都是文化市场上的成功典范么？时值二十一世纪，人类有了前所未有的文化自由选择权，但为什么从这时起人类倒变得如此犹疑不定、六神无主、手足无措、茫然无计，竟找不到自己真正信赖和需要的东西？如果人类长期处于这样一种文化消费中的自我分裂和自我对抗，那么这种所好即所疑、所乐即所耻、所爱即所憎的左右两难，是不是一种文化狂欢之下的精神死机状态？</p>
<p>​        也许需要重新启动，重新确定一个方向。</p>
<p>​        一个重建精神价值的方向。</p>
<p>​        这需要很多人的共同努力，重建一种非权力化和非利益化的文化核心、级差以及组织，即文明教化的正常体系。是的，在这里我愿意重新使用「教化」这样一个词，在人类几百年来钟情于「自由」一词以后，在有效教化与宽幅自由互为条件的奇诡历史之中。</p>
]]></content>
      <categories>
        <category>今日故事</category>
      </categories>
  </entry>
  <entry>
    <title>色彩搭配</title>
    <url>/CN/%E6%95%99%E4%BD%A0%E5%AD%A6%E4%BC%9A%E8%89%B2%E5%BD%A9%E6%90%AD%E9%85%8D/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="中国色"><a href="#中国色" class="headerlink" title="中国色"></a>中国色</h3><p><a href="http://zhongguose.com/">中国色</a></p>
<h3 id="COULEUR"><a href="#COULEUR" class="headerlink" title="COULEUR"></a>COULEUR</h3><p><a href="https://www.code-couleur.com/signification/">couleur</a></p>
<h3 id="color-space"><a href="#color-space" class="headerlink" title="color space"></a>color space</h3><p><a href="https://mycolor.space/">color space</a></p>
<h3 id="hyper-color"><a href="#hyper-color" class="headerlink" title="hyper color"></a>hyper color</h3><p><a href="https://hypercolor.dev/">Hypercolor</a></p>
<h3 id="colorable"><a href="#colorable" class="headerlink" title="colorable"></a>colorable</h3><p><a href="https://colorable.jxnblk.com/">Colorable</a></p>
<h3 id="brandcolors"><a href="#brandcolors" class="headerlink" title="brandcolors"></a>brandcolors</h3><p><a href="https://brandcolors.net/">BrandColors</a></p>
<h3 id="九月ppt"><a href="#九月ppt" class="headerlink" title="九月ppt"></a>九月ppt</h3><p><a href="https://jiuyueppt.com/#/">九月PPT</a></p>
<h3 id="huemint"><a href="#huemint" class="headerlink" title="huemint"></a>huemint</h3><p><a href="https://huemint.com/">Huemint - AI color palette generator</a></p>
<h3 id="Adobe-Color"><a href="#Adobe-Color" class="headerlink" title="Adobe Color"></a>Adobe Color</h3><p><a href="https://color.adobe.com/zh/create/color-wheel">色輪、調色盤產生器 | Adobe Color</a></p>
]]></content>
  </entry>
  <entry>
    <title>提醒幸福</title>
    <url>/CN/%E6%8F%90%E9%86%92%E5%B9%B8%E7%A6%8F/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>我们从小就习惯了在提醒中过日子。天气刚有一丝风吹草动，妈妈就说，别忘了多穿衣服。才相识了一个朋友，爸爸就说，小心<br>他是个骗子。你取得了一点成功，还没容得乐出声来，所有关切着你的人一起说，别骄傲！你沉浸在欢快中的时候，自己不停地<br>对自己说：「千万不可太高兴，苦难也许马上就要降临……」我们已经习惯了在提醒中过日子。看得见的恐惧和看不见的恐惧始<br>终像乌鸦盘旋在头顶。</p>
<p>在皓月当空的良宵，提醒会走出来对你说：注意风暴。于是我们忽略了皎洁的月光，急急忙忙做好风暴来临前的一切准备。当我们大睁着眼睛枕戈待旦之时，风暴却像迟归的羊群，不知在哪里徘徊。当我们实在忍受不了等待灾难的煎熬时，我们甚至会恶意<br>地祈盼风暴早些到来。</p>
<p>风暴终于姗姗地来了。我们怅然发现，所做的准备多半是没有用的。事先能够抵御的风险毕竟有限，世上无法预计的灾难却是无<br>限的。战胜灾难靠的更多的是临门一脚，先前的惴惴不安帮不上忙。</p>
<p>当风暴的尾巴终于远去，我们守住零乱的家园。气还没有喘匀，新的提醒又智慧地响起来，我们又开始对未来充满恐惧的期待。</p>
<p>人生总是有灾难。其实大多数人早已练就了对灾难的从容，我们只是还没有学会灾难间隙的快活。我们太多注重了自己警觉苦难<br>，我们太忽视提醒幸福。请从此注意幸福！幸福也需要提醒吗？</p>
<p>提醒注意跌倒……提醒注意路滑……提醒受骗上当……提醒荣辱不惊……先哲们提醒了我们一万零一次，却不提醒我们幸福。</p>
<p>也许他们认为幸福不提醒也跑不了的。也许他们以为好的东西你自会珍惜，犯不上谆谆告诫。也许他们太崇尚血与火，觉得幸福<br>无足挂齿。他们总是站在危崖上，指点我们逃离未来的苦难。但避去苦难之后的时间是什么？</p>
<p>那就是幸福啊！</p>
<p>享受幸福是需要学习的，当幸福即将来临的时刻需要提醒。人可以自然而然地学会感官的享乐，人却无法天生地掌握幸福的韵律。灵魂的快意同器官的舒适像一对孪生兄弟，时而相傍相依，时而南辕北辙。幸福是一种心灵的振颤。它像会倾听音乐的耳朵一样，需要不断地训练。</p>
<p>简言之，幸福就是没有痛苦的时刻。它出现的频率并不像我们想象的那样少。</p>
<p>人们常常只是在幸福的金马车已经驶过去很远，捡起地上的金鬃毛说，原来我见过它。</p>
<p>人们喜爱回味幸福的标本，却忽略幸福披着露水散发清香的时刻。那时候我们往往步履匆匆，瞻前顾后不知在忙着什么。</p>
<p>世上有预报台风的，有预报蝗虫的，有预报瘟疫的，有预报地震的。没有人预报幸福。其实幸福和世界万物一样，有它的征兆。</p>
<p>幸福常常是朦胧的，很有节制地向我们喷洒甘霖。你不要总希冀轰轰烈烈的幸福，它多半只是悄悄地扑面而来。你也不要企图把水龙头拧得更大，使幸福很快地流失。而需静静地以平和之心，体验幸福的真谛。幸福绝大多数是朴素的。它不会像信号弹似的，在很高的天际闪烁红色的光芒。它披着本色外衣，亲切温暖地包裹起我们。</p>
<p>幸福不喜欢喧嚣浮华，常常在暗淡中降临。贫困中相濡以沫的一块糕饼，患难中心心相印的一个眼神，父亲一次粗糙的抚摸，女<br>友一个温馨的字条……这都是千金难买的幸福啊。像一粒粒缀在旧绸子上的红宝石，在凄凉中愈发熠熠夺目。</p>
<p>幸福有时会同我们开一个玩笑，乔装打扮而来。机遇、友情、成功、团圆……</p>
<p>它们都酷似幸福，但它们并不等同于幸福。幸福会借了它们的衣裙，袅袅婷婷而来，走得近了，揭去帏幔，才发觉它有钢铁般的<br>内核。幸福有时会很短暂，不像苦难似的笼罩天空。如果把人生的苦难和幸福分置天平两端，苦难体积庞大，幸福可能只是一块小小的矿石。但指针一定要向幸福这一侧倾斜，因为它有生命的黄金。</p>
<p>幸福有梯形的切面，它可以扩大也可以缩小，就看你是否珍惜。</p>
<p>我们要提高对于幸福的警惕，当它到来的时刻，激情地享受每一分钟。据科学家研究，有意注意的结果比无意要好得多。</p>
<p>当春天来临的时候，我们要对自己说，这是春天啦！心里就会泛起茸茸的绿意。</p>
<p>幸福的时候，我们要对自己说，请记住这一刻！幸福就会长久地伴随我们。那我们岂不是拥有了更多的幸福！</p>
<p>所以，丰收的季节，先不要去想可能的灾年，我们还有漫长的冬季来得及考虑这件事。我们要和朋友们跳舞唱歌，渲染喜悦。既<br>然种子已经回报了汗水，我们就有权沉浸幸福。不要管以后的风霜雨雪，让我们先把麦子磨成面，烘一个香喷喷的面包。</p>
<p>所以，当我们从天涯海角相聚在一起的时候，请不要踌躇片刻后的别离。在今后漫长的岁月里，有无数孤寂的夜晚可以独自品尝<br>愁绪。现在的每一分钟，都让它像纯净的酒精，燃烧成幸福的淡蓝色火焰，不留一丝渣滓。让我们一起举杯，说：我们幸福。</p>
<p>所以，当我们守候在年迈的父母膝下时，哪怕他们鬓发苍苍，哪怕他们垂垂老矣，你都要有勇气对自己说：我很幸福。因为天地<br>无常，总有一天你会失去他们，会无限追悔此刻的时光。</p>
<p>幸福并不与财富地位声望婚姻同步，这只是你心灵的感觉。</p>
<p>所以，当我们一无所有的时候，我们也能够说：我很幸福。因为我们还有健康的身体。当我们不再享有健康的时候，那些最勇敢<br>的人可以依然微笑着说：我很幸福。因为我还有一颗健康的心。甚至当我们连心也不再存在的时候，那些人类最优秀的分子仍旧可以对宇宙大声说：我很幸福。因为我曾经生活过。</p>
<p>常常提醒自己注意幸福，就像在寒冷的日子里经常看看太阳，心就不知不觉暖洋洋亮光光。</p>
]]></content>
      <categories>
        <category>今日故事</category>
      </categories>
  </entry>
  <entry>
    <title>数据处理</title>
    <url>/CN/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="单进程"><a href="#单进程" class="headerlink" title="单进程"></a>单进程</h2><p>​        在单进程模式下，DataLoader 初始化的进程和取数据的进程是一样的 。因此，数据加载可能会阻止计算。但是，当用于在进程之间共享数据的资源（例如共享内存，文件描述符）有限时，或者当整个数据集很小并且可以完全加载到内存中时，此模式可能是我们首选。此外，单进程加载通常可以显示更多可读的错误跟踪，这<strong>对于我们调试代码很有用</strong>。</p>
<h2 id="多进程"><a href="#多进程" class="headerlink" title="多进程"></a>多进程</h2><p>·多进程处理</p>
<p>​        为了避免在加载数据时阻塞计算，PyTorch 提供了一个简单的开关，只需将参数设置 num_workers 为正整数即可执行多进程数据加载，而设置为 0 时执行单线程数据加载。</p>
<p>​        在设置多进程模式时，每次 DataLoader 创建 iterator 时（例如，当调用 enumerate(dataloader) 时），都会创建 num_workers 个工作进程。此时dataset, collate_fn, worker_init_fn 都会被传到每个worker中，而每个 worker 都用独立的进程。</p>
<p>​        对于 map-style 数据，主线程会用 Sampler 产生 indices，并将它们送到 worker 里。因此，shuffle 是在主线程做的。</p>
<p>​        而对于 iterable-style 数据，因为每个 worker 都有相同的 data 复制样本，并在各个进程里进行不同的操作，以防止每个进程输出的数据是重复的，所以一般会使用 torch.utils.data.get_worker_info() 来进行辅助处理。这里，torch.utils.data.get_worker_info() 会返回 worker 进程的一些信息(如id, dataset, num_workers, seed)，如果在主线程的话返回 None。</p>
<p>​        <strong>注意</strong>，通常不建议在多进程加载中返回 CUDA 张量，因为在使用 CUDA 和在多处理中共享 CUDA 张量时存在许多微妙之处（文档中提出：只要接收过程保留张量的副本，就需要发送过程来保留原始张量）。建议采用 pin_memory=True ，以将数据快速传输到支持 CUDA 的 GPU。简而言之，<strong>不建议在使用多线程的情况下返回 CUDA 的 Tensor</strong>。</p>
<h2 id="锁页内存"><a href="#锁页内存" class="headerlink" title="锁页内存"></a>锁页内存</h2><p>​        首先我们先了解一下锁页内存的概念。</p>
<p>​        主机中的内存，有两种存在方式，一是锁页，二是不锁页。锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。主机到 GPU 副本源自固定（页面锁定）内存时，速度要快得多。CPU 张量和存储暴露了一种 pin_memory() 方法，该方法返回对象的副本，并将数据放在固定的区域中。</p>
<p>​        <strong>而显卡中的显存全部是锁页内存！当计算机的内存充足的时候，可以设置 pin_memory=True</strong>。设置 pin_memory=True，则意味着生成的 Tensor 数据最开始是属于内存中的锁页内存，这样将内存的 Tensor 转义到 GPU 的显存就会更快一些。同时，由于 pin_memory 的作用是将张量返回之前将其复制到 CUDA 固定的内存中，所以只有在 CUDA 环境支持下才有用。</p>
<p>​        PyTorch 原生的 pin_memory 方法如下，其支持大部分 python 数据类型的处理：</p>
<pre><code class="lang-python">def pin_memory(data):
    if isinstance(data, torch.Tensor):
        return data.pin_memory()
    elif isinstance(data, string_classes):
        return data
    elif isinstance(data, container_abcs.Mapping):
        return &#123;k: pin_memory(sample) for k, sample in data.items()&#125;
    elif isinstance(data, tuple) and hasattr(data, &#39;_fields&#39;):  # namedtuple
        return type(data)(*(pin_memory(sample) for sample in data))
    elif isinstance(data, container_abcs.Sequence):
        return [pin_memory(sample) for sample in data]
    elif hasattr(data, &quot;pin_memory&quot;):
        return data.pin_memory()
    else:
        return data
</code></pre>
<p>​        默认情况下，如果固定逻辑对于一个属于自定义类型（custom type）的 batch（如果有一个 collate_fn 返回自定义批处理类型的批处理，则会发生），或者如果该批处理的每个元素都是 custom type，则该固定逻辑将无法识别它们，它会返回该批处理（或那些元素）而无需固定内存。而要为自定义批处理或数据类型启用内存固定，我们需使用 pin_memory() 在自定义类型上自定义一个方法。如下：</p>
<pre><code class="lang-python">class SimpleCustomBatch:
    # 自定义一个类，该类不能被PyTorch原生的pin_memory方法所支持

    def __init__(self, data):
        transposed_data = list(zip(*data))
        self.inp = torch.stack(transposed_data[0], 0)
        self.tgt = torch.stack(transposed_data[1], 0)

    # custom memory pinning method on custom type
    def pin_memory(self):
        self.inp = self.inp.pin_memory()
        self.tgt = self.tgt.pin_memory()
        return self

def collate_wrapper(batch):
    return SimpleCustomBatch(batch)

inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)
tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)
dataset = TensorDataset(inps, tgts)

loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,
                    pin_memory=True)

for batch_ndx, sample in enumerate(loader):
    print(sample.inp.is_pinned())  # True
    print(sample.tgt.is_pinned())  # True
</code></pre>
<h2 id="预取"><a href="#预取" class="headerlink" title="预取"></a>预取</h2><p>DataLoader 通过指定 prefetch_factor （默认为 2）来进行数据的预取。</p>
<pre><code class="lang-python">class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
    def __init__(self, loader):
        ...
        self._reset(loader, first_iter=True)

    def _reset(self, loader, first_iter=False):
        ...
        # prime the prefetch loop
        for _ in range(self._prefetch_factor * self._num_workers):
            self._try_put_index()
</code></pre>
<p>通过源码可以看到，prefetch 功能仅适用于多进程加载中（下面也会有多进程 dataloader 的部分代码分析）。</p>
<h2 id="代码详解"><a href="#代码详解" class="headerlink" title="代码详解"></a>代码详解</h2><p>那么现在让我们来看看具体的代码调用流程：</p>
<pre><code class="lang-python">for data, label in train_loader:
    ......
</code></pre>
<p>for 循环会调用 dataloader 的 <strong><strong>iter</strong>(self)</strong> 方法，以此获得迭代器来遍历 dataset。</p>
<pre><code class="lang-python">class DataLoader(Generic[T_co]):
    ...
    def __iter__(self) -&gt; &#39;_BaseDataLoaderIter&#39;:

        if self.persistent_workers and self.num_workers &gt; 0:
            if self._iterator is None:
                self._iterator = self._get_iterator()
            else:
                self._iterator._reset(self)
            return self._iterator
        else:
            return self._get_iterator()
</code></pre>
<p>在 <strong><strong>iter</strong>(self)</strong> 方法中，dataloader 调用了 self._get_iterator() 方法，根据 num_workers 获得迭代器，并指示是进行单进程还是多进程处理。</p>
<pre><code class="lang-python">class DataLoader(Generic[T_co]):
    ...
    def _get_iterator(self) -&gt; &#39;_BaseDataLoaderIter&#39;:
        if self.num_workers == 0:
            return _SingleProcessDataLoaderIter(self)
        else:
            self.check_worker_number_rationality()
            return _MultiProcessingDataLoaderIter(self)
</code></pre>
<p>为了描述更加清晰，我们只考虑单进程的代码。下面是 class _SingleProcessDataLoaderIter(_BaseDataLoaderIter) ，以及其父类 class _BaseDataLoaderIter(object): 的重点代码片段：</p>
<pre><code class="lang-python">
class _BaseDataLoaderIter(object):
    def __init__(self, loader: DataLoader) -&gt; None:
        # 初始化赋值一些 DataLoader 参数，
        # 以及用户输入合法性进行校验
        self._dataset = loader.dataset
        self._dataset_kind = loader._dataset_kind
        self._index_sampler = loader._index_sampler
        ...

    def __iter__(self) -&gt; &#39;_BaseDataLoaderIter&#39;:
        return self

    def _reset(self, loader, first_iter=False):
        self._sampler_iter = iter(self._index_sampler)
        self._num_yielded = 0
        self._IterableDataset_len_called = loader._IterableDataset_len_called

    def _next_index(self):
        return next(self._sampler_iter)  # may raise StopIteration

    def _next_data(self):
        raise NotImplementedError

    def __next__(self) -&gt; Any:
        with torch.autograd.profiler.record_function(self._profile_name):
            if self._sampler_iter is None:
                self._reset()
            data = self._next_data() # 重点代码行，通过此获取数据
            self._num_yielded += 1
            ...
            return data

    next = __next__  # Python 2 compatibility

    def __len__(self) -&gt; int:
        return len(self._index_sampler) # len(_BaseDataLoaderIter) == len(self._index_sampler)

    def __getstate__(self):
        raise NotImplementedError(&quot;&#123;&#125; cannot be pickled&quot;, self.__class__.__name__)
</code></pre>
<p><em>BaseDataLoaderIter 是所有 DataLoaderIter 的父类。dataloader获得了迭代器之后，for 循环需要调用 <strong><strong>next</strong>()</strong> 来获得下一个对象，从而实现遍历。通过 **<em>_next</em></em>()** 方法调用 _next_data() 获取数据。</p>
<pre><code class="lang-python">class _SingleProcessDataLoaderIter(_BaseDataLoaderIter):
    def __init__(self, loader):
        super(_SingleProcessDataLoaderIter, self).__init__(loader)
        assert self._timeout == 0
        assert self._num_workers == 0

        self._dataset_fetcher = _DatasetKind.create_fetcher(
            self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last)

    def _next_data(self):
        index = self._next_index()  # may raise StopIteration
        data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
        if self._pin_memory:
            data = _utils.pin_memory.pin_memory(data)
        return data
</code></pre>
<p>从 _SingleProcessDataLoaderIter 的初始化参数可以看到，其在父类 _BaseDataLoaderIter 的基础上定义了 _dataset_fetcher，并传入 _dataset，_auto_collation，_collate_fn 等参数，用于定义获取数据的方式。其具体实现会在稍后解释。</p>
<p>在 _next_data() 被调用后，其需要 _next_index() 获取 index，并通过获得的 index 传入 _dataset_fetcher 中获取对应样本。</p>
<pre><code class="lang-python">class DataLoader(Generic[T_co]):
    ...
    @property
    def _auto_collation(self):
        return self.batch_sampler is not None

    @property
    def _index_sampler(self):
        if self._auto_collation:
            return self.batch_sampler
        else:
            return self.sampler

class _BaseDataLoaderIter(object):
    ...
    def _reset(self, loader, first_iter=False):
        self._sampler_iter = iter(self._index_sampler)
        ...

    def _next_index(self):
        # sampler_iter 来自于 index_sampler
        return next(self._sampler_iter)  # may raise StopIteration
</code></pre>
<p>从这里看出，dataloader 提供了 sampler（可以是batch_sampler 或者是其他 sampler 子类），然后 _SingleProcessDataLoaderIter 迭代 sampler 获得索引。</p>
<p>下面我们来看看 fetcher，fetcher 需要 index 来获取元素，并同时支持 Map-style dataset（对应 _MapDatasetFetcher）和 Iterable-style dataset（对应 _IterableDatasetFetcher），使其在 Dataloader 内能使用相同的接口 fetch，代码更加简洁。</p>
<p>· 对于 Map-style：直接输入索引 index，作为 map 的 key，获得对应的样本（即 value）。</p>
<pre><code class="lang-python">class _MapDatasetFetcher(_BaseDatasetFetcher):
    def __init__(self, dataset, auto_collation, collate_fn, drop_last):
        super(_MapDatasetFetcher, self).__init__(dataset, auto_collation, collate_fn, drop_last)

    def fetch(self, possibly_batched_index):
        if self.auto_collation:
            # 有batch_sampler，_auto_collation就为True，
            # 就优先使用batch_sampler，对应在fetcher中传入的就是一个batch的索引
            data = [self.dataset[idx] for idx in possibly_batched_index]
        else:
            data = self.dataset[possibly_batched_index]
        return self.collate_fn(data)
</code></pre>
<p>· 对于 Iterable-style: <strong><strong>init</strong></strong> 方法内设置了 dataset 初始的迭代器，fetch 方法内获取元素，此时 index 其实已经没有多大作用了。</p>
<pre><code class="lang-python">class _IterableDatasetFetcher(_BaseDatasetFetcher):
    def __init__(self, dataset, auto_collation, collate_fn, drop_last):
        super(_IterableDatasetFetcher, self).__init__(dataset, auto_collation, collate_fn, drop_last)
        self.dataset_iter = iter(dataset)

    def fetch(self, possibly_batched_index):
        if self.auto_collation:
            # 对于batch_sampler（即auto_collation==True）
            # 直接使用往后遍历并提取len(possibly_batched_index)个样本（即1个batch的样本）
            data = []
            for _ in possibly_batched_index:
                try:
                    data.append(next(self.dataset_iter))
                except StopIteration:
                    break
            if len(data) == 0 or (self.drop_last and len(data) &lt; len(possibly_batched_index)):
                raise StopIteration
        else:
            # 对于sampler，直接往后遍历并提取1个样本
            data = next(self.dataset_iter)
        return self.collate_fn(data)
</code></pre>
<p>最后，我们通过索引传入 fetcher，fetch 得到想要的样本。因此，整个过程调用关系总结如下：</p>
<pre><code>loader.iter --&gt; self._get_iterator() --&gt; class _SingleProcessDataLoaderIter --&gt; class _BaseDataLoaderIter --&gt; __next__() --&gt; self._next_data() --&gt; self._next_index() --&gt;next(self._sampler_iter) 即 next(iter(self._index_sampler)) --&gt; 获得 index --&gt; self._dataset_fetcher.fetch(index) --&gt; 获得 data
</code></pre><p>而对于多进程而言，借用 PyTorch 内源码的注释，其运行流程解释如下：</p>
<pre><code>
# Our data model looks like this (queues are indicated with curly brackets):
#
#                main process                              ||
#                     |                                    ||
#               &#123;index_queue&#125;                              ||
#                     |                                    ||
#              worker processes                            ||     DATA
#                     |                                    ||
#            &#123;worker_result_queue&#125;                         ||     FLOW
#                     |                                    ||
#      pin_memory_thread of main process                   ||   DIRECTION
#                     |                                    ||
#               &#123;data_queue&#125;                               ||
#                     |                                    ||
#                data output                               \/
#
# P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if
#      `pin_memory=False`.
</code></pre><p>首先 dataloader 基于 multiprocessing 产生多进程，每个子进程的输入输出通过两个主要的队列（multiprocessing.Queue() 类）产生，分别为：</p>
<p>· index_queue：每个子进程的队列中需要处理的任务的下标</p>
<p>· _worker_result_queue：返回时处理完任务的下标</p>
<p>· data_queue：表明经过 pin_memory 处理后的数据队列</p>
<p>并且有以下这些比较重要的 flag 参数来协调各个 worker 之间的工作：</p>
<p>· _send_idx: 发送索引，用来记录这次要放 index_queue 中 batch 的 idx</p>
<p>· _rcvd_idx: 接受索引，记录要从 data_queue 中取出的 batch 的 idx</p>
<p>· _task_info: 存储将要产生的 data 信息的 dict，key为 task idx（由 0 开始的整形索引），value 为 (worker_id,) 或 (worker_id, data)，分别对应数据未取和已取的情况</p>
<p>· _tasks_outstanding: 整形，代表已经准备好的 task/batch 的数量（可能有些正在准备中）</p>
<p>每个 worker 一次产生一个 batch 的数据，返回 batch 数据前放入下一个批次要处理的数据下标，对应构造函数子进程初始化如下：</p>
<pre><code class="lang-python">
class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
    def __init__(self, loader):
        super(_MultiProcessingDataLoaderIter, self).__init__(loader)
        ...
        self._worker_result_queue = multiprocessing_context.Queue()  # 把该worker取出的数放入该队列，用于进程间通信
        ...
        self._workers_done_event = multiprocessing_context.Event()
        self._index_queues = []
        self._workers = []
        for i in range(self._num_workers):
            index_queue = multiprocessing_context.Queue()  # 索引队列，每个子进程一个队列放要处理的下标
            index_queue.cancel_join_thread()
            # _worker_loop 的作用是：从index_queue中取索引，然后通过collate_fn处理数据，
            # 然后再将处理好的 batch 数据放到 data_queue 中。（发送到队列中的idx是self.send_idx）
            w = multiprocessing_context.Process(
                target=_utils.worker._worker_loop,  # 每个worker子进程循环执行的函数，主要将数据以(idx, data)的方式传入_worker_result_queue中
                args=(self._dataset_kind, self._dataset, index_queue, 
                      self._worker_result_queue, self._workers_done_event,
                      self._auto_collation, self._collate_fn, self._drop_last,
                      self._base_seed + i, self._worker_init_fn, i, self._num_workers,
                      self._persistent_workers))
            w.daemon = True
            w.start()
            self._index_queues.append(index_queue)
            self._workers.append(w)
        if self._pin_memory:
            self._pin_memory_thread_done_event = threading.Event()
            self._data_queue = queue.Queue()  # 用于存取出的数据进行 pin_memory 操作后的结果
            pin_memory_thread = threading.Thread(
                target=_utils.pin_memory._pin_memory_loop,
                args=(self._worker_result_queue, self._data_queue,
                      torch.cuda.current_device(),
                      self._pin_memory_thread_done_event))
            pin_memory_thread.daemon = True
            pin_memory_thread.start()
            # Similar to workers (see comment above), we only register
            # pin_memory_thread once it is started.
            self._pin_memory_thread = pin_memory_thread
        else:
            self._data_queue = self._worker_result_queue
        ...
        self._reset(loader, first_iter=True)
    def _reset(self, loader, first_iter=False):
        super()._reset(loader, first_iter)
        self._send_idx = 0  # idx of the next task to be sent to workers，发送索引，用来记录这次要放 index_queue 中 batch 的 idx
        self._rcvd_idx = 0  # idx of the next task to be returned in __next__，接受索引，记录要从 data_queue 中取出的 batch 的 idx
        # information about data not yet yielded, i.e., tasks w/ indices in range [rcvd_idx, send_idx).
        # map: task idx =&gt; - (worker_id,)        if data isn&#39;t fetched (outstanding)
        #                  \ (worker_id, data)   if data is already fetched (out-of-order)
        self._task_info = &#123;&#125;
        # _tasks_outstanding 指示当前已经准备好的 task/batch 的数量（可能有些正在准备中）
        # 初始值为 0, 在 self._try_put_index() 中 +1,在 self._next_data 中-1
        self._tasks_outstanding = 0  # always equal to count(v for v in task_info.values() if len(v) == 1)
        # this indicates status that a worker still has work to do *for this epoch*.
        self._workers_status = [True for i in range(self._num_workers)] 
        # We resume the prefetching in case it was enabled
        if not first_iter:
            for idx in range(self._num_workers):
                self._index_queues[idx].put(_utils.worker._ResumeIteration())
            resume_iteration_cnt = self._num_workers
            while resume_iteration_cnt &gt; 0:
                data = self._get_data()
                if isinstance(data, _utils.worker._ResumeIteration):
                    resume_iteration_cnt -= 1
        ...
        # 初始化的时候，就将 2*num_workers 个 (batch_idx, sampler_indices) 放到 index_queue 中
        for _ in range(self._prefetch_factor * self._num_workers):
            self._try_put_index() # 进行预取
</code></pre>
<p>dataloader 初始化的时候，每个 worker 的 index_queue 默认会放入<strong>两个</strong> batch 的 index，从 index_queue 中取出要处理的下标。</p>
<pre><code class="lang-python">def _try_put_index(self):
        # self._prefetch_factor 默认为 2
        assert self._tasks_outstanding &lt; self._prefetch_factor * self._num_workers
        try:
            index = self._next_index()
        except StopIteration:
            return
        for _ in range(self._num_workers):  # find the next active worker, if any
            worker_queue_idx = next(self._worker_queue_idx_cycle)
            if self._workers_status[worker_queue_idx]:
                break
        else:
            # not found (i.e., didn&#39;t break)
            return
        self._index_queues[worker_queue_idx].put((self._send_idx, index)) # 放入 任务下标 和 数据下标
        self._task_info[self._send_idx] = (worker_queue_idx,)
        # _tasks_outstanding + 1，表明预备好的batch个数+1
        self._tasks_outstanding += 1
        # send_idx 发送索引, 记录从sample_iter中发送索引到index_queue的次数
        self._send_idx += 1
</code></pre>
<p>调用 _next_data(self) 方法进行数据读取，其中 _process_data(self, data) 用于返回数据。</p>
<pre><code class="lang-python">
def _next_data(self):
        while True:

            while self._rcvd_idx &lt; self._send_idx: # 确保待处理的任务(待取的batch)下标 &gt; 处理完毕要返回的任务(已经取完的batch)下标
                info = self._task_info[self._rcvd_idx]
                worker_id = info[0]
                if len(info) == 2 or self._workers_status[worker_id]:  # has data or is still active
                    break
                del self._task_info[self._rcvd_idx]
                self._rcvd_idx += 1
            else:
                # no valid `self._rcvd_idx` is found (i.e., didn&#39;t break)
                if not self._persistent_workers:
                    self._shutdown_workers()
                raise StopIteration

            # Now `self._rcvd_idx` is the batch index we want to fetch

            # Check if the next sample has already been generated
            if len(self._task_info[self._rcvd_idx]) == 2:
                data = self._task_info.pop(self._rcvd_idx)[1]
                return self._process_data(data)

            assert not self._shutdown and self._tasks_outstanding &gt; 0
            idx, data = self._get_data() # 调用 self._try_get_data() 从 self._data_queue 中取数
            self._tasks_outstanding -= 1  # 表明预备好的batch个数需要减1
            if self._dataset_kind == _DatasetKind.Iterable:
                # Check for _IterableDatasetStopIteration
                if isinstance(data, _utils.worker._IterableDatasetStopIteration):
                    if self._persistent_workers:
                        self._workers_status[data.worker_id] = False
                    else:
                        self._mark_worker_as_unavailable(data.worker_id)
                    self._try_put_index()
                    continue

            if idx != self._rcvd_idx:
                # store out-of-order samples
                self._task_info[idx] += (data,)
            else:
                del self._task_info[idx]
                return self._process_data(data) # 返回数据

    def _process_data(self, data):
        self._rcvd_idx += 1
        self._try_put_index() # 同上，主要放入队列索引 以及 更新flag
        if isinstance(data, ExceptionWrapper):
            data.reraise()
        return data
</code></pre>
<p>这样，多进程模式的 dataloader 就能通过多个 worker 的协作来共同完成数据的加载。</p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
      </tags>
  </entry>
  <entry>
    <title>数据集介绍</title>
    <url>/CN/%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="参考博文"><a href="#参考博文" class="headerlink" title="参考博文"></a>参考博文</h3><p>1.<a href="https://blog.csdn.net/qq_45616304/article/details/117912347">针对 VOC2007和VOC2012 的具体用法</a></p>
<p>2.<a href="https://blog.csdn.net/a_piece_of_ppx/article/details/118701340">Pascal Voc（07+12）联合训练并在07上测试</a></p>
<h3 id="VOC2007和VOC2012用法"><a href="#VOC2007和VOC2012用法" class="headerlink" title="VOC2007和VOC2012用法"></a>VOC2007和VOC2012用法</h3><p>目前广大研究者们普遍使用的是 VOC2007和VOC2012数据集，因为二者是互斥的，不相容的。</p>
<p>论文中针对 VOC2007和VOC2012 的具体用法有以下几种：</p>
<p>1.只用VOC2007的trainval 训练，使用VOC2007的test测试。<br>2.只用VOC2012的trainval 训练，使用VOC2012的test测试，这种用法很少使用，因为大家都会结合VOC2007使用。<br>3.（推荐）使用 VOC2007 的 train+val 和 VOC2012的 train+val 训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12 ，研究者可以自己测试在VOC2007上的结果，因为VOC2007的test是公开的。<br>4.使用 VOC2007 的 train+val+test 和 VOC2012的 train+val训练，然后使用 VOC2012的test测试，这个用法是论文中经常看到的 07++12 ，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。<br>5.先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val、 VOC2012的 train+val 微调训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12+COCO 。<br>6.先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val+test 、 VOC2012的 train+val 微调训练，然后使用 VOC2012的test测试 ，这个用法是论文中经常看到的 07++12+COCO，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。</p>
<h3 id="VOC07-12联合训练并在07上测试"><a href="#VOC07-12联合训练并在07上测试" class="headerlink" title="VOC07+12联合训练并在07上测试"></a>VOC07+12联合训练并在07上测试</h3><h4 id="数据分布"><a href="#数据分布" class="headerlink" title="数据分布"></a>数据分布</h4><p>对于分类/检测任务而言，完成07 + 12数据集合并后，共得到如下数据：</p>
<pre><code># 5011+11540=16551, 12608+27450=40058
训练数据：16551张图像，共40058个目标
# 全部来自voc07_test
测试数据：4952张图像，共12032个目标
</code></pre><p>组成如下所示：</p>
<p>训练数据：</p>
<p>1.VOC2007的训练集提供了：</p>
<pre><code>训练数据：2501张图像，共6301个目标
验证数据：2510张图像，共6307个目标
训练+验证数据：5011张图像，共12608个目标
</code></pre><p>2.VOC2012的训练集提供了：</p>
<pre><code>训练数据：5717张图像，共13609个目标
验证数据：5823张图像，共13841个目标
训练+验证数据：11540张图像，共27450个目标
</code></pre><p>测试数据：</p>
<p>1.VOC2007的测试集提供了：</p>
<pre><code>测试数据：4952张图像，共12032个目标
</code></pre><h3 id="各种数据集介绍"><a href="#各种数据集介绍" class="headerlink" title="各种数据集介绍"></a>各种数据集介绍</h3><h4 id="Pascal-VOC"><a href="#Pascal-VOC" class="headerlink" title="Pascal VOC"></a>Pascal VOC</h4><p>官网地址：<a href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/">Pascal VOC Dataset Mirror (pjreddie.com)</a></p>
<h4 id="MS-COCO"><a href="#MS-COCO" class="headerlink" title="MS COCO"></a>MS COCO</h4><h4 id="ILSVRC"><a href="#ILSVRC" class="headerlink" title="ILSVRC"></a>ILSVRC</h4>]]></content>
      <categories>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>CV必备文献</title>
    <url>/CN/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E5%90%88%E9%9B%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>1.RFBNet</p>
<p>论文标题：Receptive Field Block Net for Accurate and Fast Object Detection</p>
<p>论文地址：<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper.pdf">Receptive Field Block Net </a></p>
<p>源码地址：<a href="https://github.com/ruinmessi/RFBNet">https://github.com/ruinmessi/RFBNet</a></p>
<p>2.Fast R-CNN</p>
<p>3.Faster R-CNN</p>
<p>4.ResNet</p>
<p>5.Inception</p>
<p>6.Mask R-CNN</p>
<p>7.YOLOv1</p>
<p>8.SSD</p>
<p>9.DSSD</p>
<p>10.ASDD</p>
<p>11.FSSD</p>
<p>12.FASSD</p>
<p>论文标题：FASSD: A Feature Fusion and Spatial Attention-Based Single Shot Detector for Small Object Detection</p>
<p>论文地址：<a href="https://www.mdpi.com/2079-9292/9/9/1536">https://www.mdpi.com/2079-9292/9/9/1536</a></p>
<p>源码地址：</p>
<p>13.AlexNet</p>
<p>14.SIFT</p>
<p>15.HOG</p>
<p>16.FPN</p>
<p>论文标题：Feature Pyramid Networks for Object Detection</p>
<p>论文地址：<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf">Feature Pyramid Networks for Object Detection (thecvf.com)</a></p>
<p>源码地址：<a href="https://github.com/unsky/FPN">FPN: Feature Pyramid Networks for Object Detection </a></p>
<p>17.RefineDet</p>
<p>18.M2Det</p>
<p>19.RSSD</p>
<p>论文标题：Enhancement of SSD by concatenating feature maps for object detection</p>
<p>论文地址：</p>
<p>源码地址：</p>
<p>20.MDSSD</p>
<p>论文标题：Mdssd: Multi-scale deconvolutional single shot detector for small objects</p>
<p>论文地址：</p>
<p>论文源码：</p>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>未婚妻</title>
    <url>/CN/%E6%9C%AA%E5%A9%9A%E5%A6%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>度过了几天假期之后，我要回巴黎了。</p>
<p>当我走进车站，火车已挤满了旅客。大多数的车门前，都站着一个男人或一个妇女，好像是在拦阻后来的旅客。</p>
<p>尽管如此，我还是踮起脚尖向每一个车厢内部观看，希望能找到一个座位。我发现靠近车门坐着的旅客旁边，有一个空座位，但<br>上面放着两个大篮子，里面的鸡子和鸭子把头伸在篮子外面。</p>
<p>我犹豫了好一会之后，决定走进车厢。我说很对不起了，让我来把篮子移开。可是一位穿着工作服的农民对我说：「小姐，请等一等，我就来把它们从这里拿开。」</p>
<p>当我把放在农民膝上的水果篮子提在手中时，他轻轻地把两篮家禽塞在凳子下面。</p>
<p>我们都听到鸭子叫喊，表示不高兴。母鸡却低下头，像是受委屈的样子。农民的妻子一面喊着鸭子鸡子的名字，一面对它们说着<br>话。</p>
<p>我坐下以后，鸭子也安静下来了。这时，坐在我对面的旅客问农民是否他把家禽带到市场上去卖。</p>
<p>农民回答说：「先生，不是送上市场的。后天，我的儿子就要结婚，我把鸡鸭带来送给儿子。」</p>
<p>他脸上显出幸福愉快的神情。他看了看周围的人，仿佛要向所有的人们都表达他自己的快乐。另外的旅客都留心倾听，他们似乎<br>听了之后感到很高兴。只有一个老媪是例外，她占了两人的座位，枕着三个枕头，正在叱骂拥塞在车厢中的农民。</p>
<p>火车开动了。刚才说话的旅客开始阅读报纸，这时农民对他说：「我的儿子在巴黎，他是一家商店的职员，将和一位小姐结婚，<br>她也是一家商店的职员。」</p>
<p>这个旅客把已经打开的报纸放在他膝上，同时移动身子坐在凳的边沿。他问道：「未婚妻美丽吗？」</p>
<p>农民说：「我不知道，我还没有见过她。」</p>
<p>这个旅客有些惊讶，又说：「真的吗？假如她长得丑，使你不喜欢，将怎样办？」</p>
<p>农民回答：「这种事情可能发生，但我相信，她会使我们喜欢，因为我们的儿子很爱我们，他不会娶一个难看的妻子。」</p>
<p>农民的妻子又补上一句：「再说，既然她使我们的儿子腓利普喜欢，她也会使我们喜欢的。」</p>
<p>农民的妻子转过身来向着我，我看到她一双柔和的眼睛肿充满着微笑。她的面容娇小玲珑，非常可爱，我不能相信她就是一个已<br>达结婚年龄的儿子的母亲。</p>
<p>她想知道我是否也去巴黎，当我回答说也是去巴黎时，这个旅客就开玩笑了。他说：「我打赌：这位小姐就是未婚妻，她是来迎<br>接她的公公婆婆而没有介绍自己使他们认识。」</p>
<p>所有的眼睛都向我注视，我羞得面红耳赤，这时农民夫妇同声说：「嗳！真是这样的话，我们将非常高兴。」</p>
<p>我向他们说明这完全是误会。可是这个旅客提醒他们，说我曾沿着火车走过两次，好像我是尽力找认什么人；又说我在登上车厢<br>前是多么犹豫迟疑。</p>
<p>所有的人都笑起来，我在困窘中解释说，这个座位是我能够找到的惟一的座位。</p>
<p>农民的妻子说：「这没有什么关系，你非常使我喜欢。假使我们的媳妇能像你那样，我将多么高兴。」</p>
<p>农民接着说：「是啊，我们的媳妇最好能像你。」</p>
<p>这个旅客对于他自己的这番笑话感到很得意，他带着开玩笑的样子看了我一眼后，对农民夫妇说：「你们相信我没有弄错。当你们到达巴黎时，你们的儿子会对你们说：『这位就是我的未婚妻。』」</p>
<p>他说完后，放声大笑一阵，便往凳子里边一坐，开始专心读他的报纸了。</p>
<p>一会儿以后，农民的妻子完全转身向着我；她在她带来的篮子底层找寻一会儿，便拿出一块煎饼。她一面把煎饼请我吃，一面对<br>我说，这煎饼是她今天早晨亲手做的。</p>
<p>我不知怎样辞谢才好，只得采用夸大的方法，把伤风说成发烧，她才把这块煎饼放在篮子的底层。</p>
<p>接着，她又请我吃一串葡萄，我不得不接受了。</p>
<p>当火车在一个站停下时，我很难阻止农民下车为我购买一杯热的饮料。</p>
<p>我看到这一对好心人一心只想爱他们儿子选中的未婚妻时，自己因不是他们的媳妇而感到遗憾。他们的爱情使自己觉得多么温暖<br>。我是孤女，从未见过父母的慈容；而和我一起生活的人，谁都对我漠不关心。</p>
<p>我惊异地看到他们的眼光时时注视在我身上，好像他们是在爱抚我那样。</p>
<p>到达巴黎时，我帮助他们把篮子从车上搬下来，并领他们向出口处走去。</p>
<p>当我看到一个身材高高的青年向他们扑过来，用双臂抱着他们时，我就稍稍离开他们远一些。他热情地吻着他父亲，又吻着他母<br>亲。</p>
<p>父母只管笑眯眯地接受儿子的亲吻，连服务员推着的行李车快要撞他们时所发出的警铃声，他们都听不到；急于赶路的旅客把臂肘撞在他们身上，他们好像也没有感觉到。</p>
<p>他们在前面走着，我在后面跟着。儿子的一只手臂挽着鸭篮子，另一只手臂抱着他妈妈的肩膀，他微微弯着身躯靠向妈妈，笑嘻<br>嘻地倾听母亲说话。</p>
<p>他像他父亲，一双眼睛鲜明快乐，笑声爽快而响亮。</p>
<p>外面，天几乎全黑了。我撑起大衣的领子。我落在他们后面，稍离开他们几步路。这时，他们的儿子去雇一辆车子。</p>
<p>农民爱抚着一只染有各种颜色、很美丽的花母鸡的头时，对他妻<br>子说：「假如我早知道她不是我们媳妇，那么我早就把这只花母鸡送给她了。」</p>
<p>「是啊！假如我早知道……」</p>
<p>农民的妻子向着已经走出车站的长长人群做手势，眼睛望着远处说：「她已随着人群走了。」</p>
<p>正在这时，他们的儿子已雇到一辆车子回来了。他尽可能好地把他的父母安顿在车上，他自己却在赶车人的旁边坐下，而且侧转身子，以免遮了他父母的视线。</p>
<p>他看起来长得身强力壮，性情温和，我想他的未婚妻一定是很幸福的。</p>
<p>他们的车子消失在黑暗中了，于是我沿着每一条街道慢吞吞地走去。孤零零的我不由自主地回到了自己的房间。</p>
<p>我已二十岁了，还没有一个人来向我谈过爱情。</p>
]]></content>
      <categories>
        <category>今日故事</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习</title>
    <url>/CN/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="常见概念"><a href="#常见概念" class="headerlink" title="常见概念"></a>常见概念</h3><h4 id="TP、TN、FP和FN概念"><a href="#TP、TN、FP和FN概念" class="headerlink" title="TP、TN、FP和FN概念"></a>TP、TN、FP和FN概念</h4><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>P(分类器认为是正样本)</th>
<th>N(分类器认为是负样本)</th>
</tr>
</thead>
<tbody>
<tr>
<td>T(正确分类)</td>
<td>TP</td>
<td>TN</td>
</tr>
<tr>
<td>F(错误分类)</td>
<td>FP</td>
<td>FN</td>
</tr>
</tbody>
</table>
</div>
<p>注：</p>
<p>TP（True Positives）意思就是被分为了正样本，而且分对了。（样本是正样本）<br>TN（True Negatives）意思就是被分为了负样本，而且分对了。(样本是负样本)<br>FP（False Positives）意思就是被分为了正样本，但是分错了。（样本是负样本）<br>FN（False Negatives）意思就是被分为了负样本，但是分错了。（样本是正样本）</p>
<h4 id="正确率"><a href="#正确率" class="headerlink" title="正确率"></a>正确率</h4><p>正确率是我们最常见的评价指标，通常来说，正确率越高，分类器越好。TP是分类器认为是正样本而且确实是正样本的样本数，FP是分类器认为是正样本但实际上不是正样本的样本数，Precision就是“分类器认为是正类并且确实是正类的部分占所有分类器认为是正类的比例”。</p>
<p><img src="/CN/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20220425220526167.png" class="lazyload" data-srcset="/CN/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20220425220526167.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220425220526167" style="zoom:50%;"></p>
<h4 id="召回率"><a href="#召回率" class="headerlink" title="召回率"></a>召回率</h4><p>TP是分类器认为是正样本而且确实是正样本的样本数，FN是分类器认为是负样本但实际上不是负样本的样本数，Recall就是“分类器认为是正类并且确实是正类的部分占所有确实是正类的比例”。</p>
<p><img src="/CN/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20220425221025699.png" class="lazyload" data-srcset="/CN/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/image-20220425221025699.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220425221025699" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>详解注意力机制</title>
    <url>/CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p>注意力机制就是让网络关注到它更需要关注的地方，是一种网络自适应注意的方式。注意力机制可以分为通道注意力，空间注意力以及二者的结合。</p>
<h3 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h3><h4 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">SENet</a></h4><p>2017年提出的SENet是最后一届ImageNet竞赛的冠军，其实现示意图如下所示，对于输入进来的特征层，我们关注其每一个通道的权重，对于SENet而言，其重点是获得输入进来的特征层，每一个通道的权值。利用SENet，我们可以让网络关注它最需要关注的通道。</p>
<p>其具体实现方式就是：<br>1、对输入进来的特征层进行全局平均池化。<br>2、然后进行两次全连接，第一次全连接神经元个数较少，第二次全连接神经元个数和输入特征层相同。<br>3、在完成两次全连接后，我们再取一次Sigmoid将值固定到0-1之间，此时我们获得了输入特征层每一个通道的权值（0-1之间）。<br>4、在获得这个权值后，我们将这个权值乘上原输入特征层即可。<br><img src="/CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331191026838.png" class="lazyload" data-srcset="/CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331191026838.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<pre><code class="lang-python">import torch
import torch.nn as nn
import math

class se_block(nn.Module):
    def __init__(self, channel, ratio=16):
        super(se_block, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
                nn.Linear(channel, channel // ratio, bias=False),
                nn.ReLU(inplace=True),
                nn.Linear(channel // ratio, channel, bias=False),
                nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y
</code></pre>
<h4 id="CBAM"><a href="#CBAM" class="headerlink" title="CBAM"></a><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf">CBAM</a></h4><p>CBAM将<strong>通道注意力机制和空间注意力机制</strong>进行一个结合，相比于<strong>SENet只关注通道的注意力机制</strong>可以取得更好的效果。其实现示意图如下所示，CBAM会对输入进来的特征层，分别进行<strong>通道注意力机制的处理和空间注意力机制的处理</strong>。</p>
<p><img src="/CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331155344988.png" class="lazyload" data-srcset="/CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331155344988.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>下图是通道注意力机制和空间注意力机制的具体实现方式：<br>图像的上半部分为通道注意力机制，通道注意力机制的实现可以分为两个部分，我们会对输入进来的单个特征层，分别进行全局平均池化和全局最大池化。之后对平均池化和最大池化的结果，利用共享的全连接层进行处理，我们会对处理后的两个结果进行相加，然后取一个sigmoid，此时我们获得了输入特征层每一个通道的权值（0-1之间）。在获得这个权值后，我们将这个权值乘上原输入特征层即可。</p>
<p>图像的下半部分为空间注意力机制，我们会对输入进来的特征层，在每一个特征点的通道上取最大值和平均值。之后将这两个结果进行一个堆叠，利用一次通道数为1的卷积调整通道数，然后取一个sigmoid，此时我们获得了输入特征层每一个特征点的权值（0-1之间）。在获得这个权值后，我们将这个权值乘上原输入特征层即可。</p>
<p><img src="/CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331155451951.png" class="lazyload" data-srcset="/CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331155451951.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<pre><code class="lang-python">class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=8):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        # 利用1x1卷积代替全连接
        self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), &#39;kernel size must be 3 or 7&#39;
        padding = 3 if kernel_size == 7 else 1
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)

class cbam_block(nn.Module):
    def __init__(self, channel, ratio=8, kernel_size=7):
        super(cbam_block, self).__init__()
        self.channelattention = ChannelAttention(channel, ratio=ratio)
        self.spatialattention = SpatialAttention(kernel_size=kernel_size)

    def forward(self, x):
        x = x * self.channelattention(x)
        x = x * self.spatialattention(x)
        return x
</code></pre>
<h4 id="ECANet"><a href="#ECANet" class="headerlink" title="ECANet"></a><a href="https://sci-hub.mksa.top/10.1109/cvpr42600.2020.01155">ECANet</a></h4><p>ECANet是也是通道注意力机制的一种实现形式。ECANet可以看作是SENet的改进版。<br>ECANet的作者认为SENet对通道注意力机制的预测带来了副作用，捕获所有通道的依赖关系是低效并且是不必要的。<br>在ECANet的论文中，作者认为卷积具有良好的跨通道信息获取能力。</p>
<p>ECA模块的思想是非常简单的，它去除了原来SE模块中的全连接层，直接在全局平均池化之后的特征上通过一个1D卷积进行学习。</p>
<p>既然使用到了1D卷积，那么1D卷积的卷积核大小的选择就变得非常重要了，了解过卷积原理的同学很快就可以明白，1D卷积的卷积核大小会影响注意力机制每个权重的计算要考虑的通道数量。用更专业的名词就是跨通道交互的覆盖率。</p>
<p>如下图所示，左图是常规的SE模块，右图是ECA模块。ECA模块用1D卷积替换两次全连接。<br><img src="/CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331160018390.png" class="lazyload" data-srcset="/CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331160018390.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<pre><code class="lang-python">class eca_block(nn.Module):
    def __init__(self, channel, b=1, gamma=2):
        super(eca_block, self).__init__()
        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))
        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False) 
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        y = self.avg_pool(x)
        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)
        y = self.sigmoid(y)
        return x * y.expand_as(x)
</code></pre>
<h4 id="CA"><a href="#CA" class="headerlink" title="CA"></a><a href="https://arxiv.org/pdf/2103.02907.pdf">CA</a></h4><p>Mobile Network设计的最新研究成果表明，通道注意力（例如，SE注意力）对于提升模型性能具有显著效果，但它们通常会忽略位置信息，而位置信息对于生成空间选择性attention maps是非常重要。</p>
<p>coordinate注意力将通道注意力分解为两个1维特征编码过程，分别沿2个空间方向聚合特征。这样，可以沿一个空间方向捕获远程依赖关系，同时可以沿另一空间方向保留精确的位置信息。然后将生成的特征图分别编码为一对方向感知和位置敏感的attention map，可以将其互补地应用于输入特征图，以增强关注对象的表示。</p>
<p>如下图所示，Coordinate Attention通过精确的位置信息对通道关系和长期依赖性进行编码，具体操作分为Coordinate信息嵌入和Coordinate Attention生成2个步骤。</p>
<p><img src="/CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220427215849678.png" class="lazyload" data-srcset="/CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220427215849678.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220427215849678"></p>
<p>实现代码：</p>
<pre><code class="lang-python">import torch
from torch import nn


class CA_Block(nn.Module):
    def __init__(self, channel, h, w, reduction=16):
        super(CA_Block, self).__init__()

        self.h = h
        self.w = w

        self.avg_pool_x = nn.AdaptiveAvgPool2d((h, 1))
        self.avg_pool_y = nn.AdaptiveAvgPool2d((1, w))

        self.conv_1x1 = nn.Conv2d(in_channels=channel, out_channels=channel//reduction, kernel_size=1, stride=1, bias=False)

        self.relu = nn.ReLU()
        self.bn = nn.BatchNorm2d(channel//reduction)

        self.F_h = nn.Conv2d(in_channels=channel//reduction, out_channels=channel, kernel_size=1, stride=1, bias=False)
        self.F_w = nn.Conv2d(in_channels=channel//reduction, out_channels=channel, kernel_size=1, stride=1, bias=False)

        self.sigmoid_h = nn.Sigmoid()
        self.sigmoid_w = nn.Sigmoid()

    def forward(self, x):

        x_h = self.avg_pool_x(x).permute(0, 1, 3, 2)
        x_w = self.avg_pool_y(x)

        x_cat_conv_relu = self.relu(self.conv_1x1(torch.cat((x_h, x_w), 3)))

        x_cat_conv_split_h, x_cat_conv_split_w = x_cat_conv_relu.split([self.h, self.w], 3)

        s_h = self.sigmoid_h(self.F_h(x_cat_conv_split_h.permute(0, 1, 3, 2)))
        s_w = self.sigmoid_w(self.F_w(x_cat_conv_split_w))

        out = x * s_h.expand_as(x) * s_w.expand_as(x)

        return out


if __name__ == &#39;__main__&#39;:
    x = torch.randn(1, 16, 128, 64)    # b, c, h, w
    ca_model = CA_Block(channel=16, h=128, w=64)
    y = ca_model(x)
    print(y.shape)
</code></pre>
<h3 id="注意力机制的应用"><a href="#注意力机制的应用" class="headerlink" title="注意力机制的应用"></a>注意力机制的应用</h3><p>注意力机制是一个即插即用的模块，理论上可以放在任何一个特征层后面，可以放在主干网络，也可以放在加强特征提取网络。</p>
<p>由于放置在主干会导致网络的预训练权重无法使用，本文以YoloV4-tiny为例，将注意力机制应用加强特征提取网络上。</p>
<p>如下图所示，我们在主干网络提取出来的两个有效特征层上增加了注意力机制，同时对上采样后的结果增加了注意力机制。<br><img src="/CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331160207149.png" class="lazyload" data-srcset="/CN/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20220331160207149.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<pre><code class="lang-python">attention_block = [se_block, cbam_block, eca_block]

#---------------------------------------------------#
#   特征层-&gt;最后的输出
#---------------------------------------------------#
class YoloBody(nn.Module):
    def __init__(self, anchors_mask, num_classes, phi=0):
        super(YoloBody, self).__init__()
        self.phi            = phi
        self.backbone       = darknet53_tiny(None)

        self.conv_for_P5    = BasicConv(512,256,1)
        self.yolo_headP5    = yolo_head([512, len(anchors_mask[0]) * (5 + num_classes)],256)

        self.upsample       = Upsample(256,128)
        self.yolo_headP4    = yolo_head([256, len(anchors_mask[1]) * (5 + num_classes)],384)

        if 1 &lt;= self.phi and self.phi &lt;= 3:
            self.feat1_att      = attention_block[self.phi - 1](256)
            self.feat2_att      = attention_block[self.phi - 1](512)
            self.upsample_att   = attention_block[self.phi - 1](128)

    def forward(self, x):
        #---------------------------------------------------#
        #   生成CSPdarknet53_tiny的主干模型
        #   feat1的shape为26,26,256
        #   feat2的shape为13,13,512
        #---------------------------------------------------#
        feat1, feat2 = self.backbone(x)
        if 1 &lt;= self.phi and self.phi &lt;= 3:
            feat1 = self.feat1_att(feat1)
            feat2 = self.feat2_att(feat2)

        # 13,13,512 -&gt; 13,13,256
        P5 = self.conv_for_P5(feat2)
        # 13,13,256 -&gt; 13,13,512 -&gt; 13,13,255
        out0 = self.yolo_headP5(P5) 

        # 13,13,256 -&gt; 13,13,128 -&gt; 26,26,128
        P5_Upsample = self.upsample(P5)
        # 26,26,256 + 26,26,128 -&gt; 26,26,384
        if 1 &lt;= self.phi and self.phi &lt;= 3:
            P5_Upsample = self.upsample_att(P5_Upsample)
        P4 = torch.cat([P5_Upsample,feat1],axis=1)

        # 26,26,384 -&gt; 26,26,256 -&gt; 26,26,255
        out1 = self.yolo_headP4(P4)

        return out0, out1
# 研究方向为CV的可以关注Bubbliiiing,也可以顺道关注一下博主心系五道口，谢谢！！！
————————————————
版权声明：本文为CSDN博主「Bubbliiiing」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/weixin_44791964/article/details/121371986
</code></pre>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数</title>
    <url>/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考博文：</p>
<p>1.<a href="https://blog.csdn.net/weixin_44791964/article/details/117338865">激活函数介绍</a></p>
<p>2.<a href="[torch.nn.functional — PyTorch 1.11.0 documentation](https://pytorch.org/docs/stable/nn.functional.html">pytorch激活函数官方文档</a>)</p>
<h2 id="什么是激活函数"><a href="#什么是激活函数" class="headerlink" title="什么是激活函数"></a>什么是激活函数</h2><p>活函数（Activation functions）对于神经网络模型学习与理解复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。</p>
<p>如果网络中不使用激活函数，网络每一层的输出都是上层输入的线性组合，无论神经网络有多少层，输出都是输入的线性组合。</p>
<p>如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，此时神经网络就可以应用到各类非线性场景当中了。</p>
<p>常见的激活函数如sigmoid、tanh、relu等，它们的输入输出映射均为非线性，这样才可以给网络赋予非线性逼近能力。</p>
<h2 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h2><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><p>Sigmoid函数是一个在生物学中常见的S型函数，它能够把输入的连续实值变换为0和1之间的输出，如果输入是特别小的负数，则输出为0，如果输入是特别大的正数，则输出为1。即将输入量映射到0到1之间。</p>
<p><img src="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102716672.png" class="lazyload" data-srcset="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102716672.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" style="zoom:50%;"></p>
<p>Sigmoid可以作为非线性激活函数赋予网络非线性区分能力，也可以用来做二分类。其计算公式为：</p>
<p><img src="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20190719210645402.png" class="lazyload" data-srcset="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20190719210645402.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>优点：</p>
<p>1.曲线过渡平滑，处处可导；<br>缺点：</p>
<p>1.幂函数运算较慢，激活函数计算量大；<br>2.求取反向梯度时，Sigmoid的梯度在饱和区域非常平缓，很容易造称梯度消失的问题，减缓收敛速度。</p>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>Tanh是一个奇函数，它能够把输入的连续实值变换为-1和1之间的输出<strong>，</strong>如果输入是特别小的负数，则输出为-1，如果输入是特别大的正数，则输出为1<strong>；</strong>解决了Sigmoid函数的不是0均值的问题。</p>
<p><img src="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102737126.png" class="lazyload" data-srcset="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102737126.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>Tanh可以作为非线性激活函数赋予网络非线性区分能力。其计算公式为：</p>
<p><img src="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20190719210849124.png" class="lazyload" data-srcset="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20190719210849124.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>优点：</p>
<p>1.曲线过渡平滑，处处可导；<br>2.具有良好的对称性，网络是0均值的。<br>缺点：</p>
<p>1.与Sigmoid类似，幂函数运算较慢，激活函数计算量大；<br>2.与Sigmoid类似，求取反向梯度时，Tanh的梯度在饱和区域非常平缓，很容易造称梯度消失的问题，减缓收敛速度。</p>
<h3 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h3><p>线性整流函数（Rectified Linear Unit, ReLU），是一种深度神经网络中常用的激活函数，整个函数可以分为两部分，<strong>在小于0的部分，激活函数的输出为0；在大于0的部分，激活函数的输出为输入</strong>。</p>
<p><img src="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102747835.png" class="lazyload" data-srcset="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102747835.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>计算公式为：</p>
<p><img src="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20190719211201399.png" class="lazyload" data-srcset="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20190719211201399.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>优点：</p>
<p>1.收敛速度快，不存在饱和区间，在大于0的部分梯度固定为1，有效解决了Sigmoid中存在的梯度消失的问题；<br>2.计算速度快，ReLU只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的指数运算，具有类生物性质。<br>缺点：</p>
<p>1.它在训练时可能会“死掉”。如果一个非常大的梯度经过一个ReLU神经元，更新过参数之后，这个神经元的的值都小于0，此时ReLU再也不会对任何数据有激活现象了。如果这种情况发生，那么从此所有流过这个神经元的梯度将都变成 0。合理设置学习率，会降低这种情况的发生概率。</p>
<h3 id="Swish"><a href="#Swish" class="headerlink" title="Swish"></a>Swish</h3><p>Swish是Sigmoid和ReLU的改进版，类似于ReLU和Sigmoid的结合，β是个常数或可训练的参数。Swish 具备无上界有下界、平滑、非单调的特性。Swish 在深层模型上的效果优于 ReLU。</p>
<p><img src="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102949435.png" class="lazyload" data-srcset="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528102949435.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>计算公式为：</p>
<p><img src="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/image-20220421221448860.png" class="lazyload" data-srcset="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/image-20220421221448860.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220421221448860"></p>
<p>优点：</p>
<ul>
<li>Swish具有一定ReLU函数的优点；</li>
<li>Swish具有一定Sigmoid函数的优点；</li>
<li>Swish函数可以看做是介于线性函数与ReLU函数之间的平滑函数。</li>
</ul>
<p>缺点：</p>
<ul>
<li>运算复杂，速度较慢。</li>
</ul>
<h3 id="Mish"><a href="#Mish" class="headerlink" title="Mish"></a>Mish</h3><p>Mish与Swish激活函数类似，Mish具备无上界有下界、平滑、非单调的特性。Mish在深层模型上的效果优于 ReLU。无上边界可以避免由于激活值过大而导致的函数饱和。</p>
<p><img src="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528103004303.png" class="lazyload" data-srcset="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528103004303.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<p>计算公式：</p>
<p><img src="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/image-20220421222005130.png" class="lazyload" data-srcset="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/image-20220421222005130.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="image-20220421222005130"></p>
<p>优点：</p>
<ul>
<li><strong>Mish具有一定ReLU函数的优点，收敛快速</strong>；</li>
<li><strong>Mish具有一定Sigmoid函数的优点，函数平滑</strong>；</li>
<li><strong>Mish函数可以看做是介于线性函数与ReLU函数之间的平滑函数</strong>。</li>
</ul>
<p>缺点：</p>
<ul>
<li><strong>运算复杂，速度较慢</strong>。</li>
</ul>
<h3 id="Swish和Mish的梯度对比"><a href="#Swish和Mish的梯度对比" class="headerlink" title="Swish和Mish的梯度对比"></a>Swish和Mish的梯度对比</h3><p><img src="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528105245598.png" class="lazyload" data-srcset="/CN/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/20210528105245598.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="在这里插入图片描述" style="zoom:50%;"></p>
<h2 id="绘制代码"><a href="#绘制代码" class="headerlink" title="绘制代码"></a>绘制代码</h2><p>pytorch官方文档中的激活函数基本都在这里</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
import numpy as np

def Sigmoid(x):
    y = np.exp(x) / (np.exp(x) + 1)
    return y

def Tanh(x):
    y = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))
    # y = np.tanh(x)
    return y

def ReLU(x):
    y = np.where(x &lt; 0, 0, x)
    return y

def LeakyReLU(x, a):  
    # LeakyReLU的a参数不可训练，人为指定。
    y = np.where(x &lt; 0, a * x, x)
    return y

def PReLU(x, a):  
    # PReLU的a参数可训练
    y = np.where(x &lt; 0, a * x, x)
    return y

def ReLU6(x):
    y = np.minimum(np.maximum(x, 0), 6)
    return y

def Swish(x, b):
    y = x * (np.exp(b*x) / (np.exp(b*x) + 1))
    return y

def Mish(x):
    # 这里的Mish已经经过e和ln的约运算
    temp = 1 + np.exp(x)
    y = x * ((temp*temp-1) / (temp*temp+1))
    return y

def Grad_Swish(x, b):
    y_grad = np.exp(b*x)/(1+np.exp(b*x)) + x * (b*np.exp(b*x) / ((1+np.exp(b*x))*(1+np.exp(b*x))))
    return y_grad

def Grad_Mish(x):
    temp = 1 + np.exp(x)
    y_grad = (temp*temp-1) / (temp*temp+1) + x*(4*temp*(temp-1)) / ((temp*temp+1)*(temp*temp+1))
    return y_grad


if __name__ == &#39;__main__&#39;:
    x = np.arange(-10, 10, 0.01)

    plt.plot(x, Sigmoid(x))
    plt.title(&quot;Sigmoid&quot;)
    plt.grid()
    plt.show()

    plt.plot(x, Tanh(x))
    plt.title(&quot;Tanh&quot;)
    plt.grid()
    plt.show()

    plt.plot(x, ReLU(x))
    plt.title(&quot;ReLU&quot;)
    plt.grid()
    plt.show()

    plt.plot(x, LeakyReLU(x, 0.1))
    plt.title(&quot;LeakyReLU&quot;)
    plt.grid()
    plt.show()

    plt.plot(x, PReLU(x, 0.25))
    plt.title(&quot;PReLU&quot;)
    plt.grid()
    plt.show()

    plt.plot(x, ReLU6(x))
    plt.title(&quot;ReLU6&quot;)
    plt.grid()
    plt.show()

    plt.plot(x, Swish(x, 1))
    plt.title(&quot;Swish&quot;)
    plt.grid()
    plt.show()

    plt.plot(x, Mish(x))
    plt.title(&quot;Mish&quot;)
    plt.grid()
    plt.show()

    plt.plot(x, Grad_Mish(x))
    plt.plot(x, Grad_Swish(x, 1))
    plt.title(&quot;Gradient of Mish and Swish&quot;)
    plt.legend([&#39;Mish&#39;, &#39;Swish&#39;])
    plt.grid()
    plt.show()
</code></pre>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title>烟之外</title>
    <url>/CN/%E7%83%9F%E4%B9%8B%E5%A4%96/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在涛声中唤你的名字，而你的名字<br>已在千帆之外<br>潮来潮去<br>左边的鞋印才下午<br>右边的鞋印已黄昏了<br>六月原是一本很感伤的书<br>结局如此之凄美<br>——落日西沉</p>
<p>你依然凝视<br>那人眼中展示的一片纯白<br>他跪向你，向昨日那朵美了整个<br>下午的云<br>海哟，为何在众灯之中<br>独点亮那一盏茫然<br>还能抓住什么呢？<br>你那曾被称为云的眸子<br>现有人叫作<br>烟</p>
]]></content>
      <categories>
        <category>今日故事</category>
      </categories>
  </entry>
  <entry>
    <title>目标检测知识大集合</title>
    <url>/CN/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script>]]></content>
      <categories>
        <category>cv</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉单词表</title>
    <url>/CN/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%8D%95%E8%AF%8D%E8%A1%A8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>artificial neural network,ANN        人工神经网络</p>
<p>perceptron        感知机，人工神经元</p>
<p>activation function        激活函数</p>
<p>rectified linear unit,RELU        修正线性单元</p>
<p>bias        偏置</p>
<p>loss function        损失函数</p>
<p>universal approximation theorem        万能逼近定理</p>
<p>one-hot encoding        独热编码</p>
<p>cross-entropy        交叉熵</p>
<p>dropout        丢弃</p>
<p>bagging        装袋</p>
<p>model averaging        模型平均</p>
<p>batch normalization        批归一化</p>
<p>backpropagation        反向传播</p>
<p>stochastic gradient descent,SGD        随机梯度下降</p>
<p>acquisition         学习，获得</p>
<p>integrate       整合，集成，合并</p>
<p>diverse         多样化的，不同的</p>
<p>tune          调优</p>
<p>curation        内容管理</p>
<p>projection      投影，预测</p>
<p>coherent         有条理的，连贯的</p>
<p>redundant        冗余的</p>
<p>entity          实体</p>
<p>synthetic     合成的，虚假的，不诚恳的</p>
<p>spammy     垃圾邮件式的，无聊的</p>
<p>crowdsourcing          众包</p>
<p>continuity        连续性，连贯性</p>
<p>manifold       多种多样的</p>
<p>inherent        固有的，内在的</p>
<p>pseudo     假的，仿冒的</p>
<p>ensemble         套</p>
<p>heuristic        启发式的；启发式教育法</p>
<p>erroneous        错误的，不正确的</p>
<p>resilient        有弹性的，可迅速恢复的</p>
<p>degraded        堕落的，退化的</p>
<p>converge        收敛，集中</p>
<p>outlier        离群值，异常值</p>
<p>violate        违反，违背</p>
<p>syntactic        语法的</p>
<p>cartesian        笛卡尔的</p>
<p>categorical        分类，绝对的</p>
<p>prune        修剪</p>
<p>param        停止</p>
<p>translation  invariance        平移不变性</p>
<p>suppress        抑制，镇压，阻止</p>
<p>bidirectional        双向</p>
<p>tabular        扁平的，列成表格的</p>
<p>revenue        收入，税收</p>
<p>latency        延迟</p>
<p>harmonic        和声的，谐和的，音乐般的</p>
<p>harmonic mean        调和平均数</p>
<p>harmonic series        调和级数</p>
<p>rote        死记硬背，生搬硬套</p>
<p>bid        出价，投标</p>
<p>leaderboard        排行榜，通栏广告</p>
<p>minor        较小的，次要的，轻微的</p>
<p>contaminated        受污染的，弄脏的</p>
<p>tradeoff        权衡，折中</p>
<p>ensemble learning        集成学习</p>
<p>decompose        分解，使腐烂</p>
<p>intrinsic        内在的，固有的</p>
<p>notable         显要的，值得注意的；非常成功的，令人尊敬的</p>
<p>camouflaged        伪装的</p>
<p>facilitate        促进，使便利</p>
<p>overlap    与……重叠，部分地相同；重叠的部分，互搭量</p>
<p>threshold    入口，门槛，开始，极限，临界值</p>
<p>conjecture    猜测，推测</p>
<p>within    在……之内</p>
<p>oversample:过采样</p>
<p>trade off:权衡，卖掉，折中方案</p>
<p>ultimately:最后，根本，基本上</p>
<p>robotics：机器人学</p>
<p>areial:空中的，航空的，空气的</p>
<p>underperform:表现不佳，工作不如预期</p>
<p>crucial:重要的，决定性的</p>
<p>high-resolution:高分辨率的</p>
<p>deploy:配置，展开，部署</p>
<p>barely:仅仅，勉强，几乎不</p>
<p>tumor:肿瘤，肿块</p>
<p>diagnosis:诊断</p>
<p>inspection:检查，视察</p>
<p>defect:缺陷，缺点，不足之处</p>
<p>annotate:注释，作注解</p>
<p>address:地址，编址</p>
<p>potentially:可能地，潜在地</p>
<p>imply:意味，暗示，隐含</p>
<p>diversity:多样性，差异</p>
<p>generalize:概括，推广，使……一般化</p>
<p>portion:部分</p>
<p>crop:裁剪</p>
<p>merge:合并</p>
<p>align:匹配，排列，对齐，对准</p>
<p>mask:掩码，掩膜</p>
<p>cascade:小瀑布，串联，级联</p>
<p>fuse:融合，熔接，熔化</p>
<p>computational:计算的</p>
<p>overhead:经常性费用，运营费用</p>
<p>fraction:分数，部分，小部分，稍微</p>
<p>schematic illustration:示意图</p>
<p>respect to:关于，考虑</p>
<p>validate:验证，确认，使生效</p>
<p>stochastic:随机的，猜测的</p>
<p>decay    衰退，衰减</p>
<p>coefficient    系数，率</p>
<p>explicitly    明确地，明白地</p>
<p>outline    大纲，概要</p>
<p>distillation:蒸馏</p>
<p>curvature:曲率</p>
<p>stochastic    随机</p>
<p>variance    差异，方差</p>
<p>spectrum    光谱，频谱；范围</p>
<p>neat    灵巧的，整洁的；优雅的，平滑的</p>
<p>heterogenerous    由很多种类组成的</p>
<p>intricate    复杂的，错综的</p>
<p>arbitrary    任意的，武断的</p>
<p>vanilla    香草，比较原始的</p>
<p>sketch    示意图</p>
<p>incarnation    化身，典型</p>
<p>waive    放弃，搁置</p>
<p>shrinkage    收缩，皱缩，缩水; 跌价; 抽缩</p>
<p>alleviate    缓解，减轻</p>
<p>de-facto    事实上</p>
<p>corpus    文集，语料库</p>
<p>unprecedented    前所未有的</p>
<p>inductive    归纳的</p>
<p>empirical    经验主义的</p>
<p>allergic    过敏的，反感的</p>
<p>pollen     花粉</p>
<p>badminton    羽毛球运动</p>
<p>pharmacy    药房</p>
<p>jasmine    茉莉</p>
<p>latent    潜在的，潜伏的，潜意识的</p>
<p>prepend    预先考虑</p>
<p>embedding    编码</p>
<p>alternating    交互的</p>
<p>interpolation    插入，篡改，添写</p>
<p>de-duplicate    删除重复数据</p>
<p>suite    一套，套件</p>
<p>geometric    几何图形的，几何的</p>
<p>intermediate    中间的</p>
<p>fine-tuning    微调</p>
<p>appendix    附录</p>
<p>warmup    预热</p>
<p>least-squares regression    最小二乘回归</p>
<p>on-the-fly    匆匆忙忙地；在空中；（计）运行中</p>
<p>literature    文献</p>
<p>outperform    胜过，做的比……好</p>
<p>substantially    实质上；大体上；充分地</p>
<p>standard deviation    标准差</p>
<p>co-training    协同训练</p>
<p>boost    促进，增加</p>
<p>overtake    赶上，压倒，突然来袭</p>
<p>plateau    趋于平稳，进入停滞期</p>
<p>vanish    消失</p>
<p>versus     与</p>
<p>saturate    饱和的</p>
<p>principal    最主要的</p>
<p>plausible    貌似可信的，花言巧语的；貌似真实的，貌似有理的</p>
<p>sinusoidal    正弦曲线的</p>
<p>degree    程度</p>
<p>analogous    类似的</p>
<p>preliminary    初步的</p>
<p>manual    手动的，手工的</p>
<p>insight    洞察力，领悟</p>
<p>exponentially    以指数方式的</p>
<p>holistically    整体论地</p>
<p>unidirectional    单向性的</p>
<p>incorporate    包含，吸收，体现；把……合并</p>
<p>alleviate    减轻</p>
<p>shallow    浅的，肤浅的</p>
<p>discriminate    区分，辨别</p>
<p>coarser    粗糙的</p>
<p>granularity    间隔尺寸，粒度</p>
<p>derived    导出的，衍生的，派生的</p>
<p>predecessor    前任，前辈;(被取代的)原有事物，前身</p>
<p>cloze    adj. 完形的；填充测验法的</p>
<p>cloze task    完形填空</p>
<p>recipe    秘诀，处方</p>
<p>distinctive    有特色的，与众不同的</p>
<p>unambiguously    不含糊地，明白地</p>
<p>intuitively    直观地；直觉地</p>
<p>trivially    琐细地，平凡地，无能地</p>
<p>mitigate    使缓和，使减轻</p>
<p>monolingual    单语的；仅用一种语言的；仅懂一种语言的</p>
<p>procedure    程序，手续，步骤</p>
<p>degenerate    使退化，恶化</p>
<p>de-facto    (法)实际上的</p>
<p>explicitly    显式地</p>
<p>reformulate    重新构造</p>
<p>ensemble    全体，总效果</p>
<p>nontrivial    重要的，显著的</p>
<p>obstacle    阻碍，障碍</p>
<p>notorious    臭名昭著的，声名狼藉的</p>
<p>vanishing/exploding gradients    梯度消失/梯度爆炸</p>
<p>hamper    妨碍，束缚</p>
<p>degradation    退化，降级，堕落</p>
<p>thoroughly    完全地，彻底地</p>
<p>counterpart    副本，配对物</p>
<p>feasible    可行的，可能的</p>
<p>akin to     类似于</p>
<p>generic    类的，属性的; 一般的; 不受商标保护的; [生]属的，类的</p>
<p>retrieval    检索</p>
<p>quantization    量化</p>
<p>partial differential equations    偏微分方程</p>
<p>auxiliary    辅助的，备用的</p>
<p>Concurrent    并发的，同时发生的</p>
<p>asymptotically    渐近地</p>
<p>counterintuitive    违反直觉的</p>
<p>perturbations    [流]扰动，不安</p>
<p>trial    测试</p>
<p>curse    咒骂，诅咒</p>
<p>estimation    评估，评价，判断</p>
<p>surrogate    代理的</p>
<p>prominent    突出的，显著的，卓越的，杰出的</p>
<p>thes    命题，论文</p>
<p>recalibrate    重新校准</p>
<p>pruning    剪枝</p>
<p>proxy    代理人，代表权</p>
<p>compound    加重; 使复杂化; 混合;混合的</p>
<p>criteria    标准，条件</p>
<p>panoptic    全景的</p>
<p>controversial    有争议的</p>
<p>problematic    有疑问的，有问题的</p>
<p>contrastive    对比的</p>
<p>intuitive    直觉的; 凭直觉获知的; 直观的</p>
<p>preserve    保存；保护；维持；腌；禁猎</p>
<p>intractable    棘手的；难治的；倔强的；不听话的</p>
<p>pretext    借口，托辞; 假象，掩饰</p>
<p>permutation    排列，置换</p>
<p>discrimination    区别对待; 鉴别力; 区别</p>
<p>shuffle    洗牌; 曳脚而行; 搬移; 搁置，随手放</p>
<p>neatness    整洁，干净</p>
<p>blur    模糊</p>
<p>permutation    排列，置换</p>
<p>infrared    红外线的</p>
<p>attenuation    衰减，衰变</p>
<p>tricky    棘手的，难对付的</p>
<p>plethora    过多，过剩</p>
<p>deluge    泛滥，淹没</p>
<p>elaborate    精心制作的，详尽的</p>
<p>repurpose    改换意图，重新</p>
<p>assistive    辅助性的</p>
<p>eliminate    消除</p>
<p>duplicate    重复的</p>
<p>coordinate    坐标</p>
<p>refreshingly    清爽地，有精神地，令人耳目一新地</p>
<p>millisecond     毫秒</p>
<p>implicitly    隐式地</p>
<p>delimiter    分隔符</p>
<p>diverge    分歧，相异</p>
<p>remedy    解决方法，纠正方法</p>
<p>deviation    偏差</p>
<p>coarse    粗糙的</p>
<p>begnign    无有害的，认为无关紧要的</p>
<p>malicious    恶意的，怀恨的</p>
<p>rigorous    严格的</p>
<p>outlier    离群值</p>
<p>deliberate    故意的；深思熟虑的；从容的</p>
<p>susceptible    易受影响的；易感动的；容许…的</p>
<p>leverage    利用</p>
<p>kinda    有点，有几分</p>
<p>centroid    形心，重心</p>
<p>exclusively    专门地，唯一地</p>
<p>collision    碰撞，警告</p>
<p>hint    暗示，示意</p>
<p>stand-alone    （计算机）独立运行的；（公司、组织）独立的</p>
<p>photometric distortion    光度失真</p>
<p>geometric    几何失真</p>
<p>hue    色调</p>
<p>saturation    饱和度</p>
<p>superimpose    叠加</p>
<p>adjacent    相邻的</p>
<p>incorporate    包含，吸收；体现；把……合并</p>
<p>mimic    模仿</p>
<p>tentative    初步的</p>
<p>tackle    处理</p>
<p>cortex    皮层</p>
<p>factorization    因子分解，因式分解</p>
<p>compatible    兼容的</p>
<p>substantially    实质上；大体上；充分地</p>
<p>explicitly    明确地；明白地</p>
<p>reformulate    v. 再制订；换种方式说（或表达）</p>
<p>nontrival    重要的</p>
<p>notorious    声名狼藉的，臭名昭著的</p>
<p>aggregated    聚合的，合计的</p>
<p>cardinality    基数</p>
<p>acquisition    获得物</p>
<p>obscured    遮挡</p>
<p>out-of-view    看不见的</p>
<p>precedent    先前的</p>
<p>intuitive    直观的</p>
<p>overhead    开销</p>
<p>efficacy    功效，效力</p>
<p>hierarchical    分层的</p>
<p>foeval    视网膜中心的</p>
<p>iteratively    迭代地</p>
<p>salient    重点的</p>
<p>modality    形式，形态</p>
<p>in a conditional fashion    有条件的方式</p>
<p>squeeze    挤压</p>
<p>excitation    激励</p>
<p>self-contained    独立的，设备齐全的，沉默寡言的</p>
<p>aggregate    集合，聚集</p>
<p>superscript    上标</p>
<p>an apples to apples comparison    比较两个相近的事物</p>
<p>thoroughly    彻底地，完全地</p>
<p>conjecture    推测，猜测</p>
<p>auxiliary    辅助的</p>
<p>parentheses    圆括号，插入成分</p>
<p>nest    嵌套</p>
<p>the best of both worlds    两全其美</p>
<p>incur    带来（成本、花费）等；招致，遭受</p>
<p>decimal    十进位的，小数的</p>
<p>timestamp    时间戳</p>
<p>clause    从句，分句；（法律文件的）条款</p>
<p>overlap    重叠</p>
<p>coalesce    合并，联合</p>
<p>rollup    归纳，卷曲，袅袅上升</p>
<p>coarse-to-fine    由粗到细，由繁到简</p>
<p>tic-tac-toe    井字棋，圈叉游戏</p>
<p>overlay    覆在……上面，覆盖</p>
<p>cached    贮藏起来，高速缓存</p>
<p>eigen    特征，固有的</p>
<p>tremendous    巨大的，极好的</p>
<p>versus    （比赛或诉讼中）以……为对手，与……竞争；与……相对，与……相比</p>
<p>delineating    描述，描绘</p>
<p>primitive    原始的</p>
<p>recalibrating    重新调整</p>
<p>magenta    洋红色</p>
<p>cyan    青绿色</p>
<p>pentagon    五边形</p>
<p>hexagon    六边形</p>
<p>diamond    菱形</p>
<p>line chart    折线图</p>
<p>flip    翻转</p>
<p>alias    别名</p>
<p>incurring    招致，遭受</p>
<p>ellipse    椭圆</p>
<p>tile    平铺，瓷砖</p>
<p>diversity    多样性</p>
<p>discard    丢弃</p>
<p>adequately    充分地，足够地</p>
<p>flaw    缺陷，缺点</p>
<p>susceptible    易得病的，易受影响的；（人）易受感动的，易动感情的</p>
<p>caret    脱字符号，插入符号</p>
<p>teardown    拆卸</p>
<p>untangle    理清，整顿，解开……纠结</p>
<p>duration    持续时间</p>
<p>optical    光学的；（装置）光电的</p>
<p>decent    像样的，尚好的，得体的</p>
<p>inevitable    必然发生的，不可避免的</p>
<p>leverage    n.影响力，杠杆作用adj.充分利用</p>
<p>overwhelm    压倒，压垮</p>
<p>degenerate    恶化，堕落，退化</p>
<p>elaborate    详细说明，复杂的</p>
<p>mitigate    减轻</p>
<p>severe    严重的，艰巨的</p>
<p>hamper    阻碍</p>
<p>induce    诱导，引诱</p>
<p>deteriorate    恶化，变坏</p>
<p>atomic    原子的，核能的</p>
]]></content>
      <categories>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>马裤先生</title>
    <url>/CN/%E9%A9%AC%E8%A3%A4%E5%85%88%E7%94%9F/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>火车在北平东站还没开，同屋那位睡上铺的穿马裤，戴平光的眼镜，青缎子洋服上身，胸袋插着小楷羊毫，足登青绒快靴的先生发了问：「你也是从北平上车？」很和气的。</p>
<p>火车还没动呢，不从北平上车，由哪儿呢？我只好反攻了：「你从哪儿上车？」</p>
<p>他没言语。看了看铺位，用尽全身的力气喊了声，「茶房！」</p>
<p>茶房跑来了。</p>
<p>「拿毯子！」马裤先生喊。</p>
<p>「请少待一会儿，先生。」茶房很和气的说，</p>
<p>「拿枕头！」</p>
<p>「先生，您等我忙过这会儿去，毯子和枕头就一齐全到。」茶房说的很快，可依然是很和气。</p>
<p>马裤先生没任何的表示。茶房故意地笑了笑，表示歉意。然后搭讪着慢慢地转身，腿刚预备好要走，背后打了个霹雳：「茶房！<br>」</p>
<p>茶房不是假装没听见，便是耳朵已经震聋，竟自没回头，一直地快步走开。</p>
<p>「茶房！茶房！茶房！」马裤先生连喊，一声比一声高：站台上送客的跑过一群来，以为车上失了火，要不然便是出了人命。茶<br>房始终没回头。马裤先生又挖了鼻孔一下，坐在我的床上。</p>
<p>茶房从门前走过。</p>
<p>「茶房！拿手巾把！」</p>
<p>「等等。」茶房似乎下了抵抗的决心。</p>
<p>马裤先生把领带解开，摘下领子来，分别挂在铁钩上：所有的钩子都被占了，他的帽子，大衣，已占了两个。车开了，他爬上了<br>上铺，在我的头上脱靴子，并且击打靴底上的土。枕着个手提箱，车还没到永定门，他睡着了。我心中安坦了许多。</p>
<p>到了丰台，车还没站住，上面出了声：「茶房！」</p>
<p>没等茶房答应，他又睡着了；大概这次是梦话。</p>
<p>过了丰台，大概还没到廊房，上面又打了雷：「茶房！」</p>
<p>茶房来了，眉毛拧得好像要把谁吃了才痛快。</p>
<p>「干吗？先——生——」</p>
<p>「拿茶！」</p>
<p>「好吧！」茶房的眉毛拧得直往下落毛。</p>
<p>马裤先生又入了梦乡，呼声只比「茶房」小一点。有时呼声稍低一点，用咬牙来补上。</p>
<p>到了天津。又上来些旅客。车好容易又从天津开走。刚一开车，茶房给马裤先生拿来头一份毯子枕头和手巾把。马裤先生用手巾<br>把耳鼻孔全钻得到家，这一把手巾擦了至少有一刻钟，最后用手巾擦了擦手提箱上的土。我给他数着，从老站到总站的十来分钟之间，他又喊了四五十声茶房。茶房只来了一次，他的问题是火车向哪面走呢？茶房的回答是不知道；于是又引起他的建议，车上总该有人知道，茶房应当负责去问。茶房说，连驶车的也不晓得东西南北。于是他几乎变了颜色，万一车走迷了路？！</p>
<p>茶房没再回答，可是又掉了几根眉毛。</p>
<p>他又睡了，这次是在头上摔了摔袜子，可是一口痰并没往下唾，而是照顾了车顶。</p>
<p>我的目的地是德州，天将亮就到了。谢天谢地！</p>
<p>我雇好车，进了城，还清清楚楚地听见「茶房！」</p>
<p>一个多礼拜了，我还惦记着茶房的眉毛呢。</p>
]]></content>
      <categories>
        <category>今日故事</category>
      </categories>
  </entry>
  <entry>
    <title>Q &amp; A</title>
    <url>/CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="就地操作"><a href="#就地操作" class="headerlink" title="就地操作"></a>就地操作</h3><p>问题来源：看代码发现<code>self.relu=nn.ReLU(inplace=True)</code>不明白<code>inplace=True</code>什么意思。</p>
<p>解答：查看pytorch官网关于ReLU定义，<a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU">ReLU</a>其中inplace参数表示可以选择就地执行操作，默认为False,就地执行操作是指图像处理函数的输入图像和输出图像是同一对象，即同一张图像，常规的图像处理函数是不支持输入图像和输出图像是同一图像的。</p>
<p>eg:中值滤波函数</p>
<p><code>medianBlur(src, dst, 7);  //常规操作</code></p>
<p><code>medianBlur(src,  src, 7); //就地操作</code></p>
<p>就地操作直接更改张量的内容，而无需复制它。由于它不创建输入的副本，因此在处理高维数据时减少了内存使用，就地操作有助于使用更少的GPU内存，详情请看该博客<a href="https://www.ksgszhuce.com/tetl/37.html">如何在Pytorch中执行就地操作</a></p>
<h3 id="torch-max中keepdim的作用"><a href="#torch-max中keepdim的作用" class="headerlink" title="torch.max中keepdim的作用"></a>torch.max中keepdim的作用</h3><p>torch.max的用法：</p>
<p>(max, max_indices) = torch.max(input, dim, keepdim=False)</p>
<ul>
<li>输入：</li>
</ul>
<ol>
<li>input 是输入的tensor。</li>
<li>dim 是索引的维度，dim=0寻找每一列的最大值，dim=1寻找每一行的最大值。</li>
<li>keepdim 表示是否需要保持输出的维度与输入一样，keepdim=True表示输出和输入的维度一样，keepdim=False表示输出的维度被压缩了，也就是输出会比输入低一个维度。</li>
</ol>
<ul>
<li>输出：</li>
</ul>
<ol>
<li><p>max 表示取最大值后的结果。</p>
</li>
<li><p>2max_indices 表示最大值的索引</p>
</li>
</ol>
<p><code>import torch</code></p>
<p><code>import numpy as np</code></p>
<p><code>x = torch.randint(0,9,(2,4))</code></p>
<p><code>print(x)
tensor([[7, 8, 7, 2],
        [6, 0, 3, 0]])</code></p>
<h1 id="取每一行的最大值，torch-max的输出结果"><a href="#取每一行的最大值，torch-max的输出结果" class="headerlink" title="取每一行的最大值，torch.max的输出结果"></a>取每一行的最大值，torch.max的输出结果</h1><p><code>y = torch.max(x, 1)  
print(y)
torch.return_types.max(values=tensor([8, 6]),indices=tensor([1, 0])) #索引值</code></p>
<p><code>y = torch.max(x, 1, keepdim=True)[0]</code><br><code>print(y)</code><br><code>print(np.shape(y)) # keepdim=True，输出仍然是二维的</code><br><code>tensor([[8],
         [6]])torch.Size([2, 1])</code><br><code>y = torch.max(x, 1, keepdim=False)[0]</code><br><code>print(y)</code><br><code>print(np.shape(y))
keepdim=False # 输出变成了一维</code>tensor([8, 6])<br><code>torch.Size([2])</code></p>
<h3 id="ConstantPad2d的用法"><a href="#ConstantPad2d的用法" class="headerlink" title="ConstantPad2d的用法"></a>ConstantPad2d的用法</h3><p>torch.nn.ConstantPad2d(padding, value)</p>
<p>参数：padding(int, tuple)-padding的尺寸，如果是整型，那么所有的边界都使用相同的填充，如果是四元组，使用（padding_left, padding_right, padding_top, padding_bottom)</p>
<p>形状：</p>
<ul>
<li><p>输入：</p>
<script type="math/tex; mode=display">
(N, C, H_{in}, W_{in}) or (C, H_{in}, W_{in})</script></li>
<li><p>输出：</p>
<script type="math/tex; mode=display">
(N, C, H_{out}, W_{out}) or (C, H_{out}, W_{out})</script><p>其中，</p>
<script type="math/tex; mode=display">
H_{out} = H_{in}+padding_{top}+padding_{bottom}</script><script type="math/tex; mode=display">
W_{out}=W_{in}+padding_{left}+padding_{right}</script></li>
</ul>
<p>测试用例：</p>
<pre><code>import torch
import torch.nn as nn

n1 = nn.ConstantPad2d(2, 0)
n2 = nn.ConstantPad2d((0, 1, 0, 1), 0)
n3 = nn.ConstantPad2d((-1, 0, -1, 0), 0)
input = torch.randn(1, 2, 2)
print(input)
t = n1(input)
print(t)
x = n2(input)
y = n3(x)
print(x)
print(y)
</code></pre><p>结果：</p>
<p>tensor([[[ 1.0826,  0.1191],<br>         [-0.3506,  0.1677]]])<br>tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],<br>         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],<br>         [ 0.0000,  0.0000,  1.0826,  0.1191,  0.0000,  0.0000],<br>         [ 0.0000,  0.0000, -0.3506,  0.1677,  0.0000,  0.0000],<br>         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],<br>         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])<br>tensor([[[ 1.0826,  0.1191,  0.0000],<br>         [-0.3506,  0.1677,  0.0000],<br>         [ 0.0000,  0.0000,  0.0000]]])<br>tensor([[[0.1677, 0.0000],<br>         [0.0000, 0.0000]]])</p>
<p>更多详情参考<a href="https://pytorch.org/docs/stable/generated/torch.nn.ConstantPad2d.html?highlight=constantpad2d#torch.nn.ConstantPad2d">ConstantPad2d</a></p>
<h3 id="enumerate-函数"><a href="#enumerate-函数" class="headerlink" title="enumerate()函数"></a>enumerate()函数</h3><h4 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h4><p>enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。Python 2.3. 以上版本可用，2.6 添加 start 参数。</p>
<h4 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h4><p><code>enumerate(sequence, [start=0])</code></p>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p>·sequence—一个序列、迭代器或其他支持迭代对象</p>
<p>·start—下标起始位置的值</p>
<h4 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h4><p>返回enumerate(枚举)对象</p>
<h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><p>以下展示了使用enumerate()方法的实例：</p>
<blockquote>
<p>seasons = [‘Spring’, ‘Summer’, ‘Fall’, ‘Winter’]</p>
<p>list(enumerate(seasons))</p>
<p><code>[(0, &#39;Spring&#39;), (1, &#39;Summer&#39;), (2, &#39;Fall&#39;), (3, &#39;Winter&#39;)]</code></p>
<p>list(enumerate(seasons, start=1))    # 下标从1开始</p>
<p><code>[(1, &#39;Spring&#39;), (2, &#39;Summer&#39;), (3, &#39;Fall&#39;), (4, &#39;Winter&#39;)]</code></p>
</blockquote>
<p>普通的for循环</p>
<pre><code class="lang-python">i = 0
seq = [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]
for i in enumerate(seq):
    print(i, seq[i])
    i += 1

result:
0 one
1 two
2 three
</code></pre>
<p>for循环使用enumerate</p>
<pre><code class="lang-python">seq = [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]
for i, element in enumerate(seq):
    print(i, element)

result:
0 one
1 two
2 three
</code></pre>
<h3 id="torch-clamp"><a href="#torch-clamp" class="headerlink" title="torch.clamp"></a>torch.clamp</h3><p><code>torch.clamp(input, min=None, max=None, *, out=None)-&gt;Tensor</code></p>
<p>Clamps中所有输入的元素都在[min, max]范围内，让最小值和最大值分别是min和max，将会返回：</p>
<script type="math/tex; mode=display">
y_i=min(max(x_i,min\_value_i),max\_value_i)</script><p>如果min为空，就没有下界。或者，如果max为空，没有上界。</p>
<p>注：</p>
<p>如果min大于max,<code>torch.clamp(...,min,max)</code>设置输入的所有元素为max的值。</p>
<p>参数：</p>
<p>·input(Tensor)-输入张量</p>
<p>·min(Number或Tensor,可选)-被限制范围的下界</p>
<p>·max(Number或Tensor,可选)-被限制范围的上界</p>
<p>关键字参数：</p>
<p>out(Tensor, 可选)-输出的张量</p>
<p>举例：</p>
<pre><code class="lang-python">&gt;&gt;&gt;a = torch.randn(4)
&gt;&gt;&gt;a
tensor([-1.7120, 0.1734, -0.0478, -0.0922])
&gt;&gt;&gt;torch.clamp(a, min=-0.5, max=0.5)
tensor([-0.5000, 0.1734, -0.0478, -0.0922])

&gt;&gt;&gt;min = torch.linspace(-1, 1, steps=4)
&gt;&gt;&gt;torch.clamp(a, min=min)
tensor([-1.000, 0.1734, 0.3333, 1.0000])
</code></pre>
<h3 id="FPPI"><a href="#FPPI" class="headerlink" title="FPPI"></a>FPPI</h3><p><a href="https://blog.csdn.net/Bruce_0712/article/details/78462880">(68条消息) Recall/Precision/FPPI评价方式详解_Bruce_0712的博客-CSDN博客</a></p>
<h3 id="torchvision-ops-box-iou"><a href="#torchvision-ops-box-iou" class="headerlink" title="torchvision.ops.box_iou"></a>torchvision.ops.box_iou</h3><h4 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h4><p><code>torchvision.ops.box_iou(boxes1:torch.Tensor, boxes2:torch.Tensor)-&gt;torch.Tensor</code><a href="https://pytorch.org/vision/stable/_modules/torchvision/ops/boxes.html#box_iou">SOURCE</a></p>
<p>返回两个框的交并比，这两个框的形式都是<script type="math/tex">(x_1,y_1,x_2,y_2)</script>并且<script type="math/tex">0<=x_1<x_2,0<=y_1<y_2</script></p>
<h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h4><p>·boxes1(Tensor[N, 4])-&gt;第一个框</p>
<p>·boxes2(Tensor[N, 4])-&gt;第二个框</p>
<h4 id="返回"><a href="#返回" class="headerlink" title="返回"></a>返回</h4><p>返回boxes1和boxes2逐元素的配对IOU矩阵（N×M)</p>
<h4 id="返回类型"><a href="#返回类型" class="headerlink" title="返回类型"></a>返回类型</h4><p>Tensor(N,M)</p>
<h3 id="torch-argmax（）函数"><a href="#torch-argmax（）函数" class="headerlink" title="torch.argmax（）函数"></a>torch.argmax（）函数</h3><p><code>torch.argmax(input)-&gt;LongTensor</code></p>
<p>返回<code>input</code>张量所有元素的最大值序号，这是<code>torch.max()</code>返回的第二个值。</p>
<p>注：</p>
<p>如果这里有多个最大值，则会返回第一个最大值的序号。</p>
<h4 id="参数-2"><a href="#参数-2" class="headerlink" title="参数"></a>参数</h4><p><code>input(Tensor)-&gt;输入的张量</code></p>
<p>举例：</p>
<pre><code class="lang-python">&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
        [-0.7401, -0.8805, -0.3402, -1.1936],
        [ 0.4907, -1.3948, -1.0691, -0.3132],
        [-1.6092,  0.5419, -0.2993,  0.3195]])
&gt;&gt;&gt; torch.argmax(a)
tensor(0)
</code></pre>
<p><code>torch.argmax(input, dim, keppdim=False)-&gt;LongTensor</code></p>
<p>通过指定维度返回张量最大值的序号，这个最大值将会通过<code>torch.max()</code>返回</p>
<h4 id="参数-3"><a href="#参数-3" class="headerlink" title="参数"></a>参数</h4><p>·input(Tensor):输入的张量</p>
<p>·dim(int):减少的维度，如果没有，将返回平铺后的张量的argmax.</p>
<p>·keepdim(bool):输出的张量是否保持维度,如果<code>dim=None</code>将忽略。</p>
<p>举例：</p>
<pre><code class="lang-python">&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
        [-0.7401, -0.8805, -0.3402, -1.1936],
        [ 0.4907, -1.3948, -1.0691, -0.3132],
        [-1.6092,  0.5419, -0.2993,  0.3195]])
&gt;&gt;&gt; torch.argmax(a, dim=1)
tensor([ 0,  2,  0,  1])
</code></pre>
<pre><code class="lang-python">import torch
a = torch.tensor([
              [
                  [1, 5, 5, 2],
                  [9, -6, 2, 8],
                  [-3, 7, -9, 1]
              ],

              [
                  [-1, 7, -5, 2],
                  [9, 6, 2, 8],
                  [3, 7, 9, 1]
              ]])
b = torch.argmax(a, dim=0)
print(b)
print(a.shape)

&quot;&quot;&quot;
tensor([[0, 1, 0, 1],
        [1, 1, 1, 1],
        [1, 1, 1, 1]])
torch.Size([2, 3, 4])&quot;&quot;&quot;

# dim=0,即将第一个维度消除，也就是将两个[3*4]矩阵只保留一个，因此要在两组中作比较，即将上下两个[3*4]的矩阵分别在对应的位置上比较大小

b = torch.argmax(a, dim=1)
&quot;&quot;&quot;
tensor([[1, 2, 0, 1],
        [1, 2, 2, 1]])
torch.Size([2, 3, 4])
&quot;&quot;&quot;
# dim=1，即将第二个维度消除,这么理解：矩阵维度变为[2*4];
&quot;&quot;&quot;
[1, 5, 5, 2],
[9, -6, 2, 8],
[-3, 7, -9, 1];
纵向压缩成一维，因此变为[1,2,0,1];同理得到[1,2,2,1];
&quot;&quot;&quot;
b = torch.argmax(a,dim=2)
&quot;&quot;&quot;
tensor([[2, 0, 1],
        [1, 0, 2]])
&quot;&quot;&quot;
# dim=2,即将第三个维度消除，这么理解：矩阵维度变为[2*3]
&quot;&quot;&quot;
   [1, 5, 5, 2],
   [9, -6, 2, 8],
   [-3, 7, -9, 1];
横向压缩成一维
[2,0,1],同理得到下面的&quot;&quot;&quot;
</code></pre>
<h3 id="python-call-方法"><a href="#python-call-方法" class="headerlink" title="python _call_()方法"></a>python _<em>call_</em>()方法</h3><p>本节再介绍 <a href="http://c.biancheng.net/python/">Python</a> 类中一个非常特殊的实例方法，即 _<em>call_</em>()。该方法的功能类似于在类中重载 () 运算符，使得类实例对象可以像调用普通函数那样，以“对象名()”的形式使用。</p>
<pre><code class="lang-python"># 引用来自C语言中文网，详情请参考：http://c.biancheng.net/view/2380.html
class CLanguage:
    # 定义__call__方法
    def __call__(self,name,add):
        print(&quot;调用__call__()方法&quot;,name,add)

clangs = CLanguage()
clangs(&quot;C语言中文网&quot;,&quot;http://c.biancheng.net&quot;)
</code></pre>
<p>程序执行结果：</p>
<pre><code>调用__call__()方法 C语言中文网 http://c.biancheng.net
</code></pre><p>可以看到，通过在 CLanguage 类中实现 _<em>call_</em>() 方法，使的 clangs 实例对象变为了可调用对象。</p>
<blockquote>
<p>Python 中，凡是可以将 () 直接应用到自身并执行，都称为可调用对象。可调用对象包括自定义的函数、Python 内置函数以及本节所讲的类实例对象。</p>
</blockquote>
<p>对于可调用对象，实际上“名称()”可以理解为是“名称.<strong>call</strong>()”的简写。仍以上面程序中定义的 clangs 实例对象为例，其最后一行代码还可以改写为如下形式：</p>
<pre><code>clangs.__call__(&quot;C语言中文网&quot;,&quot;http://c.biancheng.net&quot;)
</code></pre><p>运行程序会发现，其运行结果和之前完全相同。</p>
<p>自定义函数：</p>
<pre><code class="lang-python">def say():
    print(&quot;Python教程：http://c.biancheng.net/python&quot;)
say()
say.__call__()
</code></pre>
<p>程序执行结果：</p>
<pre><code class="lang-python">Python教程：http://c.biancheng.net/python
Python教程：http://c.biancheng.net/python
</code></pre>
<h4 id="用-call-弥补-hasattr-函数的短板"><a href="#用-call-弥补-hasattr-函数的短板" class="headerlink" title="用 call() 弥补 hasattr() 函数的短板"></a>用 <strong>call</strong>() 弥补 hasattr() 函数的短板</h4><p>前面章节介绍了 hasattr() 函数的用法，该函数的功能是查找类的实例对象中是否包含指定名称的属性或者方法，但该函数有一个缺陷，即它无法判断该指定的名称，到底是类属性还是类方法。</p>
<p>要解决这个问题，我们可以借助可调用对象的概念。要知道，类实例对象包含的方法，其实也属于可调用对象，但类属性却不是。举个例子：</p>
<pre><code class="lang-python">class CLanguage:
    def __init__ (self):
        self.name = &quot;C语言中文网&quot;
        self.add = &quot;http://c.biancheng.net&quot;
    def say(self):
        print(&quot;我正在学Python&quot;)

clangs = CLanguage()
if hasattr(clangs,&quot;name&quot;):
    print(hasattr(clangs.name,&quot;__call__&quot;))
print(&quot;**********&quot;)
if hasattr(clangs,&quot;say&quot;):
    print(hasattr(clangs.say,&quot;__call__&quot;))
</code></pre>
<p>程序执行结果：</p>
<pre><code class="lang-python">False
**********
True
</code></pre>
<p>可以看到，由于 name 是类属性，它没有以 <strong>call</strong> 为名的 <strong>call</strong>() 方法；而 say 是类方法，它是可调用对象，因此它有 <strong>call</strong>() 方法。</p>
<h3 id="argparse-函数"><a href="#argparse-函数" class="headerlink" title="argparse()函数"></a>argparse()函数</h3><p><a href="https://docs.python.org/zh-cn/2/library/argparse.html#module-argparse"><code>argparse</code></a> 模块可以让人轻松编写用户友好的命令行接口。程序定义它需要的参数，然后 <a href="https://docs.python.org/zh-cn/2/library/argparse.html#module-argparse"><code>argparse</code></a> 将弄清如何从 <a href="https://docs.python.org/zh-cn/2/library/sys.html#sys.argv"><code>sys.argv</code></a> 解析出那些参数。 <a href="https://docs.python.org/zh-cn/2/library/argparse.html#module-argparse"><code>argparse</code></a> 模块还会自动生成帮助和使用手册，并在用户给程序传入无效参数时报出错误信息。</p>
<p>以下代码是一个 Python 程序，它获取一个整数列表并计算总和或者最大值：</p>
<pre><code class="lang-python">import argparse

parser = argparse.ArgumentParser(description=&#39;Process some integers.&#39;)
parser.add_argument(&#39;integers&#39;, metavar=&#39;N&#39;, type=int, nargs=&#39;+&#39;,
                    help=&#39;an integer for the accumulator&#39;)
parser.add_argument(&#39;--sum&#39;, dest=&#39;accumulate&#39;, action=&#39;store_const&#39;,
                    const=sum, default=max,
                    help=&#39;sum the integers (default: find the max)&#39;)

args = parser.parse_args()
print args.accumulate(args.integers)
</code></pre>
<p>假设上面的 Python 代码保存在名为 <code>prog.py</code> 的文件中，它可以在命令行运行并提供有用的帮助信息：</p>
<pre><code class="lang-python">$ python prog.py -h
usage: prog.py [-h] [--sum] N [N ...]

Process some integers.

positional arguments:
 N           an integer for the accumulator

optional arguments:
 -h, --help  show this help message and exit
 --sum       sum the integers (default: find the max)
</code></pre>
<p>当使用适当的参数运行时，它会输出命令行传入整数的总和或者最大值：</p>
<pre><code class="lang-python">$ python prog.py 1 2 3 4
4

$ python prog.py 1 2 3 4 --sum
10
</code></pre>
<p>如果传入无效参数，则会报出错误：</p>
<pre><code class="lang-python">$ python prog.py a b c
usage: prog.py [-h] [--sum] N [N ...]
prog.py: error: argument N: invalid int value: &#39;a&#39;
</code></pre>
<p>更多详情参考python文档，网址：<a href="https://docs.python.org/zh-cn/2/library/argparse.html">15.4. argparse — 命令行选项、参数和子命令解析器 — Python 2.7.18 文档</a></p>
<h3 id="Distuils"><a href="#Distuils" class="headerlink" title="Distuils"></a>Distuils</h3><p>大部分Python程序员都知道，有很多第三方包管理器供选择，包括setuptools、distribute等等。 有些是为了替代标准库中的distutils。</p>
<p>1.1概念和术语</p>
<p>对于模块开发者以及需要安装模块的使用者来说，Distutils的使用都很简单，作为一个开发者，除了编写源码之外，还需要：</p>
<p>·编写setup脚本（一般是setup.py）；</p>
<p>·编写一个setup配置文件（可选）；</p>
<p>·创建一个源码发布；</p>
<p>·创建一个或多个构建（二进制）发布（可选）;</p>
<p>有些模块开发者在开发时不会考虑多个平台发布，所以就有了packagers的角色，它们从模块开发者那取得源码发布，然后在多个平台上面进行构建，并发布多个平台的构建版本。</p>
<p>1.2简单例子</p>
<p>由python编写的setup脚本一般都非常简单。作为autoconf类型的配置脚本，setup脚本可以在构建和安装模块发布时运行多次。</p>
<p>比如，如果需要发布一个叫做foo的模块，它包含在一个文件foo.py，那setup脚本可以这样写：</p>
<pre><code class="lang-python">from distutils.core import setup  
setup(name=&#39;foo&#39;,  
       version=&#39;1.0&#39;,  
       py_modules=[&#39;foo&#39;],  
      )
</code></pre>
<p>setup函数的参数表示提供给Distutils的信息，这些参数分为两类：包的元数据（包名、版本号）以及包的信息（本例中是一个Python模块的列表）；模块由模块名表示，而不是文件名（对于包和扩展而言也是这样）；建议可以提供更多的元数据，比如你的名字，email地址和项目的URL地址。</p>
<p>编写好setup.py之后，就可以创建该模块的源码发布了：</p>
<pre><code class="lang-python">python setup.py sdist
</code></pre>
<p>sdist命令会创建一个archive 文件（比如Unix上的tar文件，Windows上的zip文件），它包含setup.py， foo.py。该archive文件命名为foo-1.0.tar.gz(zip)，解压之后的目录名是foo-1.0。</p>
<p>如果一个用户希望安装foo模块，他只需要下载foo-1.0.tar.gz，解压，进入foo-1.0目录，然后运行：</p>
<pre><code class="lang-python">python setup.py install
</code></pre>
<p>该命令最终会将foo.py复制到Python环境存放第三方模块的目录中。在linux环境下，运行该命令的输出是：</p>
<pre><code class="lang-python"># python setup.py install  
running install  
running build  
running build_py  
creating build  
creating build/lib  
copying foo.py -&gt; build/lib  
running install_lib  
copying build/lib/foo.py -&gt; /usr/lib/python2.7/site-packages  
byte-compiling /usr/lib/python2.7/site-packages/foo.py to foo.pyc  
running install_egg_info  
Writing /usr/lib/python2.7/site-packages/foo-1.0-py2.7.egg-info
</code></pre>
<p>该命令生成的文件是：</p>
<p><code>/usr/lib/python2.7/site-packages/foo-1.0-py2.7.egg-info</code></p>
<p><code>/usr/lib/python2.7/site-packages/foo.py</code></p>
<p><code>/usr/lib/python2.7/site-packages/foo.pyc</code></p>
<h3 id="图片缩放方式"><a href="#图片缩放方式" class="headerlink" title="图片缩放方式"></a>图片缩放方式</h3><p>对图像进行预处理操作的时候，一般有两种缩放方式。</p>
<ul>
<li>一种是直接宽、高缩放至想要的宽、高，这种方式快捷，但可能会导致图像变形<br> step1: 计算宽高缩放比例，选择较小的那个缩放系数；<br> step2: 计算缩放后的尺寸: 原始图片的长宽都乘以较小的缩放系数；<br> step3：计算短边需要填充的灰边数，将短边的两边各自填充一半的灰行即可。</li>
<li>一种是等比例缩放，然后用灰色边缘填充</li>
</ul>
<h4 id="直接缩放"><a href="#直接缩放" class="headerlink" title="直接缩放"></a>直接缩放</h4><p>代码实现如下：</p>
<p><code>new_image = image.resize((target_w, target_h), Image.BICUBIC)</code></p>
<p><img src="/CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/image-20220406191420560.png" class="lazyload" data-srcset="/CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/image-20220406191420560.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h4 id="不变形缩放，两端填充灰边"><a href="#不变形缩放，两端填充灰边" class="headerlink" title="不变形缩放，两端填充灰边"></a>不变形缩放，两端填充灰边</h4><p><img src="/CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/image-20220406191519097.png" class="lazyload" data-srcset="/CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/image-20220406191519097.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<h4 id="不变形缩放，一端填充灰边"><a href="#不变形缩放，一端填充灰边" class="headerlink" title="不变形缩放，一端填充灰边"></a>不变形缩放，一端填充灰边</h4><p><img src="/CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/image-20220406191620889.png" class="lazyload" data-srcset="/CN/%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/image-20220406191620889.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII="></p>
<p>很多图片的长宽比不同导致缩放填充后，两端的黑边大小都不同。而如果填充的比较多，则存在信息冗余，影响推理速度。YOLOv5作者对letterbox的缩放策略进行了修改，对原图自适应的添加最少的黑边。<br> <strong>计算方法：</strong><br> 1.计算原始图片宽高与输入尺寸的缩放比例rw和rh，选取r = min(rw,rh)后把原图按r进行缩放<br> 2.原图宽和高中一定有一边完全贴合输入尺寸，没有达到输入尺寸的一边计算与输入尺寸的差值，然后进行上下（or左右）的填充。</p>
<p>代码如下：</p>
<pre><code class="lang-python">import matplotlib.pyplot as plt
from PIL import Image

# ---------------------------------------------------#
#   对输入图像进行resize,他人测试发现，不用letterbox_image直接resize的效果更好
# ---------------------------------------------------#
def resize_image(image, size, letterbox_image):
    iw, ih  = image.size
    w, h    = size      # w=200, h=300
    if letterbox_image:
        scale   = min(w/iw, h/ih)
        nw      = int(iw*scale)
        nh      = int(ih*scale)

        image   = image.resize((nw,nh), Image.BICUBIC)
        new_image = Image.new(&#39;RGB&#39;, size, (128,128,128))       # 新建一张image，第二个参数表示尺寸，第三个参数表示颜色
        # --------------------------------------------------#
        #   image.paste函数表示将一张图片覆盖到另一张图片的指定位置去
        #   a.paste(b, (50,50))   将b的左上顶点贴到a的坐标为（50，50）的位置，左上顶点为(0,0), b超出a的部分会被自动舍弃
        # ---------------------------------------------------#
        # new_image.paste(image, ((w-nw)//2, (h-nh)//2))    # 不变形resize，两端填充灰边
        new_image.paste(image, (0, 0))      # 不变形resize，一端填充灰边
    else:
        new_image = image.resize((w, h), Image.BICUBIC)
    return new_image


img_PIL = Image.open(&quot;Avatar.jpg&quot;)
img = resize_image(img_PIL, (200, 300), True)   # 第二参数表示目标尺寸，第三参数表示是否使用letterbox
plt.imshow(img)
plt.show()
# 作者：寻找永不遗憾
# 链接：https://www.jianshu.com/p/2ae3a497f5f4
# 来源：简书
# 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
</code></pre>
<h3 id="图像resize插值方式比较"><a href="#图像resize插值方式比较" class="headerlink" title="图像resize插值方式比较"></a>图像resize插值方式比较</h3><p>resize函数说明：</p>
<p><code>void resize(InputArray src, OutputArray dst,  Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR)</code></p>
<p>参数说明：</p>
<p><code>src</code>：输入，原图像，即待改变大小的图像；<br><code>dst</code>：输出，改变大小之后的图像，这个图像和原图像具有相同的内容，只是大小和原图像不一样而已；<br><code>dsize</code>：输出图像的大小。如果这个参数不为0，那么就代表将原图像缩放到这个Size(width，height)指定的大小；如果这个参数为0，那么原图像缩放之后的大小就要通过下面的公式来计算：</p>
<script type="math/tex; mode=display">
dsize = Size(round(fx*src.cols), round(fy*src.rows))</script><p> 其中，fx和fy就是下面要说的两个参数，是图像width方向和height方向的缩放比例。</p>
<p><code>fx</code>：width方向的缩放比例，如果它是0，那么它就会按照(double)dsize.width/src.cols来计算；<br><code>fy</code>：height方向的缩放比例，如果它是0，那么它就会按照(double)dsize.height/src.rows来计算；<br><code>interpolation</code>：这个是指定插值的方式，图像缩放之后，肯定像素要进行重新计算的，就靠这个参数来指定重新计算像素的方式，有以下几种：<br>      ·<code>INTER_NEAREST</code> - 最邻近插值<br>      ·<code>INTER_LINEAR</code> - 双线性插值，如果最后一个参数你不指定，默认使用这种方法<br>      ·<code>INTER_AREA</code>-区域插值<br>      ·<code>INTER_CUBIC</code> - 4x4像素邻域内的双立方插值<br>     ·<code>INTER_LANCZOS4</code>- 8x8像素邻域内的Lanczos插值</p>
<p>各种插值方式比较：</p>
<p>每种插值算法的前部分代码是相同的，如下：</p>
<pre><code class="lang-python">    cv::Mat matSrc, matDst1, matDst2;

    matSrc = cv::imread(&quot;lena.jpg&quot;, 2 | 4);
    matDst1 = cv::Mat(cv::Size(800, 1000), matSrc.type(), cv::Scalar::all(0));
    matDst2 = cv::Mat(matDst1.size(), matSrc.type(), cv::Scalar::all(0));

    double scale_x = (double)matSrc.cols / matDst1.cols;
    double scale_y = (double)matSrc.rows / matDst1.rows;
</code></pre>
<p>最近邻：</p>
<script type="math/tex; mode=display">
X_{src}=X_{dst}*(Width_{src}/Width_{dst})\\
Y_{src}=Y_{dst}*(Height_{src}/Height_{dst})</script><p>实现代码如下：</p>
<pre><code class="lang-python">    for (int i = 0; i &lt; matDst1.cols; ++i)
    &#123;
        int sx = cvFloor(i * scale_x);
        sx = std::min(sx, matSrc.cols - 1);
        for (int j = 0; j &lt; matDst1.rows; ++j)
        &#123;
            int sy = cvFloor(j * scale_y);
            sy = std::min(sy, matSrc.rows - 1);
            matDst1.at&lt;cv::Vec3b&gt;(j, i) = matSrc.at&lt;cv::Vec3b&gt;(sy, sx);
        &#125;
    &#125;
    cv::imwrite(&quot;nearest_1.jpg&quot;, matDst1);

    cv::resize(matSrc, matDst2, matDst1.size(), 0, 0, 0);
    cv::imwrite(&quot;nearest_2.jpg&quot;, matDst2);
</code></pre>
<p>双线性：</p>
<script type="math/tex; mode=display">
Dst(X, Y)=(1-u)*(1-v)*Src(X',Y')+(1-u)*v*Src(X', Y'+1)+\\    
        u*(1-v)*Src(X'+1, Y')+u*v*Src(X'+1, Y'+1)</script><pre><code class="lang-python">    uchar* dataDst = matDst1.data;
    int stepDst = matDst1.step;
    uchar* dataSrc = matSrc.data;
    int stepSrc = matSrc.step;
    int iWidthSrc = matSrc.cols;
    int iHiehgtSrc = matSrc.rows;

    for (int j = 0; j &lt; matDst1.rows; ++j)
    &#123;
        float fy = (float)((j + 0.5) * scale_y - 0.5);
        int sy = cvFloor(fy);
        fy -= sy;
        sy = std::min(sy, iHiehgtSrc - 2);
        sy = std::max(0, sy);

        short cbufy[2];
        cbufy[0] = cv::saturate_cast&lt;short&gt;((1.f - fy) * 2048);
        cbufy[1] = 2048 - cbufy[0];

        for (int i = 0; i &lt; matDst1.cols; ++i)
        &#123;
            float fx = (float)((i + 0.5) * scale_x - 0.5);
            int sx = cvFloor(fx);
            fx -= sx;

            if (sx &lt; 0) &#123;
                fx = 0, sx = 0;
            &#125;
            if (sx &gt;= iWidthSrc - 1) &#123;
                fx = 0, sx = iWidthSrc - 2;
            &#125;

            short cbufx[2];
            cbufx[0] = cv::saturate_cast&lt;short&gt;((1.f - fx) * 2048);
            cbufx[1] = 2048 - cbufx[0];

            for (int k = 0; k &lt; matSrc.channels(); ++k)
            &#123;
                *(dataDst+ j*stepDst + 3*i + k) = (*(dataSrc + sy*stepSrc + 3*sx + k) * cbufx[0] * cbufy[0] + 
                    *(dataSrc + (sy+1)*stepSrc + 3*sx + k) * cbufx[0] * cbufy[1] + 
                    *(dataSrc + sy*stepSrc + 3*(sx+1) + k) * cbufx[1] * cbufy[0] + 
                    *(dataSrc + (sy+1)*stepSrc + 3*(sx+1) + k) * cbufx[1] * cbufy[1]) &gt;&gt; 22;
            &#125;
        &#125;
    &#125;
    cv::imwrite(&quot;linear_1.jpg&quot;, matDst1);

    cv::resize(matSrc, matDst2, matDst1.size(), 0, 0, 1);
    cv::imwrite(&quot;linear_2.jpg&quot;, matDst2);
</code></pre>
<p>双三次：</p>
<pre><code class="lang-python">int iscale_x = cv::saturate_cast&lt;int&gt;(scale_x);
int iscale_y = cv::saturate_cast&lt;int&gt;(scale_y);

for (int j = 0; j &lt; matDst1.rows; ++j)
&#123;
    float fy = (float)((j + 0.5) * scale_y - 0.5);
    int sy = cvFloor(fy);
    fy -= sy;
    sy = std::min(sy, matSrc.rows - 3);
    sy = std::max(1, sy);

    const float A = -0.75f;

    float coeffsY[4];
    coeffsY[0] = ((A*(fy + 1) - 5*A)*(fy + 1) + 8*A)*(fy + 1) - 4*A;
    coeffsY[1] = ((A + 2)*fy - (A + 3))*fy*fy + 1;
    coeffsY[2] = ((A + 2)*(1 - fy) - (A + 3))*(1 - fy)*(1 - fy) + 1;
    coeffsY[3] = 1.f - coeffsY[0] - coeffsY[1] - coeffsY[2];

    short cbufY[4];
    cbufY[0] = cv::saturate_cast&lt;short&gt;(coeffsY[0] * 2048);
    cbufY[1] = cv::saturate_cast&lt;short&gt;(coeffsY[1] * 2048);
    cbufY[2] = cv::saturate_cast&lt;short&gt;(coeffsY[2] * 2048);
    cbufY[3] = cv::saturate_cast&lt;short&gt;(coeffsY[3] * 2048);

    for (int i = 0; i &lt; matDst1.cols; ++i)
    &#123;
        float fx = (float)((i + 0.5) * scale_x - 0.5);
        int sx = cvFloor(fx);
        fx -= sx;

        if (sx &lt; 1) &#123;
            fx = 0, sx = 1;
        &#125;
        if (sx &gt;= matSrc.cols - 3) &#123;
            fx = 0, sx = matSrc.cols - 3;
        &#125;

        float coeffsX[4];
        coeffsX[0] = ((A*(fx + 1) - 5*A)*(fx + 1) + 8*A)*(fx + 1) - 4*A;
        coeffsX[1] = ((A + 2)*fx - (A + 3))*fx*fx + 1;
        coeffsX[2] = ((A + 2)*(1 - fx) - (A + 3))*(1 - fx)*(1 - fx) + 1;
        coeffsX[3] = 1.f - coeffsX[0] - coeffsX[1] - coeffsX[2];

        short cbufX[4];
        cbufX[0] = cv::saturate_cast&lt;short&gt;(coeffsX[0] * 2048);
        cbufX[1] = cv::saturate_cast&lt;short&gt;(coeffsX[1] * 2048);
        cbufX[2] = cv::saturate_cast&lt;short&gt;(coeffsX[2] * 2048);
        cbufX[3] = cv::saturate_cast&lt;short&gt;(coeffsX[3] * 2048);

        for (int k = 0; k &lt; matSrc.channels(); ++k)
        &#123;
            matDst1.at&lt;cv::Vec3b&gt;(j, i)[k] = abs((matSrc.at&lt;cv::Vec3b&gt;(sy-1, sx-1)[k] * cbufX[0] * cbufY[0] + matSrc.at&lt;cv::Vec3b&gt;(sy, sx-1)[k] * cbufX[0] * cbufY[1] +
                matSrc.at&lt;cv::Vec3b&gt;(sy+1, sx-1)[k] * cbufX[0] * cbufY[2] + matSrc.at&lt;cv::Vec3b&gt;(sy+2, sx-1)[k] * cbufX[0] * cbufY[3] +
                matSrc.at&lt;cv::Vec3b&gt;(sy-1, sx)[k] * cbufX[1] * cbufY[0] + matSrc.at&lt;cv::Vec3b&gt;(sy, sx)[k] * cbufX[1] * cbufY[1] +
                matSrc.at&lt;cv::Vec3b&gt;(sy+1, sx)[k] * cbufX[1] * cbufY[2] + matSrc.at&lt;cv::Vec3b&gt;(sy+2, sx)[k] * cbufX[1] * cbufY[3] +
                matSrc.at&lt;cv::Vec3b&gt;(sy-1, sx+1)[k] * cbufX[2] * cbufY[0] + matSrc.at&lt;cv::Vec3b&gt;(sy, sx+1)[k] * cbufX[2] * cbufY[1] +
                matSrc.at&lt;cv::Vec3b&gt;(sy+1, sx+1)[k] * cbufX[2] * cbufY[2] + matSrc.at&lt;cv::Vec3b&gt;(sy+2, sx+1)[k] * cbufX[2] * cbufY[3] +
                matSrc.at&lt;cv::Vec3b&gt;(sy-1, sx+2)[k] * cbufX[3] * cbufY[0] + matSrc.at&lt;cv::Vec3b&gt;(sy, sx+2)[k] * cbufX[3] * cbufY[1] +
                matSrc.at&lt;cv::Vec3b&gt;(sy+1, sx+2)[k] * cbufX[3] * cbufY[2] + matSrc.at&lt;cv::Vec3b&gt;(sy+2, sx+2)[k] * cbufX[3] * cbufY[3] ) &gt;&gt; 22);
        &#125;
    &#125;
&#125;
cv::imwrite(&quot;cubic_1.jpg&quot;, matDst1);

cv::resize(matSrc, matDst2, matDst1.size(), 0, 0, 2);
cv::imwrite(&quot;cubic_2.jpg&quot;, matDst2);
</code></pre>
<p>基于像素区域关系：共分三种情况，图像放大时类似于双线性插值，图像缩小(x轴、y轴同时缩小)又分两种情况，此情况下可以避免波纹出现</p>
<p>具体实现代码可以参考<a href="https://github.com/fengbingchun/OpenCV_Test/blob/master/src/fbc_cv/include/resize.hpp，用法如下：">https://github.com/fengbingchun/OpenCV_Test/blob/master/src/fbc_cv/include/resize.hpp，用法如下：</a></p>
<pre><code class="lang-python">fbc::Mat3BGR src(matSrc.rows, matSrc.cols, matSrc.data);
fbc::Mat3BGR dst(matDst1.rows, matDst1.cols, matDst1.data);
fbc::resize(src, dst, 3);
</code></pre>
<p>兰索斯插值：略</p>
<p>测试代码：</p>
<pre><code class="lang-c++">#include &lt;chrono&gt;
#include &lt;opencv2/opencv.hpp&gt;
#define  millisecond 1000000
#define DEBUG_PRINT(...)  printf( __VA_ARGS__); printf(&quot;\n&quot;)
#define DEBUG_TIME(time_) auto time_ =std::chrono::high_resolution_clock::now()
#define RUN_TIME(time_)  (double)(time_).count()/millisecond
using namespace std;

cv::Mat image_resize(cv::Mat image, int width, int height, int interpolation, int num) &#123;
    cv::Mat dest;
    for (int i = 0; i &lt; num; ++i) &#123;
        cv::resize(image, dest, cv::Size(width, height), 0, 0, interpolation);//最近邻插值
    &#125;
    return dest;
&#125;


int main() &#123;
    string path = &quot;../1.jpg&quot;;
    cv::Mat image = cv::imread(path);
    cv::resize(image, image, cv::Size(1000, 1000));
    int re_width = 900;
    int re_height = 900;
    int  num=10;
    cv::Mat image2X_INTER_NEAREST;
    cv::Mat image2X_INTER_LINEAR;
    cv::Mat image2X_INTER_AREA;
    cv::Mat image2X_INTER_CUBIC;
    cv::Mat initMat;
    DEBUG_PRINT(&quot;image input size:%dx%d&quot;, image.rows, image.cols);
    DEBUG_TIME(T0);
    image2X_INTER_NEAREST=image_resize(image, re_width, re_height, cv::INTER_NEAREST, num);
    DEBUG_TIME(T1);
    image2X_INTER_LINEAR=image_resize(image, re_width, re_height, cv::INTER_LINEAR, num);
    DEBUG_TIME(T2);
    image2X_INTER_AREA=image_resize(image, re_width, re_height, cv::INTER_AREA, num);
    DEBUG_TIME(T3);
    image2X_INTER_CUBIC=image_resize(image, re_width, re_height, cv::INTER_CUBIC, num);
    DEBUG_TIME(T4);
    DEBUG_PRINT(&quot;resize_image:%dx%d,INTER_NEAREST:%3.3fms&quot;,
            image2X_INTER_NEAREST.rows,
            image2X_INTER_NEAREST.cols,
            RUN_TIME(T1 - T0)/num);
    DEBUG_PRINT(&quot;resize_image:%dx%d,INTER_LINEAR :%3.3fms&quot;,
            image2X_INTER_LINEAR.rows,
            image2X_INTER_LINEAR.cols,
            RUN_TIME(T2 - T1)/num);
    DEBUG_PRINT(&quot;resize_image:%dx%d,INTER_AREA   :%3.3fms&quot;,
            image2X_INTER_AREA.rows,
            image2X_INTER_AREA.cols,
            RUN_TIME(T3 - T2)/num);
    DEBUG_PRINT(&quot;resize_image:%dx%d,INTER_CUBIC  :%3.3fms&quot;,
            image2X_INTER_CUBIC.rows,
            image2X_INTER_CUBIC.cols,
            RUN_TIME(T4 - T3)/num);
    return 0;
&#125;
版权声明：本文为CSDN博主「pan_jinquan」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/guyuealian/article/details/85097633
</code></pre>
<p>运行结果：</p>
<pre><code>image input size:1000x1000
resize_image:900x900,INTER_NEAREST:0.389ms
resize_image:900x900,INTER_LINEAR :0.605ms
resize_image:900x900,INTER_AREA   :2.611ms
resize_image:900x900,INTER_CUBIC  :1.920ms
</code></pre><p>总结：</p>
<pre><code class="lang-markdown"> 速度比较：INTER_NEAREST（最近邻插值)&gt;INTER_LINEAR(线性插值)&gt;INTER_CUBIC(三次样条插值)&gt;INTER_AREA  (区域插值)
对图像进行缩小时，为了避免出现波纹现象，推荐采用INTER_AREA 区域插值方法。
OpenCV推荐：如果要缩小图像，通常推荐使用#INTER_AREA插值效果最好，而要放大图像，通常使用INTER_CUBIC(速度较慢，但效果最好)，或者使用INTER_LINEAR(速度较快，效果还可以)。至于最近邻插值INTER_NEAREST，一般不推荐使用
</code></pre>
<p>更多详情参考：<a href="https://docs.opencv.org/3.2.0/da/d54/group__imgproc__transform.html#ga47a974309e9102f5f08231edc7e7529d">OpenCV: Geometric Image Transformations</a></p>
<h3 id="torch-nn-Identity"><a href="#torch-nn-Identity" class="headerlink" title="torch.nn.Identity()"></a>torch.nn.Identity()</h3><p>dentity模块如果不改变输入，直接返回输入</p>
<p>一种编码技巧吧，比如我们要加深网络，有些层是不改变输入数据的维度的，</p>
<p>在增减网络的过程中我们就可以用identity占个位置，这样网络整体层数永远不变，</p>
<p>应用：</p>
<p>例如此时：如果此时我们使用了se_layer，那么就SELayer(dim)，否则就输入什么就输出什么（什么都不做）</p>
<pre><code class="lang-python"># 定义操作
self.attn_drop = nn.Dropout(attn_drop)
self.proj = nn.Linear(dim, dim)
self.se_layer = SELayer(dim) if se_layer else nn.Identity()
self.proj_drop = nn.Dropout(proj_drop)
# dropout
attn = self.attn_drop(attn)
# 与矩阵V相乘
x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
# 经过一层全连接层
x = self.proj(x)
# nn.Identity():建立一个输入模块，什么都不做
x = self.se_layer(x)
# dropout
x = self.proj_drop(x)
return x
</code></pre>
<p>更多详情参考<a href="https://pytorch.org/docs/stable/generated/torch.nn.Identity.html?highlight=identity#torch.nn.Identity">Identity — PyTorch 1.11.0 documentation</a></p>
]]></content>
  </entry>
  <entry>
    <title>Sanderson the Man 1</title>
    <url>/en/The_story_of_a_great_schoolmaster/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Of all the men I have met—and I have now had a fairly long and active life and have met a very great variety of interesting people—one only has stirred me to a biographical effort. This one exception is F. W. Sanderson, for many years the headmaster of Oundle School. I think him beyond question the greatest man I have ever known with any degree of <a href="http://dict.qsbdc.com/intimacy"><strong>intimacy</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_1">1</a>, and it is in the hope of conveying to others something of my sense not merely of his importance, but of his <a href="http://dict.qsbdc.com/peculiar"><strong>peculiar</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_2">2</a> genius and the rich humanity of his character, that I am setting out to write this book. He was in himself a very delightful[Pg 2] mixture of <a href="http://dict.qsbdc.com/subtlety"><strong>subtlety</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_3">3</a> and <a href="http://dict.qsbdc.com/simplicity"><strong>simplicity</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_4">4</a>, <a href="http://dict.qsbdc.com/generosity"><strong>generosity</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_5">5</a>, <a href="http://dict.qsbdc.com/adventurousness"><strong>adventurousness</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_6">6</a>, imagination and <a href="http://dict.qsbdc.com/steadfast"><strong>steadfast</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_7">7</a> purpose, and he approached the general life of our time at such an angle as to reflect the most curious and profitable lights upon it. To tell his story is to reflect upon all the main educational ideas of the last half-century, and to revise our conception of the process and purpose of the modern community in relation to education. For Sanderson had a mind like an <a href="http://dict.qsbdc.com/octopus"><strong>octopus</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_8">8</a>, it seemed always to have a <a href="http://dict.qsbdc.com/tentacle"><strong>tentacle</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_9">9</a> free to reach out beyond what was already held, and his <a href="http://dict.qsbdc.com/tentacles"><strong>tentacles</strong></a><a href="http://novel.tingroom.com/jingdian/5031/128151.html#_w_10">10</a> grew and radiated farther and farther. Before his end he had come to a vision of the school as a centre for the complete reorganisation of civilised life.</p>
]]></content>
      <categories>
        <category>novel</category>
      </categories>
  </entry>
</search>
